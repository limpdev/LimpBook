
[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/embedding_models.mdx)

# Embedding models

Prerequisites

- [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

Note

This conceptual overview focuses on text-based embedding models.

Embedding models can also be [multimodal](/docs/concepts/multimodality/) though such models are not currently supported by LangChain.

Imagine being able to capture the essence of any text - a tweet, document, or book - in a single, compact representation. This is the power of embedding models, which lie at the heart of many retrieval systems. Embedding models transform human language into a format that machines can understand and compare with speed and accuracy. These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning. Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.

## Key concepts[â€‹](#key-concepts "Direct link to Key concepts")

![Conceptual Overview](/assets/images/embeddings_concept-975a9aaba52de05b457a1aeff9a7393a.png)

(1) **Embed text as a vector**: Embeddings transform text into a numerical vector representation.

(2) **Measure similarity**: Embedding vectors can be compared using simple mathematical operations.

## Embedding[â€‹](#embedding "Direct link to Embedding")

### Historical context[â€‹](#historical-context "Direct link to Historical context")

The landscape of embedding models has evolved significantly over the years. A pivotal moment came in 2018 when Google introduced [BERT (Bidirectional Encoder Representations from Transformers)](https://www.nvidia.com/en-us/glossary/bert/). BERT applied transformer models to embed text as a simple vector representation, which lead to unprecedented performance across various NLP tasks. However, BERT wasn't optimized for generating sentence embeddings efficiently. This limitation spurred the creation of [SBERT (Sentence-BERT)](https://www.sbert.net/examples/training/sts/README.html), which adapted the BERT architecture to generate semantically rich sentence embeddings, easily comparable via similarity metrics like cosine similarity, dramatically reduced the computational overhead for tasks like finding similar sentences. Today, the embedding model ecosystem is diverse, with numerous providers offering their own implementations. To navigate this variety, researchers and practitioners often turn to benchmarks like the Massive Text Embedding Benchmark (MTEB) [here](https://huggingface.co/blog/mteb) for objective comparisons.

Further reading

- See the [seminal BERT paper](https://arxiv.org/abs/1810.04805).
- See Cameron Wolfe's [excellent review](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2) of embedding models.
- See the [Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/blog/mteb) leaderboard for a comprehensive overview of embedding models.

### Interface[â€‹](#interface "Direct link to Interface")

LangChain provides a universal interface for working with them, providing standard methods for common operations. This common interface simplifies interaction with various embedding providers through two central methods:

- `embed_documents`: For embedding multiple texts (documents)
- `embed_query`: For embedding a single text (query)

This distinction is important, as some providers employ different embedding strategies for documents (which are to be searched) versus queries (the search input itself). To illustrate, here's a practical example using LangChain's `.embed_documents` method to embed a list of strings:

```python
from langchain_openai import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings()
embeddings = embeddings_model.embed_documents(
    [
        "Hi there!",
        "Oh, hello!",
        "What's your name?",
        "My friends call me World",
        "Hello World!"
    ]
)
len(embeddings), len(embeddings[0])
(5, 1536)
```

**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

For convenience, you can also use the `embed_query` method to embed a single text:

```python
query_embedding = embeddings_model.embed_query("What is the meaning of life?")
```

Further reading

- See the full list of [LangChain embedding model integrations](/docs/integrations/text_embedding/).
- See these [how-to guides](/docs/how_to/embed_text/) for working with embedding models.

### Integrations[â€‹](#integrations "Direct link to Integrations")

LangChain offers many embedding model integrations which you can find [on the embedding models](/docs/integrations/text_embedding/) integrations page.

## Measure similarity[â€‹](#measure-similarity "Direct link to Measure similarity")

Each embedding is essentially a set of coordinates, often in a high-dimensional space. In this space, the position of each point (embedding) reflects the meaning of its corresponding text. Just as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space. This allows for intuitive comparisons between different pieces of text. By reducing text to these numerical representations, we can use simple mathematical operations to quickly measure how alike two pieces of text are, regardless of their original length or structure. Some common similarity metrics include:

- **Cosine Similarity**: Measures the cosine of the angle between two vectors.
- **Euclidean Distance**: Measures the straight-line distance between two points.
- **Dot Product**: Measures the projection of one vector onto another.

The choice of similarity metric should be chosen based on the model. As an example, [OpenAI suggests cosine similarity for their embeddings](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use), which can be easily implemented:

```python
import numpy as np

def cosine_similarity(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    return dot_product / (norm_vec1 * norm_vec2)

similarity = cosine_similarity(query_result, document_result)
print("Cosine Similarity:", similarity)
```

Further reading

- See Simon Willisonâ€™s [nice blog post and video](https://simonwillison.net/2023/Oct/23/embeddings/) on embeddings and similarity metrics.
- See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with embeddings.
- See Pinecone's [blog post](https://www.pinecone.io/learn/vector-similarity/) on similarity metrics.
- See OpenAI's [FAQ](https://platform.openai.com/docs/guides/embeddings/faq) on what similarity metric to use with OpenAI embeddings.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/embedding_models.mdx)

* * *



[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_memory/conversation_buffer_memory.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_memory/conversation_buffer_memory.ipynb)

# Migrating off ConversationBufferMemory or ConversationStringBufferMemory

[ConversationBufferMemory](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer.ConversationBufferMemory.html) and [ConversationStringBufferMemory](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer.ConversationStringBufferMemory.html) were used to keep track of a conversation between a human and an ai asstistant without any additional processing.

note

The `ConversationStringBufferMemory` is equivalent to `ConversationBufferMemory` but was targeting LLMs that were not chat models.

The methods for handling conversation history using existing modern primitives are:

1. Using [LangGraph persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) along with appropriate processing of the message history
2. Using LCEL with [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) combined with appropriate processing of the message history.

Most users will find [LangGraph persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) both easier to use and configure than the equivalent LCEL, especially for more complex use cases.

## Set up[â€‹](#set-up "Direct link to Set up")

```python
%%capture --no-stderr
%pip install --upgrade --quiet langchain-openai langchain
```

```python
import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

## Usage with LLMChain / ConversationChain[â€‹](#usage-with-llmchain--conversationchain "Direct link to Usage with LLMChain / ConversationChain")

This section shows how to migrate off `ConversationBufferMemory` or `ConversationStringBufferMemory` that's used together with either an `LLMChain` or a `ConversationChain`.

### Legacy[â€‹](#legacy "Direct link to Legacy")

Below is example usage of `ConversationBufferMemory` with an `LLMChain` or an equivalent `ConversationChain`.

Details

```python
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from langchain_core.messages import SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate(
    [
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessagePromptTemplate.from_template("{text}"),
    ]
)

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

legacy_chain = LLMChain(
    llm=ChatOpenAI(),
    prompt=prompt,
    memory=memory,
)

legacy_result = legacy_chain.invoke({"text": "my name is bob"})
print(legacy_result)

legacy_result = legacy_chain.invoke({"text": "what was my name"})
```

**API Reference:**[LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [ConversationBufferMemory](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer.ConversationBufferMemory.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [HumanMessagePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.HumanMessagePromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
{'text': 'Hello Bob! How can I assist you today?', 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={}, response_metadata={})]}
```

```python
legacy_result["text"]
```

```output
'Your name is Bob. How can I assist you today, Bob?'
```

note

Note that there is no support for separating conversation threads in a single memory object

### LangGraph[â€‹](#langgraph "Direct link to LangGraph")

The example below shows how to use LangGraph to implement a `ConversationChain` or `LLMChain` with `ConversationBufferMemory`.

This example assumes that you're already somewhat familiar with `LangGraph`. If you're not, then please see the [LangGraph Quickstart Guide](https://langchain-ai.github.io/langgraph/tutorials/introduction/) for more details.

`LangGraph` offers a lot of additional functionality (e.g., time-travel and interrupts) and will work well for other more complex (and realistic) architectures.

Details

```python
import uuid

from IPython.display import Image, display
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

# Define a new graph
workflow = StateGraph(state_schema=MessagesState)

# Define a chat model
model = ChatOpenAI()


# Define the function that calls the model
def call_model(state: MessagesState):
    response = model.invoke(state["messages"])
    # We return a list, because this will get added to the existing list
    return {"messages": response}


# Define the two nodes we will cycle between
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)


# Adding memory is straight forward in langgraph!
memory = MemorySaver()

app = workflow.compile(
    checkpointer=memory
)


# The thread id is a unique key that identifies
# this particular conversation.
# We'll just generate a random uuid here.
# This enables a single application to manage conversations among multiple users.
thread_id = uuid.uuid4()
config = {"configurable": {"thread_id": thread_id}}


input_message = HumanMessage(content="hi! I'm bob")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()

# Here, let's confirm that the AI remembers our name!
input_message = HumanMessage(content="what was my name?")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```output
================================[1m Human Message [0m=================================

hi! I'm bob
==================================[1m Ai Message [0m==================================

Hello Bob! How can I assist you today?
================================[1m Human Message [0m=================================

what was my name?
==================================[1m Ai Message [0m==================================

Your name is Bob. How can I help you today, Bob?
```

### LCEL RunnableWithMessageHistory[â€‹](#lcel-runnablewithmessagehistory "Direct link to LCEL RunnableWithMessageHistory")

Alternatively, if you have a simple chain, you can wrap the chat model of the chain within a [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html).

Please refer to the following [migration guide](/docs/versions/migrating_chains/conversation_chain/) for more information.

## Usage with a pre-built agent[â€‹](#usage-with-a-pre-built-agent "Direct link to Usage with a pre-built agent")

This example shows usage of an Agent Executor with a pre-built agent constructed using the [create\_tool\_calling\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) function.

If you are using one of the [old LangChain pre-built agents](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/), you should be able to replace that code with the new [langgraph pre-built agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/) which leverages native tool calling capabilities of chat models and will likely work better out of the box.

### Legacy Usage[â€‹](#legacy-usage "Direct link to Legacy Usage")

Details

```python
from langchain import hub
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.memory import ConversationBufferMemory
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

model = ChatOpenAI(temperature=0)


@tool
def get_user_age(name: str) -> str:
    """Use this tool to find the user's age."""
    # This is a placeholder for the actual implementation
    if "bob" in name.lower():
        return "42 years old"
    return "41 years old"


tools = [get_user_age]

prompt = ChatPromptTemplate.from_messages(
    [
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ]
)

# Construct the Tools agent
agent = create_tool_calling_agent(model, tools, prompt)
# Instantiate memory
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Create an agent
agent = create_tool_calling_agent(model, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,  # Pass the memory to the executor
)

# Verify that the agent can use tools
print(agent_executor.invoke({"input": "hi! my name is bob what is my age?"}))
print()
# Verify that the agent has access to conversation history.
# The agent should be able to answer that the user's name is bob.
print(agent_executor.invoke({"input": "do you remember my name?"}))
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create\_tool\_calling\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) | [ConversationBufferMemory](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer.ConversationBufferMemory.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
{'input': 'hi! my name is bob what is my age?', 'chat_history': [HumanMessage(content='hi! my name is bob what is my age?', additional_kwargs={}, response_metadata={}), AIMessage(content='Bob, you are 42 years old.', additional_kwargs={}, response_metadata={})], 'output': 'Bob, you are 42 years old.'}

{'input': 'do you remember my name?', 'chat_history': [HumanMessage(content='hi! my name is bob what is my age?', additional_kwargs={}, response_metadata={}), AIMessage(content='Bob, you are 42 years old.', additional_kwargs={}, response_metadata={}), HumanMessage(content='do you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes, your name is Bob.', additional_kwargs={}, response_metadata={})], 'output': 'Yes, your name is Bob.'}
```

### LangGraph[â€‹](#langgraph-1 "Direct link to LangGraph")

You can follow the standard LangChain tutorial for [building an agent](/docs/tutorials/agents/) an in depth explanation of how this works.

This example is shown here explicitly to make it easier for users to compare the legacy implementation vs. the corresponding langgraph implementation.

This example shows how to add memory to the [pre-built react agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) in langgraph.

For more details, please see the [how to add memory to the prebuilt ReAct agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-memory/) guide in langgraph.

Details

```python
import uuid

from langchain_core.messages import HumanMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent


@tool
def get_user_age(name: str) -> str:
    """Use this tool to find the user's age."""
    # This is a placeholder for the actual implementation
    if "bob" in name.lower():
        return "42 years old"
    return "41 years old"


memory = MemorySaver()
model = ChatOpenAI()
app = create_react_agent(
    model,
    tools=[get_user_age],
    checkpointer=memory,
)

# The thread id is a unique key that identifies
# this particular conversation.
# We'll just generate a random uuid here.
# This enables a single application to manage conversations among multiple users.
thread_id = uuid.uuid4()
config = {"configurable": {"thread_id": thread_id}}

# Tell the AI that our name is Bob, and ask it to use a tool to confirm
# that it's capable of working like an agent.
input_message = HumanMessage(content="hi! I'm bob. What is my age?")

for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()

# Confirm that the chat bot has access to previous conversation
# and can respond to the user saying that the user's name is Bob.
input_message = HumanMessage(content="do you remember my name?")

for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```output
================================[1m Human Message [0m=================================

hi! I'm bob. What is my age?
==================================[1m Ai Message [0m==================================
Tool Calls:
  get_user_age (call_oEDwEbIDNdokwqhAV6Azn47c)
 Call ID: call_oEDwEbIDNdokwqhAV6Azn47c
  Args:
    name: bob
=================================[1m Tool Message [0m=================================
Name: get_user_age

42 years old
==================================[1m Ai Message [0m==================================

Bob, you are 42 years old! If you need any more assistance or information, feel free to ask.
================================[1m Human Message [0m=================================

do you remember my name?
==================================[1m Ai Message [0m==================================

Yes, your name is Bob. If you have any other questions or need assistance, feel free to ask!
```

If we use a different thread ID, it'll start a new conversation and the bot will not know our name!

```python
config = {"configurable": {"thread_id": "123456789"}}

input_message = HumanMessage(content="hi! do you remember my name?")

for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

hi! do you remember my name?
==================================[1m Ai Message [0m==================================

Hello! Yes, I remember your name. It's great to see you again! How can I assist you today?
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

Explore persistence with LangGraph:

- [LangGraph quickstart tutorial](https://langchain-ai.github.io/langgraph/tutorials/introduction/)
- [How to add persistence ("memory") to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)
- [How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/)
- [How to add summary of the conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)

Add persistence with simple LCEL (favor langgraph for more complex use cases):

- [How to add message history](/docs/how_to/message_history/)

Working with message history:

- [How to trim messages](/docs/how_to/trim_messages/)
- [How to filter messages](/docs/how_to/filter_messages/)
- [How to merge message runs](/docs/how_to/merge_message_runs/)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_memory/conversation_buffer_memory.ipynb)

* * *


- [Set up](#set-up)
- [Usage with LLMChain / ConversationChain](#usage-with-llmchain--conversationchain)
  
  - [Legacy](#legacy)
  - [LangGraph](#langgraph)
  - [LCEL RunnableWithMessageHistory](#lcel-runnablewithmessagehistory)
- [Usage with a pre-built agent](#usage-with-a-pre-built-agent)
  
  - [Legacy Usage](#legacy-usage)
  - [LangGraph](#langgraph-1)
- [Next steps](#next-steps)


# Tracing

A trace is essentially a series of steps that your application takes to go from input to output. Traces contain individual steps called `runs`. These can be individual calls from a model, retriever, tool, or sub-chains. Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.

For a deeper dive, check out [this LangSmith conceptual guide](https://docs.smith.langchain.com/concepts/tracing).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/tracing.mdx)

* * *










# How to use few-shot prompting with tool calling

For more complex tool use it's very useful to add [few-shot examples](/docs/concepts/few_shot_prompting/) to the prompt. We can do this by adding `AIMessage`s with `ToolCall`s and corresponding `ToolMessage`s to our prompt.

First let's define our tools and model.

```python
from langchain_core.tools import tool


@tool
def add(a: int, b: int) -> int:
    """Adds a and b."""
    return a + b


@tool
def multiply(a: int, b: int) -> int:
    """Multiplies a and b."""
    return a * b


tools = [add, multiply]
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

```python
import os
from getpass import getpass

from langchain_openai import ChatOpenAI

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
llm_with_tools = llm.bind_tools(tools)
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

Let's run our model where we can notice that even with some special instructions our model can get tripped up by order of operations.

```python
llm_with_tools.invoke(
    "Whats 119 times 8 minus 20. Don't do any math yourself, only use tools for math. Respect order of operations"
).tool_calls
```

```output
[{'name': 'Multiply',
  'args': {'a': 119, 'b': 8},
  'id': 'call_T88XN6ECucTgbXXkyDeC2CQj'},
 {'name': 'Add',
  'args': {'a': 952, 'b': -20},
  'id': 'call_licdlmGsRqzup8rhqJSb1yZ4'}]
```

The model shouldn't be trying to add anything yet, since it technically can't know the results of 119 * 8 yet.

By adding a prompt with some examples we can correct this behavior:

```python
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

examples = [
    HumanMessage(
        "What's the product of 317253 and 128472 plus four", name="example_user"
    ),
    AIMessage(
        "",
        name="example_assistant",
        tool_calls=[
            {"name": "Multiply", "args": {"x": 317253, "y": 128472}, "id": "1"}
        ],
    ),
    ToolMessage("16505054784", tool_call_id="1"),
    AIMessage(
        "",
        name="example_assistant",
        tool_calls=[{"name": "Add", "args": {"x": 16505054784, "y": 4}, "id": "2"}],
    ),
    ToolMessage("16505054788", tool_call_id="2"),
    AIMessage(
        "The product of 317253 and 128472 plus four is 16505054788",
        name="example_assistant",
    ),
]

system = """You are bad at math but are an expert at using a calculator. 

Use past tool usage as an example of how to correctly use the tools."""
few_shot_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        *examples,
        ("human", "{query}"),
    ]
)

chain = {"query": RunnablePassthrough()} | few_shot_prompt | llm_with_tools
chain.invoke("Whats 119 times 8 minus 20").tool_calls
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)

```output
[{'name': 'Multiply',
  'args': {'a': 119, 'b': 8},
  'id': 'call_9MvuwQqg7dlJupJcoTWiEsDo'}]
```

And we get the correct output this time.

Here's what the [LangSmith trace](https://smith.langchain.com/public/f70550a1-585f-4c9d-a643-13148ab1616f/r) looks like.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tools_few_shot.ipynb)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/binding.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/binding.ipynb)

# How to add default invocation args to a Runnable

Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel/)
- [Chaining runnables](/docs/how_to/sequence/)
- [Tool calling](/docs/how_to/tool_calling/)

Sometimes we want to invoke a [`Runnable`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) within a [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html) with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use the [`Runnable.bind()`](https://python.langchain.com/api_reference/langchain_core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.bind) method to set these arguments ahead of time.

## Binding stop sequences[â€‹](#binding-stop-sequences "Direct link to Binding stop sequences")

Suppose we have a simple prompt + model chain:

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Write out the following equation using algebraic symbols then solve it. Use the format\n\nEQUATION:...\nSOLUTION:...\n\n",
        ),
        ("human", "{equation_statement}"),
    ]
)

model = ChatOpenAI(temperature=0)

runnable = (
    {"equation_statement": RunnablePassthrough()} | prompt | model | StrOutputParser()
)

print(runnable.invoke("x raised to the third plus seven equals 12"))
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
EQUATION: x^3 + 7 = 12

SOLUTION: 
Subtract 7 from both sides:
x^3 = 5

Take the cube root of both sides:
x = âˆ›5
```

and want to call the model with certain `stop` words so that we shorten the output as is useful in certain types of prompting techniques. While we can pass some arguments into the constructor, other runtime args use the `.bind()` method as follows:

```python
runnable = (
    {"equation_statement": RunnablePassthrough()}
    | prompt
    | model.bind(stop="SOLUTION")
    | StrOutputParser()
)

print(runnable.invoke("x raised to the third plus seven equals 12"))
```

```output
EQUATION: x^3 + 7 = 12
```

What you can bind to a Runnable will depend on the extra parameters you can pass when invoking it.

## Attaching OpenAI tools[â€‹](#attaching-openai-tools "Direct link to Attaching OpenAI tools")

Another common use-case is tool calling. While you should generally use the [`.bind_tools()`](/docs/how_to/tool_calling/) method for tool-calling models, you can also bind provider-specific args directly if you want lower level control:

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    },
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                },
                "required": ["location"],
            },
        },
    }
]
```

```python
model = ChatOpenAI(model="gpt-4o-mini").bind(tools=tools)
model.invoke("What's the weather in SF, NYC and LA?")
```

```output
AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z0OU2CytqENVrRTI6T8DkI3u', 'function': {'arguments': '{"location": "San Francisco, CA", "unit": "celsius"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw', 'function': {'arguments': '{"location": "New York, NY", "unit": "celsius"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH', 'function': {'arguments': '{"location": "Los Angeles, CA", "unit": "celsius"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 85, 'total_tokens': 169}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_77a673219d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d57ad5fa-b52a-4822-bc3e-74f838697e18-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA', 'unit': 'celsius'}, 'id': 'call_z0OU2CytqENVrRTI6T8DkI3u'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY', 'unit': 'celsius'}, 'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA', 'unit': 'celsius'}, 'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH'}])
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You now know how to bind runtime arguments to a Runnable.

To learn more, see the other how-to guides on runnables in this section, including:

- [Using configurable fields and alternatives](/docs/how_to/configure/) to change parameters of a step in a chain, or even swap out entire steps, at runtime

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/binding.ipynb)

* * *


- [Binding stop sequences](#binding-stop-sequences)
- [Attaching OpenAI tools](#attaching-openai-tools)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/output_parser_yaml.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/output_parser_yaml.ipynb)

# How to parse YAML output

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models/)
- [Output parsers](/docs/concepts/output_parsers/)
- [Prompt templates](/docs/concepts/prompt_templates/)
- [Structured output](/docs/how_to/structured_output/)
- [Chaining runnables together](/docs/how_to/sequence/)

LLMs from different providers often have different strengths depending on the specific data they are trained on. This also means that some may be "better" and more reliable at generating output in formats other than JSON.

This output parser allows users to specify an arbitrary schema and query LLMs for outputs that conform to that schema, using YAML to format their response.

note

Keep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed YAML.

```python
%pip install -qU langchain langchain-openai

import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

We use [Pydantic](https://docs.pydantic.dev) with the [`YamlOutputParser`](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.yaml.YamlOutputParser.html#langchain.output_parsers.yaml.YamlOutputParser) to declare our data model and give the model more context as to what type of YAML it should generate:

```python
from langchain.output_parsers import YamlOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field


# Define your desired data structure.
class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")


model = ChatOpenAI(temperature=0)

# And a query intented to prompt a language model to populate the data structure.
joke_query = "Tell me a joke."

# Set up a parser + inject instructions into the prompt template.
parser = YamlOutputParser(pydantic_object=Joke)

prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

chain = prompt | model | parser

chain.invoke({"query": joke_query})
```

**API Reference:**[YamlOutputParser](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.yaml.YamlOutputParser.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
Joke(setup="Why couldn't the bicycle find its way home?", punchline='Because it lost its bearings!')
```

The parser will automatically parse the output YAML and create a Pydantic model with the data. We can see the parser's `format_instructions`, which get added to the prompt:

```python
parser.get_format_instructions()
```

```output
'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\n\n# Examples\n## Schema\n\`\`\`\n{"title": "Players", "description": "A list of players", "type": "array", "items": {"$ref": "#/definitions/Player"}, "definitions": {"Player": {"title": "Player", "type": "object", "properties": {"name": {"title": "Name", "description": "Player name", "type": "string"}, "avg": {"title": "Avg", "description": "Batting average", "type": "number"}}, "required": ["name", "avg"]}}}\n\`\`\`\n## Well formatted instance\n\`\`\`\n- name: John Doe\n  avg: 0.3\n- name: Jane Maxfield\n  avg: 1.4\n\`\`\`\n\n## Schema\n\`\`\`\n{"properties": {"habit": { "description": "A common daily habit", "type": "string" }, "sustainable_alternative": { "description": "An environmentally friendly alternative to the habit", "type": "string"}}, "required": ["habit", "sustainable_alternative"]}\n\`\`\`\n## Well formatted instance\n\`\`\`\nhabit: Using disposable water bottles for daily hydration.\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\n\`\`\` \n\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \n\`\`\`\n{"properties": {"setup": {"title": "Setup", "description": "question to set up a joke", "type": "string"}, "punchline": {"title": "Punchline", "description": "answer to resolve the joke", "type": "string"}}, "required": ["setup", "punchline"]}\n\`\`\`\n\nMake sure to always enclose the YAML output in triple backticks (\`\`\`). Please do not add anything other than valid YAML output!'
```

You can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to prompt a model to return YAML. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other related techniques.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/output_parser_yaml.ipynb)

* * *


- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/few_shot_examples_chat.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/few_shot_examples_chat.ipynb)

# How to use few shot examples in chat models

Prerequisites

This guide assumes familiarity with the following concepts:

- [Prompt templates](/docs/concepts/prompt_templates/)
- [Example selectors](/docs/concepts/example_selectors/)
- [Chat models](/docs/concepts/chat_models/)
- [Vectorstores](/docs/concepts/vectorstores/)

This guide covers how to prompt a chat model with example inputs and outputs. Providing the model with a few such examples is called [few-shotting](/docs/concepts/few_shot_prompting/), and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.

There does not appear to be solid consensus on how best to do few-shot prompting, and the optimal prompt compilation will likely vary by model. Because of this, we provide few-shot prompt templates like the [FewShotChatMessagePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate.html?highlight=fewshot#langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate) as a flexible starting point, and you can modify or replace them as you see fit.

The goal of few-shot prompt templates are to dynamically select examples based on an input, and then format the examples in a final prompt to provide for the model.

**Note:** The following code examples are for chat models only, since `FewShotChatMessagePromptTemplates` are designed to output formatted [chat messages](/docs/concepts/messages/) rather than pure strings. For similar few-shot prompt examples for pure string templates compatible with completion models (LLMs), see the [few-shot prompt templates](/docs/how_to/few_shot_examples/) guide.

## Fixed Examples[â€‹](#fixed-examples "Direct link to Fixed Examples")

The most basic (and common) few-shot prompting technique is to use fixed prompt examples. This way you can select a chain, evaluate it, and avoid worrying about additional moving parts in production.

The basic components of the template are:

- `examples`: A list of dictionary examples to include in the final prompt.
- `example_prompt`: converts each example into 1 or more messages through its [`format_messages`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html?highlight=format_messages#langchain_core.prompts.chat.ChatPromptTemplate.format_messages) method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.

Below is a simple demonstration. First, define the examples you'd like to include. Let's give the LLM an unfamiliar mathematical operator, denoted by the "ðŸ¦œ" emoji:

```python
%pip install -qU langchain langchain-openai langchain-chroma

import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

If we try to ask the model what the result of this expression is, it will fail:

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini", temperature=0.0)

model.invoke("What is 2 ðŸ¦œ 9?")
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
AIMessage(content='The expression "2 ðŸ¦œ 9" is not a standard mathematical operation or equation. It appears to be a combination of the number 2 and the parrot emoji ðŸ¦œ followed by the number 9. It does not have a specific mathematical meaning.', response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 17, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-aad12dda-5c47-4a1e-9949-6fe94e03242a-0', usage_metadata={'input_tokens': 17, 'output_tokens': 54, 'total_tokens': 71})
```

Now let's see what happens if we give the LLM some examples to work with. We'll define some below:

```python
from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate

examples = [
    {"input": "2 ðŸ¦œ 2", "output": "4"},
    {"input": "2 ðŸ¦œ 3", "output": "5"},
]
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [FewShotChatMessagePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate.html)

Next, assemble them into the few-shot prompt template.

```python
# This is a prompt template used to format each individual example.
example_prompt = ChatPromptTemplate.from_messages(
    [
        ("human", "{input}"),
        ("ai", "{output}"),
    ]
)
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
)

print(few_shot_prompt.invoke({}).to_messages())
```

```output
[HumanMessage(content='2 ðŸ¦œ 2'), AIMessage(content='4'), HumanMessage(content='2 ðŸ¦œ 3'), AIMessage(content='5')]
```

Finally, we assemble the final prompt as shown below, passing `few_shot_prompt` directly into the `from_messages` factory method, and use it with a model:

```python
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a wondrous wizard of math."),
        few_shot_prompt,
        ("human", "{input}"),
    ]
)
```

And now let's ask the model the initial question and see how it does:

```python
from langchain_openai import ChatOpenAI

chain = final_prompt | model

chain.invoke({"input": "What is 2 ðŸ¦œ 9?"})
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
AIMessage(content='11', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 60, 'total_tokens': 61}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5ec4e051-262f-408e-ad00-3f2ebeb561c3-0', usage_metadata={'input_tokens': 60, 'output_tokens': 1, 'total_tokens': 61})
```

And we can see that the model has now inferred that the parrot emoji means addition from the given few-shot examples!

## Dynamic few-shot prompting[â€‹](#dynamic-few-shot-prompting "Direct link to Dynamic few-shot prompting")

Sometimes you may want to select only a few examples from your overall set to show based on the input. For this, you can replace the `examples` passed into `FewShotChatMessagePromptTemplate` with an `example_selector`. The other components remain the same as above! Our dynamic few-shot prompt template would look like:

- `example_selector`: responsible for selecting few-shot examples (and the order in which they are returned) for a given input. These implement the [BaseExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.base.BaseExampleSelector.html?highlight=baseexampleselector#langchain_core.example_selectors.base.BaseExampleSelector) interface. A common example is the vectorstore-backed [SemanticSimilarityExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector.html?highlight=semanticsimilarityexampleselector#langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector)
- `example_prompt`: convert each example into 1 or more messages through its [`format_messages`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html?highlight=chatprompttemplate#langchain_core.prompts.chat.ChatPromptTemplate.format_messages) method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.

These once again can be composed with other messages and chat templates to assemble your final prompt.

Let's walk through an example with the `SemanticSimilarityExampleSelector`. Since this implementation uses a vectorstore to select examples based on semantic similarity, we will want to first populate the store. Since the basic idea here is that we want to search for and return examples most similar to the text input, we embed the `values` of our prompt examples rather than considering the keys:

```python
from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings

examples = [
    {"input": "2 ðŸ¦œ 2", "output": "4"},
    {"input": "2 ðŸ¦œ 3", "output": "5"},
    {"input": "2 ðŸ¦œ 4", "output": "6"},
    {"input": "What did the cow say to the moon?", "output": "nothing at all"},
    {
        "input": "Write me a poem about the moon",
        "output": "One for the moon, and one for me, who are we to talk about the moon?",
    },
]

to_vectorize = [" ".join(example.values()) for example in examples]
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)
```

**API Reference:**[SemanticSimilarityExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

### Create the `example_selector`[â€‹](#create-the-example_selector "Direct link to create-the-example_selector")

With a vectorstore created, we can create the `example_selector`. Here we will call it in isolation, and set `k` on it to only fetch the two example closest to the input.

```python
example_selector = SemanticSimilarityExampleSelector(
    vectorstore=vectorstore,
    k=2,
)

# The prompt template will load examples by passing the input do the `select_examples` method
example_selector.select_examples({"input": "horse"})
```

```output
[{'input': 'What did the cow say to the moon?', 'output': 'nothing at all'},
 {'input': '2 ðŸ¦œ 4', 'output': '6'}]
```

### Create prompt template[â€‹](#create-prompt-template "Direct link to Create prompt template")

We now assemble the prompt template, using the `example_selector` created above.

```python
from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate

# Define the few-shot prompt.
few_shot_prompt = FewShotChatMessagePromptTemplate(
    # The input variables select the values to pass to the example_selector
    input_variables=["input"],
    example_selector=example_selector,
    # Define how each example will be formatted.
    # In this case, each example will become 2 messages:
    # 1 human, and 1 AI
    example_prompt=ChatPromptTemplate.from_messages(
        [("human", "{input}"), ("ai", "{output}")]
    ),
)

print(few_shot_prompt.invoke(input="What's 3 ðŸ¦œ 3?").to_messages())
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [FewShotChatMessagePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate.html)

```output
[HumanMessage(content='2 ðŸ¦œ 3'), AIMessage(content='5'), HumanMessage(content='2 ðŸ¦œ 4'), AIMessage(content='6')]
```

And we can pass this few-shot chat message prompt template into another chat prompt template:

```python
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a wondrous wizard of math."),
        few_shot_prompt,
        ("human", "{input}"),
    ]
)

print(few_shot_prompt.invoke(input="What's 3 ðŸ¦œ 3?"))
```

```output
messages=[HumanMessage(content='2 ðŸ¦œ 3'), AIMessage(content='5'), HumanMessage(content='2 ðŸ¦œ 4'), AIMessage(content='6')]
```

### Use with an chat model[â€‹](#use-with-an-chat-model "Direct link to Use with an chat model")

Finally, you can connect your model to the few-shot prompt.

```python
chain = final_prompt | ChatOpenAI(model="gpt-4o-mini", temperature=0.0)

chain.invoke({"input": "What's 3 ðŸ¦œ 3?"})
```

```output
AIMessage(content='6', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 60, 'total_tokens': 61}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d1863e5e-17cd-4e9d-bf7a-b9f118747a65-0', usage_metadata={'input_tokens': 60, 'output_tokens': 1, 'total_tokens': 61})
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to add few-shot examples to your chat prompts.

Next, check out the other how-to guides on prompt templates in this section, the related how-to guide on [few shotting with text completion models](/docs/how_to/few_shot_examples/), or the other [example selector how-to guides](/docs/how_to/example_selectors/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/few_shot_examples_chat.ipynb)

* * *


- [Fixed Examples](#fixed-examples)
- [Dynamic few-shot prompting](#dynamic-few-shot-prompting)
  
  - [Create the `example_selector`](#create-the-example_selector)
  - [Create prompt template](#create-prompt-template)
  - [Use with an chat model](#use-with-an-chat-model)
- [Next steps](#next-steps)









# How to force models to call a tool

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models/)
- [LangChain Tools](/docs/concepts/tools/)
- [How to use a model to call tools](/docs/how_to/tool_calling/)

In order to force our LLM to select a specific [tool](/docs/concepts/tools/), we can use the `tool_choice` parameter to ensure certain behavior. First, let's define our model and tools:

```python
from langchain_core.tools import tool


@tool
def add(a: int, b: int) -> int:
    """Adds a and b."""
    return a + b


@tool
def multiply(a: int, b: int) -> int:
    """Multiplies a and b."""
    return a * b


tools = [add, multiply]
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

For example, we can force our tool to call the multiply tool by using the following code:

```python
llm_forced_to_multiply = llm.bind_tools(tools, tool_choice="multiply")
llm_forced_to_multiply.invoke("what is 2 + 4")
```

```output
AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9cViskmLvPnHjXk9tbVla5HA', 'function': {'arguments': '{"a":2,"b":4}', 'name': 'Multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 103, 'total_tokens': 112}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-095b827e-2bdd-43bb-8897-c843f4504883-0', tool_calls=[{'name': 'Multiply', 'args': {'a': 2, 'b': 4}, 'id': 'call_9cViskmLvPnHjXk9tbVla5HA'}], usage_metadata={'input_tokens': 103, 'output_tokens': 9, 'total_tokens': 112})
```

Even if we pass it something that doesn't require multiplcation - it will still call the tool!

We can also just force our tool to select at least one of our tools by passing in the "any" (or "required" which is OpenAI specific) keyword to the `tool_choice` parameter.

```python
llm_forced_to_use_tool = llm.bind_tools(tools, tool_choice="any")
llm_forced_to_use_tool.invoke("What day is today?")
```

```output
AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mCSiJntCwHJUBfaHZVUB2D8W', 'function': {'arguments': '{"a":1,"b":2}', 'name': 'Add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 94, 'total_tokens': 109}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-28f75260-9900-4bed-8cd3-f1579abb65e5-0', tool_calls=[{'name': 'Add', 'args': {'a': 1, 'b': 2}, 'id': 'call_mCSiJntCwHJUBfaHZVUB2D8W'}], usage_metadata={'input_tokens': 94, 'output_tokens': 15, 'total_tokens': 109})
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tool_choice.ipynb)

* * *










# How to use multimodal prompts

Here we demonstrate how to use prompt templates to format [multimodal](/docs/concepts/multimodality/) inputs to models.

To use prompt templates in the context of multimodal data, we can templatize elements of the corresponding content block. For example, below we define a prompt that takes a URL for an image as a parameter:

```python
from langchain_core.prompts import ChatPromptTemplate

# Define prompt
prompt = ChatPromptTemplate(
    [
        {
            "role": "system",
            "content": "Describe the image provided.",
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source_type": "url",
                    "url": "{image_url}",
                },
            ],
        },
    ]
)
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

Let's use this prompt to pass an image to a [chat model](/docs/concepts/chat_models/#multimodality):

```python
from langchain.chat_models import init_chat_model

llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")

url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"

chain = prompt | llm
response = chain.invoke({"image_url": url})
print(response.text())
```

**API Reference:**[init\_chat\_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)

```output
This image shows a beautiful wooden boardwalk cutting through a lush green wetland or marsh area. The boardwalk extends straight ahead toward the horizon, creating a strong leading line through the composition. On either side, tall green grasses sway in what appears to be a summer or late spring setting. The sky is particularly striking, with wispy cirrus clouds streaking across a vibrant blue background. In the distance, you can see a tree line bordering the wetland area. The lighting suggests this may be during "golden hour" - either early morning or late afternoon - as there's a warm, gentle quality to the light that's illuminating the scene. The wooden planks of the boardwalk appear well-maintained and provide safe passage through what would otherwise be difficult terrain to traverse. It's the kind of scene you might find in a nature preserve or wildlife refuge designed to give visitors access to observe wetland ecosystems while protecting the natural environment.
```

Note that we can templatize arbitrary elements of the content block:

```python
prompt = ChatPromptTemplate(
    [
        {
            "role": "system",
            "content": "Describe the image provided.",
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source_type": "base64",
                    "mime_type": "{image_mime_type}",
                    "data": "{image_data}",
                    "cache_control": {"type": "{cache_type}"},
                },
            ],
        },
    ]
)
```

```python
import base64

import httpx

image_data = base64.b64encode(httpx.get(url).content).decode("utf-8")

chain = prompt | llm
response = chain.invoke(
    {
        "image_data": image_data,
        "image_mime_type": "image/jpeg",
        "cache_type": "ephemeral",
    }
)
print(response.text())
```

```output
This image shows a beautiful wooden boardwalk cutting through a lush green marsh or wetland area. The boardwalk extends straight ahead toward the horizon, creating a strong leading line in the composition. The surrounding vegetation consists of tall grass and reeds in vibrant green hues, with some bushes and trees visible in the background. The sky is particularly striking, featuring a bright blue color with wispy white clouds streaked across it. The lighting suggests this photo was taken during the "golden hour" - either early morning or late afternoon - giving the scene a warm, peaceful quality. The raised wooden path provides accessible access through what would otherwise be difficult terrain to traverse, allowing visitors to experience and appreciate this natural environment.
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/multimodal_prompts.ipynb)

* * *










# How to bind model-specific tools

Providers adopt different conventions for formatting tool schemas. For instance, OpenAI uses a format like this:

- `type`: The type of the tool. At the time of writing, this is always `"function"`.
- `function`: An object containing tool parameters.
- `function.name`: The name of the schema to output.
- `function.description`: A high level description of the schema to output.
- `function.parameters`: The nested details of the schema you want to extract, formatted as a [JSON schema](https://json-schema.org/) dict.

We can bind this model-specific format directly to the model as well if preferred. Here's an example:

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI()

model_with_tools = model.bind(
    tools=[
        {
            "type": "function",
            "function": {
                "name": "multiply",
                "description": "Multiply two integers together.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "a": {"type": "number", "description": "First integer"},
                        "b": {"type": "number", "description": "Second integer"},
                    },
                    "required": ["a", "b"],
                },
            },
        }
    ]
)

model_with_tools.invoke("Whats 119 times 8?")
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mn4ELw1NbuE0DFYhIeK0GrPe', 'function': {'arguments': '{"a":119,"b":8}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 62, 'total_tokens': 79}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-353e8a9a-7125-4f94-8c68-4f3da4c21120-0', tool_calls=[{'name': 'multiply', 'args': {'a': 119, 'b': 8}, 'id': 'call_mn4ELw1NbuE0DFYhIeK0GrPe'}])
```

This is functionally equivalent to the `bind_tools()` method.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tools_model_specific.ipynb)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/tools_human.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/tools_human.ipynb)

# How to add a human-in-the-loop for tools

There are certain tools that we don't trust a model to execute on its own. One thing we can do in such situations is require human approval before the tool is invoked.

info

This how-to guide shows a simple way to add human-in-the-loop for code running in a jupyter notebook or in a terminal.

To build a production application, you will need to do more work to keep track of application state appropriately.

We recommend using `langgraph` for powering such a capability. For more details, please see this [guide](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/).

## Setup[â€‹](#setup "Direct link to Setup")

We'll need to install the following packages:

```python
%pip install --upgrade --quiet langchain
```

And set these environment variables:

```python
import getpass
import os

# If you'd like to use LangSmith, uncomment the below:
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

## Chain[â€‹](#chain "Direct link to Chain")

Let's create a few simple (dummy) tools and a tool-calling chain:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
from typing import Dict, List

from langchain_core.messages import AIMessage
from langchain_core.runnables import Runnable, RunnablePassthrough
from langchain_core.tools import tool


@tool
def count_emails(last_n_days: int) -> int:
    """Dummy function to count number of e-mails. Returns 2 * last_n_days."""
    return last_n_days * 2


@tool
def send_email(message: str, recipient: str) -> str:
    """Dummy function for sending an e-mail."""
    return f"Successfully sent email to {recipient}."


tools = [count_emails, send_email]
llm_with_tools = llm.bind_tools(tools)


def call_tools(msg: AIMessage) -> List[Dict]:
    """Simple sequential tool calling helper."""
    tool_map = {tool.name: tool for tool in tools}
    tool_calls = msg.tool_calls.copy()
    for tool_call in tool_calls:
        tool_call["output"] = tool_map[tool_call["name"]].invoke(tool_call["args"])
    return tool_calls


chain = llm_with_tools | call_tools
chain.invoke("how many emails did i get in the last 5 days?")
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

```output
[{'name': 'count_emails',
  'args': {'last_n_days': 5},
  'id': 'toolu_01QYZdJ4yPiqsdeENWHqioFW',
  'output': 10}]
```

## Adding human approval[â€‹](#adding-human-approval "Direct link to Adding human approval")

Let's add a step in the chain that will ask a person to approve or reject the tool call request.

On rejection, the step will raise an exception which will stop execution of the rest of the chain.

```python
import json


class NotApproved(Exception):
    """Custom exception."""


def human_approval(msg: AIMessage) -> AIMessage:
    """Responsible for passing through its input or raising an exception.

    Args:
        msg: output from the chat model

    Returns:
        msg: original output from the msg
    """
    tool_strs = "\n\n".join(
        json.dumps(tool_call, indent=2) for tool_call in msg.tool_calls
    )
    input_msg = (
        f"Do you approve of the following tool invocations\n\n{tool_strs}\n\n"
        "Anything except 'Y'/'Yes' (case-insensitive) will be treated as a no.\n >>>"
    )
    resp = input(input_msg)
    if resp.lower() not in ("yes", "y"):
        raise NotApproved(f"Tool invocations not approved:\n\n{tool_strs}")
    return msg
```

```python
chain = llm_with_tools | human_approval | call_tools
chain.invoke("how many emails did i get in the last 5 days?")
```

```output
Do you approve of the following tool invocations

{
  "name": "count_emails",
  "args": {
    "last_n_days": 5
  },
  "id": "toolu_01WbD8XeMoQaRFtsZezfsHor"
}

Anything except 'Y'/'Yes' (case-insensitive) will be treated as a no.
 >>> yes
```

```output
[{'name': 'count_emails',
  'args': {'last_n_days': 5},
  'id': 'toolu_01WbD8XeMoQaRFtsZezfsHor',
  'output': 10}]
```

```python
try:
    chain.invoke("Send sally@gmail.com an email saying 'What's up homie'")
except NotApproved as e:
    print()
    print(e)
```

```````output
Do you approve of the following tool invocations

{
  "name": "send_email",
  "args": {
    "recipient": "sally@gmail.com",
    "message": "What's up homie"
  },
  "id": "toolu_014XccHFzBiVcc9GV1harV9U"
}

Anything except 'Y'/'Yes' (case-insensitive) will be treated as a no.
 >>> no
``````output

Tool invocations not approved:

{
  "name": "send_email",
  "args": {
    "recipient": "sally@gmail.com",
    "message": "What's up homie"
  },
  "id": "toolu_014XccHFzBiVcc9GV1harV9U"
}
```````

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tools_human.ipynb)

* * *


- [Setup](#setup)
- [Chain](#chain)
- [Adding human approval](#adding-human-approval)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/convert_runnable_to_tool.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/convert_runnable_to_tool.ipynb)

# How to convert Runnables to Tools

Prerequisites

This guide assumes familiarity with the following concepts:

- [Runnables](/docs/concepts/runnables/)
- [Tools](/docs/concepts/tools/)
- [Agents](/docs/tutorials/agents/)

Here we will demonstrate how to convert a LangChain `Runnable` into a tool that can be used by agents, chains, or chat models.

## Dependencies[â€‹](#dependencies "Direct link to Dependencies")

**Note**: this guide requires `langchain-core` &gt;= 0.2.13. We will also use [OpenAI](/docs/integrations/providers/openai/) for embeddings, but any LangChain embeddings should suffice. We will use a simple [LangGraph](https://langchain-ai.github.io/langgraph/) agent for demonstration purposes.

```python
%%capture --no-stderr
%pip install -U langchain-core langchain-openai langgraph
```

LangChain [tools](/docs/concepts/tools/) are interfaces that an agent, chain, or chat model can use to interact with the world. See [here](/docs/how_to/#tools) for how-to guides covering tool-calling, built-in tools, custom tools, and more information.

LangChain tools-- instances of [BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.BaseTool.html)-- are [Runnables](/docs/concepts/runnables/) with additional constraints that enable them to be invoked effectively by language models:

- Their inputs are constrained to be serializable, specifically strings and Python `dict` objects;
- They contain names and descriptions indicating how and when they should be used;
- They may contain a detailed [args\_schema](https://python.langchain.com/docs/how_to/custom_tools/) for their arguments. That is, while a tool (as a `Runnable`) might accept a single `dict` input, the specific keys and type information needed to populate a dict should be specified in the `args_schema`.

Runnables that accept string or `dict` input can be converted to tools using the [as\_tool](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.as_tool) method, which allows for the specification of names, descriptions, and additional schema information for arguments.

## Basic usage[â€‹](#basic-usage "Direct link to Basic usage")

With typed `dict` input:

```python
from typing import List

from langchain_core.runnables import RunnableLambda
from typing_extensions import TypedDict


class Args(TypedDict):
    a: int
    b: List[int]


def f(x: Args) -> str:
    return str(x["a"] * max(x["b"]))


runnable = RunnableLambda(f)
as_tool = runnable.as_tool(
    name="My tool",
    description="Explanation of when to use tool.",
)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```python
print(as_tool.description)

as_tool.args_schema.schema()
```

```output
Explanation of when to use tool.
```

```output
{'title': 'My tool',
 'type': 'object',
 'properties': {'a': {'title': 'A', 'type': 'integer'},
  'b': {'title': 'B', 'type': 'array', 'items': {'type': 'integer'}}},
 'required': ['a', 'b']}
```

```python
as_tool.invoke({"a": 3, "b": [1, 2]})
```

```output
'6'
```

Without typing information, arg types can be specified via `arg_types`:

```python
from typing import Any, Dict


def g(x: Dict[str, Any]) -> str:
    return str(x["a"] * max(x["b"]))


runnable = RunnableLambda(g)
as_tool = runnable.as_tool(
    name="My tool",
    description="Explanation of when to use tool.",
    arg_types={"a": int, "b": List[int]},
)
```

Alternatively, the schema can be fully specified by directly passing the desired [args\_schema](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.BaseTool.html#langchain_core.tools.BaseTool.args_schema) for the tool:

```python
from pydantic import BaseModel, Field


class GSchema(BaseModel):
    """Apply a function to an integer and list of integers."""

    a: int = Field(..., description="Integer")
    b: List[int] = Field(..., description="List of ints")


runnable = RunnableLambda(g)
as_tool = runnable.as_tool(GSchema)
```

String input is also supported:

```python
def f(x: str) -> str:
    return x + "a"


def g(x: str) -> str:
    return x + "z"


runnable = RunnableLambda(f) | g
as_tool = runnable.as_tool()
```

```python
as_tool.invoke("b")
```

```output
'baz'
```

## In agents[â€‹](#in-agents "Direct link to In agents")

Below we will incorporate LangChain Runnables as tools in an [agent](/docs/concepts/agents/) application. We will demonstrate with:

- a document [retriever](/docs/concepts/retrievers/);
- a simple [RAG](/docs/tutorials/rag/) chain, allowing an agent to delegate relevant queries to it.

We first instantiate a chat model that supports [tool calling](/docs/how_to/tool_calling/):

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

Following the [RAG tutorial](/docs/tutorials/rag/), let's first construct a retriever:

```python
from langchain_core.documents import Document
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings

documents = [
    Document(
        page_content="Dogs are great companions, known for their loyalty and friendliness.",
    ),
    Document(
        page_content="Cats are independent pets that often enjoy their own space.",
    ),
]

vectorstore = InMemoryVectorStore.from_documents(
    documents, embedding=OpenAIEmbeddings()
)

retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 1},
)
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

We next create use a simple pre-built [LangGraph agent](https://python.langchain.com/docs/tutorials/agents/) and provide it the tool:

```python
from langgraph.prebuilt import create_react_agent

tools = [
    retriever.as_tool(
        name="pet_info_retriever",
        description="Get information about pets.",
    )
]
agent = create_react_agent(llm, tools)
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```python
for chunk in agent.stream({"messages": [("human", "What are dogs known for?")]}):
    print(chunk)
    print("----")
```

```output
{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_W8cnfOjwqEn4cFcg19LN9mYD', 'function': {'arguments': '{"__arg1":"dogs"}', 'name': 'pet_info_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 60, 'total_tokens': 79}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d7f81de9-1fb7-4caf-81ed-16dcdb0b2ab4-0', tool_calls=[{'name': 'pet_info_retriever', 'args': {'__arg1': 'dogs'}, 'id': 'call_W8cnfOjwqEn4cFcg19LN9mYD'}], usage_metadata={'input_tokens': 60, 'output_tokens': 19, 'total_tokens': 79})]}}
----
{'tools': {'messages': [ToolMessage(content="[Document(id='86f835fe-4bbe-4ec6-aeb4-489a8b541707', page_content='Dogs are great companions, known for their loyalty and friendliness.')]", name='pet_info_retriever', tool_call_id='call_W8cnfOjwqEn4cFcg19LN9mYD')]}}
----
{'agent': {'messages': [AIMessage(content='Dogs are known for being great companions, known for their loyalty and friendliness.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 134, 'total_tokens': 152}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9ca5847a-a5eb-44c0-a774-84cc2c5bbc5b-0', usage_metadata={'input_tokens': 134, 'output_tokens': 18, 'total_tokens': 152})]}}
----
```

See [LangSmith trace](https://smith.langchain.com/public/44e438e3-2faf-45bd-b397-5510fc145eb9/r) for the above run.

Going further, we can create a simple [RAG](/docs/tutorials/rag/) chain that takes an additional parameter-- here, the "style" of the answer.

```python
from operator import itemgetter

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

system_prompt = """
You are an assistant for question-answering tasks.
Use the below context to answer the question. If
you don't know the answer, say you don't know.
Use three sentences maximum and keep the answer
concise.

Answer in the style of {answer_style}.

Question: {question}

Context: {context}
"""

prompt = ChatPromptTemplate.from_messages([("system", system_prompt)])

rag_chain = (
    {
        "context": itemgetter("question") | retriever,
        "question": itemgetter("question"),
        "answer_style": itemgetter("answer_style"),
    }
    | prompt
    | llm
    | StrOutputParser()
)
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)

Note that the input schema for our chain contains the required arguments, so it converts to a tool without further specification:

```python
rag_chain.input_schema.schema()
```

```output
{'title': 'RunnableParallel<context,question,answer_style>Input',
 'type': 'object',
 'properties': {'question': {'title': 'Question'},
  'answer_style': {'title': 'Answer Style'}}}
```

```python
rag_tool = rag_chain.as_tool(
    name="pet_expert",
    description="Get information about pets.",
)
```

Below we again invoke the agent. Note that the agent populates the required parameters in its `tool_calls`:

```python
agent = create_react_agent(llm, [rag_tool])

for chunk in agent.stream(
    {"messages": [("human", "What would a pirate say dogs are known for?")]}
):
    print(chunk)
    print("----")
```

```output
{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_17iLPWvOD23zqwd1QVQ00Y63', 'function': {'arguments': '{"question":"What are dogs known for according to pirates?","answer_style":"quote"}', 'name': 'pet_expert'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 59, 'total_tokens': 87}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7fef44f3-7bba-4e63-8c51-2ad9c5e65e2e-0', tool_calls=[{'name': 'pet_expert', 'args': {'question': 'What are dogs known for according to pirates?', 'answer_style': 'quote'}, 'id': 'call_17iLPWvOD23zqwd1QVQ00Y63'}], usage_metadata={'input_tokens': 59, 'output_tokens': 28, 'total_tokens': 87})]}}
----
{'tools': {'messages': [ToolMessage(content='"Dogs are known for their loyalty and friendliness, making them great companions for pirates on long sea voyages."', name='pet_expert', tool_call_id='call_17iLPWvOD23zqwd1QVQ00Y63')]}}
----
{'agent': {'messages': [AIMessage(content='According to pirates, dogs are known for their loyalty and friendliness, making them great companions for pirates on long sea voyages.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 119, 'total_tokens': 146}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5a30edc3-7be0-4743-b980-ca2f8cad9b8d-0', usage_metadata={'input_tokens': 119, 'output_tokens': 27, 'total_tokens': 146})]}}
----
```

See [LangSmith trace](https://smith.langchain.com/public/147ae4e6-4dfb-4dd9-8ca0-5c5b954f08ac/r) for the above run.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/convert_runnable_to_tool.ipynb)

* * *


- [Dependencies](#dependencies)
- [Basic usage](#basic-usage)
- [In agents](#in-agents)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/output_parser_custom.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/output_parser_custom.ipynb)

# How to create a custom Output Parser

In some situations you may want to implement a custom [parser](/docs/concepts/output_parsers/) to structure the model output into a custom format.

There are two ways to implement a custom parser:

1. Using `RunnableLambda` or `RunnableGenerator` in [LCEL](/docs/concepts/lcel/) -- we strongly recommend this for most use cases
2. By inheriting from one of the base classes for out parsing -- this is the hard way of doing things

The difference between the two approaches are mostly superficial and are mainly in terms of which callbacks are triggered (e.g., `on_chain_start` vs. `on_parser_start`), and how a runnable lambda vs. a parser might be visualized in a tracing platform like LangSmith.

## Runnable Lambdas and Generators[â€‹](#runnable-lambdas-and-generators "Direct link to Runnable Lambdas and Generators")

The recommended way to parse is using **runnable lambdas** and **runnable generators**!

Here, we will make a simple parse that inverts the case of the output from the model.

For example, if the model outputs: "Meow", the parser will produce "mEOW".

```python
from typing import Iterable

from langchain_anthropic.chat_models import ChatAnthropic
from langchain_core.messages import AIMessage, AIMessageChunk

model = ChatAnthropic(model_name="claude-2.1")


def parse(ai_message: AIMessage) -> str:
    """Parse the AI message."""
    return ai_message.content.swapcase()


chain = model | parse
chain.invoke("hello")
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [AIMessageChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html)

```output
'hELLO!'
```

tip

LCEL automatically upgrades the function `parse` to `RunnableLambda(parse)` when composed using a `|` syntax.

If you don't like that you can manually import `RunnableLambda` and then run`parse = RunnableLambda(parse)`.

Does streaming work?

```python
for chunk in chain.stream("tell me about yourself in one sentence"):
    print(chunk, end="|", flush=True)
```

```output
i'M cLAUDE, AN ai ASSISTANT CREATED BY aNTHROPIC TO BE HELPFUL, HARMLESS, AND HONEST.|
```

No, it doesn't because the parser aggregates the input before parsing the output.

If we want to implement a streaming parser, we can have the parser accept an iterable over the input instead and yield the results as they're available.

```python
from langchain_core.runnables import RunnableGenerator


def streaming_parse(chunks: Iterable[AIMessageChunk]) -> Iterable[str]:
    for chunk in chunks:
        yield chunk.content.swapcase()


streaming_parse = RunnableGenerator(streaming_parse)
```

**API Reference:**[RunnableGenerator](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableGenerator.html)

important

Please wrap the streaming parser in `RunnableGenerator` as we may stop automatically upgrading it with the `|` syntax.

```python
chain = model | streaming_parse
chain.invoke("hello")
```

```output
'hELLO!'
```

Let's confirm that streaming works!

```python
for chunk in chain.stream("tell me about yourself in one sentence"):
    print(chunk, end="|", flush=True)
```

```output
i|'M| cLAUDE|,| AN| ai| ASSISTANT| CREATED| BY| aN|THROP|IC| TO| BE| HELPFUL|,| HARMLESS|,| AND| HONEST|.|
```

## Inheriting from Parsing Base Classes[â€‹](#inheriting-from-parsing-base-classes "Direct link to Inheriting from Parsing Base Classes")

Another approach to implement a parser is by inheriting from `BaseOutputParser`, `BaseGenerationOutputParser` or another one of the base parsers depending on what you need to do.

In general, we **do not** recommend this approach for most use cases as it results in more code to write without significant benefits.

The simplest kind of output parser extends the `BaseOutputParser` class and must implement the following methods:

- `parse`: takes the string output from the model and parses it
- (optional) `_type`: identifies the name of the parser.

When the output from the chat model or LLM is malformed, the can throw an `OutputParserException` to indicate that parsing fails because of bad input. Using this exception allows code that utilizes the parser to handle the exceptions in a consistent manner.

Parsers are Runnables! ðŸƒ

Because `BaseOutputParser` implements the `Runnable` interface, any custom parser you will create this way will become valid LangChain Runnables and will benefit from automatic async support, batch interface, logging support etc.

### Simple Parser[â€‹](#simple-parser "Direct link to Simple Parser")

Here's a simple parser that can parse a **string** representation of a boolean (e.g., `YES` or `NO`) and convert it into the corresponding `boolean` type.

```python
from langchain_core.exceptions import OutputParserException
from langchain_core.output_parsers import BaseOutputParser


# The [bool] desribes a parameterization of a generic.
# It's basically indicating what the return type of parse is
# in this case the return type is either True or False
class BooleanOutputParser(BaseOutputParser[bool]):
    """Custom boolean parser."""

    true_val: str = "YES"
    false_val: str = "NO"

    def parse(self, text: str) -> bool:
        cleaned_text = text.strip().upper()
        if cleaned_text not in (self.true_val.upper(), self.false_val.upper()):
            raise OutputParserException(
                f"BooleanOutputParser expected output value to either be "
                f"{self.true_val} or {self.false_val} (case-insensitive). "
                f"Received {cleaned_text}."
            )
        return cleaned_text == self.true_val.upper()

    @property
    def _type(self) -> str:
        return "boolean_output_parser"
```

**API Reference:**[OutputParserException](https://python.langchain.com/api_reference/core/exceptions/langchain_core.exceptions.OutputParserException.html) | [BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)

```python
parser = BooleanOutputParser()
parser.invoke("YES")
```

```output
True
```

```python
try:
    parser.invoke("MEOW")
except Exception as e:
    print(f"Triggered an exception of type: {type(e)}")
```

```output
Triggered an exception of type: <class 'langchain_core.exceptions.OutputParserException'>
```

Let's test changing the parameterization

```python
parser = BooleanOutputParser(true_val="OKAY")
parser.invoke("OKAY")
```

```output
True
```

Let's confirm that other LCEL methods are present

```python
parser.batch(["OKAY", "NO"])
```

```output
[True, False]
```

```python
await parser.abatch(["OKAY", "NO"])
```

```output
[True, False]
```

```python
from langchain_anthropic.chat_models import ChatAnthropic

anthropic = ChatAnthropic(model_name="claude-2.1")
anthropic.invoke("say OKAY or NO")
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)

```output
AIMessage(content='OKAY')
```

Let's test that our parser works!

```python
chain = anthropic | parser
chain.invoke("say OKAY or NO")
```

```output
True
```

note

The parser will work with either the output from an LLM (a string) or the output from a chat model (an `AIMessage`)!

### Parsing Raw Model Outputs[â€‹](#parsing-raw-model-outputs "Direct link to Parsing Raw Model Outputs")

Sometimes there is additional metadata on the model output that is important besides the raw text. One example of this is tool calling, where arguments intended to be passed to called functions are returned in a separate property. If you need this finer-grained control, you can instead subclass the `BaseGenerationOutputParser` class.

This class requires a single method `parse_result`. This method takes raw model output (e.g., list of `Generation` or `ChatGeneration`) and returns the parsed output.

Supporting both `Generation` and `ChatGeneration` allows the parser to work with both regular LLMs as well as with Chat Models.

```python
from typing import List

from langchain_core.exceptions import OutputParserException
from langchain_core.messages import AIMessage
from langchain_core.output_parsers import BaseGenerationOutputParser
from langchain_core.outputs import ChatGeneration, Generation


class StrInvertCase(BaseGenerationOutputParser[str]):
    """An example parser that inverts the case of the characters in the message.

    This is an example parse shown just for demonstration purposes and to keep
    the example as simple as possible.
    """

    def parse_result(self, result: List[Generation], *, partial: bool = False) -> str:
        """Parse a list of model Generations into a specific format.

        Args:
            result: A list of Generations to be parsed. The Generations are assumed
                to be different candidate outputs for a single model input.
                Many parsers assume that only a single generation is passed it in.
                We will assert for that
            partial: Whether to allow partial results. This is used for parsers
                     that support streaming
        """
        if len(result) != 1:
            raise NotImplementedError(
                "This output parser can only be used with a single generation."
            )
        generation = result[0]
        if not isinstance(generation, ChatGeneration):
            # Say that this one only works with chat generations
            raise OutputParserException(
                "This output parser can only be used with a chat generation."
            )
        return generation.message.content.swapcase()


chain = anthropic | StrInvertCase()
```

**API Reference:**[OutputParserException](https://python.langchain.com/api_reference/core/exceptions/langchain_core.exceptions.OutputParserException.html) | [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [BaseGenerationOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseGenerationOutputParser.html) | [ChatGeneration](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.chat_generation.ChatGeneration.html) | [Generation](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.generation.Generation.html)

Let's the new parser! It should be inverting the output from the model.

```python
chain.invoke("Tell me a short sentence about yourself")
```

```output
'hELLO! mY NAME IS cLAUDE.'
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/output_parser_custom.ipynb)

* * *


- [Runnable Lambdas and Generators](#runnable-lambdas-and-generators)
- [Inheriting from Parsing Base Classes](#inheriting-from-parsing-base-classes)
  
  - [Simple Parser](#simple-parser)
  - [Parsing Raw Model Outputs](#parsing-raw-model-outputs)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/sql_large_db.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/sql_large_db.ipynb)

# How to deal with large databases when doing SQL question-answering

In order to write valid queries against a database, we need to feed the model the table names, table schemas, and feature values for it to query over. When there are many tables, columns, and/or high-cardinality columns, it becomes impossible for us to dump the full information about our database in every prompt. Instead, we must find ways to dynamically insert into the prompt only the most relevant information.

In this guide we demonstrate methods for identifying such relevant information, and feeding this into a query-generation step. We will cover:

1. Identifying a relevant subset of tables;
2. Identifying a relevant subset of column values.

## Setup[â€‹](#setup "Direct link to Setup")

First, get required packages and set environment variables:

```python
%pip install --upgrade --quiet  langchain langchain-community langchain-openai
```

```python
# Uncomment the below to use LangSmith. Not required.
# import os
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
# os.environ["LANGSMITH_TRACING"] = "true"
```

The below example will use a SQLite connection with Chinook database. Follow [these installation steps](https://database.guide/2-sample-databases-sqlite/) to create `Chinook.db` in the same directory as this notebook:

- Save [this file](https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql) as `Chinook_Sqlite.sql`
- Run `sqlite3 Chinook.db`
- Run `.read Chinook_Sqlite.sql`
- Test `SELECT * FROM Artist LIMIT 10;`

Now, `Chinook.db` is in our directory and we can interface with it using the SQLAlchemy-driven [SQLDatabase](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.sql_database.SQLDatabase.html) class:

```python
from langchain_community.utilities import SQLDatabase

db = SQLDatabase.from_uri("sqlite:///Chinook.db")
print(db.dialect)
print(db.get_usable_table_names())
print(db.run("SELECT * FROM Artist LIMIT 10;"))
```

**API Reference:**[SQLDatabase](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.sql_database.SQLDatabase.html)

```output
sqlite
['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']
[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'AntÃ´nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]
```

## Many tables[â€‹](#many-tables "Direct link to Many tables")

One of the main pieces of information we need to include in our prompt is the schemas of the relevant tables. When we have very many tables, we can't fit all of the schemas in a single prompt. What we can do in such cases is first extract the names of the tables related to the user input, and then include only their schemas.

One easy and reliable way to do this is using [tool-calling](/docs/how_to/tool_calling/). Below, we show how we can use this feature to obtain output conforming to a desired format (in this case, a list of table names). We use the chat model's `.bind_tools` method to bind a tool in Pydantic format, and feed this into an output parser to reconstruct the object from the model's response.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
from langchain_core.output_parsers.openai_tools import PydanticToolsParser
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field


class Table(BaseModel):
    """Table in SQL database."""

    name: str = Field(description="Name of table in SQL database.")


table_names = "\n".join(db.get_usable_table_names())
system = f"""Return the names of ALL the SQL tables that MIGHT be relevant to the user question. \
The tables are:

{table_names}

Remember to include ALL POTENTIALLY RELEVANT tables, even if you're not sure that they're needed."""

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{input}"),
    ]
)
llm_with_tools = llm.bind_tools([Table])
output_parser = PydanticToolsParser(tools=[Table])

table_chain = prompt | llm_with_tools | output_parser

table_chain.invoke({"input": "What are all the genres of Alanis Morisette songs"})
```

**API Reference:**[PydanticToolsParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.openai_tools.PydanticToolsParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
[Table(name='Genre')]
```

This works pretty well! Except, as we'll see below, we actually need a few other tables as well. This would be pretty difficult for the model to know based just on the user question. In this case, we might think to simplify our model's job by grouping the tables together. We'll just ask the model to choose between categories "Music" and "Business", and then take care of selecting all the relevant tables from there:

```python
system = """Return the names of any SQL tables that are relevant to the user question.
The tables are:

Music
Business
"""

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{input}"),
    ]
)

category_chain = prompt | llm_with_tools | output_parser
category_chain.invoke({"input": "What are all the genres of Alanis Morisette songs"})
```

```output
[Table(name='Music'), Table(name='Business')]
```

```python
from typing import List


def get_tables(categories: List[Table]) -> List[str]:
    tables = []
    for category in categories:
        if category.name == "Music":
            tables.extend(
                [
                    "Album",
                    "Artist",
                    "Genre",
                    "MediaType",
                    "Playlist",
                    "PlaylistTrack",
                    "Track",
                ]
            )
        elif category.name == "Business":
            tables.extend(["Customer", "Employee", "Invoice", "InvoiceLine"])
    return tables


table_chain = category_chain | get_tables
table_chain.invoke({"input": "What are all the genres of Alanis Morisette songs"})
```

```output
['Album',
 'Artist',
 'Genre',
 'MediaType',
 'Playlist',
 'PlaylistTrack',
 'Track',
 'Customer',
 'Employee',
 'Invoice',
 'InvoiceLine']
```

Now that we've got a chain that can output the relevant tables for any query we can combine this with our [create\_sql\_query\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.sql_database.query.create_sql_query_chain.html), which can accept a list of `table_names_to_use` to determine which table schemas are included in the prompt:

```python
from operator import itemgetter

from langchain.chains import create_sql_query_chain
from langchain_core.runnables import RunnablePassthrough

query_chain = create_sql_query_chain(llm, db)
# Convert "question" key to the "input" key expected by current table_chain.
table_chain = {"input": itemgetter("question")} | table_chain
# Set table_names_to_use using table_chain.
full_chain = RunnablePassthrough.assign(table_names_to_use=table_chain) | query_chain
```

**API Reference:**[create\_sql\_query\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.sql_database.query.create_sql_query_chain.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)

```python
query = full_chain.invoke(
    {"question": "What are all the genres of Alanis Morisette songs"}
)
print(query)
```

```output
SELECT DISTINCT "g"."Name"
FROM "Genre" g
JOIN "Track" t ON "g"."GenreId" = "t"."GenreId"
JOIN "Album" a ON "t"."AlbumId" = "a"."AlbumId"
JOIN "Artist" ar ON "a"."ArtistId" = "ar"."ArtistId"
WHERE "ar"."Name" = 'Alanis Morissette'
LIMIT 5;
```

```python
db.run(query)
```

```output
"[('Rock',)]"
```

We can see the LangSmith trace for this run [here](https://smith.langchain.com/public/4fbad408-3554-4f33-ab47-1e510a1b52a3/r).

We've seen how to dynamically include a subset of table schemas in a prompt within a chain. Another possible approach to this problem is to let an Agent decide for itself when to look up tables by giving it a Tool to do so. You can see an example of this in the [SQL: Agents](/docs/tutorials/agents/) guide.

## High-cardinality columns[â€‹](#high-cardinality-columns "Direct link to High-cardinality columns")

In order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.

One naive strategy it to create a vector store with all the distinct proper nouns that exist in the database. We can then query that vector store each user input and inject the most relevant proper nouns into the prompt.

First we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:

```python
import ast
import re


def query_as_list(db, query):
    res = db.run(query)
    res = [el for sub in ast.literal_eval(res) for el in sub if el]
    res = [re.sub(r"\b\d+\b", "", string).strip() for string in res]
    return res


proper_nouns = query_as_list(db, "SELECT Name FROM Artist")
proper_nouns += query_as_list(db, "SELECT Title FROM Album")
proper_nouns += query_as_list(db, "SELECT Name FROM Genre")
len(proper_nouns)
proper_nouns[:5]
```

```output
['AC/DC', 'Accept', 'Aerosmith', 'Alanis Morissette', 'Alice In Chains']
```

Now we can embed and store all of our values in a vector database:

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

vector_db = FAISS.from_texts(proper_nouns, OpenAIEmbeddings())
retriever = vector_db.as_retriever(search_kwargs={"k": 15})
```

**API Reference:**[FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

And put together a query construction chain that first retrieves values from the database and inserts them into the prompt:

```python
from operator import itemgetter

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

system = """You are a SQLite expert. Given an input question, create a syntactically
correct SQLite query to run. Unless otherwise specificed, do not return more than
{top_k} rows.

Only return the SQL query with no markup or explanation.

Here is the relevant table info: {table_info}

Here is a non-exhaustive list of possible feature values. If filtering on a feature
value make sure to check its spelling against this list first:

{proper_nouns}
"""

prompt = ChatPromptTemplate.from_messages([("system", system), ("human", "{input}")])

query_chain = create_sql_query_chain(llm, db, prompt=prompt)
retriever_chain = (
    itemgetter("question")
    | retriever
    | (lambda docs: "\n".join(doc.page_content for doc in docs))
)
chain = RunnablePassthrough.assign(proper_nouns=retriever_chain) | query_chain
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)

To try out our chain, let's see what happens when we try filtering on "elenis moriset", a misspelling of Alanis Morissette, without and with retrieval:

```python
# Without retrieval
query = query_chain.invoke(
    {"question": "What are all the genres of elenis moriset songs", "proper_nouns": ""}
)
print(query)
db.run(query)
```

```output
SELECT DISTINCT g.Name 
FROM Track t
JOIN Album a ON t.AlbumId = a.AlbumId
JOIN Artist ar ON a.ArtistId = ar.ArtistId
JOIN Genre g ON t.GenreId = g.GenreId
WHERE ar.Name = 'Elenis Moriset';
```

```output
''
```

```python
# With retrieval
query = chain.invoke({"question": "What are all the genres of elenis moriset songs"})
print(query)
db.run(query)
```

```output
SELECT DISTINCT g.Name
FROM Genre g
JOIN Track t ON g.GenreId = t.GenreId
JOIN Album a ON t.AlbumId = a.AlbumId
JOIN Artist ar ON a.ArtistId = ar.ArtistId
WHERE ar.Name = 'Alanis Morissette';
```

```output
"[('Rock',)]"
```

We can see that with retrieval we're able to correct the spelling from "Elenis Moriset" to "Alanis Morissette" and get back a valid result.

Another possible approach to this problem is to let an Agent decide for itself when to look up proper nouns. You can see an example of this in the [SQL: Agents](/docs/tutorials/agents/) guide.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/sql_large_db.ipynb)

* * *


- [Setup](#setup)
- [Many tables](#many-tables)
- [High-cardinality columns](#high-cardinality-columns)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/summarize_refine.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/summarize_refine.ipynb)

# How to summarize text through iterative refinement

LLMs can summarize and otherwise distill desired information from text, including large volumes of text. In many cases, especially when the amount of text is large compared to the size of the model's context window, it can be helpful (or necessary) to break up the summarization task into smaller components.

Iterative refinement represents one strategy for summarizing long texts. The strategy is as follows:

- Split a text into smaller documents;
- Summarize the first document;
- Refine or update the result based on the next document;
- Repeat through the sequence of documents until finished.

Note that this strategy is not parallelized. It is especially effective when understanding of a sub-document depends on prior context-- for instance, when summarizing a novel or body of text with an inherent sequence.

[LangGraph](https://langchain-ai.github.io/langgraph/), built on top of `langchain-core`, is well-suited to this problem:

- LangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;
- LangGraph's [checkpointing](https://langchain-ai.github.io/langgraph/how-tos/persistence/) supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.
- Because it is assembled from modular components, it is also simple to extend or modify (e.g., to incorporate [tool calling](/docs/concepts/tool_calling/) or other behavior).

Below, we demonstrate how to summarize text via iterative refinement.

## Load chat model[â€‹](#load-chat-model "Direct link to Load chat model")

Let's first load a chat model:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

## Load documents[â€‹](#load-documents "Direct link to Load documents")

Next, we need some documents to summarize. Below, we generate some toy documents for illustrative purposes. See the document loader [how-to guides](/docs/how_to/#document-loaders) and [integration pages](/docs/integrations/document_loaders/) for additional sources of data. The [summarization tutorial](/docs/tutorials/summarization/) also includes an example summarizing a blog post.

```python
from langchain_core.documents import Document

documents = [
    Document(page_content="Apples are red", metadata={"title": "apple_book"}),
    Document(page_content="Blueberries are blue", metadata={"title": "blueberry_book"}),
    Document(page_content="Bananas are yelow", metadata={"title": "banana_book"}),
]
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

## Create graph[â€‹](#create-graph "Direct link to Create graph")

Below we show a LangGraph implementation of this process:

- We generate a simple chain for the initial summary that plucks out the first document, formats it into a prompt and runs inference with our LLM.
- We generate a second `refine_summary_chain` that operates on each successive document, refining the initial summary.

We will need to install `langgraph`:

```python
pip install -qU langgraph
```

```python
import operator
from typing import List, Literal, TypedDict

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langgraph.constants import Send
from langgraph.graph import END, START, StateGraph

# Initial summary
summarize_prompt = ChatPromptTemplate(
    [
        ("human", "Write a concise summary of the following: {context}"),
    ]
)
initial_summary_chain = summarize_prompt | llm | StrOutputParser()

# Refining the summary with new docs
refine_template = """
Produce a final summary.

Existing summary up to this point:
{existing_answer}

New context:
------------
{context}
------------

Given the new context, refine the original summary.
"""
refine_prompt = ChatPromptTemplate([("human", refine_template)])

refine_summary_chain = refine_prompt | llm | StrOutputParser()


# We will define the state of the graph to hold the document
# contents and summary. We also include an index to keep track
# of our position in the sequence of documents.
class State(TypedDict):
    contents: List[str]
    index: int
    summary: str


# We define functions for each node, including a node that generates
# the initial summary:
async def generate_initial_summary(state: State, config: RunnableConfig):
    summary = await initial_summary_chain.ainvoke(
        state["contents"][0],
        config,
    )
    return {"summary": summary, "index": 1}


# And a node that refines the summary based on the next document
async def refine_summary(state: State, config: RunnableConfig):
    content = state["contents"][state["index"]]
    summary = await refine_summary_chain.ainvoke(
        {"existing_answer": state["summary"], "context": content},
        config,
    )

    return {"summary": summary, "index": state["index"] + 1}


# Here we implement logic to either exit the application or refine
# the summary.
def should_refine(state: State) -> Literal["refine_summary", END]:
    if state["index"] >= len(state["contents"]):
        return END
    else:
        return "refine_summary"


graph = StateGraph(State)
graph.add_node("generate_initial_summary", generate_initial_summary)
graph.add_node("refine_summary", refine_summary)

graph.add_edge(START, "generate_initial_summary")
graph.add_conditional_edges("generate_initial_summary", should_refine)
graph.add_conditional_edges("refine_summary", should_refine)
app = graph.compile()
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [Send](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

LangGraph allows the graph structure to be plotted to help visualize its function:

```python
from IPython.display import Image

Image(app.get_graph().draw_mermaid_png())
```

![](data:image/jpg;base64,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)

## Invoke graph[â€‹](#invoke-graph "Direct link to Invoke graph")

We can step through the execution as follows, printing out the summary as it is refined:

```python
async for step in app.astream(
    {"contents": [doc.page_content for doc in documents]},
    stream_mode="values",
):
    if summary := step.get("summary"):
        print(summary)
```

```output
Apples are characterized by their red color.
Apples are characterized by their red color, while blueberries are known for their blue hue.
Apples are characterized by their red color, blueberries are known for their blue hue, and bananas are recognized for their yellow color.
```

The final `step` contains the summary as synthesized from the entire set of documents.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

Check out the summarization [how-to guides](/docs/how_to/#summarization) for additional summarization strategies, including those designed for larger volumes of text.

See [this tutorial](/docs/tutorials/summarization/) for more detail on summarization.

See also the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) for detail on building with LangGraph.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/summarize_refine.ipynb)

* * *


- [Load chat model](#load-chat-model)
- [Load documents](#load-documents)
- [Create graph](#create-graph)
- [Invoke graph](#invoke-graph)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/debugging.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/debugging.ipynb)

# How to debug your LLM apps

Like building any type of software, at some point you'll need to debug when building with LLMs. A model call will fail, or model output will be misformatted, or there will be some nested model calls and it won't be clear where along the way an incorrect output was created.

There are three main methods for debugging:

- Verbose Mode: This adds print statements for "important" events in your chain.
- Debug Mode: This add logging statements for ALL events in your chain.
- LangSmith Tracing: This logs events to [LangSmith](https://docs.smith.langchain.com/) to allow for visualization there.

|                        | Verbose Mode | Debug Mode | LangSmith Tracing |
|------------------------|--------------|------------|-------------------|
| Free                   | âœ…            | âœ…          | âœ…                 |
| UI                     | âŒ            | âŒ          | âœ…                 |
| Persisted              | âŒ            | âŒ          | âœ…                 |
| See all events         | âŒ            | âœ…          | âœ…                 |
| See "important" events | âœ…            | âŒ          | âœ…                 |
| Runs Locally           | âœ…            | âœ…          | âŒ                 |

## Tracing[â€‹](#tracing "Direct link to Tracing")

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
```

Or, if in a notebook, you can set them with:

```python
import getpass
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

Let's suppose we have an agent, and want to visualize the actions it takes and tool outputs it receives. Without any debugging, here's what we see:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate

tools = [TavilySearchResults(max_results=1)]
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant.",
        ),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ]
)

# Construct the Tools agent
agent = create_tool_calling_agent(llm, tools, prompt)

# Create an agent executor by passing in the agent and tools
agent_executor = AgentExecutor(agent=agent, tools=tools)
agent_executor.invoke(
    {"input": "Who directed the 2023 film Oppenheimer and what is their age in days?"}
)
```

**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create\_tool\_calling\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) | [TavilySearchResults](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
{'input': 'Who directed the 2023 film Oppenheimer and what is their age in days?',
 'output': 'The 2023 film "Oppenheimer" was directed by Christopher Nolan.\n\nTo calculate Christopher Nolan\'s age in days, we first need his birthdate, which is July 30, 1970. Let\'s calculate his age in days from his birthdate to today\'s date, December 7, 2023.\n\n1. Calculate the total number of days from July 30, 1970, to December 7, 2023.\n2. Nolan was born on July 30, 1970. From July 30, 1970, to July 30, 2023, is 53 years.\n3. From July 30, 2023, to December 7, 2023, is 130 days.\n\nNow, calculate the total days:\n- 53 years = 53 x 365 = 19,345 days\n- Adding leap years from 1970 to 2023: There are 13 leap years (1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020). So, add 13 days.\n- Total days from years and leap years = 19,345 + 13 = 19,358 days\n- Add the days from July 30, 2023, to December 7, 2023 = 130 days\n\nTotal age in days = 19,358 + 130 = 19,488 days\n\nChristopher Nolan is 19,488 days old as of December 7, 2023.'}
```

We don't get much output, but since we set up LangSmith we can easily see what happened under the hood:

[https://smith.langchain.com/public/a89ff88f-9ddc-4757-a395-3a1b365655bf/r](https://smith.langchain.com/public/a89ff88f-9ddc-4757-a395-3a1b365655bf/r)

## `set_debug` and `set_verbose`[â€‹](#set_debug-and-set_verbose "Direct link to set_debug-and-set_verbose")

If you're prototyping in Jupyter Notebooks or running Python scripts, it can be helpful to print out the intermediate steps of a chain run.

There are a number of ways to enable printing at varying degrees of verbosity.

Note: These still work even with LangSmith enabled, so you can have both turned on and running at the same time

### `set_verbose(True)`[â€‹](#set_verbosetrue "Direct link to set_verbosetrue")

Setting the `verbose` flag will print out inputs and outputs in a slightly more readable format and will skip logging certain raw outputs (like the token usage stats for an LLM call) so that you can focus on application logic.

```python
from langchain.globals import set_verbose

set_verbose(True)
agent_executor = AgentExecutor(agent=agent, tools=tools)
agent_executor.invoke(
    {"input": "Who directed the 2023 film Oppenheimer and what is their age in days?"}
)
```

**API Reference:**[set\_verbose](https://python.langchain.com/api_reference/langchain/globals/langchain.globals.set_verbose.html)

```output


[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `tavily_search_results_json` with `{'query': 'director of the 2023 film Oppenheimer'}`


[0m[36;1m[1;3m[{'url': 'https://m.imdb.com/title/tt15398776/', 'content': 'Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.'}][0m[32;1m[1;3m
Invoking: `tavily_search_results_json` with `{'query': 'birth date of Christopher Nolan'}`


[0m[36;1m[1;3m[{'url': 'https://m.imdb.com/name/nm0634240/bio/', 'content': 'Christopher Nolan. Writer: Tenet. Best known for his cerebral, often nonlinear, storytelling, acclaimed Academy Award winner writer/director/producer Sir Christopher Nolan CBE was born in London, England. Over the course of more than 25 years of filmmaking, Nolan has gone from low-budget independent films to working on some of the biggest blockbusters ever made and became one of the most ...'}][0m[32;1m[1;3m
Invoking: `tavily_search_results_json` with `{'query': 'Christopher Nolan birth date'}`
responded: The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.

To calculate Christopher Nolan's age in days, I need his exact birth date. Let me find that information for you.

[0m[36;1m[1;3m[{'url': 'https://m.imdb.com/name/nm0634240/bio/', 'content': 'Christopher Nolan. Writer: Tenet. Best known for his cerebral, often nonlinear, storytelling, acclaimed Academy Award winner writer/director/producer Sir Christopher Nolan CBE was born in London, England. Over the course of more than 25 years of filmmaking, Nolan has gone from low-budget independent films to working on some of the biggest blockbusters ever made and became one of the most ...'}][0m[32;1m[1;3m
Invoking: `tavily_search_results_json` with `{'query': 'Christopher Nolan date of birth'}`
responded: It appears that I need to refine my search to get the exact birth date of Christopher Nolan. Let me try again to find that specific information.

[0m[36;1m[1;3m[{'url': 'https://m.imdb.com/name/nm0634240/bio/', 'content': 'Christopher Nolan. Writer: Tenet. Best known for his cerebral, often nonlinear, storytelling, acclaimed Academy Award winner writer/director/producer Sir Christopher Nolan CBE was born in London, England. Over the course of more than 25 years of filmmaking, Nolan has gone from low-budget independent films to working on some of the biggest blockbusters ever made and became one of the most ...'}][0m[32;1m[1;3mI am currently unable to retrieve the exact birth date of Christopher Nolan from the sources available. However, it is widely known that he was born on July 30, 1970. Using this date, I can calculate his age in days as of today.

Let's calculate:

- Christopher Nolan's birth date: July 30, 1970.
- Today's date: December 7, 2023.

The number of days between these two dates can be calculated as follows:

1. From July 30, 1970, to July 30, 2023, is 53 years.
2. From July 30, 2023, to December 7, 2023, is 130 days.

Calculating the total days for 53 years (considering leap years):
- 53 years Ã— 365 days/year = 19,345 days
- Adding leap years (1972, 1976, ..., 2020, 2024 - 13 leap years): 13 days

Total days from birth until July 30, 2023: 19,345 + 13 = 19,358 days
Adding the days from July 30, 2023, to December 7, 2023: 130 days

Total age in days as of December 7, 2023: 19,358 + 130 = 19,488 days.

Therefore, Christopher Nolan is 19,488 days old as of December 7, 2023.[0m

[1m> Finished chain.[0m
```

```output
{'input': 'Who directed the 2023 film Oppenheimer and what is their age in days?',
 'output': "I am currently unable to retrieve the exact birth date of Christopher Nolan from the sources available. However, it is widely known that he was born on July 30, 1970. Using this date, I can calculate his age in days as of today.\n\nLet's calculate:\n\n- Christopher Nolan's birth date: July 30, 1970.\n- Today's date: December 7, 2023.\n\nThe number of days between these two dates can be calculated as follows:\n\n1. From July 30, 1970, to July 30, 2023, is 53 years.\n2. From July 30, 2023, to December 7, 2023, is 130 days.\n\nCalculating the total days for 53 years (considering leap years):\n- 53 years Ã— 365 days/year = 19,345 days\n- Adding leap years (1972, 1976, ..., 2020, 2024 - 13 leap years): 13 days\n\nTotal days from birth until July 30, 2023: 19,345 + 13 = 19,358 days\nAdding the days from July 30, 2023, to December 7, 2023: 130 days\n\nTotal age in days as of December 7, 2023: 19,358 + 130 = 19,488 days.\n\nTherefore, Christopher Nolan is 19,488 days old as of December 7, 2023."}
```

### `set_debug(True)`[â€‹](#set_debugtrue "Direct link to set_debugtrue")

Setting the global `debug` flag will cause all LangChain components with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate. This is the most verbose setting and will fully log raw inputs and outputs.

```python
from langchain.globals import set_debug

set_debug(True)
set_verbose(False)
agent_executor = AgentExecutor(agent=agent, tools=tools)

agent_executor.invoke(
    {"input": "Who directed the 2023 film Oppenheimer and what is their age in days?"}
)
```

**API Reference:**[set\_debug](https://python.langchain.com/api_reference/langchain/globals/langchain.globals.set_debug.html)

```````output
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor] Entering Chain run with input:
[0m{
  "input": "Who directed the 2023 film Oppenheimer and what is their age in days?"
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence] Entering Chain run with input:
[0m{
  "input": ""
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:
[0m{
  "input": ""
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:
[0m{
  "input": ""
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad> > 5:chain:RunnableLambda] Entering Chain run with input:
[0m{
  "input": ""
}
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad> > 5:chain:RunnableLambda] [1ms] Exiting Chain run with output:
[0m{
  "output": []
}
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad>] [2ms] Exiting Chain run with output:
[0m{
  "agent_scratchpad": []
}
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad>] [5ms] Exiting Chain run with output:
[0m{
  "input": "Who directed the 2023 film Oppenheimer and what is their age in days?",
  "intermediate_steps": [],
  "agent_scratchpad": []
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] Entering Prompt run with input:
[0m{
  "input": "Who directed the 2023 film Oppenheimer and what is their age in days?",
  "intermediate_steps": [],
  "agent_scratchpad": []
}
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:
[0m[outputs]
[32;1m[1;3m[llm/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 7:llm:ChatOpenAI] Entering LLM run with input:
[0m{
  "prompts": [
    "System: You are a helpful assistant.\nHuman: Who directed the 2023 film Oppenheimer and what is their age in days?"
  ]
}
[36;1m[1;3m[llm/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 7:llm:ChatOpenAI] [3.17s] Exiting LLM run with output:
[0m{
  "generations": [
    [
      {
        "text": "",
        "generation_info": {
          "finish_reason": "tool_calls"
        },
        "type": "ChatGenerationChunk",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "schema",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": "",
            "example": false,
            "additional_kwargs": {
              "tool_calls": [
                {
                  "index": 0,
                  "id": "call_fnfq6GjSQED4iF6lo4rxkUup",
                  "function": {
                    "arguments": "{\"query\": \"director of the 2023 film Oppenheimer\"}",
                    "name": "tavily_search_results_json"
                  },
                  "type": "function"
                },
                {
                  "index": 1,
                  "id": "call_mwhVi6pk49f4OIo5rOWrr4TD",
                  "function": {
                    "arguments": "{\"query\": \"birth date of Christopher Nolan\"}",
                    "name": "tavily_search_results_json"
                  },
                  "type": "function"
                }
              ]
            },
            "tool_call_chunks": [
              {
                "name": "tavily_search_results_json",
                "args": "{\"query\": \"director of the 2023 film Oppenheimer\"}",
                "id": "call_fnfq6GjSQED4iF6lo4rxkUup",
                "index": 0
              },
              {
                "name": "tavily_search_results_json",
                "args": "{\"query\": \"birth date of Christopher Nolan\"}",
                "id": "call_mwhVi6pk49f4OIo5rOWrr4TD",
                "index": 1
              }
            ],
            "response_metadata": {
              "finish_reason": "tool_calls"
            },
            "id": "run-6e160323-15f9-491d-aadf-b5d337e9e2a1",
            "tool_calls": [
              {
                "name": "tavily_search_results_json",
                "args": {
                  "query": "director of the 2023 film Oppenheimer"
                },
                "id": "call_fnfq6GjSQED4iF6lo4rxkUup"
              },
              {
                "name": "tavily_search_results_json",
                "args": {
                  "query": "birth date of Christopher Nolan"
                },
                "id": "call_mwhVi6pk49f4OIo5rOWrr4TD"
              }
            ],
            "invalid_tool_calls": []
          }
        }
      }
    ]
  ],
  "llm_output": null,
  "run": null
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 8:parser:ToolsAgentOutputParser] Entering Parser run with input:
[0m[inputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 8:parser:ToolsAgentOutputParser] [1ms] Exiting Parser run with output:
[0m[outputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence] [3.18s] Exiting Chain run with output:
[0m[outputs]
[32;1m[1;3m[tool/start][0m [1m[1:chain:AgentExecutor > 9:tool:tavily_search_results_json] Entering Tool run with input:
[0m"{'query': 'director of the 2023 film Oppenheimer'}"
``````output
Error in ConsoleCallbackHandler.on_tool_end callback: AttributeError("'list' object has no attribute 'strip'")
``````output
[32;1m[1;3m[tool/start][0m [1m[1:chain:AgentExecutor > 10:tool:tavily_search_results_json] Entering Tool run with input:
[0m"{'query': 'birth date of Christopher Nolan'}"
``````output
Error in ConsoleCallbackHandler.on_tool_end callback: AttributeError("'list' object has no attribute 'strip'")
``````output
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence] Entering Chain run with input:
[0m{
  "input": ""
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 12:chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:
[0m{
  "input": ""
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 12:chain:RunnableAssign<agent_scratchpad> > 13:chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:
[0m{
  "input": ""
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 12:chain:RunnableAssign<agent_scratchpad> > 13:chain:RunnableParallel<agent_scratchpad> > 14:chain:RunnableLambda] Entering Chain run with input:
[0m{
  "input": ""
}
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 12:chain:RunnableAssign<agent_scratchpad> > 13:chain:RunnableParallel<agent_scratchpad> > 14:chain:RunnableLambda] [1ms] Exiting Chain run with output:
[0m[outputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 12:chain:RunnableAssign<agent_scratchpad> > 13:chain:RunnableParallel<agent_scratchpad>] [4ms] Exiting Chain run with output:
[0m[outputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 12:chain:RunnableAssign<agent_scratchpad>] [8ms] Exiting Chain run with output:
[0m[outputs]
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 15:prompt:ChatPromptTemplate] Entering Prompt run with input:
[0m[inputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 15:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:
[0m[outputs]
[32;1m[1;3m[llm/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 16:llm:ChatOpenAI] Entering LLM run with input:
[0m{
  "prompts": [
    "System: You are a helpful assistant.\nHuman: Who directed the 2023 film Oppenheimer and what is their age in days?\nAI: \nTool: [{\"url\": \"https://m.imdb.com/title/tt15398776/fullcredits/\", \"content\": \"Oppenheimer (2023) cast and crew credits, including actors, actresses, directors, writers and more. Menu. ... director of photography: behind-the-scenes Jason Gary ... best boy grip ... film loader Luc Poullain ... aerial coordinator\"}]\nTool: [{\"url\": \"https://en.wikipedia.org/wiki/Christopher_Nolan\", \"content\": \"In early 2003, Nolan approached Warner Bros. with the idea of making a new Batman film, based on the character's origin story.[58] Nolan was fascinated by the notion of grounding it in a more realistic world than a comic-book fantasy.[59] He relied heavily on traditional stunts and miniature effects during filming, with minimal use of computer-generated imagery (CGI).[60] Batman Begins (2005), the biggest project Nolan had undertaken to that point,[61] was released to critical acclaim and commercial success.[62][63] Starring Christian Bale as Bruce Wayne / Batmanâ€”along with Michael Caine, Gary Oldman, Morgan Freeman and Liam Neesonâ€”Batman Begins revived the franchise.[64][65] Batman Begins was 2005's ninth-highest-grossing film and was praised for its psychological depth and contemporary relevance;[63][66] it is cited as one of the most influential films of the 2000s.[67] Film author Ian Nathan wrote that within five years of his career, Nolan \\\"[went] from unknown to indie darling to gaining creative control over one of the biggest properties in Hollywood, and (perhaps unwittingly) fomenting the genre that would redefine the entire industry\\\".[68]\\nNolan directed, co-wrote and produced The Prestige (2006), an adaptation of the Christopher Priest novel about two rival 19th-century magicians.[69] He directed, wrote and edited the short film Larceny (1996),[19] which was filmed over a weekend in black and white with limited equipment and a small cast and crew.[12][20] Funded by Nolan and shot with the UCL Union Film society's equipment, it appeared at the Cambridge Film Festival in 1996 and is considered one of UCL's best shorts.[21] For unknown reasons, the film has since been removed from public view.[19] Nolan filmed a third short, Doodlebug (1997), about a man seemingly chasing an insect with his shoe, only to discover that it is a miniature of himself.[14][22] Nolan and Thomas first attempted to make a feature in the mid-1990s with Larry Mahoney, which they scrapped.[23] During this period in his career, Nolan had little to no success getting his projects off the ground, facing several rejections; he added, \\\"[T]here's a very limited pool of finance in the UK. Philosophy professor David Kyle Johnson wrote that \\\"Inception became a classic almost as soon as it was projected on silver screens\\\", praising its exploration of philosophical ideas, including leap of faith and allegory of the cave.[97] The film grossed over $836Â million worldwide.[98] Nominated for eight Academy Awardsâ€”including Best Picture and Best Original Screenplayâ€”it won Best Cinematography, Best Sound Mixing, Best Sound Editing and Best Visual Effects.[99] Nolan was nominated for a BAFTA Award and a Golden Globe Award for Best Director, among other accolades.[40]\\nAround the release of The Dark Knight Rises (2012), Nolan's third and final Batman film, Joseph Bevan of the British Film Institute wrote a profile on him: \\\"In the space of just over a decade, Christopher Nolan has shot from promising British indie director to undisputed master of a new brand of intelligent escapism. He further wrote that Nolan's body of work reflect \\\"a heterogeneity of conditions of products\\\" extending from low-budget films to lucrative blockbusters, \\\"a wide range of genres and settings\\\" and \\\"a diversity of styles that trumpet his versatility\\\".[193]\\nDavid Bordwell, a film theorist, wrote that Nolan has been able to blend his \\\"experimental impulses\\\" with the demands of mainstream entertainment, describing his oeuvre as \\\"experiments with cinematic time by means of techniques of subjective viewpoint and crosscutting\\\".[194] Nolan's use of practical, in-camera effects, miniatures and models, as well as shooting on celluloid film, has been highly influential in early 21st century cinema.[195][196] IndieWire wrote in 2019 that, Nolan \\\"kept a viable alternate model of big-budget filmmaking alive\\\", in an era where blockbuster filmmaking has become \\\"a largely computer-generated art form\\\".[196] Initially reluctant to make a sequel, he agreed after Warner Bros. repeatedly insisted.[78] Nolan wanted to expand on the noir quality of the first film by broadening the canvas and taking on \\\"the dynamic of a story of the city, a large crime storyÂ ... where you're looking at the police, the justice system, the vigilante, the poor people, the rich people, the criminals\\\".[79] Continuing to minimalise the use of CGI, Nolan employed high-resolution IMAX cameras, making it the first major motion picture to use this technology.[80][81]\"}]"
  ]
}
[36;1m[1;3m[llm/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 16:llm:ChatOpenAI] [20.22s] Exiting LLM run with output:
[0m{
  "generations": [
    [
      {
        "text": "The 2023 film \"Oppenheimer\" was directed by Christopher Nolan.\n\nTo calculate Christopher Nolan's age in days, we first need his birth date, which is July 30, 1970. Let's calculate his age in days from his birth date to today's date, December 7, 2023.\n\n1. Calculate the total number of days from July 30, 1970, to December 7, 2023.\n2. Christopher Nolan was born on July 30, 1970. From July 30, 1970, to July 30, 2023, is 53 years.\n3. From July 30, 2023, to December 7, 2023, is 130 days.\n\nNow, calculate the total days for 53 years:\n- Each year has 365 days, so 53 years Ã— 365 days/year = 19,345 days.\n- Adding the leap years from 1970 to 2023: 1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020, and 2024 (up to February). This gives us 14 leap years.\n- Total days from leap years: 14 days.\n\nAdding all together:\n- Total days = 19,345 days (from years) + 14 days (from leap years) + 130 days (from July 30, 2023, to December 7, 2023) = 19,489 days.\n\nTherefore, as of December 7, 2023, Christopher Nolan is 19,489 days old.",
        "generation_info": {
          "finish_reason": "stop"
        },
        "type": "ChatGenerationChunk",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "schema",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": "The 2023 film \"Oppenheimer\" was directed by Christopher Nolan.\n\nTo calculate Christopher Nolan's age in days, we first need his birth date, which is July 30, 1970. Let's calculate his age in days from his birth date to today's date, December 7, 2023.\n\n1. Calculate the total number of days from July 30, 1970, to December 7, 2023.\n2. Christopher Nolan was born on July 30, 1970. From July 30, 1970, to July 30, 2023, is 53 years.\n3. From July 30, 2023, to December 7, 2023, is 130 days.\n\nNow, calculate the total days for 53 years:\n- Each year has 365 days, so 53 years Ã— 365 days/year = 19,345 days.\n- Adding the leap years from 1970 to 2023: 1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020, and 2024 (up to February). This gives us 14 leap years.\n- Total days from leap years: 14 days.\n\nAdding all together:\n- Total days = 19,345 days (from years) + 14 days (from leap years) + 130 days (from July 30, 2023, to December 7, 2023) = 19,489 days.\n\nTherefore, as of December 7, 2023, Christopher Nolan is 19,489 days old.",
            "example": false,
            "additional_kwargs": {},
            "tool_call_chunks": [],
            "response_metadata": {
              "finish_reason": "stop"
            },
            "id": "run-1c08a44f-db70-4836-935b-417caaf422a5",
            "tool_calls": [],
            "invalid_tool_calls": []
          }
        }
      }
    ]
  ],
  "llm_output": null,
  "run": null
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 17:parser:ToolsAgentOutputParser] Entering Parser run with input:
[0m[inputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 17:parser:ToolsAgentOutputParser] [2ms] Exiting Parser run with output:
[0m[outputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence] [20.27s] Exiting Chain run with output:
[0m[outputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor] [26.37s] Exiting Chain run with output:
[0m{
  "output": "The 2023 film \"Oppenheimer\" was directed by Christopher Nolan.\n\nTo calculate Christopher Nolan's age in days, we first need his birth date, which is July 30, 1970. Let's calculate his age in days from his birth date to today's date, December 7, 2023.\n\n1. Calculate the total number of days from July 30, 1970, to December 7, 2023.\n2. Christopher Nolan was born on July 30, 1970. From July 30, 1970, to July 30, 2023, is 53 years.\n3. From July 30, 2023, to December 7, 2023, is 130 days.\n\nNow, calculate the total days for 53 years:\n- Each year has 365 days, so 53 years Ã— 365 days/year = 19,345 days.\n- Adding the leap years from 1970 to 2023: 1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020, and 2024 (up to February). This gives us 14 leap years.\n- Total days from leap years: 14 days.\n\nAdding all together:\n- Total days = 19,345 days (from years) + 14 days (from leap years) + 130 days (from July 30, 2023, to December 7, 2023) = 19,489 days.\n\nTherefore, as of December 7, 2023, Christopher Nolan is 19,489 days old."
}
```````

```output
{'input': 'Who directed the 2023 film Oppenheimer and what is their age in days?',
 'output': 'The 2023 film "Oppenheimer" was directed by Christopher Nolan.\n\nTo calculate Christopher Nolan\'s age in days, we first need his birth date, which is July 30, 1970. Let\'s calculate his age in days from his birth date to today\'s date, December 7, 2023.\n\n1. Calculate the total number of days from July 30, 1970, to December 7, 2023.\n2. Christopher Nolan was born on July 30, 1970. From July 30, 1970, to July 30, 2023, is 53 years.\n3. From July 30, 2023, to December 7, 2023, is 130 days.\n\nNow, calculate the total days for 53 years:\n- Each year has 365 days, so 53 years Ã— 365 days/year = 19,345 days.\n- Adding the leap years from 1970 to 2023: 1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020, and 2024 (up to February). This gives us 14 leap years.\n- Total days from leap years: 14 days.\n\nAdding all together:\n- Total days = 19,345 days (from years) + 14 days (from leap years) + 130 days (from July 30, 2023, to December 7, 2023) = 19,489 days.\n\nTherefore, as of December 7, 2023, Christopher Nolan is 19,489 days old.'}
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/debugging.ipynb)

* * *


- [Tracing](#tracing)
- [`set_debug` and `set_verbose`](#set_debug-and-set_verbose)
  
  - [`set_verbose(True)`](#set_verbosetrue)
  - [`set_debug(True)`](#set_debugtrue)









# How to pass runtime secrets to runnables

Requires `langchain-core >= 0.2.22`

We can pass in secrets to our [runnables](/docs/concepts/runnables/) at runtime using the `RunnableConfig`. Specifically we can pass in secrets with a `__` prefix to the `configurable` field. This will ensure that these secrets aren't traced as part of the invocation:

```python
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool


@tool
def foo(x: int, config: RunnableConfig) -> int:
    """Sum x and a secret int"""
    return x + config["configurable"]["__top_secret_int"]


foo.invoke({"x": 5}, {"configurable": {"__top_secret_int": 2, "traced_key": "bar"}})
```

**API Reference:**[RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

```output
7
```

Looking at the LangSmith trace for this run, we can see that "traced\_key" was recorded (as part of Metadata) while our secret int was not: [https://smith.langchain.com/public/aa7e3289-49ca-422d-a408-f6b927210170/r](https://smith.langchain.com/public/aa7e3289-49ca-422d-a408-f6b927210170/r)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/runnable_runtime_secrets.ipynb)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/stuff_docs_chain.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/stuff_docs_chain.ipynb)

# Migrating from StuffDocumentsChain

[StuffDocumentsChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html) combines documents by concatenating them into a single context window. It is a straightforward and effective strategy for combining documents for question-answering, summarization, and other purposes.

[create\_stuff\_documents\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html) is the recommended alternative. It functions the same as `StuffDocumentsChain`, with better support for streaming and batch functionality. Because it is a simple combination of [LCEL primitives](/docs/concepts/lcel/), it is also easier to extend and incorporate into other LangChain applications.

Below we will go through both `StuffDocumentsChain` and `create_stuff_documents_chain` on a simple example for illustrative purposes.

Let's first load a chat model:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

## Example[â€‹](#example "Direct link to Example")

Let's go through an example where we analyze a set of documents. We first generate some simple documents for illustrative purposes:

```python
from langchain_core.documents import Document

documents = [
    Document(page_content="Apples are red", metadata={"title": "apple_book"}),
    Document(page_content="Blueberries are blue", metadata={"title": "blueberry_book"}),
    Document(page_content="Bananas are yelow", metadata={"title": "banana_book"}),
]
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

### Legacy[â€‹](#legacy "Direct link to Legacy")

Details

Below we show an implementation with `StuffDocumentsChain`. We define the prompt template for a summarization task and instantiate a [LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) object for this purpose. We define how documents are formatted into the prompt and ensure consistency among the keys in the various prompts.

```python
from langchain.chains import LLMChain, StuffDocumentsChain
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate

# This controls how each document will be formatted. Specifically,
# it will be passed to `format_document` - see that function for more
# details.
document_prompt = PromptTemplate(
    input_variables=["page_content"], template="{page_content}"
)
document_variable_name = "context"
# The prompt here should take as an input variable the
# `document_variable_name`
prompt = ChatPromptTemplate.from_template("Summarize this content: {context}")

llm_chain = LLMChain(llm=llm, prompt=prompt)
chain = StuffDocumentsChain(
    llm_chain=llm_chain,
    document_prompt=document_prompt,
    document_variable_name=document_variable_name,
)
```

**API Reference:**[LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [StuffDocumentsChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

We can now invoke our chain:

```python
result = chain.invoke(documents)
result["output_text"]
```

```output
'This content describes the colors of different fruits: apples are red, blueberries are blue, and bananas are yellow.'
```

```python
for chunk in chain.stream(documents):
    print(chunk)
```

```output
{'input_documents': [Document(metadata={'title': 'apple_book'}, page_content='Apples are red'), Document(metadata={'title': 'blueberry_book'}, page_content='Blueberries are blue'), Document(metadata={'title': 'banana_book'}, page_content='Bananas are yelow')], 'output_text': 'This content describes the colors of different fruits: apples are red, blueberries are blue, and bananas are yellow.'}
```

### LCEL[â€‹](#lcel "Direct link to LCEL")

Details

Below we show an implementation using `create_stuff_documents_chain`:

```python
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("Summarize this content: {context}")
chain = create_stuff_documents_chain(llm, prompt)
```

**API Reference:**[create\_stuff\_documents\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

Invoking the chain, we obtain a similar result as before:

```python
result = chain.invoke({"context": documents})
result
```

```output
'This content describes the colors of different fruits: apples are red, blueberries are blue, and bananas are yellow.'
```

Note that this implementation supports streaming of output tokens:

```python
for chunk in chain.stream({"context": documents}):
    print(chunk, end=" | ")
```

```output
 | This |  content |  describes |  the |  colors |  of |  different |  fruits | : |  apples |  are |  red | , |  blue | berries |  are |  blue | , |  and |  bananas |  are |  yellow | . |  |
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

Check out the [LCEL conceptual docs](/docs/concepts/lcel/) for more background information.

See these [how-to guides](/docs/how_to/#qa-with-rag) for more on question-answering tasks with RAG.

See [this tutorial](/docs/tutorials/summarization/) for more LLM-based summarization strategies.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_chains/stuff_docs_chain.ipynb)

* * *


- [Example](#example)
  
  - [Legacy](#legacy)
  - [LCEL](#lcel)
- [Next steps](#next-steps)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/document_loaders.mdx)

# Document loaders

Prerequisites

- [Document loaders API reference](/docs/how_to/#document-loaders)

Document loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.

## Integrations[â€‹](#integrations "Direct link to Integrations")

You can find available integrations on the [Document loaders integrations page](/docs/integrations/document_loaders/).

## Interface[â€‹](#interface "Direct link to Interface")

Documents loaders implement the [BaseLoader interface](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html).

Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the `.load` method or `.lazy_load`.

Here's a simple example:

```python
from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(
    ...  # <-- Integration specific parameters here
)
data = loader.load()
```

**API Reference:**[CSVLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.csv_loader.CSVLoader.html)

When working with large datasets, you can use the `.lazy_load` method:

```python
for document in loader.lazy_load():
    print(document)
```

## Related resources[â€‹](#related-resources "Direct link to Related resources")

Please see the following resources for more information:

- [How-to guides for document loaders](/docs/how_to/#document-loaders)
- [Document API reference](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)
- [Document loaders integrations](/docs/integrations/document_loaders/)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/document_loaders.mdx)

* * *


- [Integrations](#integrations)
- [Interface](#interface)
- [Related resources](#related-resources)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/retrieval.mdx)

# Retrieval

Prerequisites

- [Retrievers](/docs/concepts/retrievers/)
- [Vector stores](/docs/concepts/vectorstores/)
- [Embeddings](/docs/concepts/embedding_models/)
- [Text splitters](/docs/concepts/text_splitters/)

Security

Some of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases). There are inherent risks in doing this. Make sure that your database connection permissions are scoped as narrowly as possible for your application's needs. This will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases. For more on general security best practices, see our [security guide](/docs/security/).

## Overview[â€‹](#overview "Direct link to Overview")

Retrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets. These systems accommodate various data formats:

- Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.
- Structured data is typically housed in relational or graph databases with defined schemas.

Despite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces. Models play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database. This translation enables more intuitive and flexible interactions with complex data structures.

## Key concepts[â€‹](#key-concepts "Direct link to Key concepts")

![Retrieval](/assets/images/retrieval_concept-2bcff1b2518f194b34eaf472ac748ffa.png)

(1) **Query analysis**: A process where models transform or construct search queries to optimize retrieval.

(2) **Information retrieval**: Search queries are used to fetch information from various retrieval systems.

## Query analysis[â€‹](#query-analysis "Direct link to Query analysis")

While users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords. Query analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:

1. **Query Re-writing**: Queries can be re-written or expanded to improve semantic or lexical searches.
2. **Query Construction**: Search indexes may require structured queries (e.g., SQL for databases).

Query analysis employs models to transform or construct optimized search queries from raw user input.

### Query re-writing[â€‹](#query-re-writing "Direct link to Query re-writing")

Retrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions. To achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries. This transformation can range from simple keyword extraction to sophisticated query expansion and reformulation. Here are some key benefits of using models for query analysis in unstructured data retrieval:

1. **Query Clarification**: Models can rephrase ambiguous or poorly worded queries for clarity.
2. **Semantic Understanding**: They can capture the intent behind a query, going beyond literal keyword matching.
3. **Query Expansion**: Models can generate related terms or concepts to broaden the search scope.
4. **Complex Query Handling**: They can break down multi-part questions into simpler sub-queries.

Various techniques have been developed to leverage models for query re-writing, including:

| Name                                                                                                      | When to use                                                                                     | Description                                                                                                                                                                                                                                                                            |
|-----------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Multi-query](/docs/how_to/MultiQueryRetriever/)                                                          | When you want to ensure high recall in retrieval by providing multiple phrasings of a question. | Rewrite the user question with multiple phrasings, retrieve documents for each rewritten question, return the unique documents for all queries.                                                                                                                                        |
| [Decomposition](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb) | When a question can be broken down into smaller subproblems.                                    | Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).                                                           |
| [Step-back](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)     | When a higher-level conceptual understanding is required.                                       | First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question. [Paper](https://arxiv.org/pdf/2310.06117).                                            |
| [HyDE](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)          | If you have challenges retrieving relevant documents using the raw user inputs.                 | Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches. [Paper](https://arxiv.org/abs/2212.10496). |

As an example, query decomposition can simply be accomplished using prompting and a structured output that enforces a list of sub-questions. These can then be run sequentially or in parallel on a downstream retrieval system.

```python
from typing import List

from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# Define a pydantic model to enforce the output structure
class Questions(BaseModel):
    questions: List[str] = Field(
        description="A list of sub-questions related to the input query."
    )

# Create an instance of the model and enforce the output structure
model = ChatOpenAI(model="gpt-4o", temperature=0) 
structured_model = model.with_structured_output(Questions)

# Define the system prompt
system = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
The goal is to break down the input into a set of sub-problems / sub-questions that can be answered independently. \n"""

# Pass the question to the model
question = """What are the main components of an LLM-powered autonomous agent system?"""
questions = structured_model.invoke([SystemMessage(content=system)]+[HumanMessage(content=question)])
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

tip

See our RAG from Scratch videos for a few different specific approaches:

- [Multi-query](https://youtu.be/JChPi0CRnDY?feature=shared)
- [Decomposition](https://youtu.be/h0OPWlEOank?feature=shared)
- [Step-back](https://youtu.be/xn1jEjRyJ2U?feature=shared)
- [HyDE](https://youtu.be/SaDzIVkYqyY?feature=shared)

### Query construction[â€‹](#query-construction "Direct link to Query construction")

Query analysis also can focus on translating natural language queries into specialized query languages or filters. This translation is crucial for effectively interacting with various types of databases that house structured or semi-structured data.

1. **Structured Data examples**: For relational and graph databases, Domain-Specific Languages (DSLs) are used to query data.
   
   - **Text-to-SQL**: [Converts natural language to SQL](https://paperswithcode.com/task/text-to-sql) for relational databases.
   - **Text-to-Cypher**: [Converts natural language to Cypher](https://neo4j.com/labs/neodash/2.4/user-guide/extensions/natural-language-queries/) for graph databases.
2. **Semi-structured Data examples**: For vectorstores, queries can combine semantic search with metadata filtering.
   
   - **Natural Language to Metadata Filters**: Converts user queries into [appropriate metadata filters](https://docs.pinecone.io/guides/data/filter-with-metadata).

These approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:

| Name                                     | When to Use                                                                                                                          | Description                                                                                                                                                                                                                                          |
|------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Self Query](/docs/how_to/self_query/)   | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself). |
| [Text to SQL](/docs/tutorials/sql_qa/)   | If users are asking questions that require information housed in a relational database, accessible via SQL.                          | This uses an LLM to transform user input into a SQL query.                                                                                                                                                                                           |
| [Text-to-Cypher](/docs/tutorials/graph/) | If users are asking questions that require information housed in a graph database, accessible via Cypher.                            | This uses an LLM to transform user input into a Cypher query.                                                                                                                                                                                        |

As an example, here is how to use the `SelfQueryRetriever` to convert natural language queries into metadata filters.

```python
metadata_field_info = schema_for_metadata 
document_content_description = "Brief summary of a movie"
llm = ChatOpenAI(temperature=0)
retriever = SelfQueryRetriever.from_llm(
    llm,
    vectorstore,
    document_content_description,
    metadata_field_info,
)
```

Further reading

- See our tutorials on [text-to-SQL](/docs/tutorials/sql_qa/), [text-to-Cypher](/docs/tutorials/graph/), and [query analysis for metadata filters](/docs/tutorials/rag/#query-analysis).
- See our [blog post overview](https://blog.langchain.dev/query-construction/).
- See our RAG from Scratch video on [query construction](https://youtu.be/kl6NwWYxvbM?feature=shared).

## Information retrieval[â€‹](#information-retrieval "Direct link to Information retrieval")

### Common retrieval systems[â€‹](#common-retrieval-systems "Direct link to Common retrieval systems")

#### Lexical search indexes[â€‹](#lexical-search-indexes "Direct link to Lexical search indexes")

Many search engines are based upon matching words in a query to the words in each document. This approach is called lexical retrieval, using search [algorithms that are typically based upon word frequencies](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2). The intution is simple: a word appears frequently both in the userâ€™s query and a particular document, then this document might be a good match.

The particular data structure used to implement this is often an [*inverted index*](https://www.geeksforgeeks.org/inverted-index/). This types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents. Using this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear. [BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).

Further reading

- See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
- See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.

#### Vector indexes[â€‹](#vector-indexes "Direct link to Vector indexes")

Vector indexes are an alternative way to index and store unstructured data. See our conceptual guide on [vectorstores](/docs/concepts/vectorstores/) for a detailed overview.  
In short, rather than using word frequencies, vectorstores use an [embedding model](/docs/concepts/embedding_models/) to compress documents into high-dimensional vector representation. This allows for efficient similarity search over embedding vectors using simple mathematical operations like cosine similarity.

Further reading

- See our [how-to guide](/docs/how_to/vectorstore_retriever/) for more details on working with vectorstores.
- See our [list of vectorstore integrations](/docs/integrations/vectorstores/).
- See Cameron Wolfe's [blog post](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2) on the basics of vector search.

#### Relational databases[â€‹](#relational-databases "Direct link to Relational databases")

Relational databases are a fundamental type of structured data storage used in many applications. They organize data into tables with predefined schemas, where each table represents an entity or relationship. Data is stored in rows (records) and columns (attributes), allowing for efficient querying and manipulation through SQL (Structured Query Language). Relational databases excel at maintaining data integrity, supporting complex queries, and handling relationships between different data entities.

Further reading

- See our [tutorial](/docs/tutorials/sql_qa/) for working with SQL databases.
- See our [SQL database toolkit](/docs/integrations/tools/sql_database/).

#### Graph databases[â€‹](#graph-databases "Direct link to Graph databases")

Graph databases are a specialized type of database designed to store and manage highly interconnected data. Unlike traditional relational databases, graph databases use a flexible structure consisting of nodes (entities), edges (relationships), and properties. This structure allows for efficient representation and querying of complex, interconnected data. Graph databases store data in a graph structure, with nodes, edges, and properties. They are particularly useful for storing and querying complex relationships between data points, such as social networks, supply-chain management, fraud detection, and recommendation services

Further reading

- See our [tutorial](/docs/tutorials/graph/) for working with graph databases.
- See our [list of graph database integrations](/docs/integrations/graphs/).
- See Neo4j's [starter kit for LangChain](https://neo4j.com/developer-blog/langchain-neo4j-starter-kit/).

### Retriever[â€‹](#retriever "Direct link to Retriever")

LangChain provides a unified interface for interacting with various retrieval systems through the [retriever](/docs/concepts/retrievers/) concept. The interface is straightforward:

1. Input: A query (string)
2. Output: A list of documents (standardized LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects)

You can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for databases that typically require structured query languages. For example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) to be transformed into a SQL query behind the scenes. Regardless of the underlying retrieval system, all retrievers in LangChain share a common interface. You can use them with the simple `invoke` method:

```python
docs = retriever.invoke(query)
```

Further reading

- See our [conceptual guide on retrievers](/docs/concepts/retrievers/).
- See our [how-to guide](/docs/how_to/#retrievers) on working with retrievers.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/retrieval.mdx)

* * *


- [Overview](#overview)
- [Key concepts](#key-concepts)
- [Query analysis](#query-analysis)
  
  - [Query re-writing](#query-re-writing)
  - [Query construction](#query-construction)
- [Information retrieval](#information-retrieval)
  
  - [Common retrieval systems](#common-retrieval-systems)
  - [Retriever](#retriever)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/parallel.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/parallel.ipynb)

# How to invoke runnables in parallel

Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel/)
- [Chaining runnables](/docs/how_to/sequence/)

The [`RunnableParallel`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html) primitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the `RunnableParallel`. The final return value is a dict with the results of each value under its appropriate key.

## Formatting with `RunnableParallels`[â€‹](#formatting-with-runnableparallels "Direct link to formatting-with-runnableparallels")

`RunnableParallels` are useful for parallelizing operations, but can also be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence. You can use them to split or fork the chain so that multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following:

```text
     Input
      / \
     /   \
 Branch1 Branch2
     \   /
      \ /
      Combine
```

Below, the input to prompt is expected to be a map with keys `"context"` and `"question"`. The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the `"question"` key.

```python
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

vectorstore = FAISS.from_texts(
    ["harrison worked at kensho"], embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""

# The prompt expects input with keys for "context" and "question"
prompt = ChatPromptTemplate.from_template(template)

model = ChatOpenAI()

retrieval_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

retrieval_chain.invoke("where did harrison work?")
```

**API Reference:**[FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```output
'Harrison worked at Kensho.'
```

tip

Note that when composing a RunnableParallel with another Runnable we don't even need to wrap our dictionary in the RunnableParallel class â€”Â the type conversion is handled for us. In the context of a chain, these are equivalent:

```text
{"context": retriever, "question": RunnablePassthrough()}
```

```text
RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
```

```text
RunnableParallel(context=retriever, question=RunnablePassthrough())
```

See the section on [coercion for more](/docs/how_to/sequence/#coercion).

## Using itemgetter as shorthand[â€‹](#using-itemgetter-as-shorthand "Direct link to Using itemgetter as shorthand")

Note that you can use Python's `itemgetter` as shorthand to extract data from the map when combining with `RunnableParallel`. You can find more information about itemgetter in the [Python Documentation](https://docs.python.org/3/library/operator.html#operator.itemgetter).

In the example below, we use itemgetter to extract specific keys from the map:

```python
from operator import itemgetter

from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

vectorstore = FAISS.from_texts(
    ["harrison worked at kensho"], embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()

template = """Answer the question based only on the following context:
{context}

Question: {question}

Answer in the following language: {language}
"""
prompt = ChatPromptTemplate.from_template(template)

chain = (
    {
        "context": itemgetter("question") | retriever,
        "question": itemgetter("question"),
        "language": itemgetter("language"),
    }
    | prompt
    | model
    | StrOutputParser()
)

chain.invoke({"question": "where did harrison work", "language": "italian"})
```

**API Reference:**[FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```output
'Harrison ha lavorato a Kensho.'
```

## Parallelize steps[â€‹](#parallelize-steps "Direct link to Parallelize steps")

RunnableParallels make it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel
from langchain_openai import ChatOpenAI

model = ChatOpenAI()
joke_chain = ChatPromptTemplate.from_template("tell me a joke about {topic}") | model
poem_chain = (
    ChatPromptTemplate.from_template("write a 2-line poem about {topic}") | model
)

map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)

map_chain.invoke({"topic": "bear"})
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
{'joke': AIMessage(content="Why don't bears like fast food? Because they can't catch it!", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None}, id='run-fe024170-c251-4b7a-bfd4-64a3737c67f2-0'),
 'poem': AIMessage(content='In the quiet of the forest, the bear roams free\nMajestic and wild, a sight to see.', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 15, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-2707913e-a743-4101-b6ec-840df4568a76-0')}
```

## Parallelism[â€‹](#parallelism "Direct link to Parallelism")

RunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier `joke_chain`, `poem_chain` and `map_chain` all have about the same runtime, even though `map_chain` executes both of the other two.

```python
%%timeit

joke_chain.invoke({"topic": "bear"})
```

```output
610 ms Â± 64 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
```

```python
%%timeit

poem_chain.invoke({"topic": "bear"})
```

```output
599 ms Â± 73.3 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
```

```python
%%timeit

map_chain.invoke({"topic": "bear"})
```

```output
643 ms Â± 77.8 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You now know some ways to format and parallelize chain steps with `RunnableParallel`.

To learn more, see the other how-to guides on runnables in this section.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/parallel.ipynb)

* * *


- [Formatting with `RunnableParallels`](#formatting-with-runnableparallels)
- [Using itemgetter as shorthand](#using-itemgetter-as-shorthand)
- [Parallelize steps](#parallelize-steps)
- [Parallelism](#parallelism)
- [Next steps](#next-steps)








[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/security.md)

# Security Policy

LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.

## Best practices[â€‹](#best-practices "Direct link to Best practices")

When building such applications developers should remember to follow good security practices:

- [**Limit Permissions**](https://en.wikipedia.org/wiki/Principle_of_least_privilege): Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), specifying proxy configurations to control external requests, etc. as appropriate for your application.
- **Anticipate Potential Misuse**: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, itâ€™s safest to assume that any LLM able to use those credentials may in fact delete data.
- [**Defense in Depth**](https://en.wikipedia.org/wiki/Defense_in_depth_%28computing%29): No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. Itâ€™s best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.

Risks of not doing so include, but are not limited to:

- Data corruption or loss.
- Unauthorized access to confidential information.
- Compromised performance or availability of critical resources.

Example scenarios with mitigation strategies:

- A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container.
- A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse.
- A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials.

If you're building applications that access external resources like file systems, APIs or databases, consider speaking with your company's security team to determine how to best design and secure your applications.

## Reporting OSS Vulnerabilities[â€‹](#reporting-oss-vulnerabilities "Direct link to Reporting OSS Vulnerabilities")

LangChain is partnered with [huntr by Protect AI](https://huntr.com/) to provide a bounty program for our open source projects.

Please report security vulnerabilities associated with the LangChain open source projects by visiting the following link:

[https://huntr.com/bounties/disclose/](https://huntr.com/bounties/disclose/?target=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain&validSearch=true)

Before reporting a vulnerability, please review:

1. In-Scope Targets and Out-of-Scope Targets below.
2. The [langchain-ai/langchain](https://python.langchain.com/docs/contributing/repo_structure) monorepo structure.
3. The [Best practicies](#best-practices) above to understand what we consider to be a security vulnerability vs. developer responsibility.

### In-Scope Targets[â€‹](#in-scope-targets "Direct link to In-Scope Targets")

The following packages and repositories are eligible for bug bounties:

- langchain-core
- langchain (see exceptions)
- langchain-community (see exceptions)
- langgraph
- langserve

### Out of Scope Targets[â€‹](#out-of-scope-targets "Direct link to Out of Scope Targets")

All out of scope targets defined by huntr as well as:

- **langchain-experimental**: This repository is for experimental code and is not eligible for bug bounties (see [package warning](https://pypi.org/project/langchain-experimental/)), bug reports to it will be marked as interesting or waste of time and published with no bounty attached.
- **tools**: Tools in either langchain or langchain-community are not eligible for bug bounties. This includes the following directories
  
  - libs/langchain/langchain/tools
  - libs/community/langchain\_community/tools
  - Please review the [best practices](#best-practices) for more details, but generally tools interact with the real world. Developers are expected to understand the security implications of their code and are responsible for the security of their tools.
- Code documented with security notices. This will be decided done on a case by case basis, but likely will not be eligible for a bounty as the code is already documented with guidelines for developers that should be followed for making their application secure.
- Any LangSmith related repositories or APIs (see [Reporting LangSmith Vulnerabilities](#reporting-langsmith-vulnerabilities)).

## Reporting LangSmith Vulnerabilities[â€‹](#reporting-langsmith-vulnerabilities "Direct link to Reporting LangSmith Vulnerabilities")

Please report security vulnerabilities associated with LangSmith by email to `security@langchain.dev`.

- LangSmith site: [https://smith.langchain.com](https://smith.langchain.com)
- SDK client: [https://github.com/langchain-ai/langsmith-sdk](https://github.com/langchain-ai/langsmith-sdk)

### Other Security Concerns[â€‹](#other-security-concerns "Direct link to Other Security Concerns")

For any other security concerns, please contact us at `security@langchain.dev`.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/security.md)

* * *

  - [In-Scope Targets](#in-scope-targets)
  - [Out of Scope Targets](#out-of-scope-targets)
- [Reporting LangSmith Vulnerabilities](#reporting-langsmith-vulnerabilities)
  
  - [Other Security Concerns](#other-security-concerns)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/v0_3/index.mdx)

# LangChain v0.3

*Last updated: 09.16.24*

## What's changed[â€‹](#whats-changed "Direct link to What's changed")

- All packages have been upgraded from Pydantic 1 to Pydantic 2 internally. Use of Pydantic 2 in user code is fully supported with all packages without the need for bridges like `langchain_core.pydantic_v1` or `pydantic.v1`.
- Pydantic 1 will no longer be supported as it reached its end-of-life in June 2024.
- Python 3.8 will no longer be supported as its end-of-life is October 2024.

**These are the only breaking changes.**

## Whatâ€™s new[â€‹](#whats-new "Direct link to Whatâ€™s new")

The following features have been added during the development of 0.2.x:

- Moved more integrations from `langchain-community` to their own `langchain-x` packages. This is a non-breaking change, as the legacy implementations are left in `langchain-community` and marked as deprecated. This allows us to better manage the dependencies of, test, and version these integrations. You can see all the latest integration packages in the [API reference](https://python.langchain.com/v0.2/api_reference/reference.html#integrations).
- Simplified tool definition and usage. Read more [here](https://blog.langchain.dev/improving-core-tool-interfaces-and-docs-in-langchain/).
- Added utilities for interacting with chat models: [universal model constructor](https://python.langchain.com/v0.2/docs/how_to/chat_models_universal_init/), [rate limiter](https://python.langchain.com/v0.2/docs/how_to/chat_model_rate_limiting/), [message utilities](https://python.langchain.com/v0.2/docs/how_to/#messages),
- Added the ability to [dispatch custom events](https://python.langchain.com/v0.2/docs/how_to/callbacks_custom_events/).
- Revamped integration docs and API reference. Read more [here](https://blog.langchain.dev/langchain-integration-docs-revamped/).
- Marked as deprecated a number of legacy chains and added migration guides for all of them. These are slated for removal in `langchain` 1.0.0. See the deprecated chains and associated [migration guides here](https://python.langchain.com/v0.2/docs/versions/migrating_chains/).

## How to update your code[â€‹](#how-to-update-your-code "Direct link to How to update your code")

If you're using `langchain` / `langchain-community` / `langchain-core` 0.0 or 0.1, we recommend that you first [upgrade to 0.2](https://python.langchain.com/v0.2/docs/versions/v0_2/).

If you're using `langgraph`, upgrade to `langgraph>=0.2.20,<0.3`. This will work with either 0.2 or 0.3 versions of all the base packages.

Here is a complete list of all packages that have been released and what we recommend upgrading your version constraints to. Any package that now requires `langchain-core` 0.3 had a minor version bump. Any package that is now compatible with both `langchain-core` 0.2 and 0.3 had a patch version bump.

You can use the `langchain-cli` to update deprecated imports automatically. The CLI will handle updating deprecated imports that were introduced in LangChain 0.0.x and LangChain 0.1, as well as updating the `langchain_core.pydantic_v1` and `langchain.pydantic_v1` imports.

### Base packages[â€‹](#base-packages "Direct link to Base packages")

| Package                  | Latest | Recommended constraint |
|--------------------------|--------|------------------------|
| langchain                | 0.3.0  | &gt;=0.3,&lt;0.4       |
| langchain-community      | 0.3.0  | &gt;=0.3,&lt;0.4       |
| langchain-text-splitters | 0.3.0  | &gt;=0.3,&lt;0.4       |
| langchain-core           | 0.3.0  | &gt;=0.3,&lt;0.4       |
| langchain-experimental   | 0.3.0  | &gt;=0.3,&lt;0.4       |

### Downstream packages[â€‹](#downstream-packages "Direct link to Downstream packages")

| Package   | Latest | Recommended constraint |
|-----------|--------|------------------------|
| langgraph | 0.2.20 | &gt;=0.2.20,&lt;0.3    |
| langserve | 0.3.0  | &gt;=0.3,&lt;0.4       |

### Integration packages[â€‹](#integration-packages "Direct link to Integration packages")

| Package                          | Latest | Recommended constraint |
|----------------------------------|--------|------------------------|
| langchain-ai21                   | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-aws                    | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-anthropic              | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-astradb                | 0.4.1  | &gt;=0.4.1,&lt;0.5     |
| langchain-azure-dynamic-sessions | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-box                    | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-chroma                 | 0.1.4  | &gt;=0.1.4,&lt;0.2     |
| langchain-cohere                 | 0.3.0  | &gt;=0.3,&lt;0.4       |
| langchain-elasticsearch          | 0.3.0  | &gt;=0.3,&lt;0.4       |
| langchain-exa                    | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-fireworks              | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-groq                   | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-google-community       | 2.0.0  | &gt;=2,&lt;3           |
| langchain-google-genai           | 2.0.0  | &gt;=2,&lt;3           |
| langchain-google-vertexai        | 2.0.0  | &gt;=2,&lt;3           |
| langchain-huggingface            | 0.1.0  | &gt;=0.1,&lt;0.2       |
| langchain-ibm                    | 0.3.0  | &gt;=0.3,&lt;0.4       |
| langchain-milvus                 | 0.1.6  | &gt;=0.1.6,&lt;0.2     |
| langchain-mistralai              | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-mongodb                | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-nomic                  | 0.1.3  | &gt;=0.1.3,&lt;0.2     |
| langchain-nvidia                 | 0.3.0  | &gt;=0.3,&lt;0.4       |
| langchain-ollama                 | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-openai                 | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-pinecone               | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-postgres               | 0.0.13 | &gt;=0.0.13,&lt;0.1    |
| langchain-prompty                | 0.1.0  | &gt;=0.1,&lt;0.2       |
| langchain-qdrant                 | 0.1.4  | &gt;=0.1.4,&lt;0.2     |
| langchain-redis                  | 0.1.0  | &gt;=0.1,&lt;0.2       |
| langchain-sema4                  | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-together               | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-unstructured           | 0.1.4  | &gt;=0.1.4,&lt;0.2     |
| langchain-upstage                | 0.3.0  | &gt;=0.3,&lt;0.4       |
| langchain-voyageai               | 0.2.0  | &gt;=0.2,&lt;0.3       |
| langchain-weaviate               | 0.0.3  | &gt;=0.0.3,&lt;0.1     |

Once you've updated to recent versions of the packages, you may need to address the following issues stemming from the internal switch from Pydantic v1 to Pydantic v2:

- If your code depends on Pydantic aside from LangChain, you will need to upgrade your pydantic version constraints to be `pydantic>=2,<3`. See [Pydanticâ€™s migration guide](https://docs.pydantic.dev/latest/migration/) for help migrating your non-LangChain code to Pydantic v2 if you use pydantic v1.
- There are a number of side effects to LangChain components caused by the internal switch from Pydantic v1 to v2. We have listed some of the common cases below together with the recommended solutions.

## Common issues when transitioning to Pydantic 2[â€‹](#common-issues-when-transitioning-to-pydantic-2 "Direct link to Common issues when transitioning to Pydantic 2")

### 1. Do not use the `langchain_core.pydantic_v1` namespace[â€‹](#1-do-not-use-the-langchain_corepydantic_v1-namespace "Direct link to 1-do-not-use-the-langchain_corepydantic_v1-namespace")

Replace any usage of `langchain_core.pydantic_v1` or `langchain.pydantic_v1` with direct imports from `pydantic`.

For example,

```python
from langchain_core.pydantic_v1 import BaseModel
```

to:

```python
from pydantic import BaseModel
```

This may require you to make additional updates to your Pydantic code given that there are a number of breaking changes in Pydantic 2. See the [Pydantic Migration](https://docs.pydantic.dev/latest/migration/) for how to upgrade your code from Pydantic 1 to 2.

### 2. Passing Pydantic objects to LangChain APIs[â€‹](#2-passing-pydantic-objects-to-langchain-apis "Direct link to 2. Passing Pydantic objects to LangChain APIs")

Users using the following APIs:

- `BaseChatModel.bind_tools`
- `BaseChatModel.with_structured_output`
- `Tool.from_function`
- `StructuredTool.from_function`

should ensure that they are passing Pydantic 2 objects to these APIs rather than Pydantic 1 objects (created via the `pydantic.v1` namespace of pydantic 2).

caution

While `v1` objects may be accepted by some of these APIs, users are advised to use Pydantic 2 objects to avoid future issues.

### 3. Sub-classing LangChain models[â€‹](#3-sub-classing-langchain-models "Direct link to 3. Sub-classing LangChain models")

Any sub-classing from existing LangChain models (e.g., `BaseTool`, `BaseChatModel`, `LLM`) should upgrade to use Pydantic 2 features.

For example, any user code that's relying on Pydantic 1 features (e.g., `validator`) should be updated to the Pydantic 2 equivalent (e.g., `field_validator`), and any references to `pydantic.v1`, `langchain_core.pydantic_v1`, `langchain.pydantic_v1` should be replaced with imports from `pydantic`.

```python
from pydantic.v1 import validator, Field # if pydantic 2 is installed
# from pydantic import validator, Field # if pydantic 1 is installed
# from langchain_core.pydantic_v1 import validator, Field
# from langchain.pydantic_v1 import validator, Field

class CustomTool(BaseTool): # BaseTool is v1 code
    x: int = Field(default=1)

    def _run(*args, **kwargs):
        return "hello"

    @validator('x') # v1 code
    @classmethod
    def validate_x(cls, x: int) -> int:
        return 1
```

Should change to:

```python
from pydantic import Field, field_validator # pydantic v2
from langchain_core.pydantic_v1 import BaseTool

class CustomTool(BaseTool): # BaseTool is v1 code
    x: int = Field(default=1)

    def _run(*args, **kwargs):
        return "hello"

    @field_validator('x') # v2 code
    @classmethod
    def validate_x(cls, x: int) -> int:
        return 1


CustomTool(
    name='custom_tool',
    description="hello",
    x=1,
)
```

### 4. model\_rebuild()[â€‹](#4-model_rebuild "Direct link to 4. model_rebuild()")

When sub-classing from LangChain models, users may need to add relevant imports to the file and rebuild the model.

You can read more about `model_rebuild` [here](https://docs.pydantic.dev/latest/concepts/models/#rebuilding-model-schema).

```python
from langchain_core.output_parsers import BaseOutputParser


class FooParser(BaseOutputParser):
    ...
```

**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)

New code:

```python
from typing import Optional as Optional

from langchain_core.output_parsers import BaseOutputParser

class FooParser(BaseOutputParser):
    ...

FooParser.model_rebuild()
```

**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html)

## Migrate using langchain-cli[â€‹](#migrate-using-langchain-cli "Direct link to Migrate using langchain-cli")

The `langchain-cli` can help update deprecated LangChain imports in your code automatically.

Please note that the `langchain-cli` only handles deprecated LangChain imports and cannot help to upgrade your code from pydantic 1 to pydantic 2.

For help with the Pydantic 1 to 2 migration itself please refer to the [Pydantic Migration Guidelines](https://docs.pydantic.dev/latest/migration/).

As of 0.0.31, the `langchain-cli` relies on [gritql](https://about.grit.io/) for applying code mods.

### Installation[â€‹](#installation "Direct link to Installation")

```bash
pip install -U langchain-cli
langchain-cli --version # <-- Make sure the version is at least 0.0.31
```

### Usage[â€‹](#usage "Direct link to Usage")

Given that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like `git`).

The `langchain-cli` will handle the `langchain_core.pydantic_v1` deprecation introduced in LangChain 0.3 as well as older deprecations (e.g.,`from langchain.chat_models import ChatOpenAI` which should be `from langchain_openai import ChatOpenAI`),

You will need to run the migration script **twice** as it only applies one import replacement per run.

For example, say that your code is still using the old import `from langchain.chat_models import ChatOpenAI`:

After the first run, youâ€™ll get: `from langchain_community.chat_models import ChatOpenAI` After the second run, youâ€™ll get: `from langchain_openai import ChatOpenAI`

```bash
# Run a first time
# Will replace from langchain.chat_models import ChatOpenAI
langchain-cli migrate --help [path to code] # Help
langchain-cli migrate [path to code] # Apply

# Run a second time to apply more import replacements
langchain-cli migrate --diff [path to code] # Preview
langchain-cli migrate [path to code] # Apply
```

### Other options[â€‹](#other-options "Direct link to Other options")

```bash
# See help menu
langchain-cli migrate --help
# Preview Changes without applying
langchain-cli migrate --diff [path to code]
# Approve changes interactively
langchain-cli migrate --interactive [path to code]
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/v0_3/index.mdx)

* * *


- [What's changed](#whats-changed)
- [Whatâ€™s new](#whats-new)
- [How to update your code](#how-to-update-your-code)
  
  - [Base packages](#base-packages)
  - [Downstream packages](#downstream-packages)
  - [Integration packages](#integration-packages)
- [Common issues when transitioning to Pydantic 2](#common-issues-when-transitioning-to-pydantic-2)
  
  - [1. Do not use the `langchain_core.pydantic_v1` namespace](#1-do-not-use-the-langchain_corepydantic_v1-namespace)
  - [2. Passing Pydantic objects to LangChain APIs](#2-passing-pydantic-objects-to-langchain-apis)
  - [3. Sub-classing LangChain models](#3-sub-classing-langchain-models)
  - [4. model\_rebuild()](#4-model_rebuild)
- [Migrate using langchain-cli](#migrate-using-langchain-cli)
  
  - [Installation](#installation)
  - [Usage](#usage)
  - [Other options](#other-options)









# String-in, string-out llms

tip

You are probably looking for the [Chat Model Concept Guide](/docs/concepts/chat_models/) page for more information.

LangChain has implementations for older language models that take a string as input and return a string as output. These models are typically named without the "Chat" prefix (e.g., `Ollama`, `Anthropic`, `OpenAI`, etc.), and may include the "LLM" suffix (e.g., `OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.). These models implement the [BaseLLM](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseLLM.html#langchain_core.language_models.llms.BaseLLM) interface.

Users should be using almost exclusively the newer [Chat Models](/docs/concepts/chat_models/) as most model providers have adopted a chat like interface for interacting with language models.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/text_llms.mdx)

* * *









- [LangSmith](https://docs.smith.langchain.com)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [LangChain Hub](https://smith.langchain.com/hub)
- [LangChain JS/TS](https://js.langchain.com)

[v0.3](#)

- [v0.3](/docs/introduction/)
- [v0.2](https://python.langchain.com/v0.2/docs/introduction)
- [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)

[ðŸ’¬](https://chat.langchain.com)

Search

- [Providers](/docs/integrations/providers/)
  
  - [Anthropic](/docs/integrations/providers/anthropic/)
  - [AWS](/docs/integrations/providers/aws/)
  - [Google](/docs/integrations/providers/google/)
  - [Hugging Face](/docs/integrations/providers/huggingface/)
  - [Microsoft](/docs/integrations/providers/microsoft/)
  - [OpenAI](/docs/integrations/providers/openai/)
  - [More](/docs/integrations/providers/all/)
    
    - [Providers](/docs/integrations/providers/)
    - [Abso](/docs/integrations/providers/abso/)
    - [Acreom](/docs/integrations/providers/acreom/)
    - [Activeloop Deep Lake](/docs/integrations/providers/activeloop_deeplake/)
    - [ADS4GPTs](/docs/integrations/providers/ads4gpts/)
    - [Aerospike](/docs/integrations/providers/aerospike/)
    - [AgentQL](/docs/integrations/providers/agentql/)
    - [AI21 Labs](/docs/integrations/providers/ai21/)
    - [Aim](/docs/integrations/providers/aim_tracking/)
    - [AINetwork](/docs/integrations/providers/ainetwork/)
    - [Airbyte](/docs/integrations/providers/airbyte/)
    - [Airtable](/docs/integrations/providers/airtable/)
    - [Alchemy](/docs/integrations/providers/alchemy/)
    - [Aleph Alpha](/docs/integrations/providers/aleph_alpha/)
    - [Alibaba Cloud](/docs/integrations/providers/alibaba_cloud/)
    - [AnalyticDB](/docs/integrations/providers/analyticdb/)
    - [Annoy](/docs/integrations/providers/annoy/)
    - [Anthropic](/docs/integrations/providers/anthropic/)
    - [Anyscale](/docs/integrations/providers/anyscale/)
    - [Apache Software Foundation](/docs/integrations/providers/apache/)
    - [Apache Doris](/docs/integrations/providers/apache_doris/)
    - [Apify](/docs/integrations/providers/apify/)
    - [Apple](/docs/integrations/providers/apple/)
    - [ArangoDB](/docs/integrations/providers/arangodb/)
    - [Arcee](/docs/integrations/providers/arcee/)
    - [ArcGIS](/docs/integrations/providers/arcgis/)
    - [Argilla](/docs/integrations/providers/argilla/)
    - [Arize](/docs/integrations/providers/arize/)
    - [Arthur](/docs/integrations/providers/arthur_tracking/)
    - [Arxiv](/docs/integrations/providers/arxiv/)
    - [Ascend](/docs/integrations/providers/ascend/)
    - [AskNews](/docs/integrations/providers/asknews/)
    - [AssemblyAI](/docs/integrations/providers/assemblyai/)
    - [Astra DB](/docs/integrations/providers/astradb/)
    - [Atlas](/docs/integrations/providers/atlas/)
    - [AwaDB](/docs/integrations/providers/awadb/)
    - [AWS](/docs/integrations/providers/aws/)
    - [AZLyrics](/docs/integrations/providers/azlyrics/)
    - [Azure AI](/docs/integrations/providers/azure_ai/)
    - [BAAI](/docs/integrations/providers/baai/)
    - [Bagel](/docs/integrations/providers/bagel/)
    - [BagelDB](/docs/integrations/providers/bageldb/)
    - [Baichuan](/docs/integrations/providers/baichuan/)
    - [Baidu](/docs/integrations/providers/baidu/)
    - [Banana](/docs/integrations/providers/bananadev/)
    - [Baseten](/docs/integrations/providers/baseten/)
    - [Beam](/docs/integrations/providers/beam/)
    - [Beautiful Soup](/docs/integrations/providers/beautiful_soup/)
    - [BibTeX](/docs/integrations/providers/bibtex/)
    - [BiliBili](/docs/integrations/providers/bilibili/)
    - [Bittensor](/docs/integrations/providers/bittensor/)
    - [Blackboard](/docs/integrations/providers/blackboard/)
    - [bookend.ai](/docs/integrations/providers/bookendai/)
    - [Box](/docs/integrations/providers/box/)
    - [Brave Search](/docs/integrations/providers/brave_search/)
    - [Breebs (Open Knowledge)](/docs/integrations/providers/breebs/)
    - [Browserbase](/docs/integrations/providers/browserbase/)
    - [Browserless](/docs/integrations/providers/browserless/)
    - [ByteDance](/docs/integrations/providers/byte_dance/)
    - [Cassandra](/docs/integrations/providers/cassandra/)
    - [Cerebras](/docs/integrations/providers/cerebras/)
    - [CerebriumAI](/docs/integrations/providers/cerebriumai/)
    - [Chaindesk](/docs/integrations/providers/chaindesk/)
    - [Chroma](/docs/integrations/providers/chroma/)
    - [Clarifai](/docs/integrations/providers/clarifai/)
    - [ClearML](/docs/integrations/providers/clearml_tracking/)
    - [ClickHouse](/docs/integrations/providers/clickhouse/)
    - [ClickUp](/docs/integrations/providers/clickup/)
    - [Cloudflare](/docs/integrations/providers/cloudflare/)
    - [Clova](/docs/integrations/providers/clova/)
    - [CnosDB](/docs/integrations/providers/cnosdb/)
    - [Cognee](/docs/integrations/providers/cognee/)
    - [CogniSwitch](/docs/integrations/providers/cogniswitch/)
    - [Cohere](/docs/integrations/providers/cohere/)
    - [College Confidential](/docs/integrations/providers/college_confidential/)
    - [Comet](/docs/integrations/providers/comet_tracking/)
    - [Confident AI](/docs/integrations/providers/confident/)
    - [Confluence](/docs/integrations/providers/confluence/)
    - [Connery](/docs/integrations/providers/connery/)
    - [Context](/docs/integrations/providers/context/)
    - [Contextual AI](/docs/integrations/providers/contextual/)
    - [Couchbase](/docs/integrations/providers/couchbase/)
    - [Coze](/docs/integrations/providers/coze/)
    - [CrateDB](/docs/integrations/providers/cratedb/)
    - [C Transformers](/docs/integrations/providers/ctransformers/)
    - [CTranslate2](/docs/integrations/providers/ctranslate2/)
    - [Cube](/docs/integrations/providers/cube/)
    - [Dappier](/docs/integrations/providers/dappier/)
    - [DashVector](/docs/integrations/providers/dashvector/)
    - [Databricks](/docs/integrations/providers/databricks/)
    - [Datadog Tracing](/docs/integrations/providers/datadog/)
    - [Datadog Logs](/docs/integrations/providers/datadog_logs/)
    - [DataForSEO](/docs/integrations/providers/dataforseo/)
    - [Dataherald](/docs/integrations/providers/dataherald/)
    - [Dedoc](/docs/integrations/providers/dedoc/)
    - [DeepInfra](/docs/integrations/providers/deepinfra/)
    - [Deeplake](/docs/integrations/providers/deeplake/)
    - [DeepSeek](/docs/integrations/providers/deepseek/)
    - [DeepSparse](/docs/integrations/providers/deepsparse/)
    - [Dell](/docs/integrations/providers/dell/)
    - [Diffbot](/docs/integrations/providers/diffbot/)
    - [DingoDB](/docs/integrations/providers/dingo/)
    - [Discord](/docs/integrations/providers/discord-shikenso/)
    - [Discord (community loader)](/docs/integrations/providers/discord/)
    - [DocArray](/docs/integrations/providers/docarray/)
    - [Docling](/docs/integrations/providers/docling/)
    - [Doctran](/docs/integrations/providers/doctran/)
    - [Docugami](/docs/integrations/providers/docugami/)
    - [Docusaurus](/docs/integrations/providers/docusaurus/)
    - [Dria](/docs/integrations/providers/dria/)
    - [Dropbox](/docs/integrations/providers/dropbox/)
    - [DSPy](/docs/integrations/providers/dspy/)
    - [DuckDB](/docs/integrations/providers/duckdb/)
    - [DuckDuckGo Search](/docs/integrations/providers/duckduckgo_search/)
    - [E2B](/docs/integrations/providers/e2b/)
    - [Eden AI](/docs/integrations/providers/edenai/)
    - [Elasticsearch](/docs/integrations/providers/elasticsearch/)
    - [ElevenLabs](/docs/integrations/providers/elevenlabs/)
    - [Embedchain](/docs/integrations/providers/embedchain/)
    - [Epsilla](/docs/integrations/providers/epsilla/)
    - [Etherscan](/docs/integrations/providers/etherscan/)
    - [Everly AI](/docs/integrations/providers/everlyai/)
    - [EverNote](/docs/integrations/providers/evernote/)
    - [Exa](/docs/integrations/providers/exa_search/)
    - [Facebook - Meta](/docs/integrations/providers/facebook/)
    - [FalkorDB](/docs/integrations/providers/falkordb/)
    - [Fauna](/docs/integrations/providers/fauna/)
    - [Fiddler](/docs/integrations/providers/fiddler/)
    - [Figma](/docs/integrations/providers/figma/)
    - [FireCrawl](/docs/integrations/providers/firecrawl/)
    - [Fireworks AI](/docs/integrations/providers/fireworks/)
    - [Flyte](/docs/integrations/providers/flyte/)
    - [FMP Data (Financial Data Prep)](/docs/integrations/providers/fmp-data/)
    - [Forefront AI](/docs/integrations/providers/forefrontai/)
    - [Friendli AI](/docs/integrations/providers/friendli/)
    - [Smabbler](/docs/integrations/providers/galaxia/)
    - [Geopandas](/docs/integrations/providers/geopandas/)
    - [Git](/docs/integrations/providers/git/)
    - [GitBook](/docs/integrations/providers/gitbook/)
    - [GitHub](/docs/integrations/providers/github/)
    - [GitLab](/docs/integrations/providers/gitlab/)
    - [GOAT](/docs/integrations/providers/goat/)
    - [Golden](/docs/integrations/providers/golden/)
    - [Goodfire](/docs/integrations/providers/goodfire/)
    - [Google](/docs/integrations/providers/google/)
    - [Serper - Google Search API](/docs/integrations/providers/google_serper/)
    - [GooseAI](/docs/integrations/providers/gooseai/)
    - [GPT4All](/docs/integrations/providers/gpt4all/)
    - [Gradient](/docs/integrations/providers/gradient/)
    - [Graph RAG](/docs/integrations/providers/graph_rag/)
    - [Graphsignal](/docs/integrations/providers/graphsignal/)
    - [Grobid](/docs/integrations/providers/grobid/)
    - [Groq](/docs/integrations/providers/groq/)
    - [Gutenberg](/docs/integrations/providers/gutenberg/)
    - [Hacker News](/docs/integrations/providers/hacker_news/)
    - [Hazy Research](/docs/integrations/providers/hazy_research/)
    - [Helicone](/docs/integrations/providers/helicone/)
    - [Hologres](/docs/integrations/providers/hologres/)
    - [HTML to text](/docs/integrations/providers/html2text/)
    - [Huawei](/docs/integrations/providers/huawei/)
    - [Hugging Face](/docs/integrations/providers/huggingface/)
    - [Hyperbrowser](/docs/integrations/providers/hyperbrowser/)
    - [IBM](/docs/integrations/providers/ibm/)
    - [IEIT Systems](/docs/integrations/providers/ieit_systems/)
    - [iFixit](/docs/integrations/providers/ifixit/)
    - [iFlytek](/docs/integrations/providers/iflytek/)
    - [IMSDb](/docs/integrations/providers/imsdb/)
    - [Infinispan VS](/docs/integrations/providers/infinispanvs/)
    - [Infinity](/docs/integrations/providers/infinity/)
    - [Infino](/docs/integrations/providers/infino/)
    - [Intel](/docs/integrations/providers/intel/)
    - [Iugu](/docs/integrations/providers/iugu/)
    - [Jaguar](/docs/integrations/providers/jaguar/)
    - [Javelin AI Gateway](/docs/integrations/providers/javelin_ai_gateway/)
    - [Jenkins](/docs/integrations/providers/jenkins/)
    - [Jina AI](/docs/integrations/providers/jina/)
    - [Johnsnowlabs](/docs/integrations/providers/johnsnowlabs/)
    - [Joplin](/docs/integrations/providers/joplin/)
    - [KDB.AI](/docs/integrations/providers/kdbai/)
    - [Kinetica](/docs/integrations/providers/kinetica/)
    - [KoboldAI](/docs/integrations/providers/koboldai/)
    - [Konko](/docs/integrations/providers/konko/)
    - [KoNLPY](/docs/integrations/providers/konlpy/)
    - [KÃ¹zu](/docs/integrations/providers/kuzu/)
    - [Label Studio](/docs/integrations/providers/labelstudio/)
    - [lakeFS](/docs/integrations/providers/lakefs/)
    - [LanceDB](/docs/integrations/providers/lancedb/)
    - [LangChain Decorators âœ¨](/docs/integrations/providers/langchain_decorators/)
    - [LangFair: Use-Case Level LLM Bias and Fairness Assessments](/docs/integrations/providers/langfair/)
    - [Langfuse ðŸª¢](/docs/integrations/providers/langfuse/)
    - [Lantern](/docs/integrations/providers/lantern/)
    - [Lindorm](/docs/integrations/providers/lindorm/)
    - [Linkup](/docs/integrations/providers/linkup/)
    - [LiteLLM](/docs/integrations/providers/litellm/)
    - [LlamaIndex](/docs/integrations/providers/llama_index/)
    - [Llama.cpp](/docs/integrations/providers/llamacpp/)
    - [LlamaEdge](/docs/integrations/providers/llamaedge/)
    - [llamafile](/docs/integrations/providers/llamafile/)
    - [LLMonitor](/docs/integrations/providers/llmonitor/)
    - [LocalAI](/docs/integrations/providers/localai/)
    - [Log10](/docs/integrations/providers/log10/)
    - [MariaDB](/docs/integrations/providers/mariadb/)
    - [MariTalk](/docs/integrations/providers/maritalk/)
    - [Marqo](/docs/integrations/providers/marqo/)
    - [MediaWikiDump](/docs/integrations/providers/mediawikidump/)
    - [Meilisearch](/docs/integrations/providers/meilisearch/)
    - [Memcached](/docs/integrations/providers/memcached/)
    - [Memgraph](/docs/integrations/providers/memgraph/)
    - [Metal](/docs/integrations/providers/metal/)
    - [Microsoft](/docs/integrations/providers/microsoft/)
    - [Milvus](/docs/integrations/providers/milvus/)
    - [MindsDB](/docs/integrations/providers/mindsdb/)
    - [Minimax](/docs/integrations/providers/minimax/)
    - [MistralAI](/docs/integrations/providers/mistralai/)
    - [MLflow AI Gateway for LLMs](/docs/integrations/providers/mlflow/)
    - [MLflow](/docs/integrations/providers/mlflow_tracking/)
    - [MLX](/docs/integrations/providers/mlx/)
    - [Modal](/docs/integrations/providers/modal/)
    - [ModelScope](/docs/integrations/providers/modelscope/)
    - [Modern Treasury](/docs/integrations/providers/modern_treasury/)
    - [Momento](/docs/integrations/providers/momento/)
    - [MongoDB](/docs/integrations/providers/mongodb/)
    - [MongoDB Atlas](/docs/integrations/providers/mongodb_atlas/)
    - [Motherduck](/docs/integrations/providers/motherduck/)
    - [MotÃ¶rhead](/docs/integrations/providers/motorhead/)
    - [MyScale](/docs/integrations/providers/myscale/)
    - [NAVER](/docs/integrations/providers/naver/)
    - [Neo4j](/docs/integrations/providers/neo4j/)
    - [Netmind](/docs/integrations/providers/netmind/)
    - [Nimble](/docs/integrations/providers/nimble/)
    - [NLPCloud](/docs/integrations/providers/nlpcloud/)
    - [Nomic](/docs/integrations/providers/nomic/)
    - [Notion DB](/docs/integrations/providers/notion/)
    - [Nuclia](/docs/integrations/providers/nuclia/)
    - [NVIDIA](/docs/integrations/providers/nvidia/)
    - [Obsidian](/docs/integrations/providers/obsidian/)
    - [OceanBase](/docs/integrations/providers/oceanbase/)
    - [Oracle Cloud Infrastructure (OCI)](/docs/integrations/providers/oci/)
    - [OctoAI](/docs/integrations/providers/octoai/)
    - [Ollama](/docs/integrations/providers/ollama/)
    - [Ontotext GraphDB](/docs/integrations/providers/ontotext_graphdb/)
    - [OpenAI](/docs/integrations/providers/openai/)
    - [OpenGradient](/docs/integrations/providers/opengradient/)
    - [OpenLLM](/docs/integrations/providers/openllm/)
    - [OpenSearch](/docs/integrations/providers/opensearch/)
    - [OpenWeatherMap](/docs/integrations/providers/openweathermap/)
    - [OracleAI Vector Search](/docs/integrations/providers/oracleai/)
    - [Outline](/docs/integrations/providers/outline/)
    - [Outlines](/docs/integrations/providers/outlines/)
    - [Oxylabs](/docs/integrations/providers/oxylabs/)
    - [Pandas](/docs/integrations/providers/pandas/)
    - [PaymanAI](/docs/integrations/providers/payman-tool/)
    - [Pebblo](/docs/integrations/providers/pebblo/)
    - [Permit](/docs/integrations/providers/permit/)
    - [Perplexity](/docs/integrations/providers/perplexity/)
    - [Petals](/docs/integrations/providers/petals/)
    - [Postgres Embedding](/docs/integrations/providers/pg_embedding/)
    - [PGVector](/docs/integrations/providers/pgvector/)
    - [Pinecone](/docs/integrations/providers/pinecone/)
    - [PipelineAI](/docs/integrations/providers/pipelineai/)
    - [Pipeshift](/docs/integrations/providers/pipeshift/)
    - [Portkey](/docs/integrations/providers/portkey/)
    - [Predibase](/docs/integrations/providers/predibase/)
    - [Prediction Guard](/docs/integrations/providers/predictionguard/)
    - [PremAI](/docs/integrations/providers/premai/)
    - [SWI-Prolog](/docs/integrations/providers/prolog/)
    - [PromptLayer](/docs/integrations/providers/promptlayer/)
    - [Psychic](/docs/integrations/providers/psychic/)
    - [PubMed](/docs/integrations/providers/pubmed/)
    - [PullMd Loader](/docs/integrations/providers/pull-md/)
    - [PygmalionAI](/docs/integrations/providers/pygmalionai/)
    - [PyMuPDF4LLM](/docs/integrations/providers/pymupdf4llm/)
    - [Qdrant](/docs/integrations/providers/qdrant/)
    - [RAGatouille](/docs/integrations/providers/ragatouille/)
    - [rank\_bm25](/docs/integrations/providers/rank_bm25/)
    - [Ray Serve](/docs/integrations/providers/ray_serve/)
    - [Rebuff](/docs/integrations/providers/rebuff/)
    - [Reddit](/docs/integrations/providers/reddit/)
    - [Redis](/docs/integrations/providers/redis/)
    - [Remembrall](/docs/integrations/providers/remembrall/)
    - [Replicate](/docs/integrations/providers/replicate/)
    - [Roam](/docs/integrations/providers/roam/)
    - [Sema4 (fka Robocorp)](/docs/integrations/providers/robocorp/)
    - [Rockset](/docs/integrations/providers/rockset/)
    - [Runhouse](/docs/integrations/providers/runhouse/)
    - [Runpod](/docs/integrations/providers/runpod/)
    - [RWKV-4](/docs/integrations/providers/rwkv/)
    - [Salesforce](/docs/integrations/providers/salesforce/)
    - [Salute Devices](/docs/integrations/providers/salute_devices/)
    - [SambaNova](/docs/integrations/providers/sambanova/)
    - [SAP](/docs/integrations/providers/sap/)
    - [ScrapeGraph AI](/docs/integrations/providers/scrapegraph/)
    - [SearchApi](/docs/integrations/providers/searchapi/)
    - [SearxNG Search API](/docs/integrations/providers/searx/)
    - [SemaDB](/docs/integrations/providers/semadb/)
    - [SerpAPI](/docs/integrations/providers/serpapi/)
    - [Shale Protocol](/docs/integrations/providers/shaleprotocol/)
    - [SingleStore Integration](/docs/integrations/providers/singlestore/)
    - [scikit-learn](/docs/integrations/providers/sklearn/)
    - [Slack](/docs/integrations/providers/slack/)
    - [Snowflake](/docs/integrations/providers/snowflake/)
    - [spaCy](/docs/integrations/providers/spacy/)
    - [Spark](/docs/integrations/providers/spark/)
    - [SparkLLM](/docs/integrations/providers/sparkllm/)
    - [Spreedly](/docs/integrations/providers/spreedly/)
    - [SQLite](/docs/integrations/providers/sqlite/)
    - [Stack Exchange](/docs/integrations/providers/stackexchange/)
    - [StarRocks](/docs/integrations/providers/starrocks/)
    - [StochasticAI](/docs/integrations/providers/stochasticai/)
    - [Streamlit](/docs/integrations/providers/streamlit/)
    - [Stripe](/docs/integrations/providers/stripe/)
    - [Supabase (Postgres)](/docs/integrations/providers/supabase/)
    - [Nebula](/docs/integrations/providers/symblai_nebula/)
    - [Tableau](/docs/integrations/providers/tableau/)
    - [Taiga](/docs/integrations/providers/taiga/)
    - [Tair](/docs/integrations/providers/tair/)
    - [Tavily](/docs/integrations/providers/tavily/)
    - [Telegram](/docs/integrations/providers/telegram/)
    - [Tencent](/docs/integrations/providers/tencent/)
    - [TensorFlow Datasets](/docs/integrations/providers/tensorflow_datasets/)
    - [TiDB](/docs/integrations/providers/tidb/)
    - [TigerGraph](/docs/integrations/providers/tigergraph/)
    - [Tigris](/docs/integrations/providers/tigris/)
    - [Tilores](/docs/integrations/providers/tilores/)
    - [Together AI](/docs/integrations/providers/together/)
    - [2Markdown](/docs/integrations/providers/tomarkdown/)
    - [Transwarp](/docs/integrations/providers/transwarp/)
    - [Trello](/docs/integrations/providers/trello/)
    - [Trubrics](/docs/integrations/providers/trubrics/)
    - [TruLens](/docs/integrations/providers/trulens/)
    - [Twitter](/docs/integrations/providers/twitter/)
    - [Typesense](/docs/integrations/providers/typesense/)
    - [Unstructured](/docs/integrations/providers/unstructured/)
    - [Upstage](/docs/integrations/providers/upstage/)
    - [upstash](/docs/integrations/providers/upstash/)
    - [UpTrain](/docs/integrations/providers/uptrain/)
    - [USearch](/docs/integrations/providers/usearch/)
    - [Valthera](/docs/integrations/providers/valthera/)
    - [VDMS](/docs/integrations/providers/vdms/)
    - [Vearch](/docs/integrations/providers/vearch/)
    - [Vectara](/docs/integrations/providers/vectara/)
    - [Vectorize](/docs/integrations/providers/vectorize/)
    - [Vespa](/docs/integrations/providers/vespa/)
    - [vlite](/docs/integrations/providers/vlite/)
    - [VoyageAI](/docs/integrations/providers/voyageai/)
    - [Weights &amp; Biases](/docs/integrations/providers/wandb/)
    - [Weights &amp; Biases tracing](/docs/integrations/providers/wandb_tracing/)
    - [Weights &amp; Biases tracking](/docs/integrations/providers/wandb_tracking/)
    - [Weather](/docs/integrations/providers/weather/)
    - [Weaviate](/docs/integrations/providers/weaviate/)
    - [WhatsApp](/docs/integrations/providers/whatsapp/)
    - [WhyLabs](/docs/integrations/providers/whylabs_profiling/)
    - [Wikipedia](/docs/integrations/providers/wikipedia/)
    - [Wolfram Alpha](/docs/integrations/providers/wolfram_alpha/)
    - [Writer, Inc.](/docs/integrations/providers/writer/)
    - [xAI](/docs/integrations/providers/xai/)
    - [Xata](/docs/integrations/providers/xata/)
    - [Xorbits Inference (Xinference)](/docs/integrations/providers/xinference/)
    - [Yahoo](/docs/integrations/providers/yahoo/)
    - [Yandex](/docs/integrations/providers/yandex/)
    - [YDB](/docs/integrations/providers/ydb/)
    - [Yeager.ai](/docs/integrations/providers/yeagerai/)
    - [Yellowbrick](/docs/integrations/providers/yellowbrick/)
    - [01.AI](/docs/integrations/providers/yi/)
    - [You](/docs/integrations/providers/you/)
    - [YouTube](/docs/integrations/providers/youtube/)
    - [Zep](/docs/integrations/providers/zep/)
    - [Zhipu AI](/docs/integrations/providers/zhipuai/)
    - [Zilliz](/docs/integrations/providers/zilliz/)
    - [Zotero](/docs/integrations/providers/zotero/)
- [Components](/docs/integrations/components/)
  
  - [Chat models](/docs/integrations/chat/)
    
    - [Chat models](/docs/integrations/chat/)
    - [Abso](/docs/integrations/chat/abso/)
    - [AI21 Labs](/docs/integrations/chat/ai21/)
    - [Alibaba Cloud PAI EAS](/docs/integrations/chat/alibaba_cloud_pai_eas/)
    - [Anthropic](/docs/integrations/chat/anthropic/)
    - [\[Deprecated\] Experimental Anthropic Tools Wrapper](/docs/integrations/chat/anthropic_functions/)
    - [Anyscale](/docs/integrations/chat/anyscale/)
    - [AzureAIChatCompletionsModel](/docs/integrations/chat/azure_ai/)
    - [Azure OpenAI](/docs/integrations/chat/azure_chat_openai/)
    - [Azure ML Endpoint](/docs/integrations/chat/azureml_chat_endpoint/)
    - [Baichuan Chat](/docs/integrations/chat/baichuan/)
    - [Baidu Qianfan](/docs/integrations/chat/baidu_qianfan_endpoint/)
    - [AWS Bedrock](/docs/integrations/chat/bedrock/)
    - [Cerebras](/docs/integrations/chat/cerebras/)
    - [CloudflareWorkersAI](/docs/integrations/chat/cloudflare_workersai/)
    - [Cohere](/docs/integrations/chat/cohere/)
    - [ContextualAI](/docs/integrations/chat/contextual/)
    - [Coze Chat](/docs/integrations/chat/coze/)
    - [Dappier AI](/docs/integrations/chat/dappier/)
    - [Databricks](/docs/integrations/chat/databricks/)
    - [DeepInfra](/docs/integrations/chat/deepinfra/)
    - [DeepSeek](/docs/integrations/chat/deepseek/)
    - [Eden AI](/docs/integrations/chat/edenai/)
    - [Ernie Bot Chat](/docs/integrations/chat/ernie/)
    - [EverlyAI](/docs/integrations/chat/everlyai/)
    - [Fireworks](/docs/integrations/chat/fireworks/)
    - [ChatFriendli](/docs/integrations/chat/friendli/)
    - [GigaChat](/docs/integrations/chat/gigachat/)
    - [Goodfire](/docs/integrations/chat/goodfire/)
    - [Google AI](/docs/integrations/chat/google_generative_ai/)
    - [Google Cloud Vertex AI](/docs/integrations/chat/google_vertex_ai_palm/)
    - [GPTRouter](/docs/integrations/chat/gpt_router/)
    - [Groq](/docs/integrations/chat/groq/)
    - [ChatHuggingFace](/docs/integrations/chat/huggingface/)
    - [IBM watsonx.ai](/docs/integrations/chat/ibm_watsonx/)
    - [JinaChat](/docs/integrations/chat/jinachat/)
    - [Kinetica](/docs/integrations/chat/kinetica/)
    - [Konko](/docs/integrations/chat/konko/)
    - [LiteLLM](/docs/integrations/chat/litellm/)
    - [LiteLLM Router](/docs/integrations/chat/litellm_router/)
    - [Llama 2 Chat](/docs/integrations/chat/llama2_chat/)
    - [Llama API](/docs/integrations/chat/llama_api/)
    - [LlamaEdge](/docs/integrations/chat/llama_edge/)
    - [Llama.cpp](/docs/integrations/chat/llamacpp/)
    - [maritalk](/docs/integrations/chat/maritalk/)
    - [MiniMax](/docs/integrations/chat/minimax/)
    - [MistralAI](/docs/integrations/chat/mistralai/)
    - [MLX](/docs/integrations/chat/mlx/)
    - [ModelScope](/docs/integrations/chat/modelscope_chat_endpoint/)
    - [Moonshot](/docs/integrations/chat/moonshot/)
    - [Naver](/docs/integrations/chat/naver/)
    - [Netmind](/docs/integrations/chat/netmind/)
    - [NVIDIA AI Endpoints](/docs/integrations/chat/nvidia_ai_endpoints/)
    - [ChatOCIModelDeployment](/docs/integrations/chat/oci_data_science/)
    - [OCIGenAI](/docs/integrations/chat/oci_generative_ai/)
    - [ChatOctoAI](/docs/integrations/chat/octoai/)
    - [Ollama](/docs/integrations/chat/ollama/)
    - [OpenAI](/docs/integrations/chat/openai/)
    - [Outlines](/docs/integrations/chat/outlines/)
    - [Perplexity](/docs/integrations/chat/perplexity/)
    - [Pipeshift](/docs/integrations/chat/pipeshift/)
    - [ChatPredictionGuard](/docs/integrations/chat/predictionguard/)
    - [PremAI](/docs/integrations/chat/premai/)
    - [PromptLayer ChatOpenAI](/docs/integrations/chat/promptlayer_chatopenai/)
    - [Qwen QwQ](/docs/integrations/chat/qwq/)
    - [Reka](/docs/integrations/chat/reka/)
    - [RunPod Chat Model](/docs/integrations/chat/runpod/)
    - [SambaNovaCloud](/docs/integrations/chat/sambanova/)
    - [SambaStudio](/docs/integrations/chat/sambastudio/)
    - [ChatSeekrFlow](/docs/integrations/chat/seekrflow/)
    - [Snowflake Cortex](/docs/integrations/chat/snowflake/)
    - [solar](/docs/integrations/chat/solar/)
    - [SparkLLM Chat](/docs/integrations/chat/sparkllm/)
    - [Nebula (Symbl.ai)](/docs/integrations/chat/symblai_nebula/)
    - [Tencent Hunyuan](/docs/integrations/chat/tencent_hunyuan/)
    - [Together](/docs/integrations/chat/together/)
    - [Tongyi Qwen](/docs/integrations/chat/tongyi/)
    - [Upstage](/docs/integrations/chat/upstage/)
    - [vectara](/docs/integrations/chat/vectara/)
    - [vLLM Chat](/docs/integrations/chat/vllm/)
    - [Volc Enging Maas](/docs/integrations/chat/volcengine_maas/)
    - [Chat Writer](/docs/integrations/chat/writer/)
    - [xAI](/docs/integrations/chat/xai/)
    - [Xinference](/docs/integrations/chat/xinference/)
    - [YandexGPT](/docs/integrations/chat/yandex/)
    - [ChatYI](/docs/integrations/chat/yi/)
    - [Yuan2.0](/docs/integrations/chat/yuan2/)
    - [ZHIPU AI](/docs/integrations/chat/zhipuai/)
  - [Retrievers](/docs/integrations/retrievers/)
    
    - [Retrievers](/docs/integrations/retrievers/)
    - [Activeloop Deep Memory](/docs/integrations/retrievers/activeloop/)
    - [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/)
    - [Arcee](/docs/integrations/retrievers/arcee/)
    - [Arxiv](/docs/integrations/retrievers/arxiv/)
    - [AskNews](/docs/integrations/retrievers/asknews/)
    - [Azure AI Search](/docs/integrations/retrievers/azure_ai_search/)
    - [Bedrock (Knowledge Bases)](/docs/integrations/retrievers/bedrock/)
    - [BM25](/docs/integrations/retrievers/bm25/)
    - [Box](/docs/integrations/retrievers/box/)
    - [BREEBS (Open Knowledge)](/docs/integrations/retrievers/breebs/)
    - [Chaindesk](/docs/integrations/retrievers/chaindesk/)
    - [ChatGPT plugin](/docs/integrations/retrievers/chatgpt-plugin/)
    - [Cognee](/docs/integrations/retrievers/cognee/)
    - [Cohere reranker](/docs/integrations/retrievers/cohere-reranker/)
    - [Cohere RAG](/docs/integrations/retrievers/cohere/)
    - [Contextual AI Reranker](/docs/integrations/retrievers/contextual/)
    - [Dappier](/docs/integrations/retrievers/dappier/)
    - [DocArray](/docs/integrations/retrievers/docarray_retriever/)
    - [Dria](/docs/integrations/retrievers/dria_index/)
    - [ElasticSearch BM25](/docs/integrations/retrievers/elastic_search_bm25/)
    - [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/)
    - [Embedchain](/docs/integrations/retrievers/embedchain/)
    - [FlashRank reranker](/docs/integrations/retrievers/flashrank-reranker/)
    - [Fleet AI Context](/docs/integrations/retrievers/fleet_context/)
    - [Galaxia](/docs/integrations/retrievers/galaxia-retriever/)
    - [Google Drive](/docs/integrations/retrievers/google_drive/)
    - [Google Vertex AI Search](/docs/integrations/retrievers/google_vertex_ai_search/)
    - [Graph RAG](/docs/integrations/retrievers/graph_rag/)
    - [IBM watsonx.ai](/docs/integrations/retrievers/ibm_watsonx_ranker/)
    - [JaguarDB Vector Database](/docs/integrations/retrievers/jaguar/)
    - [Kay.ai](/docs/integrations/retrievers/kay/)
    - [Kinetica Vectorstore based Retriever](/docs/integrations/retrievers/kinetica/)
    - [kNN](/docs/integrations/retrievers/knn/)
    - [LinkupSearchRetriever](/docs/integrations/retrievers/linkup_search/)
    - [LLMLingua Document Compressor](/docs/integrations/retrievers/llmlingua/)
    - [LOTR (Merger Retriever)](/docs/integrations/retrievers/merger_retriever/)
    - [Metal](/docs/integrations/retrievers/metal/)
    - [Milvus Hybrid Search](/docs/integrations/retrievers/milvus_hybrid_search/)
    - [NanoPQ (Product Quantization)](/docs/integrations/retrievers/nanopq/)
    - [needle](/docs/integrations/retrievers/needle/)
    - [Nimble](/docs/integrations/retrievers/nimble/)
    - [Outline](/docs/integrations/retrievers/outline/)
    - [Permit](/docs/integrations/retrievers/permit/)
    - [Pinecone Hybrid Search](/docs/integrations/retrievers/pinecone_hybrid_search/)
    - [PubMed](/docs/integrations/retrievers/pubmed/)
    - [Qdrant Sparse Vector](/docs/integrations/retrievers/qdrant-sparse/)
    - [RAGatouille](/docs/integrations/retrievers/ragatouille/)
    - [RePhraseQuery](/docs/integrations/retrievers/re_phrase/)
    - [Rememberizer](/docs/integrations/retrievers/rememberizer/)
    - [SEC filing](/docs/integrations/retrievers/sec_filings/)
    - [Self-querying retrievers](/docs/integrations/retrievers/self_query/)
    - [SVM](/docs/integrations/retrievers/svm/)
    - [TavilySearchAPI](/docs/integrations/retrievers/tavily/)
    - [TF-IDF](/docs/integrations/retrievers/tf_idf/)
    - [\*\*NeuralDB\*\*](/docs/integrations/retrievers/thirdai_neuraldb/)
    - [Vectorize](/docs/integrations/retrievers/vectorize/)
    - [Vespa](/docs/integrations/retrievers/vespa/)
    - [Wikipedia](/docs/integrations/retrievers/wikipedia/)
    - [You.com](/docs/integrations/retrievers/you-retriever/)
    - [Zep Cloud](/docs/integrations/retrievers/zep_cloud_memorystore/)
    - [Zep Open Source](/docs/integrations/retrievers/zep_memorystore/)
    - [Zilliz Cloud Pipeline](/docs/integrations/retrievers/zilliz_cloud_pipeline/)
    - [Zotero](/docs/integrations/retrievers/zotero/)
  - [Tools/Toolkits](/docs/integrations/tools/)
    
    - [Tools](/docs/integrations/tools/)
    - [ADS4GPTs](/docs/integrations/tools/ads4gpts/)
    - [AgentQL](/docs/integrations/tools/agentql/)
    - [AINetwork Toolkit](/docs/integrations/tools/ainetwork/)
    - [Alpha Vantage](/docs/integrations/tools/alpha_vantage/)
    - [Amadeus Toolkit](/docs/integrations/tools/amadeus/)
    - [Apify Actor](/docs/integrations/tools/apify_actors/)
    - [ArXiv](/docs/integrations/tools/arxiv/)
    - [AskNews](/docs/integrations/tools/asknews/)
    - [AWS Lambda](/docs/integrations/tools/awslambda/)
    - [Azure AI Services Toolkit](/docs/integrations/tools/azure_ai_services/)
    - [Azure Cognitive Services Toolkit](/docs/integrations/tools/azure_cognitive_services/)
    - [Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions/)
    - [Shell (bash)](/docs/integrations/tools/bash/)
    - [Bearly Code Interpreter](/docs/integrations/tools/bearly/)
    - [Bing Search](/docs/integrations/tools/bing_search/)
    - [Brave Search](/docs/integrations/tools/brave_search/)
    - [Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database/)
    - [CDP](/docs/integrations/tools/cdp_agentkit/)
    - [ChatGPT Plugins](/docs/integrations/tools/chatgpt_plugins/)
    - [ClickUp Toolkit](/docs/integrations/tools/clickup/)
    - [Cogniswitch Toolkit](/docs/integrations/tools/cogniswitch/)
    - [Connery Toolkit and Tools](/docs/integrations/tools/connery/)
    - [Dall-E Image Generator](/docs/integrations/tools/dalle_image_generator/)
    - [Dappier](/docs/integrations/tools/dappier/)
    - [Databricks Unity Catalog (UC)](/docs/integrations/tools/databricks/)
    - [DataForSEO](/docs/integrations/tools/dataforseo/)
    - [Dataherald](/docs/integrations/tools/dataherald/)
    - [DuckDuckGo Search](/docs/integrations/tools/ddg/)
    - [Discord](/docs/integrations/tools/discord/)
    - [E2B Data Analysis](/docs/integrations/tools/e2b_data_analysis/)
    - [Eden AI](/docs/integrations/tools/edenai_tools/)
    - [ElevenLabs Text2Speech](/docs/integrations/tools/eleven_labs_tts/)
    - [Exa Search](/docs/integrations/tools/exa_search/)
    - [File System](/docs/integrations/tools/filesystem/)
    - [FinancialDatasets Toolkit](/docs/integrations/tools/financial_datasets/)
    - [FMP Data](/docs/integrations/tools/fmp-data/)
    - [Github Toolkit](/docs/integrations/tools/github/)
    - [Gitlab Toolkit](/docs/integrations/tools/gitlab/)
    - [Gmail Toolkit](/docs/integrations/tools/gmail/)
    - [GOAT](/docs/integrations/tools/goat/)
    - [Golden Query](/docs/integrations/tools/golden_query/)
    - [Google Books](/docs/integrations/tools/google_books/)
    - [Google Calendar Toolkit](/docs/integrations/tools/google_calendar/)
    - [Google Cloud Text-to-Speech](/docs/integrations/tools/google_cloud_texttospeech/)
    - [Google Drive](/docs/integrations/tools/google_drive/)
    - [Google Finance](/docs/integrations/tools/google_finance/)
    - [Google Imagen](/docs/integrations/tools/google_imagen/)
    - [Google Jobs](/docs/integrations/tools/google_jobs/)
    - [Google Lens](/docs/integrations/tools/google_lens/)
    - [Google Places](/docs/integrations/tools/google_places/)
    - [Google Scholar](/docs/integrations/tools/google_scholar/)
    - [Google Search](/docs/integrations/tools/google_search/)
    - [Google Serper](/docs/integrations/tools/google_serper/)
    - [Google Trends](/docs/integrations/tools/google_trends/)
    - [Gradio](/docs/integrations/tools/gradio_tools/)
    - [GraphQL](/docs/integrations/tools/graphql/)
    - [HuggingFace Hub Tools](/docs/integrations/tools/huggingface_tools/)
    - [Human as a tool](/docs/integrations/tools/human_tools/)
    - [Hyperbrowser Browser Agent Tools](/docs/integrations/tools/hyperbrowser_browser_agent_tools/)
    - [Hyperbrowser Web Scraping Tools](/docs/integrations/tools/hyperbrowser_web_scraping_tools/)
    - [IBM watsonx.ai](/docs/integrations/tools/ibm_watsonx/)
    - [IFTTT WebHooks](/docs/integrations/tools/ifttt/)
    - [Infobip](/docs/integrations/tools/infobip/)
    - [Ionic Shopping Tool](/docs/integrations/tools/ionic_shopping/)
    - [Jenkins](/docs/integrations/tools/jenkins/)
    - [Jina Search](/docs/integrations/tools/jina_search/)
    - [Jira Toolkit](/docs/integrations/tools/jira/)
    - [JSON Toolkit](/docs/integrations/tools/json/)
    - [Lemon Agent](/docs/integrations/tools/lemonai/)
    - [LinkupSearchTool](/docs/integrations/tools/linkup_search/)
    - [Memgraph](/docs/integrations/tools/memgraph/)
    - [Memorize](/docs/integrations/tools/memorize/)
    - [Mojeek Search](/docs/integrations/tools/mojeek_search/)
    - [MultiOn Toolkit](/docs/integrations/tools/multion/)
    - [NASA Toolkit](/docs/integrations/tools/nasa/)
    - [Naver Search](/docs/integrations/tools/naver_search/)
    - [Nuclia Understanding](/docs/integrations/tools/nuclia/)
    - [NVIDIA Riva: ASR and TTS](/docs/integrations/tools/nvidia_riva/)
    - [Office365 Toolkit](/docs/integrations/tools/office365/)
    - [OpenAPI Toolkit](/docs/integrations/tools/openapi/)
    - [Natural Language API Toolkits](/docs/integrations/tools/openapi_nla/)
    - [OpenGradient](/docs/integrations/tools/opengradient_toolkit/)
    - [OpenWeatherMap](/docs/integrations/tools/openweathermap/)
    - [Oracle AI Vector Search: Generate Summary](/docs/integrations/tools/oracleai/)
    - [Oxylabs](/docs/integrations/tools/oxylabs/)
    - [Pandas Dataframe](/docs/integrations/tools/pandas/)
    - [Passio NutritionAI](/docs/integrations/tools/passio_nutrition_ai/)
    - [PaymanAI](/docs/integrations/tools/payman-tool/)
    - [Permit](/docs/integrations/tools/permit/)
    - [PlayWright Browser Toolkit](/docs/integrations/tools/playwright/)
    - [Polygon IO Toolkit and Tools](/docs/integrations/tools/polygon/)
    - [PowerBI Toolkit](/docs/integrations/tools/powerbi/)
    - [Prolog](/docs/integrations/tools/prolog_tool/)
    - [PubMed](/docs/integrations/tools/pubmed/)
    - [Python REPL](/docs/integrations/tools/python/)
    - [Reddit Search](/docs/integrations/tools/reddit_search/)
    - [Requests Toolkit](/docs/integrations/tools/requests/)
    - [Riza Code Interpreter](/docs/integrations/tools/riza/)
    - [Robocorp Toolkit](/docs/integrations/tools/robocorp/)
    - [Salesforce](/docs/integrations/tools/salesforce/)
    - [SceneXplain](/docs/integrations/tools/sceneXplain/)
    - [ScrapeGraph](/docs/integrations/tools/scrapegraph/)
    - [SearchApi](/docs/integrations/tools/searchapi/)
    - [SearxNG Search](/docs/integrations/tools/searx_search/)
    - [Semantic Scholar API Tool](/docs/integrations/tools/semanticscholar/)
    - [SerpAPI](/docs/integrations/tools/serpapi/)
    - [Slack Toolkit](/docs/integrations/tools/slack/)
    - [Spark SQL Toolkit](/docs/integrations/tools/spark_sql/)
    - [SQLDatabase Toolkit](/docs/integrations/tools/sql_database/)
    - [StackExchange](/docs/integrations/tools/stackexchange/)
    - [Steam Toolkit](/docs/integrations/tools/steam/)
    - [Stripe](/docs/integrations/tools/stripe/)
    - [Tableau](/docs/integrations/tools/tableau/)
    - [Taiga](/docs/integrations/tools/taiga/)
    - [Tavily Extract](/docs/integrations/tools/tavily_extract/)
    - [Tavily Search](/docs/integrations/tools/tavily_search/)
    - [Tilores](/docs/integrations/tools/tilores/)
    - [Twilio](/docs/integrations/tools/twilio/)
    - [Upstage](/docs/integrations/tools/upstage_groundedness_check/)
    - [Valthera](/docs/integrations/tools/valthera/)
    - [Wikidata](/docs/integrations/tools/wikidata/)
    - [Wikipedia](/docs/integrations/tools/wikipedia/)
    - [Wolfram Alpha](/docs/integrations/tools/wolfram_alpha/)
    - [Writer Tools](/docs/integrations/tools/writer/)
    - [Yahoo Finance News](/docs/integrations/tools/yahoo_finance_news/)
    - [You.com Search](/docs/integrations/tools/you/)
    - [YouTube](/docs/integrations/tools/youtube/)
    - [Zapier Natural Language Actions](/docs/integrations/tools/zapier/)
    - [ZenGuard AI](/docs/integrations/tools/zenguard/)
  - [Document loaders](/docs/integrations/document_loaders/)
    
    - [Document loaders](/docs/integrations/document_loaders/)
    - [acreom](/docs/integrations/document_loaders/acreom/)
    - [AgentQLLoader](/docs/integrations/document_loaders/agentql/)
    - [AirbyteLoader](/docs/integrations/document_loaders/airbyte/)
    - [Airbyte CDK (Deprecated)](/docs/integrations/document_loaders/airbyte_cdk/)
    - [Airbyte Gong (Deprecated)](/docs/integrations/document_loaders/airbyte_gong/)
    - [Airbyte Hubspot (Deprecated)](/docs/integrations/document_loaders/airbyte_hubspot/)
    - [Airbyte JSON (Deprecated)](/docs/integrations/document_loaders/airbyte_json/)
    - [Airbyte Salesforce (Deprecated)](/docs/integrations/document_loaders/airbyte_salesforce/)
    - [Airbyte Shopify (Deprecated)](/docs/integrations/document_loaders/airbyte_shopify/)
    - [Airbyte Stripe (Deprecated)](/docs/integrations/document_loaders/airbyte_stripe/)
    - [Airbyte Typeform (Deprecated)](/docs/integrations/document_loaders/airbyte_typeform/)
    - [Airbyte Zendesk Support (Deprecated)](/docs/integrations/document_loaders/airbyte_zendesk_support/)
    - [Airtable](/docs/integrations/document_loaders/airtable/)
    - [Alibaba Cloud MaxCompute](/docs/integrations/document_loaders/alibaba_cloud_maxcompute/)
    - [Amazon Textract](/docs/integrations/document_loaders/amazon_textract/)
    - [Apify Dataset](/docs/integrations/document_loaders/apify_dataset/)
    - [ArcGIS](/docs/integrations/document_loaders/arcgis/)
    - [ArxivLoader](/docs/integrations/document_loaders/arxiv/)
    - [AssemblyAI Audio Transcripts](/docs/integrations/document_loaders/assemblyai/)
    - [AstraDB](/docs/integrations/document_loaders/astradb/)
    - [Async Chromium](/docs/integrations/document_loaders/async_chromium/)
    - [AsyncHtml](/docs/integrations/document_loaders/async_html/)
    - [Athena](/docs/integrations/document_loaders/athena/)
    - [AWS S3 Directory](/docs/integrations/document_loaders/aws_s3_directory/)
    - [AWS S3 File](/docs/integrations/document_loaders/aws_s3_file/)
    - [AZLyrics](/docs/integrations/document_loaders/azlyrics/)
    - [Azure AI Data](/docs/integrations/document_loaders/azure_ai_data/)
    - [Azure Blob Storage Container](/docs/integrations/document_loaders/azure_blob_storage_container/)
    - [Azure Blob Storage File](/docs/integrations/document_loaders/azure_blob_storage_file/)
    - [Azure AI Document Intelligence](/docs/integrations/document_loaders/azure_document_intelligence/)
    - [BibTeX](/docs/integrations/document_loaders/bibtex/)
    - [BiliBili](/docs/integrations/document_loaders/bilibili/)
    - [Blackboard](/docs/integrations/document_loaders/blackboard/)
    - [Blockchain](/docs/integrations/document_loaders/blockchain/)
    - [Box](/docs/integrations/document_loaders/box/)
    - [Brave Search](/docs/integrations/document_loaders/brave_search/)
    - [Browserbase](/docs/integrations/document_loaders/browserbase/)
    - [Browserless](/docs/integrations/document_loaders/browserless/)
    - [BSHTMLLoader](/docs/integrations/document_loaders/bshtml/)
    - [Cassandra](/docs/integrations/document_loaders/cassandra/)
    - [ChatGPT Data](/docs/integrations/document_loaders/chatgpt_loader/)
    - [College Confidential](/docs/integrations/document_loaders/college_confidential/)
    - [Concurrent Loader](/docs/integrations/document_loaders/concurrent/)
    - [Confluence](/docs/integrations/document_loaders/confluence/)
    - [CoNLL-U](/docs/integrations/document_loaders/conll-u/)
    - [Copy Paste](/docs/integrations/document_loaders/copypaste/)
    - [Couchbase](/docs/integrations/document_loaders/couchbase/)
    - [CSV](/docs/integrations/document_loaders/csv/)
    - [Cube Semantic Layer](/docs/integrations/document_loaders/cube_semantic/)
    - [Datadog Logs](/docs/integrations/document_loaders/datadog_logs/)
    - [Dedoc](/docs/integrations/document_loaders/dedoc/)
    - [Diffbot](/docs/integrations/document_loaders/diffbot/)
    - [Discord](/docs/integrations/document_loaders/discord/)
    - [Docling](/docs/integrations/document_loaders/docling/)
    - [Docugami](/docs/integrations/document_loaders/docugami/)
    - [Docusaurus](/docs/integrations/document_loaders/docusaurus/)
    - [Dropbox](/docs/integrations/document_loaders/dropbox/)
    - [DuckDB](/docs/integrations/document_loaders/duckdb/)
    - [Email](/docs/integrations/document_loaders/email/)
    - [EPub](/docs/integrations/document_loaders/epub/)
    - [Etherscan](/docs/integrations/document_loaders/etherscan/)
    - [EverNote](/docs/integrations/document_loaders/evernote/)
    - [example\_data](/docs/integrations/document_loaders/example_data/example/)
    - [Facebook Chat](/docs/integrations/document_loaders/facebook_chat/)
    - [Fauna](/docs/integrations/document_loaders/fauna/)
    - [Figma](/docs/integrations/document_loaders/figma/)
    - [FireCrawl](/docs/integrations/document_loaders/firecrawl/)
    - [Geopandas](/docs/integrations/document_loaders/geopandas/)
    - [Git](/docs/integrations/document_loaders/git/)
    - [GitBook](/docs/integrations/document_loaders/gitbook/)
    - [GitHub](/docs/integrations/document_loaders/github/)
    - [Glue Catalog](/docs/integrations/document_loaders/glue_catalog/)
    - [Google AlloyDB for PostgreSQL](/docs/integrations/document_loaders/google_alloydb/)
    - [Google BigQuery](/docs/integrations/document_loaders/google_bigquery/)
    - [Google Bigtable](/docs/integrations/document_loaders/google_bigtable/)
    - [Google Cloud SQL for SQL server](/docs/integrations/document_loaders/google_cloud_sql_mssql/)
    - [Google Cloud SQL for MySQL](/docs/integrations/document_loaders/google_cloud_sql_mysql/)
    - [Google Cloud SQL for PostgreSQL](/docs/integrations/document_loaders/google_cloud_sql_pg/)
    - [Google Cloud Storage Directory](/docs/integrations/document_loaders/google_cloud_storage_directory/)
    - [Google Cloud Storage File](/docs/integrations/document_loaders/google_cloud_storage_file/)
    - [Google Firestore in Datastore Mode](/docs/integrations/document_loaders/google_datastore/)
    - [Google Drive](/docs/integrations/document_loaders/google_drive/)
    - [Google El Carro for Oracle Workloads](/docs/integrations/document_loaders/google_el_carro/)
    - [Google Firestore (Native Mode)](/docs/integrations/document_loaders/google_firestore/)
    - [Google Memorystore for Redis](/docs/integrations/document_loaders/google_memorystore_redis/)
    - [Google Spanner](/docs/integrations/document_loaders/google_spanner/)
    - [Google Speech-to-Text Audio Transcripts](/docs/integrations/document_loaders/google_speech_to_text/)
    - [Grobid](/docs/integrations/document_loaders/grobid/)
    - [Gutenberg](/docs/integrations/document_loaders/gutenberg/)
    - [Hacker News](/docs/integrations/document_loaders/hacker_news/)
    - [Huawei OBS Directory](/docs/integrations/document_loaders/huawei_obs_directory/)
    - [Huawei OBS File](/docs/integrations/document_loaders/huawei_obs_file/)
    - [HuggingFace dataset](/docs/integrations/document_loaders/hugging_face_dataset/)
    - [HyperbrowserLoader](/docs/integrations/document_loaders/hyperbrowser/)
    - [iFixit](/docs/integrations/document_loaders/ifixit/)
    - [Images](/docs/integrations/document_loaders/image/)
    - [Image captions](/docs/integrations/document_loaders/image_captions/)
    - [IMSDb](/docs/integrations/document_loaders/imsdb/)
    - [Iugu](/docs/integrations/document_loaders/iugu/)
    - [Joplin](/docs/integrations/document_loaders/joplin/)
    - [JSONLoader](/docs/integrations/document_loaders/json/)
    - [Jupyter Notebook](/docs/integrations/document_loaders/jupyter_notebook/)
    - [Kinetica](/docs/integrations/document_loaders/kinetica/)
    - [lakeFS](/docs/integrations/document_loaders/lakefs/)
    - [LangSmith](/docs/integrations/document_loaders/langsmith/)
    - [LarkSuite (FeiShu)](/docs/integrations/document_loaders/larksuite/)
    - [LLM Sherpa](/docs/integrations/document_loaders/llmsherpa/)
    - [Mastodon](/docs/integrations/document_loaders/mastodon/)
    - [MathPixPDFLoader](/docs/integrations/document_loaders/mathpix/)
    - [MediaWiki Dump](/docs/integrations/document_loaders/mediawikidump/)
    - [Merge Documents Loader](/docs/integrations/document_loaders/merge_doc/)
    - [mhtml](/docs/integrations/document_loaders/mhtml/)
    - [Microsoft Excel](/docs/integrations/document_loaders/microsoft_excel/)
    - [Microsoft OneDrive](/docs/integrations/document_loaders/microsoft_onedrive/)
    - [Microsoft OneNote](/docs/integrations/document_loaders/microsoft_onenote/)
    - [Microsoft PowerPoint](/docs/integrations/document_loaders/microsoft_powerpoint/)
    - [Microsoft SharePoint](/docs/integrations/document_loaders/microsoft_sharepoint/)
    - [Microsoft Word](/docs/integrations/document_loaders/microsoft_word/)
    - [Near Blockchain](/docs/integrations/document_loaders/mintbase/)
    - [Modern Treasury](/docs/integrations/document_loaders/modern_treasury/)
    - [MongoDB](/docs/integrations/document_loaders/mongodb/)
    - [Needle Document Loader](/docs/integrations/document_loaders/needle/)
    - [News URL](/docs/integrations/document_loaders/news/)
    - [Notion DB 2/2](/docs/integrations/document_loaders/notion/)
    - [Nuclia](/docs/integrations/document_loaders/nuclia/)
    - [Obsidian](/docs/integrations/document_loaders/obsidian/)
    - [Open Document Format (ODT)](/docs/integrations/document_loaders/odt/)
    - [Open City Data](/docs/integrations/document_loaders/open_city_data/)
    - [Oracle Autonomous Database](/docs/integrations/document_loaders/oracleadb_loader/)
    - [Oracle AI Vector Search: Document Processing](/docs/integrations/document_loaders/oracleai/)
    - [Org-mode](/docs/integrations/document_loaders/org_mode/)
    - [Pandas DataFrame](/docs/integrations/document_loaders/pandas_dataframe/)
    - [parsers](/docs/integrations/document_loaders/parsers/azure_openai_whisper_parser/)
    - [PDFMinerLoader](/docs/integrations/document_loaders/pdfminer/)
    - [PDFPlumber](/docs/integrations/document_loaders/pdfplumber/)
    - [Pebblo Safe DocumentLoader](/docs/integrations/document_loaders/pebblo/)
    - [Polars DataFrame](/docs/integrations/document_loaders/polars_dataframe/)
    - [Dell PowerScale Document Loader](/docs/integrations/document_loaders/powerscale/)
    - [Psychic](/docs/integrations/document_loaders/psychic/)
    - [PubMed](/docs/integrations/document_loaders/pubmed/)
    - [PullMdLoader](/docs/integrations/document_loaders/pull_md/)
    - [PyMuPDFLoader](/docs/integrations/document_loaders/pymupdf/)
    - [PyMuPDF4LLM](/docs/integrations/document_loaders/pymupdf4llm/)
    - [PyPDFDirectoryLoader](/docs/integrations/document_loaders/pypdfdirectory/)
    - [PyPDFium2Loader](/docs/integrations/document_loaders/pypdfium2/)
    - [PyPDFLoader](/docs/integrations/document_loaders/pypdfloader/)
    - [PySpark](/docs/integrations/document_loaders/pyspark_dataframe/)
    - [Quip](/docs/integrations/document_loaders/quip/)
    - [ReadTheDocs Documentation](/docs/integrations/document_loaders/readthedocs_documentation/)
    - [Recursive URL](/docs/integrations/document_loaders/recursive_url/)
    - [Reddit](/docs/integrations/document_loaders/reddit/)
    - [Roam](/docs/integrations/document_loaders/roam/)
    - [Rockset](/docs/integrations/document_loaders/rockset/)
    - [rspace](/docs/integrations/document_loaders/rspace/)
    - [RSS Feeds](/docs/integrations/document_loaders/rss/)
    - [RST](/docs/integrations/document_loaders/rst/)
    - [scrapfly](/docs/integrations/document_loaders/scrapfly/)
    - [ScrapingAnt](/docs/integrations/document_loaders/scrapingant/)
    - [SingleStore](/docs/integrations/document_loaders/singlestore/)
    - [Sitemap](/docs/integrations/document_loaders/sitemap/)
    - [Slack](/docs/integrations/document_loaders/slack/)
    - [Snowflake](/docs/integrations/document_loaders/snowflake/)
    - [Source Code](/docs/integrations/document_loaders/source_code/)
    - [Spider](/docs/integrations/document_loaders/spider/)
    - [Spreedly](/docs/integrations/document_loaders/spreedly/)
    - [Stripe](/docs/integrations/document_loaders/stripe/)
    - [Subtitle](/docs/integrations/document_loaders/subtitle/)
    - [SurrealDB](/docs/integrations/document_loaders/surrealdb/)
    - [Telegram](/docs/integrations/document_loaders/telegram/)
    - [Tencent COS Directory](/docs/integrations/document_loaders/tencent_cos_directory/)
    - [Tencent COS File](/docs/integrations/document_loaders/tencent_cos_file/)
    - [TensorFlow Datasets](/docs/integrations/document_loaders/tensorflow_datasets/)
    - [TiDB](/docs/integrations/document_loaders/tidb/)
    - [2Markdown](/docs/integrations/document_loaders/tomarkdown/)
    - [TOML](/docs/integrations/document_loaders/toml/)
    - [Trello](/docs/integrations/document_loaders/trello/)
    - [TSV](/docs/integrations/document_loaders/tsv/)
    - [Twitter](/docs/integrations/document_loaders/twitter/)
    - [Unstructured](/docs/integrations/document_loaders/unstructured_file/)
    - [UnstructuredMarkdownLoader](/docs/integrations/document_loaders/unstructured_markdown/)
    - [UnstructuredPDFLoader](/docs/integrations/document_loaders/unstructured_pdfloader/)
    - [Upstage](/docs/integrations/document_loaders/upstage/)
    - [URL](/docs/integrations/document_loaders/url/)
    - [Vsdx](/docs/integrations/document_loaders/vsdx/)
    - [Weather](/docs/integrations/document_loaders/weather/)
    - [WebBaseLoader](/docs/integrations/document_loaders/web_base/)
    - [WhatsApp Chat](/docs/integrations/document_loaders/whatsapp_chat/)
    - [Wikipedia](/docs/integrations/document_loaders/wikipedia/)
    - [UnstructuredXMLLoader](/docs/integrations/document_loaders/xml/)
    - [Xorbits Pandas DataFrame](/docs/integrations/document_loaders/xorbits/)
    - [YouTube audio](/docs/integrations/document_loaders/youtube_audio/)
    - [YouTube transcripts](/docs/integrations/document_loaders/youtube_transcript/)
    - [YoutubeLoaderDL](/docs/integrations/document_loaders/yt_dlp/)
    - [Yuque](/docs/integrations/document_loaders/yuque/)
    - [ZeroxPDFLoader](/docs/integrations/document_loaders/zeroxpdfloader/)
  - [Vector stores](/docs/integrations/vectorstores/)
    
    - [Vector stores](/docs/integrations/vectorstores/)
    - [Activeloop Deep Lake](/docs/integrations/vectorstores/activeloop_deeplake/)
    - [Aerospike](/docs/integrations/vectorstores/aerospike/)
    - [Alibaba Cloud OpenSearch](/docs/integrations/vectorstores/alibabacloud_opensearch/)
    - [AnalyticDB](/docs/integrations/vectorstores/analyticdb/)
    - [Annoy](/docs/integrations/vectorstores/annoy/)
    - [Apache Doris](/docs/integrations/vectorstores/apache_doris/)
    - [ApertureDB](/docs/integrations/vectorstores/aperturedb/)
    - [Astra DB Vector Store](/docs/integrations/vectorstores/astradb/)
    - [Atlas](/docs/integrations/vectorstores/atlas/)
    - [AwaDB](/docs/integrations/vectorstores/awadb/)
    - [Azure Cosmos DB Mongo vCore](/docs/integrations/vectorstores/azure_cosmos_db/)
    - [Azure Cosmos DB No SQL](/docs/integrations/vectorstores/azure_cosmos_db_no_sql/)
    - [Azure AI Search](/docs/integrations/vectorstores/azuresearch/)
    - [Bagel](/docs/integrations/vectorstores/bagel/)
    - [BagelDB](/docs/integrations/vectorstores/bageldb/)
    - [Baidu Cloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/baiducloud_vector_search/)
    - [Baidu VectorDB](/docs/integrations/vectorstores/baiduvectordb/)
    - [Apache Cassandra](/docs/integrations/vectorstores/cassandra/)
    - [Chroma](/docs/integrations/vectorstores/chroma/)
    - [Clarifai](/docs/integrations/vectorstores/clarifai/)
    - [ClickHouse](/docs/integrations/vectorstores/clickhouse/)
    - [CloudflareVectorize](/docs/integrations/vectorstores/cloudflare_vectorize/)
    - [Couchbase](/docs/integrations/vectorstores/couchbase/)
    - [DashVector](/docs/integrations/vectorstores/dashvector/)
    - [Databricks](/docs/integrations/vectorstores/databricks_vector_search/)
    - [DingoDB](/docs/integrations/vectorstores/dingo/)
    - [DocArray HnswSearch](/docs/integrations/vectorstores/docarray_hnsw/)
    - [DocArray InMemorySearch](/docs/integrations/vectorstores/docarray_in_memory/)
    - [Amazon Document DB](/docs/integrations/vectorstores/documentdb/)
    - [DuckDB](/docs/integrations/vectorstores/duckdb/)
    - [China Mobile ECloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/ecloud_vector_search/)
    - [Elasticsearch](/docs/integrations/vectorstores/elasticsearch/)
    - [Epsilla](/docs/integrations/vectorstores/epsilla/)
    - [Faiss](/docs/integrations/vectorstores/faiss/)
    - [Faiss (Async)](/docs/integrations/vectorstores/faiss_async/)
    - [FalkorDBVectorStore](/docs/integrations/vectorstores/falkordbvector/)
    - [Google AlloyDB for PostgreSQL](/docs/integrations/vectorstores/google_alloydb/)
    - [Google BigQuery Vector Search](/docs/integrations/vectorstores/google_bigquery_vector_search/)
    - [Google Cloud SQL for MySQL](/docs/integrations/vectorstores/google_cloud_sql_mysql/)
    - [Google Cloud SQL for PostgreSQL](/docs/integrations/vectorstores/google_cloud_sql_pg/)
    - [Firestore](/docs/integrations/vectorstores/google_firestore/)
    - [Google Memorystore for Redis](/docs/integrations/vectorstores/google_memorystore_redis/)
    - [Google Spanner](/docs/integrations/vectorstores/google_spanner/)
    - [Google Vertex AI Feature Store](/docs/integrations/vectorstores/google_vertex_ai_feature_store/)
    - [Google Vertex AI Vector Search](/docs/integrations/vectorstores/google_vertex_ai_vector_search/)
    - [Hippo](/docs/integrations/vectorstores/hippo/)
    - [Hologres](/docs/integrations/vectorstores/hologres/)
    - [Infinispan](/docs/integrations/vectorstores/infinispanvs/)
    - [Jaguar Vector Database](/docs/integrations/vectorstores/jaguar/)
    - [KDB.AI](/docs/integrations/vectorstores/kdbai/)
    - [Kinetica](/docs/integrations/vectorstores/kinetica/)
    - [LanceDB](/docs/integrations/vectorstores/lancedb/)
    - [Lantern](/docs/integrations/vectorstores/lantern/)
    - [Lindorm](/docs/integrations/vectorstores/lindorm/)
    - [LLMRails](/docs/integrations/vectorstores/llm_rails/)
    - [ManticoreSearch VectorStore](/docs/integrations/vectorstores/manticore_search/)
    - [MariaDB](/docs/integrations/vectorstores/mariadb/)
    - [Marqo](/docs/integrations/vectorstores/marqo/)
    - [Meilisearch](/docs/integrations/vectorstores/meilisearch/)
    - [Amazon MemoryDB](/docs/integrations/vectorstores/memorydb/)
    - [Milvus](/docs/integrations/vectorstores/milvus/)
    - [Momento Vector Index (MVI)](/docs/integrations/vectorstores/momento_vector_index/)
    - [MongoDB Atlas](/docs/integrations/vectorstores/mongodb_atlas/)
    - [MyScale](/docs/integrations/vectorstores/myscale/)
    - [Neo4j Vector Index](/docs/integrations/vectorstores/neo4jvector/)
    - [NucliaDB](/docs/integrations/vectorstores/nucliadb/)
    - [Oceanbase](/docs/integrations/vectorstores/oceanbase/)
    - [openGauss](/docs/integrations/vectorstores/opengauss/)
    - [OpenSearch](/docs/integrations/vectorstores/opensearch/)
    - [Oracle AI Vector Search: Vector Store](/docs/integrations/vectorstores/oracle/)
    - [Pathway](/docs/integrations/vectorstores/pathway/)
    - [Postgres Embedding](/docs/integrations/vectorstores/pgembedding/)
    - [PGVecto.rs](/docs/integrations/vectorstores/pgvecto_rs/)
    - [PGVector](/docs/integrations/vectorstores/pgvector/)
    - [Pinecone](/docs/integrations/vectorstores/pinecone/)
    - [Qdrant](/docs/integrations/vectorstores/qdrant/)
    - [Redis](/docs/integrations/vectorstores/redis/)
    - [Relyt](/docs/integrations/vectorstores/relyt/)
    - [Rockset](/docs/integrations/vectorstores/rockset/)
    - [SAP HANA Cloud Vector Engine](/docs/integrations/vectorstores/sap_hanavector/)
    - [ScaNN](/docs/integrations/vectorstores/scann/)
    - [SemaDB](/docs/integrations/vectorstores/semadb/)
    - [SingleStore](/docs/integrations/vectorstores/singlestore/)
    - [scikit-learn](/docs/integrations/vectorstores/sklearn/)
    - [SQLiteVec](/docs/integrations/vectorstores/sqlitevec/)
    - [SQLite-VSS](/docs/integrations/vectorstores/sqlitevss/)
    - [SQLServer](/docs/integrations/vectorstores/sqlserver/)
    - [StarRocks](/docs/integrations/vectorstores/starrocks/)
    - [Supabase (Postgres)](/docs/integrations/vectorstores/supabase/)
    - [SurrealDB](/docs/integrations/vectorstores/surrealdb/)
    - [Tablestore](/docs/integrations/vectorstores/tablestore/)
    - [Tair](/docs/integrations/vectorstores/tair/)
    - [Tencent Cloud VectorDB](/docs/integrations/vectorstores/tencentvectordb/)
    - [ThirdAI NeuralDB](/docs/integrations/vectorstores/thirdai_neuraldb/)
    - [TiDB Vector](/docs/integrations/vectorstores/tidb_vector/)
    - [Tigris](/docs/integrations/vectorstores/tigris/)
    - [TileDB](/docs/integrations/vectorstores/tiledb/)
    - [Timescale Vector (Postgres)](/docs/integrations/vectorstores/timescalevector/)
    - [Typesense](/docs/integrations/vectorstores/typesense/)
    - [Upstash Vector](/docs/integrations/vectorstores/upstash/)
    - [USearch](/docs/integrations/vectorstores/usearch/)
    - [Vald](/docs/integrations/vectorstores/vald/)
    - [VDMS](/docs/integrations/vectorstores/vdms/)
    - [Vearch](/docs/integrations/vectorstores/vearch/)
    - [Vectara](/docs/integrations/vectorstores/vectara/)
    - [Vespa](/docs/integrations/vectorstores/vespa/)
    - [viking DB](/docs/integrations/vectorstores/vikingdb/)
    - [vlite](/docs/integrations/vectorstores/vlite/)
    - [Weaviate](/docs/integrations/vectorstores/weaviate/)
    - [Xata](/docs/integrations/vectorstores/xata/)
    - [YDB](/docs/integrations/vectorstores/ydb/)
    - [Yellowbrick](/docs/integrations/vectorstores/yellowbrick/)
    - [Zep](/docs/integrations/vectorstores/zep/)
    - [Zep Cloud](/docs/integrations/vectorstores/zep_cloud/)
    - [Zilliz](/docs/integrations/vectorstores/zilliz/)
  - [Embedding models](/docs/integrations/text_embedding/)
    
    - [Embedding models](/docs/integrations/text_embedding/)
    - [AI21](/docs/integrations/text_embedding/ai21/)
    - [Aleph Alpha](/docs/integrations/text_embedding/aleph_alpha/)
    - [Anyscale](/docs/integrations/text_embedding/anyscale/)
    - [ascend](/docs/integrations/text_embedding/ascend/)
    - [AwaDB](/docs/integrations/text_embedding/awadb/)
    - [AzureOpenAI](/docs/integrations/text_embedding/azureopenai/)
    - [Baichuan Text Embeddings](/docs/integrations/text_embedding/baichuan/)
    - [Baidu Qianfan](/docs/integrations/text_embedding/baidu_qianfan_endpoint/)
    - [Bedrock](/docs/integrations/text_embedding/bedrock/)
    - [BGE on Hugging Face](/docs/integrations/text_embedding/bge_huggingface/)
    - [Bookend AI](/docs/integrations/text_embedding/bookend/)
    - [Clarifai](/docs/integrations/text_embedding/clarifai/)
    - [Cloudflare Workers AI](/docs/integrations/text_embedding/cloudflare_workersai/)
    - [Clova Embeddings](/docs/integrations/text_embedding/clova/)
    - [Cohere](/docs/integrations/text_embedding/cohere/)
    - [DashScope](/docs/integrations/text_embedding/dashscope/)
    - [Databricks](/docs/integrations/text_embedding/databricks/)
    - [DeepInfra](/docs/integrations/text_embedding/deepinfra/)
    - [EDEN AI](/docs/integrations/text_embedding/edenai/)
    - [Elasticsearch](/docs/integrations/text_embedding/elasticsearch/)
    - [Embaas](/docs/integrations/text_embedding/embaas/)
    - [ERNIE](/docs/integrations/text_embedding/ernie/)
    - [Fake Embeddings](/docs/integrations/text_embedding/fake/)
    - [FastEmbed by Qdrant](/docs/integrations/text_embedding/fastembed/)
    - [Fireworks](/docs/integrations/text_embedding/fireworks/)
    - [GigaChat](/docs/integrations/text_embedding/gigachat/)
    - [Google Generative AI Embeddings](/docs/integrations/text_embedding/google_generative_ai/)
    - [Google Vertex AI](/docs/integrations/text_embedding/google_vertex_ai_palm/)
    - [GPT4All](/docs/integrations/text_embedding/gpt4all/)
    - [Gradient](/docs/integrations/text_embedding/gradient/)
    - [Hugging Face](/docs/integrations/text_embedding/huggingfacehub/)
    - [IBM watsonx.ai](/docs/integrations/text_embedding/ibm_watsonx/)
    - [Infinity](/docs/integrations/text_embedding/infinity/)
    - [Instruct Embeddings on Hugging Face](/docs/integrations/text_embedding/instruct_embeddings/)
    - [IPEX-LLM: Local BGE Embeddings on Intel CPU](/docs/integrations/text_embedding/ipex_llm/)
    - [IPEX-LLM: Local BGE Embeddings on Intel GPU](/docs/integrations/text_embedding/ipex_llm_gpu/)
    - [IntelÂ® Extension for Transformers Quantized Text Embeddings](/docs/integrations/text_embedding/itrex/)
    - [Jina](/docs/integrations/text_embedding/jina/)
    - [John Snow Labs](/docs/integrations/text_embedding/johnsnowlabs_embedding/)
    - [LASER Language-Agnostic SEntence Representations Embeddings by Meta AI](/docs/integrations/text_embedding/laser/)
    - [Lindorm](/docs/integrations/text_embedding/lindorm/)
    - [Llama.cpp](/docs/integrations/text_embedding/llamacpp/)
    - [llamafile](/docs/integrations/text_embedding/llamafile/)
    - [LLMRails](/docs/integrations/text_embedding/llm_rails/)
    - [LocalAI](/docs/integrations/text_embedding/localai/)
    - [MiniMax](/docs/integrations/text_embedding/minimax/)
    - [MistralAI](/docs/integrations/text_embedding/mistralai/)
    - [model2vec](/docs/integrations/text_embedding/model2vec/)
    - [ModelScope](/docs/integrations/text_embedding/modelscope_embedding/)
    - [MosaicML](/docs/integrations/text_embedding/mosaicml/)
    - [Naver](/docs/integrations/text_embedding/naver/)
    - [Netmind](/docs/integrations/text_embedding/netmind/)
    - [NLP Cloud](/docs/integrations/text_embedding/nlp_cloud/)
    - [Nomic](/docs/integrations/text_embedding/nomic/)
    - [NVIDIA NIMs](/docs/integrations/text_embedding/nvidia_ai_endpoints/)
    - [Oracle Cloud Infrastructure Generative AI](/docs/integrations/text_embedding/oci_generative_ai/)
    - [Ollama](/docs/integrations/text_embedding/ollama/)
    - [OpenClip](/docs/integrations/text_embedding/open_clip/)
    - [OpenAI](/docs/integrations/text_embedding/openai/)
    - [OpenVINO](/docs/integrations/text_embedding/openvino/)
    - [Embedding Documents using Optimized and Quantized Embedders](/docs/integrations/text_embedding/optimum_intel/)
    - [Oracle AI Vector Search: Generate Embeddings](/docs/integrations/text_embedding/oracleai/)
    - [OVHcloud](/docs/integrations/text_embedding/ovhcloud/)
    - [Pinecone Embeddings](/docs/integrations/text_embedding/pinecone/)
    - [PredictionGuardEmbeddings](/docs/integrations/text_embedding/predictionguard/)
    - [PremAI](/docs/integrations/text_embedding/premai/)
    - [SageMaker](/docs/integrations/text_embedding/sagemaker-endpoint/)
    - [SambaNovaCloud](/docs/integrations/text_embedding/sambanova/)
    - [SambaStudio](/docs/integrations/text_embedding/sambastudio/)
    - [Self Hosted](/docs/integrations/text_embedding/self-hosted/)
    - [Sentence Transformers on Hugging Face](/docs/integrations/text_embedding/sentence_transformers/)
    - [Solar](/docs/integrations/text_embedding/solar/)
    - [SpaCy](/docs/integrations/text_embedding/spacy_embedding/)
    - [SparkLLM Text Embeddings](/docs/integrations/text_embedding/sparkllm/)
    - [TensorFlow Hub](/docs/integrations/text_embedding/tensorflowhub/)
    - [Text Embeddings Inference](/docs/integrations/text_embedding/text_embeddings_inference/)
    - [TextEmbed - Embedding Inference Server](/docs/integrations/text_embedding/textembed/)
    - [Titan Takeoff](/docs/integrations/text_embedding/titan_takeoff/)
    - [Together AI](/docs/integrations/text_embedding/together/)
    - [Upstage](/docs/integrations/text_embedding/upstage/)
    - [Volc Engine](/docs/integrations/text_embedding/volcengine/)
    - [Voyage AI](/docs/integrations/text_embedding/voyageai/)
    - [Xorbits inference (Xinference)](/docs/integrations/text_embedding/xinference/)
    - [YandexGPT](/docs/integrations/text_embedding/yandex/)
    - [ZhipuAI](/docs/integrations/text_embedding/zhipuai/)
  - [Other](/docs/integrations/llms/)

<!--THE END-->

- [Components](/docs/integrations/components/)
- Tools/Toolkits


# Tools

[Tools](/docs/concepts/tools/) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.

A [toolkit](/docs/concepts/tools/#toolkits) is a collection of tools meant to be used together.

info

If you'd like to write your own tool, see [this how-to](/docs/how_to/custom_tools/). If you'd like to contribute an integration, see [Contributing integrations](/docs/contributing/how_to/integrations/).

## Search[â€‹](#search "Direct link to Search")

The following table shows tools that execute online searches in some shape or form:

| Tool/Toolkit                                             | Free/Paid                    | Return Data                                           |
|----------------------------------------------------------|------------------------------|-------------------------------------------------------|
| [Bing Search](/docs/integrations/tools/bing_search/)     | Paid                         | URL, Snippet, Title                                   |
| [Brave Search](/docs/integrations/tools/brave_search/)   | Free                         | URL, Snippet, Title                                   |
| [DuckDuckgoSearch](/docs/integrations/tools/ddg/)        | Free                         | URL, Snippet, Title                                   |
| [Exa Search](/docs/integrations/tools/exa_search/)       | 1000 free searches/month     | URL, Author, Title, Published Date                    |
| [Google Search](/docs/integrations/tools/google_search/) | Paid                         | URL, Snippet, Title                                   |
| [Google Serper](/docs/integrations/tools/google_serper/) | Free                         | URL, Snippet, Title, Search Rank, Site Links          |
| [Jina Search](/docs/integrations/tools/jina_search/)     | 1M Response Tokens Free      | URL, Snippet, Title, Page Content                     |
| [Mojeek Search](/docs/integrations/tools/mojeek_search/) | Paid                         | URL, Snippet, Title                                   |
| [SearchApi](/docs/integrations/tools/searchapi/)         | 100 Free Searches on Sign Up | URL, Snippet, Title, Search Rank, Site Links, Authors |
| [SearxNG Search](/docs/integrations/tools/searx_search/) | Free                         | URL, Snippet, Title, Category                         |
| [SerpAPI](/docs/integrations/tools/serpapi/)             | 100 Free Searches/Month      | Answer                                                |
| [Tavily Search](/docs/integrations/tools/tavily_search/) | 1000 free searches/month     | URL, Content, Title, Images, Answer                   |
| [You.com Search](/docs/integrations/tools/you/)          | Free for 60 days             | URL, Title, Page Content                              |

## Code Interpreter[â€‹](#code-interpreter "Direct link to Code Interpreter")

The following table shows tools that can be used as code interpreters:

| Tool/Toolkit                                                                              | Supported Languages           | Sandbox Lifetime    | Supports File Uploads | Return Types | Supports Self-Hosting |
|-------------------------------------------------------------------------------------------|-------------------------------|---------------------|-----------------------|--------------|-----------------------|
| [Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions/) | Python                        | 1 Hour              | âœ…                     | Text, Images | âŒ                     |
| [Bearly Code Interpreter](/docs/integrations/tools/bearly/)                               | Python                        | Resets on Execution | âœ…                     | Text         | âŒ                     |
| [Riza Code Interpreter](/docs/integrations/tools/riza/)                                   | Python, JavaScript, PHP, Ruby | Resets on Execution | âœ…                     | Text         | âœ…                     |

## Productivity[â€‹](#productivity "Direct link to Productivity")

The following table shows tools that can be used to automate tasks in productivity tools:

| Tool/Toolkit                                             | Pricing                                                                                                |
|----------------------------------------------------------|--------------------------------------------------------------------------------------------------------|
| [Github Toolkit](/docs/integrations/tools/github/)       | Free                                                                                                   |
| [Gitlab Toolkit](/docs/integrations/tools/gitlab/)       | Free for personal project                                                                              |
| [Gmail Toolkit](/docs/integrations/tools/gmail/)         | Free, with limit of 250 quota units per user per second                                                |
| [Infobip Tool](/docs/integrations/tools/infobip/)        | Free trial, with variable pricing after                                                                |
| [Jira Toolkit](/docs/integrations/tools/jira/)           | Free, with [rate limits](https://developer.atlassian.com/cloud/jira/platform/rate-limiting/)           |
| [Office365 Toolkit](/docs/integrations/tools/office365/) | Free with Office365, includes [rate limits](https://learn.microsoft.com/en-us/graph/throttling-limits) |
| [Slack Toolkit](/docs/integrations/tools/slack/)         | Free                                                                                                   |
| [Twilio Tool](/docs/integrations/tools/twilio/)          | Free trial, with [pay-as-you-go pricing](https://www.twilio.com/en-us/pricing) after                   |

## Web Browsing[â€‹](#web-browsing "Direct link to Web Browsing")

The following table shows tools that can be used to automate tasks in web browsers:

| Tool/Toolkit                                                                                   | Pricing                                                     | Supports Interacting with the Browser |
|------------------------------------------------------------------------------------------------|-------------------------------------------------------------|---------------------------------------|
| [AgentQL Toolkit](/docs/integrations/tools/agentql/)                                           | Free trial, with pay-as-you-go and flat rate plans after    | âœ…                                     |
| [Hyperbrowser Browser Agent Tools](/docs/integrations/tools/hyperbrowser_browser_agent_tools/) | Free trial, with flat rate plans and pre-paid credits after | âœ…                                     |
| [Hyperbrowser Web Scraping Tools](/docs/integrations/tools/hyperbrowser_web_scraping_tools/)   | Free trial, with flat rate plans and pre-paid credits after | âŒ                                     |
| [MultiOn Toolkit](/docs/integrations/tools/multion/)                                           | 40 free requests/day                                        | âœ…                                     |
| [PlayWright Browser Toolkit](/docs/integrations/tools/playwright/)                             | Free                                                        | âœ…                                     |
| [Requests Toolkit](/docs/integrations/tools/requests/)                                         | Free                                                        | âŒ                                     |

## Database[â€‹](#database "Direct link to Database")

The following table shows tools that can be used to automate tasks in databases:

| Tool/Toolkit                                                               | Allowed Operations              |
|----------------------------------------------------------------------------|---------------------------------|
| [Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database/) | SELECT and schema introspection |
| [SQLDatabase Toolkit](/docs/integrations/tools/sql_database/)              | Any SQL operation               |
| [Spark SQL Toolkit](/docs/integrations/tools/spark_sql/)                   | Any SQL operation               |

## Finance[â€‹](#finance "Direct link to Finance")

The following table shows tools that can be used to execute financial transactions such as payments, purchases, and more:

| Tool/Toolkit                           | Pricing | Capabilities                                                                      |
|----------------------------------------|---------|-----------------------------------------------------------------------------------|
| [GOAT](/docs/integrations/tools/goat/) | Free    | Create and receive payments, purchase physical goods, make investments, and more. |

## All tools[â€‹](#all-tools "Direct link to All tools")

| Name                                                                                          | Description                                                                  |
|-----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| [ADS4GPTs](/docs/integrations/tools/ads4gpts)                                                 | Integrate AI native advertising into your Agentic application.               |
| [AgentQL](/docs/integrations/tools/agentql)                                                   | AgentQL tools provides web interaction and structured data extraction...     |
| [AINetwork Toolkit](/docs/integrations/tools/ainetwork)                                       | AI Network is a layer 1 blockchain designed to accommodate large-scal...     |
| [Alpha Vantage](/docs/integrations/tools/alpha_vantage)                                       | Alpha Vantage Alpha Vantage provides realtime and historical financia...     |
| [Amadeus Toolkit](/docs/integrations/tools/amadeus)                                           | This notebook walks you through connecting LangChain to the Amadeus t...     |
| [Apify Actor](/docs/integrations/tools/apify_actors)                                          | Apify Actors are cloud programs designed for a wide range of web scra...     |
| [ArXiv](/docs/integrations/tools/arxiv)                                                       | This notebook goes over how to use the arxiv tool with an agent.             |
| [AskNews](/docs/integrations/tools/asknews)                                                   | AskNews infuses any LLM with the latest global news (or historical ne...     |
| [AWS Lambda](/docs/integrations/tools/awslambda)                                              | Amazon AWS Lambda is a serverless computing service provided by Amazo...     |
| [Azure AI Services Toolkit](/docs/integrations/tools/azure_ai_services)                       | This toolkit is used to interact with the Azure AI Services API to ac...     |
| [Azure Cognitive Services Toolkit](/docs/integrations/tools/azure_cognitive_services)         | This toolkit is used to interact with the Azure Cognitive Services AP...     |
| [Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions)      | Azure Container Apps dynamic sessions provides a secure and scalable ...     |
| [Shell (bash)](/docs/integrations/tools/bash)                                                 | Giving agents access to the shell is powerful (though risky outside a...     |
| [Bearly Code Interpreter](/docs/integrations/tools/bearly)                                    | Bearly Code Interpreter allows for remote execution of code. This mak...     |
| [Bing Search](/docs/integrations/tools/bing_search)                                           | Bing Search is an Azure service and enables safe, ad-free, location-a...     |
| [Brave Search](/docs/integrations/tools/brave_search)                                         | This notebook goes over how to use the Brave Search tool.                    |
| [Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database)                     | Apache CassandraÂ® is a widely used database for storing transactional...     |
| [CDP](/docs/integrations/tools/cdp_agentkit)                                                  | The CDP Agentkit toolkit contains tools that enable an LLM agent to i...     |
| [ChatGPT Plugins](/docs/integrations/tools/chatgpt_plugins)                                   | OpenAI has deprecated plugins.                                               |
| [ClickUp Toolkit](/docs/integrations/tools/clickup)                                           | ClickUp is an all-in-one productivity platform that provides small an...     |
| [Cogniswitch Toolkit](/docs/integrations/tools/cogniswitch)                                   | CogniSwitch is used to build production ready applications that can c...     |
| [Connery Toolkit and Tools](/docs/integrations/tools/connery)                                 | Using the Connery toolkit and tools, you can integrate Connery Action...     |
| [Dall-E Image Generator](/docs/integrations/tools/dalle_image_generator)                      | OpenAI Dall-E are text-to-image models developed by OpenAI using deep...     |
| [Dappier](/docs/integrations/tools/dappier)                                                   | Dappier connects any LLM or your Agentic AI to real-time, rights-clea...     |
| [Databricks Unity Catalog (UC)](/docs/integrations/tools/databricks)                          | This notebook shows how to use UC functions as LangChain tools, with ...     |
| [DataForSEO](/docs/integrations/tools/dataforseo)                                             | DataForSeo provides comprehensive SEO and digital marketing data solu...     |
| [Dataherald](/docs/integrations/tools/dataherald)                                             | This notebook goes over how to use the dataherald component.                 |
| [DuckDuckGo Search](/docs/integrations/tools/ddg)                                             | This guide shows over how to use the DuckDuckGo search component.            |
| [Discord](/docs/integrations/tools/discord)                                                   | This notebook provides a quick overview for getting started with Disc...     |
| [E2B Data Analysis](/docs/integrations/tools/e2b_data_analysis)                               | E2B's cloud environments are great runtime sandboxes for LLMs.               |
| [Eden AI](/docs/integrations/tools/edenai_tools)                                              | This Jupyter Notebook demonstrates how to use Eden AI tools with an A...     |
| [ElevenLabs Text2Speech](/docs/integrations/tools/eleven_labs_tts)                            | This notebook shows how to interact with the ElevenLabs API to achiev...     |
| [Exa Search](/docs/integrations/tools/exa_search)                                             | Exa is a search engine fully designed for use by LLMs. Search for doc...     |
| [File System](/docs/integrations/tools/filesystem)                                            | LangChain provides tools for interacting with a local file system out...     |
| [FinancialDatasets Toolkit](/docs/integrations/tools/financial_datasets)                      | The financial datasets stock market API provides REST endpoints that ...     |
| [FMP Data](/docs/integrations/tools/fmp-data)                                                 | Access financial market data through natural language queries.               |
| [Github Toolkit](/docs/integrations/tools/github)                                             | The Github toolkit contains tools that enable an LLM agent to interac...     |
| [Gitlab Toolkit](/docs/integrations/tools/gitlab)                                             | The Gitlab toolkit contains tools that enable an LLM agent to interac...     |
| [Gmail Toolkit](/docs/integrations/tools/gmail)                                               | This will help you getting started with the GMail toolkit. This toolk...     |
| [GOAT](/docs/integrations/tools/goat)                                                         | GOAT is the finance toolkit for AI agents.                                   |
| [Golden Query](/docs/integrations/tools/golden_query)                                         | Golden provides a set of natural language APIs for querying and enric...     |
| [Google Books](/docs/integrations/tools/google_books)                                         | Overview                                                                     |
| [Google Calendar Toolkit](/docs/integrations/tools/google_calendar)                           | Google Calendar is a product of Google Workspace that allows users to...     |
| [Google Cloud Text-to-Speech](/docs/integrations/tools/google_cloud_texttospeech)             | Google Cloud Text-to-Speech enables developers to synthesize natural-...     |
| [Google Drive](/docs/integrations/tools/google_drive)                                         | This notebook walks through connecting a LangChain to the Google Driv...     |
| [Google Finance](/docs/integrations/tools/google_finance)                                     | This notebook goes over how to use the Google Finance Tool to get inf...     |
| [Google Imagen](/docs/integrations/tools/google_imagen)                                       | Imagen on Vertex AI brings Google's state of the art image generative...     |
| [Google Jobs](/docs/integrations/tools/google_jobs)                                           | This notebook goes over how to use the Google Jobs Tool to fetch curr...     |
| [Google Lens](/docs/integrations/tools/google_lens)                                           | This notebook goes over how to use the Google Lens Tool to fetch info...     |
| [Google Places](/docs/integrations/tools/google_places)                                       | This notebook goes through how to use Google Places API                      |
| [Google Scholar](/docs/integrations/tools/google_scholar)                                     | This notebook goes through how to use Google Scholar Tool                    |
| [Google Search](/docs/integrations/tools/google_search)                                       | This notebook goes over how to use the google search component.              |
| [Google Serper](/docs/integrations/tools/google_serper)                                       | This notebook goes over how to use the Google Serper component to sea...     |
| [Google Trends](/docs/integrations/tools/google_trends)                                       | This notebook goes over how to use the Google Trends Tool to fetch tr...     |
| [Gradio](/docs/integrations/tools/gradio_tools)                                               | There are many 1000s of Gradio apps on Hugging Face Spaces. This libr...     |
| [GraphQL](/docs/integrations/tools/graphql)                                                   | GraphQL is a query language for APIs and a runtime for executing thos...     |
| [HuggingFace Hub Tools](/docs/integrations/tools/huggingface_tools)                           | Huggingface Tools that supporting text I/O can be                            |
| [Human as a tool](/docs/integrations/tools/human_tools)                                       | Human are AGI so they can certainly be used as a tool to help out AI ...     |
| [Hyperbrowser Browser Agent Tools](/docs/integrations/tools/hyperbrowser_browser_agent_tools) | Hyperbrowser is a platform for running, running browser agents, and s...     |
| [Hyperbrowser Web Scraping Tools](/docs/integrations/tools/hyperbrowser_web_scraping_tools)   | Hyperbrowser is a platform for running and scaling headless browsers....     |
| [IBM watsonx.ai](/docs/integrations/tools/ibm_watsonx)                                        | WatsonxToolkit is a wrapper for IBM watsonx.ai Toolkit.                      |
| [IFTTT WebHooks](/docs/integrations/tools/ifttt)                                              | This notebook shows how to use IFTTT Webhooks.                               |
| [Infobip](/docs/integrations/tools/infobip)                                                   | This notebook that shows how to use Infobip API wrapper to send SMS m...     |
| [Ionic Shopping Tool](/docs/integrations/tools/ionic_shopping)                                | Ionic is a plug and play ecommerce marketplace for AI Assistants. By ...     |
| [Jenkins](/docs/integrations/tools/jenkins)                                                   | Tools for interacting with Jenkins.                                          |
| [Jina Search](/docs/integrations/tools/jina_search)                                           | This notebook provides a quick overview for getting started with Jina...     |
| [Jira Toolkit](/docs/integrations/tools/jira)                                                 | This notebook goes over how to use the Jira toolkit.                         |
| [JSON Toolkit](/docs/integrations/tools/json)                                                 | This notebook showcases an agent interacting with large JSON/dict obj...     |
| [Lemon Agent](/docs/integrations/tools/lemonai)                                               | Lemon Agent helps you build powerful AI assistants in minutes and aut...     |
| [LinkupSearchTool](/docs/integrations/tools/linkup_search)                                    | Linkup provides an API to connect LLMs to the web and the Linkup Prem...     |
| [Memgraph](/docs/integrations/tools/memgraph)                                                 | Overview                                                                     |
| [Memorize](/docs/integrations/tools/memorize)                                                 | Fine-tuning LLM itself to memorize information using unsupervised lea...     |
| [Mojeek Search](/docs/integrations/tools/mojeek_search)                                       | The following notebook will explain how to get results using Mojeek S...     |
| [MultiOn Toolkit](/docs/integrations/tools/multion)                                           | MultiON has built an AI Agent that can interact with a broad array of...     |
| [NASA Toolkit](/docs/integrations/tools/nasa)                                                 | This notebook shows how to use agents to interact with the NASA toolk...     |
| [Naver Search](/docs/integrations/tools/naver_search)                                         | Overview                                                                     |
| [Nuclia Understanding](/docs/integrations/tools/nuclia)                                       | Nuclia automatically indexes your unstructured data from any internal...     |
| [NVIDIA Riva: ASR and TTS](/docs/integrations/tools/nvidia_riva)                              | NVIDIA Riva                                                                  |
| [Office365 Toolkit](/docs/integrations/tools/office365)                                       | Microsoft 365 is a product family of productivity software, collabora...     |
| [OpenAPI Toolkit](/docs/integrations/tools/openapi)                                           | We can construct agents to consume arbitrary APIs, here APIs conforma...     |
| [Natural Language API Toolkits](/docs/integrations/tools/openapi_nla)                         | Natural Language API Toolkits (NLAToolkits) permit LangChain Agents t...     |
| [OpenGradient](/docs/integrations/tools/opengradient_toolkit)                                 | This notebook shows how to build tools using the OpenGradient toolkit...     |
| [OpenWeatherMap](/docs/integrations/tools/openweathermap)                                     | This notebook goes over how to use the OpenWeatherMap component to fe...     |
| [Oracle AI Vector Search: Generate Summary](/docs/integrations/tools/oracleai)                | Oracle AI Vector Search is designed for Artificial Intelligence (AI) ...     |
| [Oxylabs](/docs/integrations/tools/oxylabs)                                                   | Oxylabs is a market-leading web intelligence collection platform, dri...     |
| [Pandas Dataframe](/docs/integrations/tools/pandas)                                           | This notebook shows how to use agents to interact with a Pandas DataF...     |
| [Passio NutritionAI](/docs/integrations/tools/passio_nutrition_ai)                            | To best understand how NutritionAI can give your agents super food-nu...     |
| [PaymanAI](/docs/integrations/tools/payman-tool)                                              | PaymanAI provides functionality to send and receive payments (fiat an...     |
| [Permit](/docs/integrations/tools/permit)                                                     | Permit is an access control platform that provides fine-grained, real...     |
| [PlayWright Browser Toolkit](/docs/integrations/tools/playwright)                             | Playwright is an open-source automation tool developed by Microsoft t...     |
| [Polygon IO Toolkit and Tools](/docs/integrations/tools/polygon)                              | This notebook shows how to use agents to interact with the Polygon IO...     |
| [PowerBI Toolkit](/docs/integrations/tools/powerbi)                                           | This notebook showcases an agent interacting with a Power BI Dataset....     |
| [Prolog](/docs/integrations/tools/prolog_tool)                                                | LangChain tools that use Prolog rules to generate answers.                   |
| [PubMed](/docs/integrations/tools/pubmed)                                                     | PubMedÂ® comprises more than 35 million citations for biomedical liter...     |
| [Python REPL](/docs/integrations/tools/python)                                                | Sometimes, for complex calculations, rather than have an LLM generate...     |
| [Reddit Search](/docs/integrations/tools/reddit_search)                                       | In this notebook, we learn how the Reddit search tool works.                 |
| [Requests Toolkit](/docs/integrations/tools/requests)                                         | We can use the Requests toolkit to construct agents that generate HTT...     |
| [Riza Code Interpreter](/docs/integrations/tools/riza)                                        | The Riza Code Interpreter is a WASM-based isolated environment for ru...     |
| [Robocorp Toolkit](/docs/integrations/tools/robocorp)                                         | This notebook covers how to get started with Robocorp Action Server a...     |
| [Salesforce](/docs/integrations/tools/salesforce)                                             | Tools for interacting with Salesforce.                                       |
| [SceneXplain](/docs/integrations/tools/sceneXplain)                                           | SceneXplain is an ImageCaptioning service accessible through the Scen...     |
| [ScrapeGraph](/docs/integrations/tools/scrapegraph)                                           | This notebook provides a quick overview for getting started with Scra...     |
| [SearchApi](/docs/integrations/tools/searchapi)                                               | This notebook shows examples of how to use SearchApi to search the we...     |
| [SearxNG Search](/docs/integrations/tools/searx_search)                                       | This notebook goes over how to use a self hosted SearxNG search API t...     |
| [Semantic Scholar API Tool](/docs/integrations/tools/semanticscholar)                         | This notebook demos how to use the semantic scholar tool with an agen...     |
| [SerpAPI](/docs/integrations/tools/serpapi)                                                   | This notebook goes over how to use the SerpAPI component to search th...     |
| [Slack Toolkit](/docs/integrations/tools/slack)                                               | This will help you getting started with the Slack toolkit. For detail...     |
| [Spark SQL Toolkit](/docs/integrations/tools/spark_sql)                                       | This notebook shows how to use agents to interact with Spark SQL. Sim...     |
| [SQLDatabase Toolkit](/docs/integrations/tools/sql_database)                                  | This will help you getting started with the SQL Database toolkit. For...     |
| [StackExchange](/docs/integrations/tools/stackexchange)                                       | Stack Exchange is a network of question-and-answer (Q&amp;A) websites on ... |
| [Steam Toolkit](/docs/integrations/tools/steam)                                               | Steam (Wikipedia)) is a video game digital distribution service and s...     |
| [Stripe](/docs/integrations/tools/stripe)                                                     | This notebook provides a quick overview for getting started with Stri...     |
| [Tableau](/docs/integrations/tools/tableau)                                                   | This notebook provides a quick overview for getting started with Tabl...     |
| [Taiga](/docs/integrations/tools/taiga)                                                       | This notebook provides a quick overview for getting started with Taig...     |
| [Tavily Extract](/docs/integrations/tools/tavily_extract)                                     | Tavily is a search engine built specifically for AI agents (LLMs), de...     |
| [Tavily Search](/docs/integrations/tools/tavily_search)                                       | Tavily's Search API is a search engine built specifically for AI agen...     |
| [Tilores](/docs/integrations/tools/tilores)                                                   | This notebook covers how to get started with the Tilores tools.              |
| [Twilio](/docs/integrations/tools/twilio)                                                     | This notebook goes over how to use the Twilio API wrapper to send a m...     |
| [Upstage](/docs/integrations/tools/upstage_groundedness_check)                                | This notebook covers how to get started with Upstage groundedness che...     |
| [Valthera](/docs/integrations/tools/valthera)                                                 | Enable AI agents to engage users when they're most likely to respond.        |
| [Wikidata](/docs/integrations/tools/wikidata)                                                 | Wikidata is a free and open knowledge base that can be read and edite...     |
| [Wikipedia](/docs/integrations/tools/wikipedia)                                               | Wikipedia is a multilingual free online encyclopedia written and main...     |
| [Wolfram Alpha](/docs/integrations/tools/wolfram_alpha)                                       | This notebook goes over how to use the wolfram alpha component.              |
| [Writer Tools](/docs/integrations/tools/writer)                                               | This notebook provides a quick overview for getting started with Writ...     |
| [Yahoo Finance News](/docs/integrations/tools/yahoo_finance_news)                             | This notebook goes over how to use the yahoofinancenews tool with an ...     |
| [You.com Search](/docs/integrations/tools/you)                                                | The you.com API is a suite of tools designed to help developers groun...     |
| [YouTube](/docs/integrations/tools/youtube)                                                   | YouTube Search package searches YouTube videos avoiding using their h...     |
| [Zapier Natural Language Actions](/docs/integrations/tools/zapier)                            | Deprecated This API will be sunset on 2023-11-17//nla.zapier.com/star...     |
| [ZenGuard AI](/docs/integrations/tools/zenguard)                                              | This tool lets you quickly set up ZenGuard AI in your Langchain-power...     |

* * *


- [Search](#search)
- [Code Interpreter](#code-interpreter)
- [Productivity](#productivity)
- [Web Browsing](#web-browsing)
- [Database](#database)
- [Finance](#finance)
- [All tools](#all-tools)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/time_weighted_vectorstore.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/time_weighted_vectorstore.ipynb)

# How to use a time-weighted vector store retriever

This [retriever](/docs/concepts/retrievers/) uses a combination of semantic [similarity](/docs/concepts/embedding_models/#measure-similarity) and a time decay.

The algorithm for scoring them is:

```text
semantic_similarity + (1.0 - decay_rate) ^ hours_passed
```

Notably, `hours_passed` refers to the hours passed since the object in the retriever **was last accessed**, not since it was created. This means that frequently accessed objects remain "fresh".

```python
from datetime import datetime, timedelta

import faiss
from langchain.retrievers import TimeWeightedVectorStoreRetriever
from langchain_community.docstore import InMemoryDocstore
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
```

**API Reference:**[TimeWeightedVectorStoreRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.time_weighted_retriever.TimeWeightedVectorStoreRetriever.html) | [InMemoryDocstore](https://python.langchain.com/api_reference/community/docstore/langchain_community.docstore.in_memory.InMemoryDocstore.html) | [FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

## Low decay rate[â€‹](#low-decay-rate "Direct link to Low decay rate")

A low `decay rate` (in this, to be extreme, we will set it close to 0) means memories will be "remembered" for longer. A `decay rate` of 0 means memories never be forgotten, making this retriever equivalent to the vector lookup.

```python
# Define your embedding model
embeddings_model = OpenAIEmbeddings()
# Initialize the vectorstore as empty
embedding_size = 1536
index = faiss.IndexFlatL2(embedding_size)
vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})
retriever = TimeWeightedVectorStoreRetriever(
    vectorstore=vectorstore, decay_rate=0.0000000000000000000000001, k=1
)
```

```python
yesterday = datetime.now() - timedelta(days=1)
retriever.add_documents(
    [Document(page_content="hello world", metadata={"last_accessed_at": yesterday})]
)
retriever.add_documents([Document(page_content="hello foo")])
```

```output
['73679bc9-d425-49c2-9d74-de6356c73489']
```

```python
# "Hello World" is returned first because it is most salient, and the decay rate is close to 0., meaning it's still recent enough
retriever.invoke("hello world")
```

```output
[Document(metadata={'last_accessed_at': datetime.datetime(2024, 10, 22, 16, 37, 40, 818583), 'created_at': datetime.datetime(2024, 10, 22, 16, 37, 37, 975074), 'buffer_idx': 0}, page_content='hello world')]
```

## High decay rate[â€‹](#high-decay-rate "Direct link to High decay rate")

With a high `decay rate` (e.g., several 9's), the `recency score` quickly goes to 0! If you set this all the way to 1, `recency` is 0 for all objects, once again making this equivalent to a vector lookup.

```python
# Define your embedding model
embeddings_model = OpenAIEmbeddings()
# Initialize the vectorstore as empty
embedding_size = 1536
index = faiss.IndexFlatL2(embedding_size)
vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})
retriever = TimeWeightedVectorStoreRetriever(
    vectorstore=vectorstore, decay_rate=0.999, k=1
)
```

```python
yesterday = datetime.now() - timedelta(days=1)
retriever.add_documents(
    [Document(page_content="hello world", metadata={"last_accessed_at": yesterday})]
)
retriever.add_documents([Document(page_content="hello foo")])
```

```output
['379631f0-42c2-4773-8cc2-d36201e1e610']
```

```python
# "Hello Foo" is returned first because "hello world" is mostly forgotten
retriever.invoke("hello world")
```

```output
[Document(metadata={'last_accessed_at': datetime.datetime(2024, 10, 22, 16, 37, 46, 553633), 'created_at': datetime.datetime(2024, 10, 22, 16, 37, 43, 927429), 'buffer_idx': 1}, page_content='hello foo')]
```

## Virtual time[â€‹](#virtual-time "Direct link to Virtual time")

Using some utils in LangChain, you can mock out the time component.

```python
from langchain_core.utils import mock_now
```

**API Reference:**[mock\_now](https://python.langchain.com/api_reference/core/utils/langchain_core.utils.utils.mock_now.html)

```python
# Notice the last access time is that date time

tomorrow = datetime.now() + timedelta(days=1)

with mock_now(tomorrow):
    print(retriever.invoke("hello world"))
```

```output
[Document(metadata={'last_accessed_at': MockDateTime(2024, 10, 23, 16, 38, 19, 66711), 'created_at': datetime.datetime(2024, 10, 22, 16, 37, 43, 599877), 'buffer_idx': 0}, page_content='hello world')]
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/time_weighted_vectorstore.ipynb)

* * *


- [Low decay rate](#low-decay-rate)
- [High decay rate](#high-decay-rate)
- [Virtual time](#virtual-time)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/query_high_cardinality.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/query_high_cardinality.ipynb)

# How deal with high cardinality categoricals when doing query analysis

You may want to do query analysis to create a filter on a categorical column. One of the difficulties here is that you usually need to specify the EXACT categorical value. The issue is you need to make sure the LLM generates that categorical value exactly. This can be done relatively easy with prompting when there are only a few values that are valid. When there are a high number of valid values then it becomes more difficult, as those values may not fit in the LLM context, or (if they do) there may be too many for the LLM to properly attend to.

In this notebook we take a look at how to approach this.

## Setup[â€‹](#setup "Direct link to Setup")

#### Install dependencies[â€‹](#install-dependencies "Direct link to Install dependencies")

```python
%pip install -qU langchain langchain-community langchain-openai faker langchain-chroma
```

```output
Note: you may need to restart the kernel to use updated packages.
```

#### Set environment variables[â€‹](#set-environment-variables "Direct link to Set environment variables")

We'll use OpenAI in this example:

```python
import getpass
import os

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass()

# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

#### Set up data[â€‹](#set-up-data "Direct link to Set up data")

We will generate a bunch of fake names

```python
from faker import Faker

fake = Faker()

names = [fake.name() for _ in range(10000)]
```

Let's look at some of the names

```python
names[0]
```

```output
'Jacob Adams'
```

```python
names[567]
```

```output
'Eric Acevedo'
```

## Query Analysis[â€‹](#query-analysis "Direct link to Query Analysis")

We can now set up a baseline query analysis

```python
from pydantic import BaseModel, Field, model_validator
```

```python
class Search(BaseModel):
    query: str
    author: str
```

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

system = """Generate a relevant search query for a library system"""
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_llm = llm.with_structured_output(Search)
query_analyzer = {"question": RunnablePassthrough()} | prompt | structured_llm
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

We can see that if we spell the name exactly correctly, it knows how to handle it

```python
query_analyzer.invoke("what are books about aliens by Jesse Knight")
```

```output
Search(query='aliens', author='Jesse Knight')
```

The issue is that the values you want to filter on may NOT be spelled exactly correctly

```python
query_analyzer.invoke("what are books about aliens by jess knight")
```

```output
Search(query='aliens', author='Jess Knight')
```

### Add in all values[â€‹](#add-in-all-values "Direct link to Add in all values")

One way around this is to add ALL possible values to the prompt. That will generally guide the query in the right direction

```python
system = """Generate a relevant search query for a library system.

`author` attribute MUST be one of:

{authors}

Do NOT hallucinate author name!"""
base_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
prompt = base_prompt.partial(authors=", ".join(names))
```

```python
query_analyzer_all = {"question": RunnablePassthrough()} | prompt | structured_llm
```

However... if the list of categoricals is long enough, it may error!

```python
try:
    res = query_analyzer_all.invoke("what are books about aliens by jess knight")
except Exception as e:
    print(e)
```

We can try to use a longer context window... but with so much information in there, it is not garunteed to pick it up reliably

```python
llm_long = ChatOpenAI(model="gpt-4-turbo-preview", temperature=0)
structured_llm_long = llm_long.with_structured_output(Search)
query_analyzer_all = {"question": RunnablePassthrough()} | prompt | structured_llm_long
```

```python
query_analyzer_all.invoke("what are books about aliens by jess knight")
```

```output
Search(query='aliens', author='jess knight')
```

### Find and all relevant values[â€‹](#find-and-all-relevant-values "Direct link to Find and all relevant values")

Instead, what we can do is create an index over the relevant values and then query that for the N most relevant values,

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_texts(names, embeddings, collection_name="author_names")
```

**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```python
def select_names(question):
    _docs = vectorstore.similarity_search(question, k=10)
    _names = [d.page_content for d in _docs]
    return ", ".join(_names)
```

```python
create_prompt = {
    "question": RunnablePassthrough(),
    "authors": select_names,
} | base_prompt
```

```python
query_analyzer_select = create_prompt | structured_llm
```

```python
create_prompt.invoke("what are books by jess knight")
```

```output
ChatPromptValue(messages=[SystemMessage(content='Generate a relevant search query for a library system.\n\n`author` attribute MUST be one of:\n\nJennifer Knight, Jill Knight, John Knight, Dr. Jeffrey Knight, Christopher Knight, Andrea Knight, Brandy Knight, Jennifer Keller, Becky Chambers, Sarah Knapp\n\nDo NOT hallucinate author name!'), HumanMessage(content='what are books by jess knight')])
```

```python
query_analyzer_select.invoke("what are books about aliens by jess knight")
```

```output
Search(query='books about aliens', author='Jennifer Knight')
```

### Replace after selection[â€‹](#replace-after-selection "Direct link to Replace after selection")

Another method is to let the LLM fill in whatever value, but then convert that value to a valid value. This can actually be done with the Pydantic class itself!

```python
class Search(BaseModel):
    query: str
    author: str

    @model_validator(mode="before")
    @classmethod
    def double(cls, values: dict) -> dict:
        author = values["author"]
        closest_valid_author = vectorstore.similarity_search(author, k=1)[
            0
        ].page_content
        values["author"] = closest_valid_author
        return values
```

```python
system = """Generate a relevant search query for a library system"""
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
corrective_structure_llm = llm.with_structured_output(Search)
corrective_query_analyzer = (
    {"question": RunnablePassthrough()} | prompt | corrective_structure_llm
)
```

```python
corrective_query_analyzer.invoke("what are books about aliens by jes knight")
```

```output
Search(query='aliens', author='John Knight')
```

```python
# TODO: show trigram similarity
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/query_high_cardinality.ipynb)

* * *


- [Setup](#setup)
- [Query Analysis](#query-analysis)
  
  - [Add in all values](#add-in-all-values)
  - [Find and all relevant values](#find-and-all-relevant-values)
  - [Replace after selection](#replace-after-selection)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/chatbots_memory.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/chatbots_memory.ipynb)

# How to add memory to chatbots

A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:

- Simply stuffing previous messages into a chat model prompt.
- The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.
- More complex modifications like synthesizing summaries for long running conversations.

We'll go into more detail on a few techniques below!

note

This how-to guide previously built a chatbot using [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html). You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/).

As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.

If your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.

Please see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.

## Setup[â€‹](#setup "Direct link to Setup")

You'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:

```python
%pip install --upgrade --quiet langchain langchain-openai langgraph

import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")
```

```output
OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·
```

Let's also set up a chat model that we'll use for the below examples.

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini")
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

## Message passing[â€‹](#message-passing "Direct link to Message passing")

The simplest form of memory is simply passing chat history messages into a chain. Here's an example:

```python
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages(
    [
        SystemMessage(
            content="You are a helpful assistant. Answer all questions to the best of your ability."
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

chain = prompt | model

ai_msg = chain.invoke(
    {
        "messages": [
            HumanMessage(
                content="Translate from English to French: I love programming."
            ),
            AIMessage(content="J'adore la programmation."),
            HumanMessage(content="What did you just say?"),
        ],
    }
)
print(ai_msg.content)
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)

```output
I said, "I love programming" in French: "J'adore la programmation."
```

We can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages.

## Automatic history management[â€‹](#automatic-history-management "Direct link to Automatic history management")

The previous examples pass messages to the chain (and model) explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also provides a way to build applications that have memory using LangGraph's [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/). You can [enable persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) in LangGraph applications by providing a `checkpointer` when compiling the graph.

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

workflow = StateGraph(state_schema=MessagesState)


# Define the function that calls the model
def call_model(state: MessagesState):
    system_prompt = (
        "You are a helpful assistant. "
        "Answer all questions to the best of your ability."
    )
    messages = [SystemMessage(content=system_prompt)] + state["messages"]
    response = model.invoke(messages)
    return {"messages": response}


# Define the node and edge
workflow.add_node("model", call_model)
workflow.add_edge(START, "model")

# Add simple in-memory checkpointer
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

**API Reference:**[MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

We'll pass the latest input to the conversation here and let LangGraph keep track of the conversation history using the checkpointer:

```python
app.invoke(
    {"messages": [HumanMessage(content="Translate to French: I love programming.")]},
    config={"configurable": {"thread_id": "1"}},
)
```

```output
{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),
  AIMessage(content="J'aime programmer.", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39})]}
```

```python
app.invoke(
    {"messages": [HumanMessage(content="What did I just ask you?")]},
    config={"configurable": {"thread_id": "1"}},
)
```

```output
{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),
  AIMessage(content="J'aime programmer.", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39}),
  HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='c667529b-7c41-4cc0-9326-0af47328b816'),
  AIMessage(content='You asked me to translate "I love programming" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-134a7ea0-d3a4-4923-bd58-25e5a43f6a1f-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67})]}
```

## Modifying chat history[â€‹](#modifying-chat-history "Direct link to Modifying chat history")

Modifying stored chat messages can help your chatbot handle a variety of situations. Here are some examples:

### Trimming messages[â€‹](#trimming-messages "Direct link to Trimming messages")

LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the history messages before passing them to the model. Let's use an example history with the `app` we declared above:

```python
demo_ephemeral_chat_history = [
    HumanMessage(content="Hey there! I'm Nemo."),
    AIMessage(content="Hello!"),
    HumanMessage(content="How are you today?"),
    AIMessage(content="Fine thanks!"),
]

app.invoke(
    {
        "messages": demo_ephemeral_chat_history
        + [HumanMessage(content="What's my name?")]
    },
    config={"configurable": {"thread_id": "2"}},
)
```

```output
{'messages': [HumanMessage(content="Hey there! I'm Nemo.", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),
  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),
  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),
  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),
  HumanMessage(content="What's my name?", additional_kwargs={}, response_metadata={}, id='c933eca3-5fd8-4651-af16-20fe2d49c216'),
  AIMessage(content='Your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a0b21acc-9dbb-4fb6-a953-392020f37d88-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68})]}
```

We can see the app remembers the preloaded name.

But let's say we have a very small context window, and we want to trim the number of messages passed to the model to only the 2 most recent ones. We can use the built in [trim\_messages](/docs/how_to/trim_messages/) util to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 "token" and keep only the last two messages:

```python
from langchain_core.messages import trim_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

# Define trimmer
# count each message as 1 "token" (token_counter=len) and keep only the last two messages
trimmer = trim_messages(strategy="last", max_tokens=2, token_counter=len)

workflow = StateGraph(state_schema=MessagesState)


# Define the function that calls the model
def call_model(state: MessagesState):
    trimmed_messages = trimmer.invoke(state["messages"])
    system_prompt = (
        "You are a helpful assistant. "
        "Answer all questions to the best of your ability."
    )
    messages = [SystemMessage(content=system_prompt)] + trimmed_messages
    response = model.invoke(messages)
    return {"messages": response}


# Define the node and edge
workflow.add_node("model", call_model)
workflow.add_edge(START, "model")

# Add simple in-memory checkpointer
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

**API Reference:**[trim\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

Let's call this new app and check the response

```python
app.invoke(
    {
        "messages": demo_ephemeral_chat_history
        + [HumanMessage(content="What is my name?")]
    },
    config={"configurable": {"thread_id": "3"}},
)
```

```output
{'messages': [HumanMessage(content="Hey there! I'm Nemo.", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),
  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),
  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),
  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),
  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='a22ab7c5-8617-4821-b3e9-a9e7dca1ff78'),
  AIMessage(content="I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 39, 'total_tokens': 66, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7b32d72-9f57-4705-be7e-43bf1c3d293b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 27, 'total_tokens': 66})]}
```

We can see that `trim_messages` was called and only the two most recent messages will be passed to the model. In this case, this means that the model forgot the name we gave it.

Check out our [how to guide on trimming messages](/docs/how_to/trim_messages/) for more.

### Summary memory[â€‹](#summary-memory "Direct link to Summary memory")

We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our app. Let's recreate our chat history:

```python
demo_ephemeral_chat_history = [
    HumanMessage(content="Hey there! I'm Nemo."),
    AIMessage(content="Hello!"),
    HumanMessage(content="How are you today?"),
    AIMessage(content="Fine thanks!"),
]
```

And now, let's update the model-calling function to distill previous interactions into a summary:

```python
from langchain_core.messages import HumanMessage, RemoveMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

workflow = StateGraph(state_schema=MessagesState)


# Define the function that calls the model
def call_model(state: MessagesState):
    system_prompt = (
        "You are a helpful assistant. "
        "Answer all questions to the best of your ability. "
        "The provided chat history includes a summary of the earlier conversation."
    )
    system_message = SystemMessage(content=system_prompt)
    message_history = state["messages"][:-1]  # exclude the most recent user input
    # Summarize the messages if the chat history reaches a certain size
    if len(message_history) >= 4:
        last_human_message = state["messages"][-1]
        # Invoke the model to generate conversation summary
        summary_prompt = (
            "Distill the above chat messages into a single summary message. "
            "Include as many specific details as you can."
        )
        summary_message = model.invoke(
            message_history + [HumanMessage(content=summary_prompt)]
        )

        # Delete messages that we no longer want to show up
        delete_messages = [RemoveMessage(id=m.id) for m in state["messages"]]
        # Re-add user message
        human_message = HumanMessage(content=last_human_message.content)
        # Call the model with summary & response
        response = model.invoke([system_message, summary_message, human_message])
        message_updates = [summary_message, human_message, response] + delete_messages
    else:
        message_updates = model.invoke([system_message] + state["messages"])

    return {"messages": message_updates}


# Define the node and edge
workflow.add_node("model", call_model)
workflow.add_edge(START, "model")

# Add simple in-memory checkpointer
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [RemoveMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.modifier.RemoveMessage.html) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

Let's see if it remembers the name we gave it:

```python
app.invoke(
    {
        "messages": demo_ephemeral_chat_history
        + [HumanMessage("What did I say my name was?")]
    },
    config={"configurable": {"thread_id": "4"}},
)
```

```output
{'messages': [AIMessage(content="Nemo greeted me, and I responded positively, indicating that I'm doing well.", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 60, 'total_tokens': 76, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee42f98d-907d-4bad-8f16-af2db789701d-0', usage_metadata={'input_tokens': 60, 'output_tokens': 16, 'total_tokens': 76}),
  HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='788555ea-5b1f-4c29-a2f2-a92f15d147be'),
  AIMessage(content='You mentioned that your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 67, 'total_tokens': 75, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-099a43bd-a284-4969-bb6f-0be486614cd8-0', usage_metadata={'input_tokens': 67, 'output_tokens': 8, 'total_tokens': 75})]}
```

Note that invoking the app again will keep accumulating the history until it reaches the specified number of messages (four in our case). At that point we will generate another summary generated from the initial summary plus new messages and so on.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/chatbots_memory.ipynb)

* * *


- [Setup](#setup)
- [Message passing](#message-passing)
- [Automatic history management](#automatic-history-management)
- [Modifying chat history](#modifying-chat-history)
  
  - [Trimming messages](#trimming-messages)
  - [Summary memory](#summary-memory)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/llm_caching.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/llm_caching.ipynb)

# How to cache LLM responses

LangChain provides an optional [caching](/docs/concepts/chat_models/#caching) layer for LLMs. This is useful for two reasons:

It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times. It can speed up your application by reducing the number of API calls you make to the LLM provider.

```python
%pip install -qU langchain_openai langchain_community

import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
# Please manually enter OpenAI Key
```

```python
from langchain_core.globals import set_llm_cache
from langchain_openai import OpenAI

# To make the caching really obvious, lets use a slower and older model.
# Caching supports newer chat models as well.
llm = OpenAI(model="gpt-3.5-turbo-instruct", n=2, best_of=2)
```

**API Reference:**[set\_llm\_cache](https://python.langchain.com/api_reference/core/globals/langchain_core.globals.set_llm_cache.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

```python
%%time
from langchain_core.caches import InMemoryCache

set_llm_cache(InMemoryCache())

# The first time, it is not yet in cache, so it should take longer
llm.invoke("Tell me a joke")
```

**API Reference:**[InMemoryCache](https://python.langchain.com/api_reference/core/caches/langchain_core.caches.InMemoryCache.html)

```output
CPU times: user 546 ms, sys: 379 ms, total: 925 ms
Wall time: 1.11 s
```

```output
"\nWhy don't scientists trust atoms?\n\nBecause they make up everything!"
```

```python
%%time
# The second time it is, so it goes faster
llm.invoke("Tell me a joke")
```

```output
CPU times: user 192 Âµs, sys: 77 Âµs, total: 269 Âµs
Wall time: 270 Âµs
```

```output
"\nWhy don't scientists trust atoms?\n\nBecause they make up everything!"
```

## SQLite Cache[â€‹](#sqlite-cache "Direct link to SQLite Cache")

```python
!rm .langchain.db
```

```python
# We can do the same thing with a SQLite cache
from langchain_community.cache import SQLiteCache

set_llm_cache(SQLiteCache(database_path=".langchain.db"))
```

**API Reference:**[SQLiteCache](https://python.langchain.com/api_reference/community/cache/langchain_community.cache.SQLiteCache.html)

```python
%%time
# The first time, it is not yet in cache, so it should take longer
llm.invoke("Tell me a joke")
```

```output
CPU times: user 10.6 ms, sys: 4.21 ms, total: 14.8 ms
Wall time: 851 ms
```

```output
"\n\nWhy don't scientists trust atoms?\n\nBecause they make up everything!"
```

```python
%%time
# The second time it is, so it goes faster
llm.invoke("Tell me a joke")
```

```output
CPU times: user 59.7 ms, sys: 63.6 ms, total: 123 ms
Wall time: 134 ms
```

```output
"\n\nWhy don't scientists trust atoms?\n\nBecause they make up everything!"
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/llm_caching.ipynb)

* * *


- [SQLite Cache](#sqlite-cache)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/callbacks_custom_events.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/callbacks_custom_events.ipynb)

# How to dispatch custom callback events

Prerequisites

This guide assumes familiarity with the following concepts:

- [Callbacks](/docs/concepts/callbacks/)
- [Custom callback handlers](/docs/how_to/custom_callbacks/)
- [Astream Events API](/docs/concepts/streaming/#astream_events) the `astream_events` method will surface custom callback events.

In some situations, you may want to dispatch a custom callback event from within a [Runnable](/docs/concepts/runnables/) so it can be surfaced in a custom callback handler or via the [Astream Events API](/docs/concepts/streaming/#astream_events).

For example, if you have a long running tool with multiple steps, you can dispatch custom events between the steps and use these custom events to monitor progress. You could also surface these custom events to an end user of your application to show them how the current task is progressing.

To dispatch a custom event you need to decide on two attributes for the event: the `name` and the `data`.

| Attribute | Type | Description                                                                                              |
|-----------|------|----------------------------------------------------------------------------------------------------------|
| name      | str  | A user defined name for the event.                                                                       |
| data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable. |

important

- Dispatching custom callback events requires `langchain-core>=0.2.15`.
- Custom callback events can only be dispatched from within an existing `Runnable`.
- If using `astream_events`, you must use `version='v2'` to see custom events.
- Sending or rendering custom callbacks events in LangSmith is not yet supported.

COMPATIBILITY

LangChain cannot automatically propagate configuration, including callbacks necessary for astream\_events(), to child runnables if you are running async code in python&lt;=3.10. This is a common reason why you may fail to see events being emitted from custom runnables or tools.

If you are running python&lt;=3.10, you will need to manually propagate the `RunnableConfig` object to the child runnable in async environments. For an example of how to manually propagate the config, see the implementation of the `bar` RunnableLambda below.

If you are running python&gt;=3.11, the `RunnableConfig` will automatically propagate to child runnables in async environment. However, it is still a good idea to propagate the `RunnableConfig` manually if your code may run in other Python versions.

## Astream Events API[â€‹](#astream-events-api "Direct link to Astream Events API")

The most useful way to consume custom events is via the [Astream Events API](/docs/concepts/streaming/#astream_events).

We can use the `async` `adispatch_custom_event` API to emit custom events in an async setting.

important

To see custom events via the astream events API, you need to use the newer `v2` API of `astream_events`.

```python
from langchain_core.callbacks.manager import (
    adispatch_custom_event,
)
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.config import RunnableConfig


@RunnableLambda
async def foo(x: str) -> str:
    await adispatch_custom_event("event1", {"x": x})
    await adispatch_custom_event("event2", 5)
    return x


async for event in foo.astream_events("hello world", version="v2"):
    print(event)
```

**API Reference:**[adispatch\_custom\_event](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.adispatch_custom_event.html) | [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html)

```output
{'event': 'on_chain_start', 'data': {'input': 'hello world'}, 'name': 'foo', 'tags': [], 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'metadata': {}, 'parent_ids': []}
{'event': 'on_custom_event', 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'event1', 'tags': [], 'metadata': {}, 'data': {'x': 'hello world'}, 'parent_ids': []}
{'event': 'on_custom_event', 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'event2', 'tags': [], 'metadata': {}, 'data': 5, 'parent_ids': []}
{'event': 'on_chain_stream', 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'foo', 'tags': [], 'metadata': {}, 'data': {'chunk': 'hello world'}, 'parent_ids': []}
{'event': 'on_chain_end', 'data': {'output': 'hello world'}, 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'foo', 'tags': [], 'metadata': {}, 'parent_ids': []}
```

In python &lt;= 3.10, you must propagate the config manually!

```python
from langchain_core.callbacks.manager import (
    adispatch_custom_event,
)
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.config import RunnableConfig


@RunnableLambda
async def bar(x: str, config: RunnableConfig) -> str:
    """An example that shows how to manually propagate config.

    You must do this if you're running python<=3.10.
    """
    await adispatch_custom_event("event1", {"x": x}, config=config)
    await adispatch_custom_event("event2", 5, config=config)
    return x


async for event in bar.astream_events("hello world", version="v2"):
    print(event)
```

**API Reference:**[adispatch\_custom\_event](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.adispatch_custom_event.html) | [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html)

```output
{'event': 'on_chain_start', 'data': {'input': 'hello world'}, 'name': 'bar', 'tags': [], 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'metadata': {}, 'parent_ids': []}
{'event': 'on_custom_event', 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'event1', 'tags': [], 'metadata': {}, 'data': {'x': 'hello world'}, 'parent_ids': []}
{'event': 'on_custom_event', 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'event2', 'tags': [], 'metadata': {}, 'data': 5, 'parent_ids': []}
{'event': 'on_chain_stream', 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'bar', 'tags': [], 'metadata': {}, 'data': {'chunk': 'hello world'}, 'parent_ids': []}
{'event': 'on_chain_end', 'data': {'output': 'hello world'}, 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'bar', 'tags': [], 'metadata': {}, 'parent_ids': []}
```

## Async Callback Handler[â€‹](#async-callback-handler "Direct link to Async Callback Handler")

You can also consume the dispatched event via an async callback handler.

```python
from typing import Any, Dict, List, Optional
from uuid import UUID

from langchain_core.callbacks import AsyncCallbackHandler
from langchain_core.callbacks.manager import (
    adispatch_custom_event,
)
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.config import RunnableConfig


class AsyncCustomCallbackHandler(AsyncCallbackHandler):
    async def on_custom_event(
        self,
        name: str,
        data: Any,
        *,
        run_id: UUID,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> None:
        print(
            f"Received event {name} with data: {data}, with tags: {tags}, with metadata: {metadata} and run_id: {run_id}"
        )


@RunnableLambda
async def bar(x: str, config: RunnableConfig) -> str:
    """An example that shows how to manually propagate config.

    You must do this if you're running python<=3.10.
    """
    await adispatch_custom_event("event1", {"x": x}, config=config)
    await adispatch_custom_event("event2", 5, config=config)
    return x


async_handler = AsyncCustomCallbackHandler()
await foo.ainvoke(1, {"callbacks": [async_handler], "tags": ["foo", "bar"]})
```

**API Reference:**[AsyncCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.AsyncCallbackHandler.html) | [adispatch\_custom\_event](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.adispatch_custom_event.html) | [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html)

```output
Received event event1 with data: {'x': 1}, with tags: ['foo', 'bar'], with metadata: {} and run_id: a62b84be-7afd-4829-9947-7165df1f37d9
Received event event2 with data: 5, with tags: ['foo', 'bar'], with metadata: {} and run_id: a62b84be-7afd-4829-9947-7165df1f37d9
```

```output
1
```

## Sync Callback Handler[â€‹](#sync-callback-handler "Direct link to Sync Callback Handler")

Let's see how to emit custom events in a sync environment using `dispatch_custom_event`.

You **must** call `dispatch_custom_event` from within an existing `Runnable`.

```python
from typing import Any, Dict, List, Optional
from uuid import UUID

from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.callbacks.manager import (
    dispatch_custom_event,
)
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.config import RunnableConfig


class CustomHandler(BaseCallbackHandler):
    def on_custom_event(
        self,
        name: str,
        data: Any,
        *,
        run_id: UUID,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> None:
        print(
            f"Received event {name} with data: {data}, with tags: {tags}, with metadata: {metadata} and run_id: {run_id}"
        )


@RunnableLambda
def foo(x: int, config: RunnableConfig) -> int:
    dispatch_custom_event("event1", {"x": x})
    dispatch_custom_event("event2", {"x": x})
    return x


handler = CustomHandler()
foo.invoke(1, {"callbacks": [handler], "tags": ["foo", "bar"]})
```

**API Reference:**[BaseCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) | [dispatch\_custom\_event](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.dispatch_custom_event.html) | [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html)

```output
Received event event1 with data: {'x': 1}, with tags: ['foo', 'bar'], with metadata: {} and run_id: 27b5ce33-dc26-4b34-92dd-08a89cb22268
Received event event2 with data: {'x': 1}, with tags: ['foo', 'bar'], with metadata: {} and run_id: 27b5ce33-dc26-4b34-92dd-08a89cb22268
```

```output
1
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've seen how to emit custom events, you can check out the more in depth guide for [astream events](/docs/how_to/streaming/#using-stream-events) which is the easiest way to leverage custom events.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/callbacks_custom_events.ipynb)

* * *


- [Astream Events API](#astream-events-api)
- [Async Callback Handler](#async-callback-handler)
- [Sync Callback Handler](#sync-callback-handler)
- [Next steps](#next-steps)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/async.mdx)

# Async programming with langchain

Prerequisites

- [Runnable interface](/docs/concepts/runnables/)
- [asyncio](https://docs.python.org/3/library/asyncio.html)

LLM based applications often involve a lot of I/O-bound operations, such as making API calls to language models, databases, or other services. Asynchronous programming (or async programming) is a paradigm that allows a program to perform multiple tasks concurrently without blocking the execution of other tasks, improving efficiency and responsiveness, particularly in I/O-bound operations.

note

You are expected to be familiar with asynchronous programming in Python before reading this guide. If you are not, please find appropriate resources online to learn how to program asynchronously in Python. This guide specifically focuses on what you need to know to work with LangChain in an asynchronous context, assuming that you are already familiar with asynch

## Langchain asynchronous APIs[â€‹](#langchain-asynchronous-apis "Direct link to Langchain asynchronous APIs")

Many LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.

Typically, any method that may perform I/O operations (e.g., making API calls, reading files) will have an asynchronous counterpart.

In LangChain, async implementations are located in the same classes as their synchronous counterparts, with the asynchronous methods having an "a" prefix. For example, the synchronous `invoke` method has an asynchronous counterpart called `ainvoke`.

Many components of LangChain implement the [Runnable Interface](/docs/concepts/runnables/), which includes support for asynchronous execution. This means that you can run Runnables asynchronously using the `await` keyword in Python.

```python
await some_runnable.ainvoke(some_input)
```

Other components like [Embedding Models](/docs/concepts/embedding_models/) and [VectorStore](/docs/concepts/vectorstores/) that do not implement the [Runnable Interface](/docs/concepts/runnables/) usually still follow the same rule and include the asynchronous version of method in the same class with an "a" prefix.

For example,

```python
await some_vectorstore.aadd_documents(documents)
```

Runnables created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel/) can also be run asynchronously as they implement the full [Runnable Interface](/docs/concepts/runnables/).

For more information, please review the [API reference](https://python.langchain.com/api_reference/) for the specific component you are using.

## Delegation to sync methods[â€‹](#delegation-to-sync-methods "Direct link to Delegation to sync methods")

Most popular LangChain integrations implement asynchronous support of their APIs. For example, the `ainvoke` method of many ChatModel implementations uses the `httpx.AsyncClient` to make asynchronous HTTP requests to the model provider's API.

When an asynchronous implementation is not available, LangChain tries to provide a default implementation, even if it incurs a **slight** overhead.

By default, LangChain will delegate the execution of unimplemented asynchronous methods to the synchronous counterparts. LangChain almost always assumes that the synchronous method should be treated as a blocking operation and should be run in a separate thread. This is done using [asyncio.loop.run\_in\_executor](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor) functionality provided by the `asyncio` library. LangChain uses the default executor provided by the `asyncio` library, which lazily initializes a thread pool executor with a default number of threads that is reused in the given event loop. While this strategy incurs a slight overhead due to context switching between threads, it guarantees that every asynchronous method has a default implementation that works out of the box.

## Performance[â€‹](#performance "Direct link to Performance")

Async code in LangChain should generally perform relatively well with minimal overhead out of the box, and is unlikely to be a bottleneck in most applications.

The two main sources of overhead are:

1. Cost of context switching between threads when [delegating to synchronous methods](#delegation-to-sync-methods). This can be addressed by providing a native asynchronous implementation.
2. In [LCEL](/docs/concepts/lcel/) any "cheap functions" that appear as part of the chain will be either scheduled as tasks on the event loop (if they are async) or run in a separate thread (if they are sync), rather than just be run inline.

The latency overhead you should expect from these is between tens of microseconds to a few milliseconds.

A more common source of performance issues arises from users accidentally blocking the event loop by calling synchronous code in an async context (e.g., calling `invoke` rather than `ainvoke`).

## Compatibility[â€‹](#compatibility "Direct link to Compatibility")

LangChain is only compatible with the `asyncio` library, which is distributed as part of the Python standard library. It will not work with other async libraries like `trio` or `curio`.

In Python 3.9 and 3.10, [asyncio's tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) did not accept a `context` parameter. Due to this limitation, LangChain cannot automatically propagate the `RunnableConfig` down the call chain in certain scenarios.

If you are experiencing issues with streaming, callbacks or tracing in async code and are using Python 3.9 or 3.10, this is a likely cause.

Please read [Propagation RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig) for more details to learn how to propagate the `RunnableConfig` down the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).

## How to use in ipython and jupyter notebooks[â€‹](#how-to-use-in-ipython-and-jupyter-notebooks "Direct link to How to use in ipython and jupyter notebooks")

As of IPython 7.0, IPython supports asynchronous REPLs. This means that you can use the `await` keyword in the IPython REPL and Jupyter Notebooks without any additional setup. For more information, see the [IPython blog post](https://blog.jupyter.org/ipython-7-0-async-repl-a35ce050f7f7).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/async.mdx)

* * *


- [Langchain asynchronous APIs](#langchain-asynchronous-apis)
- [Delegation to sync methods](#delegation-to-sync-methods)
- [Performance](#performance)
- [Compatibility](#compatibility)
- [How to use in ipython and jupyter notebooks](#how-to-use-in-ipython-and-jupyter-notebooks)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/message_history.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/message_history.ipynb)

# How to add message history

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chaining runnables](/docs/how_to/sequence/)
- [Prompt templates](/docs/concepts/prompt_templates/)
- [Chat Messages](/docs/concepts/messages/)
- [LangGraph persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/)

note

This guide previously covered the [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) abstraction. You can access this version of the guide in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/message_history/).

As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.

If your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.

Please see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.

Passing conversation state into and out a chain is vital when building a chatbot. LangGraph implements a built-in persistence layer, allowing chain states to be automatically persisted in memory, or external backends such as SQLite, Postgres or Redis. Details can be found in the LangGraph [persistence documentation](https://langchain-ai.github.io/langgraph/how-tos/persistence/).

In this guide we demonstrate how to add persistence to arbitrary LangChain runnables by wrapping them in a minimal LangGraph application. This lets us persist the message history and other elements of the chain's state, simplifying the development of multi-turn applications. It also supports multiple threads, enabling a single application to interact separately with multiple users.

## Setup[â€‹](#setup "Direct link to Setup")

Let's initialize a chat model:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

## Example: message inputs[â€‹](#example-message-inputs "Direct link to Example: message inputs")

Adding memory to a [chat model](/docs/concepts/chat_models/) provides a simple example. Chat models accept a list of messages as input and output a message. LangGraph includes a built-in `MessagesState` that we can use for this purpose.

Below, we:

1. Define the graph state to be a list of messages;
2. Add a single node to the graph that calls a chat model;
3. Compile the graph with an in-memory checkpointer to store messages between runs.

info

The output of a LangGraph application is its [state](https://langchain-ai.github.io/langgraph/concepts/low_level/). This can be any Python type, but in this context it will typically be a `TypedDict` that matches the schema of your runnable.

```python
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

# Define a new graph
workflow = StateGraph(state_schema=MessagesState)


# Define the function that calls the model
def call_model(state: MessagesState):
    response = llm.invoke(state["messages"])
    # Update message history with response:
    return {"messages": response}


# Define the (single) node in the graph
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# Add memory
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

When we run the application, we pass in a configuration `dict` that specifies a `thread_id`. This ID is used to distinguish conversational threads (e.g., between different users).

```python
config = {"configurable": {"thread_id": "abc123"}}
```

We can then invoke the application:

```python
query = "Hi! I'm Bob."

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()  # output contains all messages in state
```

```output
==================================[1m Ai Message [0m==================================

It's nice to meet you, Bob! I'm Claude, an AI assistant created by Anthropic. How can I help you today?
```

```python
query = "What's my name?"

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

```output
==================================[1m Ai Message [0m==================================

Your name is Bob, as you introduced yourself at the beginning of our conversation.
```

Note that states are separated for different threads. If we issue the same query to a thread with a new `thread_id`, the model indicates that it does not know the answer:

```python
query = "What's my name?"
config = {"configurable": {"thread_id": "abc234"}}

input_messages = [HumanMessage(query)]
output = app.invoke({"messages": input_messages}, config)
output["messages"][-1].pretty_print()
```

```output
==================================[1m Ai Message [0m==================================

I'm afraid I don't actually know your name. As an AI assistant, I don't have personal information about you unless you provide it to me directly.
```

## Example: dictionary inputs[â€‹](#example-dictionary-inputs "Direct link to Example: dictionary inputs")

LangChain runnables often accept multiple inputs via separate keys in a single `dict` argument. A common example is a prompt template with multiple parameters.

Whereas before our runnable was a chat model, here we chain together a prompt template and chat model.

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Answer in {language}."),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

runnable = prompt | llm
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)

For this scenario, we define the graph state to include these parameters (in addition to the message history). We then define a single-node graph in the same way as before.

Note that in the below state:

- Updates to the `messages` list will append messages;
- Updates to the `language` string will overwrite the string.

```python
from typing import Sequence

from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict


class State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    language: str


workflow = StateGraph(state_schema=State)


def call_model(state: State):
    response = runnable.invoke(state)
    # Update message history with response:
    return {"messages": [response]}


workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

**API Reference:**[BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [add\_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)

```python
config = {"configurable": {"thread_id": "abc345"}}

input_dict = {
    "messages": [HumanMessage("Hi, I'm Bob.")],
    "language": "Spanish",
}
output = app.invoke(input_dict, config)
output["messages"][-1].pretty_print()
```

```output
==================================[1m Ai Message [0m==================================

Â¡Hola, Bob! Es un placer conocerte.
```

## Managing message history[â€‹](#managing-message-history "Direct link to Managing message history")

The message history (and other elements of the application state) can be accessed via `.get_state`:

```python
state = app.get_state(config).values

print(f'Language: {state["language"]}')
for message in state["messages"]:
    message.pretty_print()
```

```output
Language: Spanish
================================[1m Human Message [0m=================================

Hi, I'm Bob.
==================================[1m Ai Message [0m==================================

Â¡Hola, Bob! Es un placer conocerte.
```

We can also update the state via `.update_state`. For example, we can manually append a new message:

```python
from langchain_core.messages import HumanMessage

_ = app.update_state(config, {"messages": [HumanMessage("Test")]})
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

```python
state = app.get_state(config).values

print(f'Language: {state["language"]}')
for message in state["messages"]:
    message.pretty_print()
```

```output
Language: Spanish
================================[1m Human Message [0m=================================

Hi, I'm Bob.
==================================[1m Ai Message [0m==================================

Â¡Hola, Bob! Es un placer conocerte.
================================[1m Human Message [0m=================================

Test
```

For details on managing state, including deleting messages, see the LangGraph documentation:

- [How to delete messages](https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages/)
- [How to view and update past graph state](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/message_history.ipynb)

* * *


- [Setup](#setup)
- [Example: message inputs](#example-message-inputs)
- [Example: dictionary inputs](#example-dictionary-inputs)
- [Managing message history](#managing-message-history)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/callbacks_attach.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/callbacks_attach.ipynb)

# How to attach callbacks to a runnable

Prerequisites

This guide assumes familiarity with the following concepts:

- [Callbacks](/docs/concepts/callbacks/)
- [Custom callback handlers](/docs/how_to/custom_callbacks/)
- [Chaining runnables](/docs/how_to/sequence/)
- [Attach runtime arguments to a Runnable](/docs/how_to/binding/)

If you are composing a chain of runnables and want to reuse callbacks across multiple executions, you can attach callbacks with the [`.with_config()`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_config) method. This saves you the need to pass callbacks in each time you invoke the chain.

important

`with_config()` binds a configuration which will be interpreted as **runtime** configuration. So these callbacks will propagate to all child components.

Here's an example:

```python
from typing import Any, Dict, List

from langchain_anthropic import ChatAnthropic
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage
from langchain_core.outputs import LLMResult
from langchain_core.prompts import ChatPromptTemplate


class LoggingHandler(BaseCallbackHandler):
    def on_chat_model_start(
        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs
    ) -> None:
        print("Chat model started")

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        print(f"Chat model ended, response: {response}")

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs
    ) -> None:
        print(f"Chain {serialized.get('name')} started")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:
        print(f"Chain ended, outputs: {outputs}")


callbacks = [LoggingHandler()]
llm = ChatAnthropic(model="claude-3-sonnet-20240229")
prompt = ChatPromptTemplate.from_template("What is 1 + {number}?")

chain = prompt | llm

chain_with_callbacks = chain.with_config(callbacks=callbacks)

chain_with_callbacks.invoke({"number": "2"})
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [BaseCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [LLMResult](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.llm_result.LLMResult.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
Chain RunnableSequence started
Chain ChatPromptTemplate started
Chain ended, outputs: messages=[HumanMessage(content='What is 1 + 2?')]
Chat model started
Chat model ended, response: generations=[[ChatGeneration(text='1 + 2 = 3', message=AIMessage(content='1 + 2 = 3', response_metadata={'id': 'msg_01NTYMsH9YxkoWsiPYs4Lemn', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}}, id='run-d6bcfd72-9c94-466d-bac0-f39e456ad6e3-0'))]] llm_output={'id': 'msg_01NTYMsH9YxkoWsiPYs4Lemn', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}} run=None
Chain ended, outputs: content='1 + 2 = 3' response_metadata={'id': 'msg_01NTYMsH9YxkoWsiPYs4Lemn', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}} id='run-d6bcfd72-9c94-466d-bac0-f39e456ad6e3-0'
```

```output
AIMessage(content='1 + 2 = 3', response_metadata={'id': 'msg_01NTYMsH9YxkoWsiPYs4Lemn', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}}, id='run-d6bcfd72-9c94-466d-bac0-f39e456ad6e3-0')
```

The bound callbacks will run for all nested module runs.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to attach callbacks to a chain.

Next, check out the other how-to guides in this section, such as how to [pass callbacks in at runtime](/docs/how_to/callbacks_runtime/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/callbacks_attach.ipynb)

* * *


- [Next steps](#next-steps)








[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/introduction.mdx)

# Introduction

**LangChain** is a framework for developing applications powered by large language models (LLMs).

LangChain simplifies every stage of the LLM application lifecycle:

- **Development**: Build your applications using LangChain's open-source [components](/docs/concepts/) and [third-party integrations](/docs/integrations/providers/). Use [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-class streaming and human-in-the-loop support.
- **Productionization**: Use [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.
- **Deployment**: Turn your LangGraph applications into production-ready APIs and Assistants with [LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/).

![Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.](/svg/langchain_stack_112024.svg "LangChain Framework Overview")![Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.](/svg/langchain_stack_112024_dark.svg "LangChain Framework Overview")

LangChain implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers. See the [integrations](/docs/integrations/providers/) page for more.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
model.invoke("Hello, world!")
```

note

These docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.

## Architecture[â€‹](#architecture "Direct link to Architecture")

The LangChain framework consists of multiple open-source libraries. Read more in the [Architecture](/docs/concepts/architecture/) page.

- **`langchain-core`** : Base abstractions for chat models and other components.
- **Integration packages** (e.g. `langchain-openai`, `langchain-anthropic`, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.
- **`langchain`** : Chains, agents, and retrieval strategies that make up an application's cognitive architecture.
- **`langchain-community`** : Third-party integrations that are community maintained.
- **`langgraph`** : Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See [LangGraph documentation](https://langchain-ai.github.io/langgraph/).

## Guides[â€‹](#guides "Direct link to Guides")

### [Tutorials](/docs/tutorials/)[â€‹](#tutorials "Direct link to tutorials")

If you're looking to build something specific or are more of a hands-on learner, check out our [tutorials section](/docs/tutorials/). This is the best place to get started.

These are the best ones to get started with:

- [Build a Simple LLM Application](/docs/tutorials/llm_chain/)
- [Build a Chatbot](/docs/tutorials/chatbot/)
- [Build an Agent](/docs/tutorials/agents/)
- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)

Explore the full list of LangChain tutorials [here](/docs/tutorials/), and check out other [LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available [here](https://academy.langchain.com/courses/intro-to-langgraph).

### [How-to guides](/docs/how_to/)[â€‹](#how-to-guides "Direct link to how-to-guides")

[Here](/docs/how_to/) youâ€™ll find short answers to â€œHow do Iâ€¦.?â€ types of questions. These how-to guides donâ€™t cover topics in depth â€“ youâ€™ll find that material in the [Tutorials](/docs/tutorials/) and the [API Reference](https://python.langchain.com/api_reference/). However, these guides will help you quickly accomplish common tasks using [chat models](/docs/how_to/#chat-models), [vector stores](/docs/how_to/#vector-stores), and other common LangChain components.

Check out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-tos/).

### [Conceptual guide](/docs/concepts/)[â€‹](#conceptual-guide "Direct link to conceptual-guide")

Introductions to all the key parts of LangChain youâ€™ll need to know! [Here](/docs/concepts/) you'll find high level explanations of all LangChain concepts.

For a deeper dive into LangGraph concepts, check out [this page](https://langchain-ai.github.io/langgraph/concepts/).

### [Integrations](/docs/integrations/providers/)[â€‹](#integrations "Direct link to integrations")

LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. If you're looking to get up and running quickly with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/), or other LangChain components from a specific provider, check out our growing list of [integrations](/docs/integrations/providers/).

### [API reference](https://python.langchain.com/api_reference/)[â€‹](#api-reference "Direct link to api-reference")

Head to the reference section for full documentation of all classes and methods in the LangChain Python packages.

## Ecosystem[â€‹](#ecosystem "Direct link to Ecosystem")

### [ðŸ¦œðŸ› ï¸ LangSmith](https://docs.smith.langchain.com)[â€‹](#%EF%B8%8F-langsmith "Direct link to ï¸-langsmith")

Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.

### [ðŸ¦œðŸ•¸ï¸ LangGraph](https://langchain-ai.github.io/langgraph)[â€‹](#%EF%B8%8F-langgraph "Direct link to ï¸-langgraph")

Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.

## Additional resources[â€‹](#additional-resources "Direct link to Additional resources")

### [Versions](/docs/versions/v0_3/)[â€‹](#versions "Direct link to versions")

See what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.

### [Security](/docs/security/)[â€‹](#security "Direct link to security")

Read up on [security](/docs/security/) best practices to make sure you're developing safely with LangChain.

### [Contributing](/docs/contributing/)[â€‹](#contributing "Direct link to contributing")

Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/introduction.mdx)

* * *

  - [Tutorials](#tutorials)
  - [How-to guides](#how-to-guides)
  - [Conceptual guide](#conceptual-guide)
  - [Integrations](#integrations)
  - [API reference](#api-reference)
- [Ecosystem](#ecosystem)
  
  - [ðŸ¦œðŸ› ï¸ LangSmith](#%EF%B8%8F-langsmith)
  - [ðŸ¦œðŸ•¸ï¸ LangGraph](#%EF%B8%8F-langgraph)
- [Additional resources](#additional-resources)
  
  - [Versions](#versions)
  - [Security](#security)
  - [Contributing](#contributing)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/multimodality.mdx)

# Multimodality

## Overview[â€‹](#overview "Direct link to Overview")

**Multimodality** refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various components, allowing models and systems to handle and process a mix of these data types seamlessly.

- **Chat Models**: These could, in theory, accept and generate multimodal inputs and outputs, handling a variety of data types like text, images, audio, and video.
- **Embedding Models**: Embedding Models can represent multimodal content, embedding various forms of dataâ€”such as text, images, and audioâ€”into vector spaces.
- **Vector Stores**: Vector stores could search over embeddings that represent multimodal data, enabling retrieval across different types of information.

## Multimodality in chat models[â€‹](#multimodality-in-chat-models "Direct link to Multimodality in chat models")

Pre-requisites

- [Chat models](/docs/concepts/chat_models/)
- [Messages](/docs/concepts/messages/)

LangChain supports multimodal data as input to chat models:

1. Following provider-specific formats
2. Adhering to a cross-provider standard (see [how-to guides](/docs/how_to/#multimodal) for detail)

### How to use multimodal models[â€‹](#how-to-use-multimodal-models "Direct link to How to use multimodal models")

- Use the [chat model integration table](/docs/integrations/chat/) to identify which models support multimodality.
- Reference the [relevant how-to guides](/docs/how_to/#multimodal) for specific examples of how to use multimodal models.

### What kind of multimodality is supported?[â€‹](#what-kind-of-multimodality-is-supported "Direct link to What kind of multimodality is supported?")

#### Inputs[â€‹](#inputs "Direct link to Inputs")

Some models can accept multimodal inputs, such as images, audio, video, or files. The types of multimodal inputs supported depend on the model provider. For instance, [OpenAI](/docs/integrations/chat/openai/), [Anthropic](/docs/integrations/chat/anthropic/), and [Google Gemini](/docs/integrations/chat/google_generative_ai/) support documents like PDFs as inputs.

The gist of passing multimodal inputs to a chat model is to use content blocks that specify a type and corresponding data. For example, to pass an image to a chat model as URL:

```python
from langchain_core.messages import HumanMessage

message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the weather in this image:"},
        {
            "type": "image",
            "source_type": "url",
            "url": "https://...",
        },
    ],
)
response = model.invoke([message])
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

We can also pass the image as in-line data:

```python
from langchain_core.messages import HumanMessage

message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the weather in this image:"},
        {
            "type": "image",
            "source_type": "base64",
            "data": "<base64 string>",
            "mime_type": "image/jpeg",
        },
    ],
)
response = model.invoke([message])
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

To pass a PDF file as in-line data (or URL, as supported by providers such as Anthropic), just change `"type"` to `"file"` and `"mime_type"` to `"application/pdf"`.

See the [how-to guides](/docs/how_to/#multimodal) for more detail.

Most chat models that support multimodal **image** inputs also accept those values in OpenAI's [Chat Completions format](https://platform.openai.com/docs/guides/images?api-mode=chat):

```python
from langchain_core.messages import HumanMessage

message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the weather in this image:"},
        {"type": "image_url", "image_url": {"url": image_url}},
    ],
)
response = model.invoke([message])
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

Otherwise, chat models will typically accept the native, provider-specific content block format. See [chat model integrations](/docs/integrations/chat/) for detail on specific providers.

#### Outputs[â€‹](#outputs "Direct link to Outputs")

Some chat models support multimodal outputs, such as images and audio. Multimodal outputs will appear as part of the [AIMessage](/docs/concepts/messages/#aimessage) response object. See for example:

- Generating [audio outputs](/docs/integrations/chat/openai/#audio-generation-preview) with OpenAI;
- Generating [image outputs](/docs/integrations/chat/google_generative_ai/#image-generation) with Google Gemini.

#### Tools[â€‹](#tools "Direct link to Tools")

Currently, no chat model is designed to work **directly** with multimodal data in a [tool call request](/docs/concepts/tool_calling/) or [ToolMessage](/docs/concepts/tool_calling/) result.

However, a chat model can easily interact with multimodal data by invoking tools with references (e.g., a URL) to the multimodal data, rather than the data itself. For example, any model capable of [tool calling](/docs/concepts/tool_calling/) can be equipped with tools to download and process images, audio, or video.

## Multimodality in embedding models[â€‹](#multimodality-in-embedding-models "Direct link to Multimodality in embedding models")

Prerequisites

- [Embedding Models](/docs/concepts/embedding_models/)

**Embeddings** are vector representations of data used for tasks like similarity search and retrieval.

The current [embedding interface](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.embeddings.Embeddings.html#langchain_core.embeddings.embeddings.Embeddings) used in LangChain is optimized entirely for text-based data, and will **not** work with multimodal data.

As use cases involving multimodal search and retrieval tasks become more common, we expect to expand the embedding interface to accommodate other data types like images, audio, and video.

## Multimodality in vector stores[â€‹](#multimodality-in-vector-stores "Direct link to Multimodality in vector stores")

Prerequisites

- [Vector stores](/docs/concepts/vectorstores/)

Vector stores are databases for storing and retrieving embeddings, which are typically used in search and retrieval tasks. Similar to embeddings, vector stores are currently optimized for text-based data.

As use cases involving multimodal search and retrieval tasks become more common, we expect to expand the vector store interface to accommodate other data types like images, audio, and video.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/multimodality.mdx)

* * *


- [Overview](#overview)
- [Multimodality in chat models](#multimodality-in-chat-models)
  
  - [How to use multimodal models](#how-to-use-multimodal-models)
  - [What kind of multimodality is supported?](#what-kind-of-multimodality-is-supported)
- [Multimodality in embedding models](#multimodality-in-embedding-models)
- [Multimodality in vector stores](#multimodality-in-vector-stores)









# How to use toolkits

Toolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.

All Toolkits expose a `get_tools` method which returns a list of tools. You can therefore do:

```python
# Initialize a toolkit
toolkit = ExampleTookit(...)

# Get list of tools
tools = toolkit.get_tools()

# Create agent
agent = create_agent_method(llm, tools, prompt)
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/toolkits.mdx)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/query_no_queries.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/query_no_queries.ipynb)

# How to handle cases where no queries are generated

Sometimes, a query analysis technique may allow for any number of queries to be generated - including no queries! In this case, our overall chain will need to inspect the result of the query analysis before deciding whether to call the retriever or not.

We will use mock data for this example.

## Setup[â€‹](#setup "Direct link to Setup")

#### Install dependencies[â€‹](#install-dependencies "Direct link to Install dependencies")

```python
%pip install -qU langchain langchain-community langchain-openai langchain-chroma
```

```output
Note: you may need to restart the kernel to use updated packages.
```

#### Set environment variables[â€‹](#set-environment-variables "Direct link to Set environment variables")

We'll use OpenAI in this example:

```python
import getpass
import os

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass()

# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

### Create Index[â€‹](#create-index "Direct link to Create Index")

We will create a vectorstore over fake information.

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

texts = ["Harrison worked at Kensho"]
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_texts(
    texts,
    embeddings,
)
retriever = vectorstore.as_retriever()
```

**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

## Query analysis[â€‹](#query-analysis "Direct link to Query analysis")

We will use function calling to structure the output. However, we will configure the LLM such that is doesn't NEED to call the function representing a search query (should it decide not to). We will also then use a prompt to do query analysis that explicitly lays when it should and shouldn't make a search.

```python
from typing import Optional

from pydantic import BaseModel, Field


class Search(BaseModel):
    """Search over a database of job records."""

    query: str = Field(
        ...,
        description="Similarity search query applied to job record.",
    )
```

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

system = """You have the ability to issue search queries to get information to help answer user information.

You do not NEED to look things up. If you don't need to, then just respond normally."""
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_llm = llm.bind_tools([Search])
query_analyzer = {"question": RunnablePassthrough()} | prompt | structured_llm
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

We can see that by invoking this we get an message that sometimes - but not always - returns a tool call.

```python
query_analyzer.invoke("where did Harrison Work")
```

```output
AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_korLZrh08PTRL94f4L7rFqdj', 'function': {'arguments': '{"query":"Harrison"}', 'name': 'Search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 95, 'total_tokens': 109}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ea94d376-37bf-4f80-abe6-e3b42b767ea0-0', tool_calls=[{'name': 'Search', 'args': {'query': 'Harrison'}, 'id': 'call_korLZrh08PTRL94f4L7rFqdj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 95, 'output_tokens': 14, 'total_tokens': 109})
```

```python
query_analyzer.invoke("hi!")
```

```output
AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 93, 'total_tokens': 103}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-ebdfc44a-455a-4ca6-be85-84559886b1e1-0', usage_metadata={'input_tokens': 93, 'output_tokens': 10, 'total_tokens': 103})
```

## Retrieval with query analysis[â€‹](#retrieval-with-query-analysis "Direct link to Retrieval with query analysis")

So how would we include this in a chain? Let's look at an example below.

```python
from langchain_core.output_parsers.openai_tools import PydanticToolsParser
from langchain_core.runnables import chain

output_parser = PydanticToolsParser(tools=[Search])
```

**API Reference:**[PydanticToolsParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.openai_tools.PydanticToolsParser.html) | [chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)

```python
@chain
def custom_chain(question):
    response = query_analyzer.invoke(question)
    if "tool_calls" in response.additional_kwargs:
        query = output_parser.invoke(response)
        docs = retriever.invoke(query[0].query)
        # Could add more logic - like another LLM call - here
        return docs
    else:
        return response
```

```python
custom_chain.invoke("where did Harrison Work")
```

```output
Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1
```

```output
[Document(page_content='Harrison worked at Kensho')]
```

```python
custom_chain.invoke("hi!")
```

```output
AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 93, 'total_tokens': 103}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-e87f058d-30c0-4075-8a89-a01b982d557e-0', usage_metadata={'input_tokens': 93, 'output_tokens': 10, 'total_tokens': 103})
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/query_no_queries.ipynb)

* * *


- [Setup](#setup)
  
  - [Create Index](#create-index)
- [Query analysis](#query-analysis)
- [Retrieval with query analysis](#retrieval-with-query-analysis)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/streaming_llm.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/streaming_llm.ipynb)

# How to stream responses from an LLM

All `LLM`s implement the [Runnable interface](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable), which comes with **default** implementations of standard runnable methods (i.e. `ainvoke`, `batch`, `abatch`, `stream`, `astream`, `astream_events`).

The **default** streaming implementations provide an`Iterator` (or `AsyncIterator` for asynchronous streaming) that yields a single value: the final output from the underlying chat model provider.

The ability to stream the output token-by-token depends on whether the provider has implemented proper streaming support.

See which [integrations support token-by-token streaming here](/docs/integrations/llms/).

note

The **default** implementation does **not** provide support for token-by-token streaming, but it ensures that the model can be swapped in for any other model as it supports the same standard interface.

## Sync stream[â€‹](#sync-stream "Direct link to Sync stream")

Below we use a `|` to help visualize the delimiter between tokens.

```python
from langchain_openai import OpenAI

llm = OpenAI(model="gpt-3.5-turbo-instruct", temperature=0, max_tokens=512)
for chunk in llm.stream("Write me a 1 verse song about sparkling water."):
    print(chunk, end="|", flush=True)
```

**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

```output


|Spark|ling| water|,| oh| so clear|
|Bubbles dancing|,| without| fear|
|Refreshing| taste|,| a| pure| delight|
|Spark|ling| water|,| my| thirst|'s| delight||
```

## Async streaming[â€‹](#async-streaming "Direct link to Async streaming")

Let's see how to stream in an async setting using `astream`.

```python
from langchain_openai import OpenAI

llm = OpenAI(model="gpt-3.5-turbo-instruct", temperature=0, max_tokens=512)
async for chunk in llm.astream("Write me a 1 verse song about sparkling water."):
    print(chunk, end="|", flush=True)
```

**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

```output


|Spark|ling| water|,| oh| so clear|
|Bubbles dancing|,| without| fear|
|Refreshing| taste|,| a| pure| delight|
|Spark|ling| water|,| my| thirst|'s| delight||
```

## Async event streaming[â€‹](#async-event-streaming "Direct link to Async event streaming")

LLMs also support the standard [astream events](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream_events) method.

tip

`astream_events` is most useful when implementing streaming in a larger LLM application that contains multiple steps (e.g., an application that involves an `agent`).

```python
from langchain_openai import OpenAI

llm = OpenAI(model="gpt-3.5-turbo-instruct", temperature=0, max_tokens=512)

idx = 0

async for event in llm.astream_events(
    "Write me a 1 verse song about goldfish on the moon", version="v1"
):
    idx += 1
    if idx >= 5:  # Truncate the output
        print("...Truncated")
        break
    print(event)
```

**API Reference:**[OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/streaming_llm.ipynb)

* * *


- [Sync stream](#sync-stream)
- [Async streaming](#async-streaming)
- [Async event streaming](#async-event-streaming)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/key_value_stores.mdx)

# Key-value stores

## Overview[â€‹](#overview "Direct link to Overview")

LangChain provides a key-value store interface for storing and retrieving data.

LangChain includes a [`BaseStore`](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) interface, which allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a more specific `BaseStore[str, bytes]` instance that stores binary data (referred to as a `ByteStore`), and internally take care of encoding and decoding data for their specific needs.

This means that as a user, you only need to think about one type of store rather than different ones for different types of data.

## Usage[â€‹](#usage "Direct link to Usage")

The key-value store interface in LangChain is used primarily for:

1. Caching [embeddings](/docs/concepts/embedding_models/) via [CachedBackedEmbeddings](https://python.langchain.com/api_reference/langchain/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html#langchain.embeddings.cache.CacheBackedEmbeddings) to avoid recomputing embeddings for repeated queries or when re-indexing content.
2. As a simple [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) persistence layer in some retrievers.

Please see these how-to guides for more information:

- [How to cache embeddings guide](/docs/how_to/caching_embeddings/).
- [How to retriever using multiple vectors per document](/docs/how_to/custom_retriever/).

## Interface[â€‹](#interface "Direct link to Interface")

All [`BaseStores`](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) support the following interface. Note that the interface allows for modifying **multiple** key-value pairs at once:

- `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist
- `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys
- `mdelete(key: Sequence[str]) -> None`: delete multiple keys
- `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix

## Integrations[â€‹](#integrations "Direct link to Integrations")

Please reference the [stores integration page](/docs/integrations/stores/) for a list of available key-value store integrations.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/key_value_stores.mdx)

* * *


- [Overview](#overview)
- [Usage](#usage)
- [Interface](#interface)
- [Integrations](#integrations)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/graph_semantic.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/graph_semantic.ipynb)

# How to add a semantic layer over graph database

You can use database queries to retrieve information from a graph database like Neo4j. One option is to use LLMs to generate Cypher statements. While that option provides excellent flexibility, the solution could be brittle and not consistently generating precise Cypher statements. Instead of generating Cypher statements, we can implement Cypher templates as tools in a semantic layer that an LLM agent can interact with.

![graph_semantic.png](/assets/images/graph_semantic-365248d76b7862193c33f44eaa6ecaeb.png)

## Setup[â€‹](#setup "Direct link to Setup")

First, get required packages and set environment variables:

```python
%pip install --upgrade --quiet  langchain langchain-neo4j langchain-openai
```

We default to OpenAI models in this guide, but you can swap them out for the model provider of your choice.

```python
import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

# Uncomment the below to use LangSmith. Not required.
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
# os.environ["LANGSMITH_TRACING"] = "true"
```

```output
 Â·Â·Â·Â·Â·Â·Â·Â·
```

Next, we need to define Neo4j credentials. Follow [these installation steps](https://neo4j.com/docs/operations-manual/current/installation/) to set up a Neo4j database.

```python
os.environ["NEO4J_URI"] = "bolt://localhost:7687"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "password"
```

The below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.

```python
from langchain_neo4j import Neo4jGraph

graph = Neo4jGraph(refresh_schema=False)

# Import movie information

movies_query = """
LOAD CSV WITH HEADERS FROM 
'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'
AS row
MERGE (m:Movie {id:row.movieId})
SET m.released = date(row.released),
    m.title = row.title,
    m.imdbRating = toFloat(row.imdbRating)
FOREACH (director in split(row.director, '|') | 
    MERGE (p:Person {name:trim(director)})
    MERGE (p)-[:DIRECTED]->(m))
FOREACH (actor in split(row.actors, '|') | 
    MERGE (p:Person {name:trim(actor)})
    MERGE (p)-[:ACTED_IN]->(m))
FOREACH (genre in split(row.genres, '|') | 
    MERGE (g:Genre {name:trim(genre)})
    MERGE (m)-[:IN_GENRE]->(g))
"""

graph.query(movies_query)
```

**API Reference:**[Neo4jGraph](https://python.langchain.com/api_reference/neo4j/graphs/langchain_neo4j.graphs.neo4j_graph.Neo4jGraph.html)

```output
[]
```

## Custom tools with Cypher templates[â€‹](#custom-tools-with-cypher-templates "Direct link to Custom tools with Cypher templates")

A semantic layer consists of various tools exposed to an LLM that it can use to interact with a knowledge graph. They can be of various complexity. You can think of each tool in a semantic layer as a function.

The function we will implement is to retrieve information about movies or their cast.

```python
description_query = """
MATCH (m:Movie|Person)
WHERE m.title CONTAINS $candidate OR m.name CONTAINS $candidate
MATCH (m)-[r:ACTED_IN|IN_GENRE]-(t)
WITH m, type(r) as type, collect(coalesce(t.name, t.title)) as names
WITH m, type+": "+reduce(s="", n IN names | s + n + ", ") as types
WITH m, collect(types) as contexts
WITH m, "type:" + labels(m)[0] + "\ntitle: "+ coalesce(m.title, m.name) 
       + "\nyear: "+coalesce(m.released,"") +"\n" +
       reduce(s="", c in contexts | s + substring(c, 0, size(c)-2) +"\n") as context
RETURN context LIMIT 1
"""


def get_information(entity: str) -> str:
    try:
        data = graph.query(description_query, params={"candidate": entity})
        return data[0]["context"]
    except IndexError:
        return "No information was found"
```

You can observe that we have defined the Cypher statement used to retrieve information. Therefore, we can avoid generating Cypher statements and use the LLM agent to only populate the input parameters. To provide additional information to an LLM agent about when to use the tool and their input parameters, we wrap the function as a tool.

```python
from typing import Optional, Type

from langchain_core.tools import BaseTool
from pydantic import BaseModel, Field


class InformationInput(BaseModel):
    entity: str = Field(description="movie or a person mentioned in the question")


class InformationTool(BaseTool):
    name: str = "Information"
    description: str = (
        "useful for when you need to answer questions about various actors or movies"
    )
    args_schema: Type[BaseModel] = InformationInput

    def _run(
        self,
        entity: str,
    ) -> str:
        """Use the tool."""
        return get_information(entity)

    async def _arun(
        self,
        entity: str,
    ) -> str:
        """Use the tool asynchronously."""
        return get_information(entity)
```

**API Reference:**[BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html)

## LangGraph Agent[â€‹](#langgraph-agent "Direct link to LangGraph Agent")

We will implement a straightforward ReAct agent using LangGraph.

The agent consists of an LLM and tools step. As we interact with the agent, we will first call the LLM to decide if we should use tools. Then we will run a loop:

If the agent said to take an action (i.e. call tool), weâ€™ll run the tools and pass the results back to the agent. If the agent did not ask to run tools, we will finish (respond to the user).

The code implementation is as straightforward as it gets. First we bind the tools to the LLM and define the assistant step.

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import MessagesState

llm = ChatOpenAI(model="gpt-4o")

tools = [InformationTool()]
llm_with_tools = llm.bind_tools(tools)

# System message
sys_msg = SystemMessage(
    content="You are a helpful assistant tasked with finding and explaining relevant information about movies."
)


# Node
def assistant(state: MessagesState):
    return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

Next we define the LangGraph flow.

```python
from IPython.display import Image, display
from langgraph.graph import END, START, StateGraph
from langgraph.prebuilt import ToolNode, tools_condition

# Graph
builder = StateGraph(MessagesState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine how the control flow moves
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools
    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END
    tools_condition,
)
builder.add_edge("tools", "assistant")
react_graph = builder.compile()

# Show
display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))
```

**API Reference:**[StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools\_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)

![](data:image/png;base64,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)

Let's test the workflow now with an example question.

```python
input_messages = [HumanMessage(content="Who played in the Casino?")]
messages = react_graph.invoke({"messages": input_messages})
for m in messages["messages"]:
    m.pretty_print()
```

```output
================================[1m Human Message [0m=================================

Who played in the Casino?
==================================[1m Ai Message [0m==================================
Tool Calls:
  Information (call_j4usgFStGtBM16fuguRaeoGc)
 Call ID: call_j4usgFStGtBM16fuguRaeoGc
  Args:
    entity: Casino
=================================[1m Tool Message [0m=================================
Name: Information

type:Movie
title: Casino
year: 1995-11-22
ACTED_IN: Robert De Niro, Joe Pesci, Sharon Stone, James Woods
IN_GENRE: Drama, Crime

==================================[1m Ai Message [0m==================================

The movie "Casino," released in 1995, features the following actors:

- Robert De Niro
- Joe Pesci
- Sharon Stone
- James Woods

The film is in the Drama and Crime genres.
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/graph_semantic.ipynb)

* * *


- [Setup](#setup)
- [Custom tools with Cypher templates](#custom-tools-with-cypher-templates)
- [LangGraph Agent](#langgraph-agent)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/chatbots_retrieval.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/chatbots_retrieval.ipynb)

# How to add retrieval to chatbots

[Retrieval](/docs/concepts/retrieval/) is a common technique chatbots use to augment their responses with data outside a chat model's training data. This section will cover how to implement retrieval in the context of chatbots, but it's worth noting that retrieval is a very subtle and deep topic - we encourage you to explore [other parts of the documentation](/docs/how_to/#qa-with-rag) that go into greater depth!

## Setup[â€‹](#setup "Direct link to Setup")

You'll need to install a few packages, and have your OpenAI API key set as an environment variable named `OPENAI_API_KEY`:

```python
%pip install -qU langchain langchain-openai langchain-chroma beautifulsoup4

# Set env var OPENAI_API_KEY or load from a .env file:
import dotenv

dotenv.load_dotenv()
```

```output
[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.
You should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m
[0mNote: you may need to restart the kernel to use updated packages.
```

```output
True
```

Let's also set up a chat model that we'll use for the below examples.

```python
from langchain_openai import ChatOpenAI

chat = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

## Creating a retriever[â€‹](#creating-a-retriever "Direct link to Creating a retriever")

We'll use [the LangSmith documentation](https://docs.smith.langchain.com/overview) as source material and store the content in a [vector store](/docs/concepts/vectorstores/) for later retrieval. Note that this example will gloss over some of the specifics around parsing and storing a data source - you can see more [in-depth documentation on creating retrieval systems here](/docs/how_to/#qa-with-rag).

Let's use a document loader to pull text from the docs:

```python
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://docs.smith.langchain.com/overview")
data = loader.load()
```

**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)

Next, we split it into smaller chunks that the LLM's context window can handle and store it in a vector database:

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
all_splits = text_splitter.split_documents(data)
```

**API Reference:**[RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

Then we embed and store those chunks in a vector database:

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
```

**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

And finally, let's create a retriever from our initialized vectorstore:

```python
# k is the number of chunks to retrieve
retriever = vectorstore.as_retriever(k=4)

docs = retriever.invoke("Can LangSmith help test my LLM applications?")

docs
```

```output
[Document(page_content='Skip to main contentðŸ¦œï¸ðŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
 Document(page_content='LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
 Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
 Document(page_content="does that affect the output?\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'})]
```

We can see that invoking the retriever above results in some parts of the LangSmith docs that contain information about testing that our chatbot can use as context when answering questions. And now we've got a retriever that can return related data from the LangSmith docs!

## Document chains[â€‹](#document-chains "Direct link to Document chains")

Now that we have a retriever that can return LangChain docs, let's create a chain that can use them as context to answer questions. We'll use a `create_stuff_documents_chain` helper function to "stuff" all of the input documents into the prompt. It will also handle formatting the docs as strings.

In addition to a chat model, the function also expects a prompt that has a `context` variables, as well as a placeholder for chat history messages named `messages`. We'll create an appropriate prompt and pass it as shown below:

```python
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

SYSTEM_TEMPLATE = """
Answer the user's questions based on the below context. 
If the context doesn't contain any relevant information to the question, don't make something up and just say "I don't know":

<context>
{context}
</context>
"""

question_answering_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            SYSTEM_TEMPLATE,
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

document_chain = create_stuff_documents_chain(chat, question_answering_prompt)
```

**API Reference:**[create\_stuff\_documents\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)

We can invoke this `document_chain` by itself to answer questions. Let's use the docs we retrieved above and the same question, `how can langsmith help with testing?`:

```python
from langchain_core.messages import HumanMessage

document_chain.invoke(
    {
        "context": docs,
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?")
        ],
    }
)
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

```output
'Yes, LangSmith can help test and evaluate your LLM applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'
```

Looks good! For comparison, we can try it with no context docs and compare the result:

```python
document_chain.invoke(
    {
        "context": [],
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?")
        ],
    }
)
```

```output
"I don't know about LangSmith's specific capabilities for testing LLM applications. It's best to reach out to LangSmith directly to inquire about their services and how they can assist with testing your LLM applications."
```

We can see that the LLM does not return any results.

## Retrieval chains[â€‹](#retrieval-chains "Direct link to Retrieval chains")

Let's combine this document chain with the retriever. Here's one way this can look:

```python
from typing import Dict

from langchain_core.runnables import RunnablePassthrough


def parse_retriever_input(params: Dict):
    return params["messages"][-1].content


retrieval_chain = RunnablePassthrough.assign(
    context=parse_retriever_input | retriever,
).assign(
    answer=document_chain,
)
```

**API Reference:**[RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)

Given a list of input messages, we extract the content of the last message in the list and pass that to the retriever to fetch some documents. Then, we pass those documents as context to our document chain to generate a final response.

Invoking this chain combines both steps outlined above:

```python
retrieval_chain.invoke(
    {
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?")
        ],
    }
)
```

```output
{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],
 'context': [Document(page_content='Skip to main contentðŸ¦œï¸ðŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
  Document(page_content='LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
  Document(page_content="does that affect the output?\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'})],
 'answer': 'Yes, LangSmith can help test and evaluate your LLM applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'}
```

Looks good!

## Query transformation[â€‹](#query-transformation "Direct link to Query transformation")

Our retrieval chain is capable of answering questions about LangSmith, but there's a problem - chatbots interact with users conversationally, and therefore have to deal with followup questions.

The chain in its current form will struggle with this. Consider a followup question to our original question like `Tell me more!`. If we invoke our retriever with that query directly, we get documents irrelevant to LLM application testing:

```python
retriever.invoke("Tell me more!")
```

```output
[Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
 Document(page_content='playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
 Document(page_content='however, there is still no complete substitute for human review to get the utmost quality and reliability from your application.', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
 Document(page_content='Skip to main contentðŸ¦œï¸ðŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'})]
```

This is because the retriever has no innate concept of state, and will only pull documents most similar to the query given. To solve this, we can transform the query into a standalone query without any external references an LLM.

Here's an example:

```python
from langchain_core.messages import AIMessage, HumanMessage

query_transform_prompt = ChatPromptTemplate.from_messages(
    [
        MessagesPlaceholder(variable_name="messages"),
        (
            "user",
            "Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.",
        ),
    ]
)

query_transformation_chain = query_transform_prompt | chat

query_transformation_chain.invoke(
    {
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?"),
            AIMessage(
                content="Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."
            ),
            HumanMessage(content="Tell me more!"),
        ],
    }
)
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

```output
AIMessage(content='"LangSmith LLM application testing and evaluation"')
```

Awesome! That transformed query would pull up context documents related to LLM application testing.

Let's add this to our retrieval chain. We can wrap our retriever as follows:

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableBranch

query_transforming_retriever_chain = RunnableBranch(
    (
        lambda x: len(x.get("messages", [])) == 1,
        # If only one message, then we just pass that message's content to retriever
        (lambda x: x["messages"][-1].content) | retriever,
    ),
    # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever
    query_transform_prompt | chat | StrOutputParser() | retriever,
).with_config(run_name="chat_retriever_chain")
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [RunnableBranch](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.branch.RunnableBranch.html)

Then, we can use this query transformation chain to make our retrieval chain better able to handle such followup questions:

```python
SYSTEM_TEMPLATE = """
Answer the user's questions based on the below context. 
If the context doesn't contain any relevant information to the question, don't make something up and just say "I don't know":

<context>
{context}
</context>
"""

question_answering_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            SYSTEM_TEMPLATE,
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

document_chain = create_stuff_documents_chain(chat, question_answering_prompt)

conversational_retrieval_chain = RunnablePassthrough.assign(
    context=query_transforming_retriever_chain,
).assign(
    answer=document_chain,
)
```

Awesome! Let's invoke this new chain with the same inputs as earlier:

```python
conversational_retrieval_chain.invoke(
    {
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?"),
        ]
    }
)
```

```output
{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],
 'context': [Document(page_content='Skip to main contentðŸ¦œï¸ðŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
  Document(page_content='LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
  Document(page_content="does that affect the output?\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'})],
 'answer': 'Yes, LangSmith can help test and evaluate LLM (Language Model) applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'}
```

```python
conversational_retrieval_chain.invoke(
    {
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?"),
            AIMessage(
                content="Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."
            ),
            HumanMessage(content="Tell me more!"),
        ],
    }
)
```

```output
{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'),
  AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'),
  HumanMessage(content='Tell me more!')],
 'context': [Document(page_content='LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
  Document(page_content='Skip to main contentðŸ¦œï¸ðŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}),
  Document(page_content='LangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'})],
 'answer': 'LangSmith simplifies the initial setup for building reliable LLM applications, but it acknowledges that there is still work needed to bring the performance of prompts, chains, and agents up to the level where they are reliable enough to be used in production. It also provides the capability to manually review and annotate runs through annotation queues, allowing you to select runs based on criteria like model type or automatic evaluation scores for human review. This feature is particularly useful for assessing subjective qualities that automatic evaluators struggle with.'}
```

You can check out [this LangSmith trace](https://smith.langchain.com/public/bb329a3b-e92a-4063-ad78-43f720fbb5a2/r) to see the internal query transformation step for yourself.

## Streaming[â€‹](#streaming "Direct link to Streaming")

Because this chain is constructed with LCEL, you can use familiar methods like `.stream()` with it:

```python
stream = conversational_retrieval_chain.stream(
    {
        "messages": [
            HumanMessage(content="Can LangSmith help test my LLM applications?"),
            AIMessage(
                content="Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."
            ),
            HumanMessage(content="Tell me more!"),
        ],
    }
)

for chunk in stream:
    print(chunk)
```

```output
{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'), AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'), HumanMessage(content='Tell me more!')]}
{'context': [Document(page_content='LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}), Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}), Document(page_content='Skip to main contentðŸ¦œï¸ðŸ› ï¸ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'}), Document(page_content='LangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ðŸ¦œï¸ðŸ› ï¸ LangSmith'})]}
{'answer': ''}
{'answer': 'Lang'}
{'answer': 'Smith'}
{'answer': ' simpl'}
{'answer': 'ifies'}
{'answer': ' the'}
{'answer': ' initial'}
{'answer': ' setup'}
{'answer': ' for'}
{'answer': ' building'}
{'answer': ' reliable'}
{'answer': ' L'}
{'answer': 'LM'}
{'answer': ' applications'}
{'answer': '.'}
{'answer': ' It'}
{'answer': ' provides'}
{'answer': ' features'}
{'answer': ' for'}
{'answer': ' manually'}
{'answer': ' reviewing'}
{'answer': ' and'}
{'answer': ' annot'}
{'answer': 'ating'}
{'answer': ' runs'}
{'answer': ' through'}
{'answer': ' annotation'}
{'answer': ' queues'}
{'answer': ','}
{'answer': ' allowing'}
{'answer': ' you'}
{'answer': ' to'}
{'answer': ' select'}
{'answer': ' runs'}
{'answer': ' based'}
{'answer': ' on'}
{'answer': ' criteria'}
{'answer': ' like'}
{'answer': ' model'}
{'answer': ' type'}
{'answer': ' or'}
{'answer': ' automatic'}
{'answer': ' evaluation'}
{'answer': ' scores'}
{'answer': ','}
{'answer': ' and'}
{'answer': ' queue'}
{'answer': ' them'}
{'answer': ' up'}
{'answer': ' for'}
{'answer': ' human'}
{'answer': ' review'}
{'answer': '.'}
{'answer': ' As'}
{'answer': ' a'}
{'answer': ' reviewer'}
{'answer': ','}
{'answer': ' you'}
{'answer': ' can'}
{'answer': ' quickly'}
{'answer': ' step'}
{'answer': ' through'}
{'answer': ' the'}
{'answer': ' runs'}
{'answer': ','}
{'answer': ' view'}
{'answer': ' the'}
{'answer': ' input'}
{'answer': ','}
{'answer': ' output'}
{'answer': ','}
{'answer': ' and'}
{'answer': ' any'}
{'answer': ' existing'}
{'answer': ' tags'}
{'answer': ' before'}
{'answer': ' adding'}
{'answer': ' your'}
{'answer': ' own'}
{'answer': ' feedback'}
{'answer': '.'}
{'answer': ' This'}
{'answer': ' can'}
{'answer': ' be'}
{'answer': ' particularly'}
{'answer': ' useful'}
{'answer': ' for'}
{'answer': ' assessing'}
{'answer': ' subjective'}
{'answer': ' qualities'}
{'answer': ' that'}
{'answer': ' automatic'}
{'answer': ' evalu'}
{'answer': 'ators'}
{'answer': ' struggle'}
{'answer': ' with'}
{'answer': '.'}
{'answer': ''}
```

## Further reading[â€‹](#further-reading "Direct link to Further reading")

This guide only scratches the surface of retrieval techniques. For more on different ways of ingesting, preparing, and retrieving the most relevant data, check out the relevant how-to guides [here](/docs/how_to/#document-loaders).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/chatbots_retrieval.ipynb)

* * *


- [Setup](#setup)
- [Creating a retriever](#creating-a-retriever)
- [Document chains](#document-chains)
- [Retrieval chains](#retrieval-chains)
- [Query transformation](#query-transformation)
- [Streaming](#streaming)
- [Further reading](#further-reading)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/tutorials/sql_qa.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/sql_qa.ipynb)

# Build a Question/Answering system over SQL data

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models/)
- [Tools](/docs/concepts/tools/)
- [Agents](/docs/concepts/agents/)
- [LangGraph](/docs/concepts/architecture/#langgraph)

Enabling a LLM system to query structured data can be qualitatively different from unstructured text data. Whereas in the latter it is common to generate text that can be searched against a vector database, the approach for structured data is often for the LLM to write and execute queries in a DSL, such as SQL. In this guide we'll go over the basic ways to create a Q&amp;A system over tabular data in databases. We will cover implementations using both [chains](/docs/tutorials/sql_qa/#chains) and [agents](/docs/tutorials/sql_qa/#agents). These systems will allow us to ask a question about the data in a database and get back a natural language answer. The main difference between the two is that our agent can query the database in a loop as many times as it needs to answer the question.

## âš ï¸ Security note âš ï¸[â€‹](#%EF%B8%8F-security-note-%EF%B8%8F "Direct link to âš ï¸ Security note âš ï¸")

Building Q&amp;A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, [see here](/docs/security/).

## Architecture[â€‹](#architecture "Direct link to Architecture")

At a high-level, the steps of these systems are:

1. **Convert question to SQL query**: Model converts user input to a SQL query.
2. **Execute SQL query**: Execute the query.
3. **Answer the question**: Model responds to user input using the query results.

Note that querying data in CSVs can follow a similar approach. See our [how-to guide](/docs/how_to/sql_csv/) on question-answering over CSV data for more detail.

![sql_usecase.png](/assets/images/sql_usecase-d432701261f05ab69b38576093718cf3.png)

## Setup[â€‹](#setup "Direct link to Setup")

First, get required packages and set environment variables:

```python
%%capture --no-stderr
%pip install --upgrade --quiet langchain-community langchainhub langgraph
```

```python
# Comment out the below to opt-out of using LangSmith in this notebook. Not required.
if not os.environ.get("LANGSMITH_API_KEY"):
    os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
    os.environ["LANGSMITH_TRACING"] = "true"
```

### Sample data[â€‹](#sample-data "Direct link to Sample data")

The below example will use a SQLite connection with the Chinook database, which is a sample database that represents a digital media store. Follow [these installation steps](https://database.guide/2-sample-databases-sqlite/) to create `Chinook.db` in the same directory as this notebook. You can also download and build the database via the command line:

```bash
curl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db
```

Now, `Chinook.db` is in our directory and we can interface with it using the SQLAlchemy-driven `SQLDatabase` class:

```python
from langchain_community.utilities import SQLDatabase

db = SQLDatabase.from_uri("sqlite:///Chinook.db")
print(db.dialect)
print(db.get_usable_table_names())
db.run("SELECT * FROM Artist LIMIT 10;")
```

**API Reference:**[SQLDatabase](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.sql_database.SQLDatabase.html)

```output
sqlite
['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']
```

```output
"[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'AntÃ´nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]"
```

Great! We've got a SQL database that we can query. Now let's try hooking it up to an LLM.

## Chains[â€‹](#chains "Direct link to Chains")

Chains are compositions of predictable steps. In [LangGraph](/docs/concepts/architecture/#langgraph), we can represent a chain via simple sequence of nodes. Let's create a sequence of steps that, given a question, does the following:

- converts the question into a SQL query;
- executes the query;
- uses the result to answer the original question.

There are scenarios not supported by this arrangement. For example, this system will execute a SQL query for any user input-- even "hello". Importantly, as we'll see below, some questions require more than one query to answer. We will address these scenarios in the Agents section.

### Application state[â€‹](#application-state "Direct link to Application state")

The LangGraph [state](https://langchain-ai.github.io/langgraph/concepts/low_level/#state) of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a `TypedDict`, but can also be a [Pydantic BaseModel](https://langchain-ai.github.io/langgraph/how-tos/state-model/).

For this application, we can just keep track of the input question, generated query, query result, and generated answer:

```python
from typing_extensions import TypedDict


class State(TypedDict):
    question: str
    query: str
    result: str
    answer: str
```

Now we just need functions that operate on this state and populate its contents.

### Convert question to SQL query[â€‹](#convert-question-to-sql-query "Direct link to Convert question to SQL query")

The first step is to take the user input and convert it to a SQL query. To reliably obtain SQL queries (absent markdown formatting and explanations or clarifications), we will make use of LangChain's [structured output](/docs/concepts/structured_outputs/) abstraction.

Let's select a chat model for our application:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

We will pull a prompt from the [Prompt Hub](https://smith.langchain.com/hub) to instruct the model.

```python
from langchain import hub

query_prompt_template = hub.pull("langchain-ai/sql-query-system-prompt")

assert len(query_prompt_template.messages) == 2
for message in query_prompt_template.messages:
    message.pretty_print()
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html)

```output
================================[1m System Message [0m================================

Given an input question, create a syntactically correct [33;1m[1;3m{dialect}[0m query to run to help find the answer. Unless the user specifies in his question a specific number of examples they wish to obtain, always limit your query to at most [33;1m[1;3m{top_k}[0m results. You can order the results by a relevant column to return the most interesting examples in the database.

Never query for all the columns from a specific table, only ask for a the few relevant columns given the question.

Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.

Only use the following tables:
[33;1m[1;3m{table_info}[0m
================================[1m Human Message [0m=================================

Question: [33;1m[1;3m{input}[0m
```

The prompt includes several parameters we will need to populate, such as the SQL dialect and table schemas. LangChain's [SQLDatabase](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.sql_database.SQLDatabase.html) object includes methods to help with this. Our `write_query` step will just populate these parameters and prompt a model to generate the SQL query:

```python
from typing_extensions import Annotated


class QueryOutput(TypedDict):
    """Generated SQL query."""

    query: Annotated[str, ..., "Syntactically valid SQL query."]


def write_query(state: State):
    """Generate SQL query to fetch information."""
    prompt = query_prompt_template.invoke(
        {
            "dialect": db.dialect,
            "top_k": 10,
            "table_info": db.get_table_info(),
            "input": state["question"],
        }
    )
    structured_llm = llm.with_structured_output(QueryOutput)
    result = structured_llm.invoke(prompt)
    return {"query": result["query"]}
```

Let's test it out:

```python
write_query({"question": "How many Employees are there?"})
```

```output
{'query': 'SELECT COUNT(*) as employee_count FROM Employee;'}
```

### Execute query[â€‹](#execute-query "Direct link to Execute query")

**This is the most dangerous part of creating a SQL chain.** Consider carefully if it is OK to run automated queries over your data. Minimize the database connection permissions as much as possible. Consider adding a human approval step to you chains before query execution (see below).

To execute the query, we will load a tool from [langchain-community](/docs/concepts/architecture/#langchain-community). Our `execute_query` node will just wrap this tool:

```python
from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool


def execute_query(state: State):
    """Execute SQL query."""
    execute_query_tool = QuerySQLDatabaseTool(db=db)
    return {"result": execute_query_tool.invoke(state["query"])}
```

**API Reference:**[QuerySQLDatabaseTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.sql_database.tool.QuerySQLDatabaseTool.html)

Testing this step:

```python
execute_query({"query": "SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;"})
```

```output
{'result': '[(8,)]'}
```

### Generate answer[â€‹](#generate-answer "Direct link to Generate answer")

Finally, our last step generates an answer to the question given the information pulled from the database:

```python
def generate_answer(state: State):
    """Answer question using retrieved information as context."""
    prompt = (
        "Given the following user question, corresponding SQL query, "
        "and SQL result, answer the user question.\n\n"
        f'Question: {state["question"]}\n'
        f'SQL Query: {state["query"]}\n'
        f'SQL Result: {state["result"]}'
    )
    response = llm.invoke(prompt)
    return {"answer": response.content}
```

### Orchestrating with LangGraph[â€‹](#orchestrating-with-langgraph "Direct link to Orchestrating with LangGraph")

Finally, we compile our application into a single `graph` object. In this case, we are just connecting the three steps into a single sequence.

```python
from langgraph.graph import START, StateGraph

graph_builder = StateGraph(State).add_sequence(
    [write_query, execute_query, generate_answer]
)
graph_builder.add_edge(START, "write_query")
graph = graph_builder.compile()
```

**API Reference:**[StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

LangGraph also comes with built-in utilities for visualizing the control flow of your application:

```python
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

Let's test our application! Note that we can stream the results of individual steps:

```python
for step in graph.stream(
    {"question": "How many employees are there?"}, stream_mode="updates"
):
    print(step)
```

```output
{'write_query': {'query': 'SELECT COUNT(*) as employee_count FROM Employee;'}}
{'execute_query': {'result': '[(8,)]'}}
{'generate_answer': {'answer': 'There are 8 employees in total.'}}
```

Check out the [LangSmith trace](https://smith.langchain.com/public/30a79380-6ba6-46af-8bd9-5d1df0b9ccca/r).

### Human-in-the-loop[â€‹](#human-in-the-loop "Direct link to Human-in-the-loop")

LangGraph supports a number of features that can be useful for this workflow. One of them is [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/): we can interrupt our application before sensitive steps (such as the execution of a SQL query) for human review. This is enabled by LangGraph's [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) layer, which saves run progress to your storage of choice. Below, we specify storage in-memory:

```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
graph = graph_builder.compile(checkpointer=memory, interrupt_before=["execute_query"])

# Now that we're using persistence, we need to specify a thread ID
# so that we can continue the run after review.
config = {"configurable": {"thread_id": "1"}}
```

**API Reference:**[MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver)

```python
display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

Let's repeat the same run, adding in a simple yes/no approval step:

```python
for step in graph.stream(
    {"question": "How many employees are there?"},
    config,
    stream_mode="updates",
):
    print(step)

try:
    user_approval = input("Do you want to go to execute query? (yes/no): ")
except Exception:
    user_approval = "no"

if user_approval.lower() == "yes":
    # If approved, continue the graph execution
    for step in graph.stream(None, config, stream_mode="updates"):
        print(step)
else:
    print("Operation cancelled by user.")
```

```````output
{'write_query': {'query': 'SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;'}}
{'__interrupt__': ()}
``````output
Do you want to go to execute query? (yes/no):  yes
``````output
{'execute_query': {'result': '[(8,)]'}}
{'generate_answer': {'answer': 'There are 8 employees.'}}
```````

See [this](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/) LangGraph guide for more detail and examples.

### Next steps[â€‹](#next-steps "Direct link to Next steps")

For more complex query-generation, we may want to create few-shot prompts or add query-checking steps. For advanced techniques like this and more check out:

- [Prompting strategies](/docs/how_to/sql_prompting/): Advanced prompt engineering techniques.
- [Query checking](/docs/how_to/sql_query_checking/): Add query validation and error handling.
- [Large databases](/docs/how_to/sql_large_db/): Techniques for working with large databases.

## Agents[â€‹](#agents "Direct link to Agents")

[Agents](/docs/concepts/agents/) leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the query generation and execution process. Although their behavior is less predictable than the above "chain", they feature some advantages:

- They can query the database as many times as needed to answer the user question.
- They can recover from errors by running a generated query, catching the traceback and regenerating it correctly.
- They can answer questions based on the databases' schema as well as on the databases' content (like describing a specific table).

Below we assemble a minimal SQL agent. We will equip it with a set of tools using LangChain's [SQLDatabaseToolkit](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.sql.toolkit.SQLDatabaseToolkit.html). Using LangGraph's [pre-built ReAct agent constructor](https://langchain-ai.github.io/langgraph/how-tos/#langgraph.prebuilt.chat_agent_executor.create_react_agent), we can do this in one line.

tip

Check out LangGraph's [SQL Agent Tutorial](https://langchain-ai.github.io/langgraph/tutorials/sql-agent/) for a more advanced formulation of a SQL agent.

The `SQLDatabaseToolkit` includes tools that can:

- Create and execute queries
- Check query syntax
- Retrieve table descriptions
- ... and more

```python
from langchain_community.agent_toolkits import SQLDatabaseToolkit

toolkit = SQLDatabaseToolkit(db=db, llm=llm)

tools = toolkit.get_tools()

tools
```

**API Reference:**[SQLDatabaseToolkit](https://python.langchain.com/api_reference/community/agent_toolkits/langchain_community.agent_toolkits.sql.toolkit.SQLDatabaseToolkit.html)

```output
[QuerySQLDatabaseTool(description="Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>),
 InfoSQLDatabaseTool(description='Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>),
 ListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>),
 QuerySQLCheckerTool(description='Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>, llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['dialect', 'query'], input_types={}, partial_variables={}, template='\n{query}\nDouble check the {dialect} query above for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\n\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n\nOutput the final SQL query only.\n\nSQL Query: '), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}))]
```

### System Prompt[â€‹](#system-prompt "Direct link to System Prompt")

We will also want to load a system prompt for our agent. This will consist of instructions for how to behave.

```python
from langchain import hub

prompt_template = hub.pull("langchain-ai/sql-agent-system-prompt")

assert len(prompt_template.messages) == 1
prompt_template.messages[0].pretty_print()
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html)

```output
================================[1m System Message [0m================================

You are an agent designed to interact with a SQL database.
Given an input question, create a syntactically correct [33;1m[1;3m{dialect}[0m query to run, then look at the results of the query and return the answer.
Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most [33;1m[1;3m{top_k}[0m results.
You can order the results by a relevant column to return the most interesting examples in the database.
Never query for all the columns from a specific table, only ask for the relevant columns given the question.
You have access to tools for interacting with the database.
Only use the below tools. Only use the information returned by the below tools to construct your final answer.
You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.

DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.

To start you should ALWAYS look at the tables in the database to see what you can query.
Do NOT skip this step.
Then you should query the schema of the most relevant tables.
```

Let's populate the parameters highlighted in the prompt:

```python
system_message = prompt_template.format(dialect="SQLite", top_k=5)
```

### Initializing agent[â€‹](#initializing-agent "Direct link to Initializing agent")

We will use a prebuilt [LangGraph](/docs/concepts/architecture/#langgraph) agent to build our agent

```python
from langchain_core.messages import HumanMessage
from langgraph.prebuilt import create_react_agent

agent_executor = create_react_agent(llm, tools, prompt=system_message)
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

Consider how the agent responds to the below question:

```python
question = "Which country's customers spent the most?"

for step in agent_executor.stream(
    {"messages": [{"role": "user", "content": question}]},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

Which country's customers spent the most?
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_list_tables (call_tFp7HYD6sAAmCShgeqkVZH6Q)
 Call ID: call_tFp7HYD6sAAmCShgeqkVZH6Q
  Args:
=================================[1m Tool Message [0m=================================
Name: sql_db_list_tables

Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_schema (call_KJZ1Jx6JazyDdJa0uH1UeiOz)
 Call ID: call_KJZ1Jx6JazyDdJa0uH1UeiOz
  Args:
    table_names: Customer, Invoice
=================================[1m Tool Message [0m=================================
Name: sql_db_schema


CREATE TABLE "Customer" (
	"CustomerId" INTEGER NOT NULL, 
	"FirstName" NVARCHAR(40) NOT NULL, 
	"LastName" NVARCHAR(20) NOT NULL, 
	"Company" NVARCHAR(80), 
	"Address" NVARCHAR(70), 
	"City" NVARCHAR(40), 
	"State" NVARCHAR(40), 
	"Country" NVARCHAR(40), 
	"PostalCode" NVARCHAR(10), 
	"Phone" NVARCHAR(24), 
	"Fax" NVARCHAR(24), 
	"Email" NVARCHAR(60) NOT NULL, 
	"SupportRepId" INTEGER, 
	PRIMARY KEY ("CustomerId"), 
	FOREIGN KEY("SupportRepId") REFERENCES "Employee" ("EmployeeId")
)

/*
3 rows from Customer table:
CustomerId	FirstName	LastName	Company	Address	City	State	Country	PostalCode	Phone	Fax	Email	SupportRepId
1	LuÃ­s	GonÃ§alves	Embraer - Empresa Brasileira de AeronÃ¡utica S.A.	Av. Brigadeiro Faria Lima, 2170	SÃ£o JosÃ© dos Campos	SP	Brazil	12227-000	+55 (12) 3923-5555	+55 (12) 3923-5566	luisg@embraer.com.br	3
2	Leonie	KÃ¶hler	None	Theodor-Heuss-StraÃŸe 34	Stuttgart	None	Germany	70174	+49 0711 2842222	None	leonekohler@surfeu.de	5
3	FranÃ§ois	Tremblay	None	1498 rue BÃ©langer	MontrÃ©al	QC	Canada	H2G 1A7	+1 (514) 721-4711	None	ftremblay@gmail.com	3
*/


CREATE TABLE "Invoice" (
	"InvoiceId" INTEGER NOT NULL, 
	"CustomerId" INTEGER NOT NULL, 
	"InvoiceDate" DATETIME NOT NULL, 
	"BillingAddress" NVARCHAR(70), 
	"BillingCity" NVARCHAR(40), 
	"BillingState" NVARCHAR(40), 
	"BillingCountry" NVARCHAR(40), 
	"BillingPostalCode" NVARCHAR(10), 
	"Total" NUMERIC(10, 2) NOT NULL, 
	PRIMARY KEY ("InvoiceId"), 
	FOREIGN KEY("CustomerId") REFERENCES "Customer" ("CustomerId")
)

/*
3 rows from Invoice table:
InvoiceId	CustomerId	InvoiceDate	BillingAddress	BillingCity	BillingState	BillingCountry	BillingPostalCode	Total
1	2	2021-01-01 00:00:00	Theodor-Heuss-StraÃŸe 34	Stuttgart	None	Germany	70174	1.98
2	4	2021-01-02 00:00:00	UllevÃ¥lsveien 14	Oslo	None	Norway	0171	3.96
3	8	2021-01-03 00:00:00	GrÃ©trystraat 63	Brussels	None	Belgium	1000	5.94
*/
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_query_checker (call_AQuTGbgH63u4gPgyV723yrjX)
 Call ID: call_AQuTGbgH63u4gPgyV723yrjX
  Args:
    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;
=================================[1m Tool Message [0m=================================
Name: sql_db_query_checker

\`\`\`sql
SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;
\`\`\`
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_query (call_B88EwU44nwwpQL5M9nlcemSU)
 Call ID: call_B88EwU44nwwpQL5M9nlcemSU
  Args:
    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;
=================================[1m Tool Message [0m=================================
Name: sql_db_query

[('USA', 523.06)]
==================================[1m Ai Message [0m==================================

The country whose customers spent the most is the USA, with a total spending of 523.06.
```

You can also use the [LangSmith trace](https://smith.langchain.com/public/8af422aa-b651-4bfe-8683-e2a7f4ccd82c/r) to visualize these steps and associated metadata.

Note that the agent executes multiple queries until it has the information it needs:

1. List available tables;
2. Retrieves the schema for three tables;
3. Queries multiple of the tables via a join operation.

The agent is then able to use the result of the final query to generate an answer to the original question.

The agent can similarly handle qualitative questions:

```python
question = "Describe the playlisttrack table"

for step in agent_executor.stream(
    {"messages": [{"role": "user", "content": question}]},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

Describe the playlisttrack table
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_list_tables (call_fMF8eTmX5TJDJjc3Mhdg52TI)
 Call ID: call_fMF8eTmX5TJDJjc3Mhdg52TI
  Args:
=================================[1m Tool Message [0m=================================
Name: sql_db_list_tables

Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_schema (call_W8Vkk4NEodkAAIg8nexAszUH)
 Call ID: call_W8Vkk4NEodkAAIg8nexAszUH
  Args:
    table_names: PlaylistTrack
=================================[1m Tool Message [0m=================================
Name: sql_db_schema


CREATE TABLE "PlaylistTrack" (
	"PlaylistId" INTEGER NOT NULL, 
	"TrackId" INTEGER NOT NULL, 
	PRIMARY KEY ("PlaylistId", "TrackId"), 
	FOREIGN KEY("TrackId") REFERENCES "Track" ("TrackId"), 
	FOREIGN KEY("PlaylistId") REFERENCES "Playlist" ("PlaylistId")
)

/*
3 rows from PlaylistTrack table:
PlaylistId	TrackId
1	3402
1	3389
1	3390
*/
==================================[1m Ai Message [0m==================================

The `PlaylistTrack` table is designed to associate tracks with playlists. It has the following structure:

- **PlaylistId**: An integer that serves as a foreign key referencing the `Playlist` table. It is part of the composite primary key.
- **TrackId**: An integer that serves as a foreign key referencing the `Track` table. It is also part of the composite primary key.

The primary key for this table is a composite key consisting of both `PlaylistId` and `TrackId`, ensuring that each track can be uniquely associated with a playlist. The table enforces referential integrity by linking to the `Track` and `Playlist` tables through foreign keys.
```

### Dealing with high-cardinality columns[â€‹](#dealing-with-high-cardinality-columns "Direct link to Dealing with high-cardinality columns")

In order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.

We can achieve this by creating a vector store with all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query.

First we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:

```python
import ast
import re


def query_as_list(db, query):
    res = db.run(query)
    res = [el for sub in ast.literal_eval(res) for el in sub if el]
    res = [re.sub(r"\b\d+\b", "", string).strip() for string in res]
    return list(set(res))


artists = query_as_list(db, "SELECT Name FROM Artist")
albums = query_as_list(db, "SELECT Title FROM Album")
albums[:5]
```

```output
['In Through The Out Door',
 'Transmission',
 'Battlestar Galactica (Classic), Season',
 'A Copland Celebration, Vol. I',
 'Quiet Songs']
```

Using this function, we can create a **retriever tool** that the agent can execute at its discretion.

Let's select an [embeddings model](/docs/integrations/text_embedding/) and [vector store](/docs/integrations/vectorstores/) for this step:

**Select an embedding model**:

Select [embeddings model](/docs/integrations/text_embedding/):

OpenAIâ–¾

[OpenAI](#)

[Azure](#)

[Google](#)

[AWS](#)

[HuggingFace](#)

[Ollama](#)

[Cohere](#)

[MistralAI](#)

[Nomic](#)

[NVIDIA](#)

[Voyage AI](#)

[IBM watsonx](#)

[Fake](#)

```bash
pip install -qU langchain-openai
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
```

**Select a vector store**:

Select [vector store](/docs/integrations/vectorstores/):

In-memoryâ–¾

[In-memory](#)

[AstraDB](#)

[Chroma](#)

[FAISS](#)

[Milvus](#)

[MongoDB](#)

[PGVector](#)

[Pinecone](#)

[Qdrant](#)

```bash
pip install -qU langchain-core
```

```python
from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)
```

We can now construct a retrieval tool that can search over relevant proper nouns in the database:

```python
from langchain.agents.agent_toolkits import create_retriever_tool

_ = vector_store.add_texts(artists + albums)
retriever = vector_store.as_retriever(search_kwargs={"k": 5})
description = (
    "Use to look up values to filter on. Input is an approximate spelling "
    "of the proper noun, output is valid proper nouns. Use the noun most "
    "similar to the search."
)
retriever_tool = create_retriever_tool(
    retriever,
    name="search_proper_nouns",
    description=description,
)
```

**API Reference:**[create\_retriever\_tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.retriever.create_retriever_tool.html)

Let's try it out:

```python
print(retriever_tool.invoke("Alice Chains"))
```

```output
Alice In Chains

Alanis Morissette

Pearl Jam

Pearl Jam

Audioslave
```

This way, if the agent determines it needs to write a filter based on an artist along the lines of "Alice Chains", it can first use the retriever tool to observe relevant values of a column.

Putting this together:

```python
# Add to system message
suffix = (
    "If you need to filter on a proper noun like a Name, you must ALWAYS first look up "
    "the filter value using the 'search_proper_nouns' tool! Do not try to "
    "guess at the proper name - use this function to find similar ones."
)

system = f"{system_message}\n\n{suffix}"

tools.append(retriever_tool)

agent = create_react_agent(llm, tools, prompt=system)
```

```python
question = "How many albums does alis in chain have?"

for step in agent.stream(
    {"messages": [{"role": "user", "content": question}]},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

How many albums does alis in chain have?
==================================[1m Ai Message [0m==================================
Tool Calls:
  search_proper_nouns (call_8ryjsRPLAr79mM3Qvnq6gTOH)
 Call ID: call_8ryjsRPLAr79mM3Qvnq6gTOH
  Args:
    query: alis in chain
=================================[1m Tool Message [0m=================================
Name: search_proper_nouns

Alice In Chains

Aisha Duo

Xis

Da Lama Ao Caos

A-Sides
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_list_tables (call_NJjtCpU89MBMplssjn1z0xzq)
 Call ID: call_NJjtCpU89MBMplssjn1z0xzq
  Args:
  search_proper_nouns (call_1BfrueC9koSIyi4OfMu2Ao8q)
 Call ID: call_1BfrueC9koSIyi4OfMu2Ao8q
  Args:
    query: Alice In Chains
=================================[1m Tool Message [0m=================================
Name: search_proper_nouns

Alice In Chains

Pearl Jam

Pearl Jam

Foo Fighters

Soundgarden
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_schema (call_Kn09w9jd9swcNzIZ1b5MlKID)
 Call ID: call_Kn09w9jd9swcNzIZ1b5MlKID
  Args:
    table_names: Album, Artist
=================================[1m Tool Message [0m=================================
Name: sql_db_schema


CREATE TABLE "Album" (
	"AlbumId" INTEGER NOT NULL, 
	"Title" NVARCHAR(160) NOT NULL, 
	"ArtistId" INTEGER NOT NULL, 
	PRIMARY KEY ("AlbumId"), 
	FOREIGN KEY("ArtistId") REFERENCES "Artist" ("ArtistId")
)

/*
3 rows from Album table:
AlbumId	Title	ArtistId
1	For Those About To Rock We Salute You	1
2	Balls to the Wall	2
3	Restless and Wild	2
*/


CREATE TABLE "Artist" (
	"ArtistId" INTEGER NOT NULL, 
	"Name" NVARCHAR(120), 
	PRIMARY KEY ("ArtistId")
)

/*
3 rows from Artist table:
ArtistId	Name
1	AC/DC
2	Accept
3	Aerosmith
*/
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_query (call_WkHRiPcBoGN9bc58MIupRHKP)
 Call ID: call_WkHRiPcBoGN9bc58MIupRHKP
  Args:
    query: SELECT COUNT(*) FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'Alice In Chains')
=================================[1m Tool Message [0m=================================
Name: sql_db_query

[(1,)]
==================================[1m Ai Message [0m==================================

Alice In Chains has released 1 album in the database.
```

As we can see, both in the streamed steps and in the [LangSmith trace](https://smith.langchain.com/public/1d757ed2-5688-4458-9400-023594e2c5a7/r), the agent used the `search_proper_nouns` tool in order to check how to correctly query the database for this specific artist.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/tutorials/sql_qa.ipynb)

* * *


- [âš ï¸ Security note âš ï¸](#%EF%B8%8F-security-note-%EF%B8%8F)
- [Architecture](#architecture)
- [Setup](#setup)
  
  - [Sample data](#sample-data)
- [Chains](#chains)
  
  - [Application state](#application-state)
  - [Convert question to SQL query](#convert-question-to-sql-query)
  - [Execute query](#execute-query)
  - [Generate answer](#generate-answer)
  - [Orchestrating with LangGraph](#orchestrating-with-langgraph)
  - [Human-in-the-loop](#human-in-the-loop)
  - [Next steps](#next-steps)
- [Agents](#agents)
  
  - [System Prompt](#system-prompt)
  - [Initializing agent](#initializing-agent)
  - [Dealing with high-cardinality columns](#dealing-with-high-cardinality-columns)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/markdown_header_metadata_splitter.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/markdown_header_metadata_splitter.ipynb)

# How to split Markdown by Headers

### Motivation[â€‹](#motivation "Direct link to Motivation")

Many chat or Q+A applications involve chunking input documents prior to embedding and vector storage.

[These notes](https://www.pinecone.io/learn/chunking-strategies/) from Pinecone provide some useful tips:

```text
When a full paragraph or document is embedded, the embedding process considers both the overall context and the relationships between the sentences and phrases within the text. This can result in a more comprehensive vector representation that captures the broader meaning and themes of the text.
```

As mentioned, chunking often aims to keep text with common context together. With this in mind, we might want to specifically honor the structure of the document itself. For example, a markdown file is organized by headers. Creating chunks within specific header groups is an intuitive idea. To address this challenge, we can use [MarkdownHeaderTextSplitter](https://python.langchain.com/api_reference/text_splitters/markdown/langchain_text_splitters.markdown.MarkdownHeaderTextSplitter.html). This will split a markdown file by a specified set of headers.

For example, if we want to split this markdown:

```text
md = '# Foo\n\n ## Bar\n\nHi this is Jim  \nHi this is Joe\n\n ## Baz\n\n Hi this is Molly' 
```

We can specify the headers to split on:

```text
[("#", "Header 1"),("##", "Header 2")]
```

And content is grouped or split by common headers:

```text
{'content': 'Hi this is Jim  \nHi this is Joe', 'metadata': {'Header 1': 'Foo', 'Header 2': 'Bar'}}
{'content': 'Hi this is Molly', 'metadata': {'Header 1': 'Foo', 'Header 2': 'Baz'}}
```

Let's have a look at some examples below.

### Basic usage:[â€‹](#basic-usage "Direct link to Basic usage:")

```python
%pip install -qU langchain-text-splitters
```

```python
from langchain_text_splitters import MarkdownHeaderTextSplitter
```

**API Reference:**[MarkdownHeaderTextSplitter](https://python.langchain.com/api_reference/text_splitters/markdown/langchain_text_splitters.markdown.MarkdownHeaderTextSplitter.html)

```python
markdown_document = "# Foo\n\n    ## Bar\n\nHi this is Jim\n\nHi this is Joe\n\n ### Boo \n\n Hi this is Lance \n\n ## Baz\n\n Hi this is Molly"

headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)
md_header_splits = markdown_splitter.split_text(markdown_document)
md_header_splits
```

```output
[Document(page_content='Hi this is Jim  \nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),
 Document(page_content='Hi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),
 Document(page_content='Hi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]
```

```python
type(md_header_splits[0])
```

```output
langchain_core.documents.base.Document
```

By default, `MarkdownHeaderTextSplitter` strips headers being split on from the output chunk's content. This can be disabled by setting `strip_headers = False`.

```python
markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers=False)
md_header_splits = markdown_splitter.split_text(markdown_document)
md_header_splits
```

```output
[Document(page_content='# Foo  \n## Bar  \nHi this is Jim  \nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),
 Document(page_content='### Boo  \nHi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),
 Document(page_content='## Baz  \nHi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]
```

note

The default `MarkdownHeaderTextSplitter` strips white spaces and new lines. To preserve the original formatting of your Markdown documents, check out [ExperimentalMarkdownSyntaxTextSplitter](https://python.langchain.com/api_reference/text_splitters/markdown/langchain_text_splitters.markdown.ExperimentalMarkdownSyntaxTextSplitter.html).

### How to return Markdown lines as separate documents[â€‹](#how-to-return-markdown-lines-as-separate-documents "Direct link to How to return Markdown lines as separate documents")

By default, `MarkdownHeaderTextSplitter` aggregates lines based on the headers specified in `headers_to_split_on`. We can disable this by specifying `return_each_line`:

```python
markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on,
    return_each_line=True,
)
md_header_splits = markdown_splitter.split_text(markdown_document)
md_header_splits
```

```output
[Document(page_content='Hi this is Jim', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),
 Document(page_content='Hi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),
 Document(page_content='Hi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),
 Document(page_content='Hi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]
```

Note that here header information is retained in the `metadata` for each document.

### How to constrain chunk size:[â€‹](#how-to-constrain-chunk-size "Direct link to How to constrain chunk size:")

Within each markdown group we can then apply any text splitter we want, such as `RecursiveCharacterTextSplitter`, which allows for further control of the chunk size.

```python
markdown_document = "# Intro \n\n    ## History \n\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \n\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \n\n ## Rise and divergence \n\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \n\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \n\n #### Standardization \n\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \n\n ## Implementations \n\n Implementations of Markdown are available for over a dozen programming languages."

headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
]

# MD splits
markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on, strip_headers=False
)
md_header_splits = markdown_splitter.split_text(markdown_document)

# Char-level splits
from langchain_text_splitters import RecursiveCharacterTextSplitter

chunk_size = 250
chunk_overlap = 30
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size, chunk_overlap=chunk_overlap
)

# Split
splits = text_splitter.split_documents(md_header_splits)
splits
```

**API Reference:**[RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

```output
[Document(page_content='# Intro  \n## History  \nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]', metadata={'Header 1': 'Intro', 'Header 2': 'History'}),
 Document(page_content='Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.', metadata={'Header 1': 'Intro', 'Header 2': 'History'}),
 Document(page_content='## Rise and divergence  \nAs Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.', metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}),
 Document(page_content='#### Standardization  \nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.', metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}),
 Document(page_content='## Implementations  \nImplementations of Markdown are available for over a dozen programming languages.', metadata={'Header 1': 'Intro', 'Header 2': 'Implementations'})]
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/markdown_header_metadata_splitter.ipynb)

* * *


- [Motivation](#motivation)
- [Basic usage:](#basic-usage)
- [How to return Markdown lines as separate documents](#how-to-return-markdown-lines-as-separate-documents)
- [How to constrain chunk size:](#how-to-constrain-chunk-size)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/add_scores_retriever.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/add_scores_retriever.ipynb)

# How to add scores to retriever results

[Retrievers](/docs/concepts/retrievers/) will return sequences of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects, which by default include no information about the process that retrieved them (e.g., a similarity score against a query). Here we demonstrate how to add retrieval scores to the `.metadata` of documents:

1. From [vectorstore retrievers](/docs/how_to/vectorstore_retriever/);
2. From higher-order LangChain retrievers, such as [SelfQueryRetriever](/docs/how_to/self_query/) or [MultiVectorRetriever](/docs/how_to/multi_vector/).

For (1), we will implement a short wrapper function around the corresponding [vector store](/docs/concepts/vectorstores/). For (2), we will update a method of the corresponding class.

## Create vector store[â€‹](#create-vector-store "Direct link to Create vector store")

First we populate a vector store with some data. We will use a [PineconeVectorStore](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html), but this guide is compatible with any LangChain vector store that implements a `.similarity_search_with_score` method.

```python
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

docs = [
    Document(
        page_content="A bunch of scientists bring back dinosaurs and mayhem breaks loose",
        metadata={"year": 1993, "rating": 7.7, "genre": "science fiction"},
    ),
    Document(
        page_content="Leo DiCaprio gets lost in a dream within a dream within a dream within a ...",
        metadata={"year": 2010, "director": "Christopher Nolan", "rating": 8.2},
    ),
    Document(
        page_content="A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea",
        metadata={"year": 2006, "director": "Satoshi Kon", "rating": 8.6},
    ),
    Document(
        page_content="A bunch of normal-sized women are supremely wholesome and some men pine after them",
        metadata={"year": 2019, "director": "Greta Gerwig", "rating": 8.3},
    ),
    Document(
        page_content="Toys come alive and have a blast doing so",
        metadata={"year": 1995, "genre": "animated"},
    ),
    Document(
        page_content="Three men walk into the Zone, three men walk out of the Zone",
        metadata={
            "year": 1979,
            "director": "Andrei Tarkovsky",
            "genre": "thriller",
            "rating": 9.9,
        },
    ),
]

vectorstore = PineconeVectorStore.from_documents(
    docs, index_name="sample", embedding=OpenAIEmbeddings()
)
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [PineconeVectorStore](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html)

## Retriever[â€‹](#retriever "Direct link to Retriever")

To obtain scores from a vector store retriever, we wrap the underlying vector store's `.similarity_search_with_score` method in a short function that packages scores into the associated document's metadata.

We add a `@chain` decorator to the function to create a [Runnable](/docs/concepts/lcel/) that can be used similarly to a typical retriever.

```python
from typing import List

from langchain_core.documents import Document
from langchain_core.runnables import chain


@chain
def retriever(query: str) -> List[Document]:
    docs, scores = zip(*vectorstore.similarity_search_with_score(query))
    for doc, score in zip(docs, scores):
        doc.metadata["score"] = score

    return docs
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)

```python
result = retriever.invoke("dinosaur")
result
```

```output
(Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993.0, 'score': 0.84429127}),
 Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995.0, 'score': 0.792038262}),
 Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979.0, 'score': 0.751571238}),
 Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006.0, 'score': 0.747471571}))
```

Note that similarity scores from the retrieval step are included in the metadata of the above documents.

## SelfQueryRetriever[â€‹](#selfqueryretriever "Direct link to SelfQueryRetriever")

`SelfQueryRetriever` will use a LLM to generate a query that is potentially structured-- for example, it can construct filters for the retrieval on top of the usual semantic-similarity driven selection. See [this guide](/docs/how_to/self_query/) for more detail.

`SelfQueryRetriever` includes a short (1 - 2 line) method `_get_docs_with_query` that executes the `vectorstore` search. We can subclass `SelfQueryRetriever` and override this method to propagate similarity scores.

First, following the [how-to guide](/docs/how_to/self_query/), we will need to establish some metadata on which to filter:

```python
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain_openai import ChatOpenAI

metadata_field_info = [
    AttributeInfo(
        name="genre",
        description="The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']",
        type="string",
    ),
    AttributeInfo(
        name="year",
        description="The year the movie was released",
        type="integer",
    ),
    AttributeInfo(
        name="director",
        description="The name of the movie director",
        type="string",
    ),
    AttributeInfo(
        name="rating", description="A 1-10 rating for the movie", type="float"
    ),
]
document_content_description = "Brief summary of a movie"
llm = ChatOpenAI(temperature=0)
```

**API Reference:**[AttributeInfo](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.query_constructor.schema.AttributeInfo.html) | [SelfQueryRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

We then override the `_get_docs_with_query` to use the `similarity_search_with_score` method of the underlying vector store:

```python
from typing import Any, Dict


class CustomSelfQueryRetriever(SelfQueryRetriever):
    def _get_docs_with_query(
        self, query: str, search_kwargs: Dict[str, Any]
    ) -> List[Document]:
        """Get docs, adding score information."""
        docs, scores = zip(
            *self.vectorstore.similarity_search_with_score(query, **search_kwargs)
        )
        for doc, score in zip(docs, scores):
            doc.metadata["score"] = score

        return docs
```

Invoking this retriever will now include similarity scores in the document metadata. Note that the underlying structured-query capabilities of `SelfQueryRetriever` are retained.

```python
retriever = CustomSelfQueryRetriever.from_llm(
    llm,
    vectorstore,
    document_content_description,
    metadata_field_info,
)


result = retriever.invoke("dinosaur movie with rating less than 8")
result
```

```output
(Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993.0, 'score': 0.84429127}),)
```

## MultiVectorRetriever[â€‹](#multivectorretriever "Direct link to MultiVectorRetriever")

`MultiVectorRetriever` allows you to associate multiple vectors with a single document. This can be useful in a number of applications. For example, we can index small chunks of a larger document and run the retrieval on the chunks, but return the larger "parent" document when invoking the retriever. [ParentDocumentRetriever](/docs/how_to/parent_document_retriever/), a subclass of `MultiVectorRetriever`, includes convenience methods for populating a vector store to support this. Further applications are detailed in this [how-to guide](/docs/how_to/multi_vector/).

To propagate similarity scores through this retriever, we can again subclass `MultiVectorRetriever` and override a method. This time we will override `_get_relevant_documents`.

First, we prepare some fake data. We generate fake "whole documents" and store them in a document store; here we will use a simple [InMemoryStore](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.InMemoryBaseStore.html).

```python
from langchain.storage import InMemoryStore
from langchain_text_splitters import RecursiveCharacterTextSplitter

# The storage layer for the parent documents
docstore = InMemoryStore()
fake_whole_documents = [
    ("fake_id_1", Document(page_content="fake whole document 1")),
    ("fake_id_2", Document(page_content="fake whole document 2")),
]
docstore.mset(fake_whole_documents)
```

**API Reference:**[InMemoryStore](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.InMemoryStore.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

Next we will add some fake "sub-documents" to our vector store. We can link these sub-documents to the parent documents by populating the `"doc_id"` key in its metadata.

```python
docs = [
    Document(
        page_content="A snippet from a larger document discussing cats.",
        metadata={"doc_id": "fake_id_1"},
    ),
    Document(
        page_content="A snippet from a larger document discussing discourse.",
        metadata={"doc_id": "fake_id_1"},
    ),
    Document(
        page_content="A snippet from a larger document discussing chocolate.",
        metadata={"doc_id": "fake_id_2"},
    ),
]

vectorstore.add_documents(docs)
```

```output
['62a85353-41ff-4346-bff7-be6c8ec2ed89',
 '5d4a0e83-4cc5-40f1-bc73-ed9cbad0ee15',
 '8c1d9a56-120f-45e4-ba70-a19cd19a38f4']
```

To propagate the scores, we subclass `MultiVectorRetriever` and override its `_get_relevant_documents` method. Here we will make two changes:

1. We will add similarity scores to the metadata of the corresponding "sub-documents" using the `similarity_search_with_score` method of the underlying vector store as above;
2. We will include a list of these sub-documents in the metadata of the retrieved parent document. This surfaces what snippets of text were identified by the retrieval, together with their corresponding similarity scores.

```python
from collections import defaultdict

from langchain.retrievers import MultiVectorRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun


class CustomMultiVectorRetriever(MultiVectorRetriever):
    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """Get documents relevant to a query.
        Args:
            query: String to find relevant documents for
            run_manager: The callbacks handler to use
        Returns:
            List of relevant documents
        """
        results = self.vectorstore.similarity_search_with_score(
            query, **self.search_kwargs
        )

        # Map doc_ids to list of sub-documents, adding scores to metadata
        id_to_doc = defaultdict(list)
        for doc, score in results:
            doc_id = doc.metadata.get("doc_id")
            if doc_id:
                doc.metadata["score"] = score
                id_to_doc[doc_id].append(doc)

        # Fetch documents corresponding to doc_ids, retaining sub_docs in metadata
        docs = []
        for _id, sub_docs in id_to_doc.items():
            docstore_docs = self.docstore.mget([_id])
            if docstore_docs:
                if doc := docstore_docs[0]:
                    doc.metadata["sub_docs"] = sub_docs
                    docs.append(doc)

        return docs
```

**API Reference:**[MultiVectorRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html) | [CallbackManagerForRetrieverRun](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.CallbackManagerForRetrieverRun.html)

Invoking this retriever, we can see that it identifies the correct parent document, including the relevant snippet from the sub-document with similarity score.

```python
retriever = CustomMultiVectorRetriever(vectorstore=vectorstore, docstore=docstore)

retriever.invoke("cat")
```

```output
[Document(page_content='fake whole document 1', metadata={'sub_docs': [Document(page_content='A snippet from a larger document discussing cats.', metadata={'doc_id': 'fake_id_1', 'score': 0.831276655})]})]
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/add_scores_retriever.ipynb)

* * *


- [Create vector store](#create-vector-store)
- [Retriever](#retriever)
- [SelfQueryRetriever](#selfqueryretriever)
- [MultiVectorRetriever](#multivectorretriever)









# How to use LangChain with different Pydantic versions

As of the `0.3` release, LangChain uses Pydantic 2 internally.

Users should install Pydantic 2 and are advised to **avoid** using the `pydantic.v1` namespace of Pydantic 2 with LangChain APIs.

If you're working with prior versions of LangChain, please see the following guide on [Pydantic compatibility](https://python.langchain.com/v0.2/docs/how_to/pydantic_compatibility).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/pydantic_compatibility.md)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_web.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_web.ipynb)

# How to load web pages

This guide covers how to [load](/docs/concepts/document_loaders/) web pages into the LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.

LangChain integrates with a host of parsers that are appropriate for web pages. The right parser will depend on your needs. Below we demonstrate two possibilities:

- [Simple and fast](/docs/how_to/document_loader_web/#simple-and-fast-text-extraction) parsing, in which we recover one `Document` per web page with its content represented as a "flattened" string;
- [Advanced](/docs/how_to/document_loader_web/#advanced-parsing) parsing, in which we recover multiple `Document` objects per page, allowing one to identify and traverse sections, links, tables, and other structures.

## Setup[â€‹](#setup "Direct link to Setup")

For the "simple and fast" parsing, we will need `langchain-community` and the `beautifulsoup4` library:

```python
%pip install -qU langchain-community beautifulsoup4
```

For advanced parsing, we will use `langchain-unstructured`:

```python
%pip install -qU langchain-unstructured
```

## Simple and fast text extraction[â€‹](#simple-and-fast-text-extraction "Direct link to Simple and fast text extraction")

If you are looking for a simple string representation of text that is embedded in a web page, the method below is appropriate. It will return a list of `Document` objects -- one per page -- containing a single string of the page's text. Under the hood it uses the `beautifulsoup4` Python library.

LangChain document loaders implement `lazy_load` and its async variant, `alazy_load`, which return iterators of `Document objects`. We will use these below.

```python
import bs4
from langchain_community.document_loaders import WebBaseLoader

page_url = "https://python.langchain.com/docs/how_to/chatbots_memory/"

loader = WebBaseLoader(web_paths=[page_url])
docs = []
async for doc in loader.alazy_load():
    docs.append(doc)

assert len(docs) == 1
doc = docs[0]
```

**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)

```output
USER_AGENT environment variable not set, consider setting it to identify your requests.
```

```python
print(f"{doc.metadata}\n")
print(doc.page_content[:500].strip())
```

```output
{'source': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'title': 'How to add memory to chatbots | \uf8ffÃ¼Â¶ÃºÃ”âˆÃ¨\uf8ffÃ¼Ã®Ã³ LangChain', 'description': 'A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:', 'language': 'en'}

How to add memory to chatbots | ï£¿Ã¼Â¶ÃºÃ”âˆÃ¨ï£¿Ã¼Ã®Ã³ LangChain







Skip to main contentShare your thoughts on AI agents. Take the 3-min survey.IntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1ï£¿Ã¼Ã­Â¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingd
```

This is essentially a dump of the text from the page's HTML. It may contain extraneous information like headings and navigation bars. If you are familiar with the expected HTML, you can specify desired `<div>` classes and other parameters via BeautifulSoup. Below we parse only the body text of the article:

```python
loader = WebBaseLoader(
    web_paths=[page_url],
    bs_kwargs={
        "parse_only": bs4.SoupStrainer(class_="theme-doc-markdown markdown"),
    },
    bs_get_text_kwargs={"separator": " | ", "strip": True},
)

docs = []
async for doc in loader.alazy_load():
    docs.append(doc)

assert len(docs) == 1
doc = docs[0]
```

```python
print(f"{doc.metadata}\n")
print(doc.page_content[:500])
```

```output
{'source': 'https://python.langchain.com/docs/how_to/chatbots_memory/'}

How to add memory to chatbots | A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including: | Simply stuffing previous messages into a chat model prompt. | The above, but trimming old messages to reduce the amount of distracting information the model has to deal with. | More complex modifications like synthesizing summaries for long running conversations. | We'll go into more detail on a few techniq
```

```python
print(doc.page_content[-500:])
```

```output
a greeting. Nemo then asks the AI how it is doing, and the AI responds that it is fine.'), | HumanMessage(content='What did I say my name was?'), | AIMessage(content='You introduced yourself as Nemo. How can I assist you today, Nemo?')] | Note that invoking the chain again will generate another summary generated from the initial summary plus new messages and so on. You could also design a hybrid approach where a certain number of messages are retained in chat history while others are summarized.
```

Note that this required advance technical knowledge of how the body text is represented in the underlying HTML.

We can parameterize `WebBaseLoader` with a variety of settings, allowing for specification of request headers, rate limits, and parsers and other kwargs for BeautifulSoup. See its [API reference](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) for detail.

## Advanced parsing[â€‹](#advanced-parsing "Direct link to Advanced parsing")

This method is appropriate if we want more granular control or processing of the page content. Below, instead of generating one `Document` per page and controlling its content via BeautifulSoup, we generate multiple `Document` objects representing distinct structures on a page. These structures can include section titles and their corresponding body texts, lists or enumerations, tables, and more.

Under the hood it uses the `langchain-unstructured` library. See the [integration docs](/docs/integrations/document_loaders/unstructured_file/) for more information about using [Unstructured](https://docs.unstructured.io/welcome) with LangChain.

```python
from langchain_unstructured import UnstructuredLoader

page_url = "https://python.langchain.com/docs/how_to/chatbots_memory/"
loader = UnstructuredLoader(web_url=page_url)

docs = []
async for doc in loader.alazy_load():
    docs.append(doc)
```

**API Reference:**[UnstructuredLoader](https://python.langchain.com/api_reference/unstructured/document_loaders/langchain_unstructured.document_loaders.UnstructuredLoader.html)

```output
INFO: Note: NumExpr detected 12 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: NumExpr defaulting to 8 threads.
```

Note that with no advance knowledge of the page HTML structure, we recover a natural organization of the body text:

```python
for doc in docs[:5]:
    print(doc.page_content)
```

```output
How to add memory to chatbots
A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:
Simply stuffing previous messages into a chat model prompt.
The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.
More complex modifications like synthesizing summaries for long running conversations.
ERROR! Session/line number was not unique in database. History logging moved to new session 2747
```

### Extracting content from specific sections[â€‹](#extracting-content-from-specific-sections "Direct link to Extracting content from specific sections")

Each `Document` object represents an element of the page. Its metadata contains useful information, such as its category:

```python
for doc in docs[:5]:
    print(f'{doc.metadata["category"]}: {doc.page_content}')
```

```output
Title: How to add memory to chatbots
NarrativeText: A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:
ListItem: Simply stuffing previous messages into a chat model prompt.
ListItem: The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.
ListItem: More complex modifications like synthesizing summaries for long running conversations.
```

Elements may also have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding `Document` objects.

As an example, below we load the content of the "Setup" sections for two web pages:

```python
from typing import List

from langchain_core.documents import Document


async def _get_setup_docs_from_url(url: str) -> List[Document]:
    loader = UnstructuredLoader(web_url=url)

    setup_docs = []
    parent_id = -1
    async for doc in loader.alazy_load():
        if doc.metadata["category"] == "Title" and doc.page_content.startswith("Setup"):
            parent_id = doc.metadata["element_id"]
        if doc.metadata.get("parent_id") == parent_id:
            setup_docs.append(doc)

    return setup_docs


page_urls = [
    "https://python.langchain.com/docs/how_to/chatbots_memory/",
    "https://python.langchain.com/docs/how_to/chatbots_tools/",
]
setup_docs = []
for url in page_urls:
    page_setup_docs = await _get_setup_docs_from_url(url)
    setup_docs.extend(page_setup_docs)
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

```python
from collections import defaultdict

setup_text = defaultdict(str)

for doc in setup_docs:
    url = doc.metadata["url"]
    setup_text[url] += f"{doc.page_content}\n"

dict(setup_text)
```

```output
{'https://python.langchain.com/docs/how_to/chatbots_memory/': "You'll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\n%pip install --upgrade --quiet langchain langchain-openai\n\n# Set env var OPENAI_API_KEY or load from a .env file:\nimport dotenv\n\ndotenv.load_dotenv()\n[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\nYou should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m\n[0mNote: you may need to restart the kernel to use updated packages.\n",
 'https://python.langchain.com/docs/how_to/chatbots_tools/': "For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\nYou'll need to sign up for an account on the Tavily website, and install the following packages:\n%pip install --upgrade --quiet langchain-community langchain-openai tavily-python\n\n# Set env var OPENAI_API_KEY or load from a .env file:\nimport dotenv\n\ndotenv.load_dotenv()\nYou will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set as TAVILY_API_KEY.\n"}
```

### Vector search over page content[â€‹](#vector-search-over-page-content "Direct link to Vector search over page content")

Once we have loaded the page contents into LangChain `Document` objects, we can index them (e.g., for a RAG application) in the usual way. Below we use OpenAI [embeddings](/docs/concepts/embedding_models/), although any LangChain embeddings model will suffice.

```python
%pip install -qU langchain-openai
```

```python
import getpass
import os

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")
```

```python
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings

vector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())
retrieved_docs = vector_store.similarity_search("Install Tavily", k=2)
for doc in retrieved_docs:
    print(f'Page {doc.metadata["url"]}: {doc.page_content[:300]}\n')
```

**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```````output
INFO: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
INFO: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
``````output
Page https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:

Page https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.
```````

## Other web page loaders[â€‹](#other-web-page-loaders "Direct link to Other web page loaders")

For a list of available LangChain web page loaders, please see [this table](/docs/integrations/document_loaders/#webpages).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/document_loader_web.ipynb)

* * *


- [Setup](#setup)
- [Simple and fast text extraction](#simple-and-fast-text-extraction)
- [Advanced parsing](#advanced-parsing)
  
  - [Extracting content from specific sections](#extracting-content-from-specific-sections)
  - [Vector search over page content](#vector-search-over-page-content)
- [Other web page loaders](#other-web-page-loaders)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/constitutional_chain.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/constitutional_chain.ipynb)

# Migrating from ConstitutionalChain

[ConstitutionalChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.constitutional_ai.base.ConstitutionalChain.html) allowed for a LLM to critique and revise generations based on [principles](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.constitutional_ai.models.ConstitutionalPrinciple.html), structured as combinations of critique and revision requests. For example, a principle might include a request to identify harmful content, and a request to rewrite the content.

`Constitutional AI principles` are based on the [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073) paper.

In `ConstitutionalChain`, this structure of critique requests and associated revisions was formatted into a LLM prompt and parsed out of string responses. This is more naturally achieved via [structured output](/docs/how_to/structured_output/) features of chat models. We can construct a simple chain in [LangGraph](https://langchain-ai.github.io/langgraph/) for this purpose. Some advantages of this approach include:

- Leverage tool-calling capabilities of chat models that have been fine-tuned for this purpose;
- Reduce parsing errors from extracting expression from a string LLM response;
- Delegation of instructions to [message roles](/docs/concepts/messages/) (e.g., chat models can understand what a `ToolMessage` represents without the need for additional prompting);
- Support for streaming, both of individual tokens and chain steps.

```python
%pip install --upgrade --quiet langchain-openai
```

```python
import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

## Legacy[â€‹](#legacy "Direct link to Legacy")

Details

```python
from langchain.chains import ConstitutionalChain, LLMChain
from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAI

llm = OpenAI()

qa_prompt = PromptTemplate(
    template="Q: {question} A:",
    input_variables=["question"],
)
qa_chain = LLMChain(llm=llm, prompt=qa_prompt)

constitutional_chain = ConstitutionalChain.from_llm(
    llm=llm,
    chain=qa_chain,
    constitutional_principles=[
        ConstitutionalPrinciple(
            critique_request="Tell if this answer is good.",
            revision_request="Give a better answer.",
        )
    ],
    return_intermediate_steps=True,
)

result = constitutional_chain.invoke("What is the meaning of life?")
```

**API Reference:**[ConstitutionalChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.constitutional_ai.base.ConstitutionalChain.html) | [LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [ConstitutionalPrinciple](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.constitutional_ai.models.ConstitutionalPrinciple.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

```python
result
```

```output
{'question': 'What is the meaning of life?',
 'output': 'The meaning of life is a deeply personal and ever-evolving concept. It is a journey of self-discovery and growth, and can be different for each individual. Some may find meaning in relationships, others in achieving their goals, and some may never find a concrete answer. Ultimately, the meaning of life is what we make of it.',
 'initial_output': ' The meaning of life is a subjective concept that can vary from person to person. Some may believe that the purpose of life is to find happiness and fulfillment, while others may see it as a journey of self-discovery and personal growth. Ultimately, the meaning of life is something that each individual must determine for themselves.',
 'critiques_and_revisions': [('This answer is good in that it recognizes and acknowledges the subjective nature of the question and provides a valid and thoughtful response. However, it could have also mentioned that the meaning of life is a complex and deeply personal concept that can also change and evolve over time for each individual. Critique Needed.',
   'The meaning of life is a deeply personal and ever-evolving concept. It is a journey of self-discovery and growth, and can be different for each individual. Some may find meaning in relationships, others in achieving their goals, and some may never find a concrete answer. Ultimately, the meaning of life is what we make of it.')]}
```

Above, we've returned intermediate steps showing:

- The original question;
- The initial output;
- Critiques and revisions;
- The final output (matching a revision).

## LangGraph[â€‹](#langgraph "Direct link to LangGraph")

Details

Below, we use the [.with\_structured\_output](/docs/how_to/structured_output/) method to simultaneously generate (1) a judgment of whether a critique is needed, and (2) the critique. We surface all prompts involved for clarity and ease of customizability.

Note that we are also able to stream intermediate steps with this implementation, so we can monitor and if needed intervene during its execution.

```python
from typing import List, Optional, Tuple

from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple
from langchain.chains.constitutional_ai.prompts import (
    CRITIQUE_PROMPT,
    REVISION_PROMPT,
)
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from typing_extensions import Annotated, TypedDict

llm = ChatOpenAI(model="gpt-4o-mini")


class Critique(TypedDict):
    """Generate a critique, if needed."""

    critique_needed: Annotated[bool, ..., "Whether or not a critique is needed."]
    critique: Annotated[str, ..., "If needed, the critique."]


critique_prompt = ChatPromptTemplate.from_template(
    "Critique this response according to the critique request. "
    "If no critique is needed, specify that.\n\n"
    "Query: {query}\n\n"
    "Response: {response}\n\n"
    "Critique request: {critique_request}"
)

revision_prompt = ChatPromptTemplate.from_template(
    "Revise this response according to the critique and reivsion request.\n\n"
    "Query: {query}\n\n"
    "Response: {response}\n\n"
    "Critique request: {critique_request}\n\n"
    "Critique: {critique}\n\n"
    "If the critique does not identify anything worth changing, ignore the "
    "revision request and return 'No revisions needed'. If the critique "
    "does identify something worth changing, revise the response based on "
    "the revision request.\n\n"
    "Revision Request: {revision_request}"
)

chain = llm | StrOutputParser()
critique_chain = critique_prompt | llm.with_structured_output(Critique)
revision_chain = revision_prompt | llm | StrOutputParser()


class State(TypedDict):
    query: str
    constitutional_principles: List[ConstitutionalPrinciple]
    initial_response: str
    critiques_and_revisions: List[Tuple[str, str]]
    response: str


async def generate_response(state: State):
    """Generate initial response."""
    response = await chain.ainvoke(state["query"])
    return {"response": response, "initial_response": response}


async def critique_and_revise(state: State):
    """Critique and revise response according to principles."""
    critiques_and_revisions = []
    response = state["initial_response"]
    for principle in state["constitutional_principles"]:
        critique = await critique_chain.ainvoke(
            {
                "query": state["query"],
                "response": response,
                "critique_request": principle.critique_request,
            }
        )
        if critique["critique_needed"]:
            revision = await revision_chain.ainvoke(
                {
                    "query": state["query"],
                    "response": response,
                    "critique_request": principle.critique_request,
                    "critique": critique["critique"],
                    "revision_request": principle.revision_request,
                }
            )
            response = revision
            critiques_and_revisions.append((critique["critique"], revision))
        else:
            critiques_and_revisions.append((critique["critique"], ""))
    return {
        "critiques_and_revisions": critiques_and_revisions,
        "response": response,
    }


graph = StateGraph(State)
graph.add_node("generate_response", generate_response)
graph.add_node("critique_and_revise", critique_and_revise)

graph.add_edge(START, "generate_response")
graph.add_edge("generate_response", "critique_and_revise")
graph.add_edge("critique_and_revise", END)
app = graph.compile()
```

**API Reference:**[ConstitutionalPrinciple](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.constitutional_ai.models.ConstitutionalPrinciple.html) | [CRITIQUE\_PROMPT](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.CRITIQUE_PROMPT.html) | [REVISION\_PROMPT](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.REVISION_PROMPT.html) | [StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```python
constitutional_principles = [
    ConstitutionalPrinciple(
        critique_request="Tell if this answer is good.",
        revision_request="Give a better answer.",
    )
]

query = "What is the meaning of life? Answer in 10 words or fewer."

async for step in app.astream(
    {"query": query, "constitutional_principles": constitutional_principles},
    stream_mode="values",
):
    subset = ["initial_response", "critiques_and_revisions", "response"]
    print({k: v for k, v in step.items() if k in subset})
```

```output
{}
{'initial_response': 'Finding purpose, connection, and joy in our experiences and relationships.', 'response': 'Finding purpose, connection, and joy in our experiences and relationships.'}
{'initial_response': 'Finding purpose, connection, and joy in our experiences and relationships.', 'critiques_and_revisions': [("The response exceeds the 10-word limit, providing a more elaborate answer than requested. A concise response, such as 'To seek purpose and joy in life,' would better align with the query.", 'To seek purpose and joy in life.')], 'response': 'To seek purpose and joy in life.'}
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

See guides for generating structured output [here](/docs/how_to/structured_output/).

Check out the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) for detail on building with LangGraph.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_chains/constitutional_chain.ipynb)

* * *


- [Legacy](#legacy)
- [LangGraph](#langgraph)
- [Next steps](#next-steps)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_office_file.mdx)

# How to load Microsoft Office files

The [Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.

This covers how to load commonly used file formats including `DOCX`, `XLSX` and `PPTX` documents into a LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) object that we can use downstream.

## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader[â€‹](#loading-docx-xlsx-pptx-with-azureaidocumentintelligenceloader "Direct link to Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader")

[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning based service that extracts texts (including handwriting), tables, document structures (e.g., titles, section headings, etc.) and key-value-pairs from digital or scanned PDFs, images, Office and HTML files. Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`.

This [current implementation](https://aka.ms/di-langchain) of a loader using `Document Intelligence` can incorporate content page-wise and turn it into LangChain documents. The default output format is markdown, which can be easily chained with `MarkdownHeaderTextSplitter` for semantic document chunking. You can also use `mode="single"` or `mode="page"` to return pure texts in a single page or document split by page.

### Prerequisite[â€‹](#prerequisite "Direct link to Prerequisite")

An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0) to create one if you don't have. You will be passing `<endpoint>` and `<key>` as parameters to the loader.

```python
%pip install --upgrade --quiet  langchain langchain-community azure-ai-documentintelligence

from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader

file_path = "<filepath>"
endpoint = "<endpoint>"
key = "<key>"
loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint, api_key=key, file_path=file_path, api_model="prebuilt-layout"
)

documents = loader.load()
```

**API Reference:**[AzureAIDocumentIntelligenceLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.doc_intelligence.AzureAIDocumentIntelligenceLoader.html)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/document_loader_office_file.mdx)

* * *


- [Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader](#loading-docx-xlsx-pptx-with-azureaidocumentintelligenceloader)
  
  - [Prerequisite](#prerequisite)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb)

# Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory

Follow this guide if you're trying to migrate off one of the old memory classes listed below:

| Memory Type                      | Description                                                                                                                                                       |
|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `ConversationBufferWindowMemory` | Keeps the last `n` messages of the conversation. Drops the oldest messages when there are more than `n` messages.                                                 |
| `ConversationTokenBufferMemory`  | Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |

`ConversationBufferWindowMemory` and `ConversationTokenBufferMemory` apply additional processing on top of the raw conversation history to trim the conversation history to a size that fits inside the context window of a chat model.

This processing functionality can be accomplished using LangChain's built-in [trim\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) function.

important

Weâ€™ll begin by exploring a straightforward method that involves applying processing logic to the entire conversation history.

While this approach is easy to implement, it has a downside: as the conversation grows, so does the latency, since the logic is re-applied to all previous exchanges in the conversation at each turn.

More advanced strategies focus on incrementally updating the conversation history to avoid redundant processing.

For instance, the langgraph [how-to guide on summarization](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/) demonstrates how to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.

## Set up[â€‹](#set-up "Direct link to Set up")

```python
%%capture --no-stderr
%pip install --upgrade --quiet langchain-openai langchain
```

```python
import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

## Legacy usage with LLMChain / Conversation Chain[â€‹](#legacy-usage-with-llmchain--conversation-chain "Direct link to Legacy usage with LLMChain / Conversation Chain")

Details

```python
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.messages import SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts.chat import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate(
    [
        SystemMessage(content="You are a helpful assistant."),
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessagePromptTemplate.from_template("{text}"),
    ]
)

memory = ConversationBufferWindowMemory(memory_key="chat_history", return_messages=True)

legacy_chain = LLMChain(
    llm=ChatOpenAI(),
    prompt=prompt,
    memory=memory,
)

legacy_result = legacy_chain.invoke({"text": "my name is bob"})
print(legacy_result)

legacy_result = legacy_chain.invoke({"text": "what was my name"})
print(legacy_result)
```

**API Reference:**[LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [ConversationBufferWindowMemory](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer_window.ConversationBufferWindowMemory.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [HumanMessagePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.HumanMessagePromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
{'text': 'Nice to meet you, Bob! How can I assist you today?', 'chat_history': []}
{'text': 'Your name is Bob. How can I assist you further, Bob?', 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, Bob! How can I assist you today?', additional_kwargs={}, response_metadata={})]}
```

## Reimplementing ConversationBufferWindowMemory logic[â€‹](#reimplementing-conversationbufferwindowmemory-logic "Direct link to Reimplementing ConversationBufferWindowMemory logic")

Let's first create appropriate logic to process the conversation history, and then we'll see how to integrate it into an application. You can later replace this basic setup with more advanced logic tailored to your specific needs.

We'll use `trim_messages` to implement logic that keeps the last `n` messages of the conversation. It will drop the oldest messages when the number of messages exceeds `n`.

In addition, we will also keep the system message if it's present -- when present, it's the first message in a conversation that includes instructions for the chat model.

```python
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    trim_messages,
)
from langchain_openai import ChatOpenAI

messages = [
    SystemMessage("you're a good assistant, you always respond with a joke."),
    HumanMessage("i wonder why it's called langchain"),
    AIMessage(
        'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
    ),
    HumanMessage("and who is harrison chasing anyways"),
    AIMessage(
        "Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"
    ),
    HumanMessage("why is 42 always the answer?"),
    AIMessage(
        "Because itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!"
    ),
    HumanMessage("What did the cow say?"),
]
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [trim\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```python
from langchain_core.messages import trim_messages

selected_messages = trim_messages(
    messages,
    token_counter=len,  # <-- len will simply count the number of messages rather than tokens
    max_tokens=5,  # <-- allow up to 5 messages.
    strategy="last",
    # Most chat models expect that chat history starts with either:
    # (1) a HumanMessage or
    # (2) a SystemMessage followed by a HumanMessage
    # start_on="human" makes sure we produce a valid chat history
    start_on="human",
    # Usually, we want to keep the SystemMessage
    # if it's present in the original history.
    # The SystemMessage has special instructions for the model.
    include_system=True,
    allow_partial=False,
)

for msg in selected_messages:
    msg.pretty_print()
```

**API Reference:**[trim\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html)

```output
================================[1m System Message [0m================================

you're a good assistant, you always respond with a joke.
==================================[1m Ai Message [0m==================================

Hmmm let me think.

Why, he's probably chasing after the last cup of coffee in the office!
================================[1m Human Message [0m=================================

why is 42 always the answer?
==================================[1m Ai Message [0m==================================

Because itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!
================================[1m Human Message [0m=================================

What did the cow say?
```

## Reimplementing ConversationTokenBufferMemory logic[â€‹](#reimplementing-conversationtokenbuffermemory-logic "Direct link to Reimplementing ConversationTokenBufferMemory logic")

Here, we'll use `trim_messages` to keeps the system message and the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.

```python
from langchain_core.messages import trim_messages

selected_messages = trim_messages(
    messages,
    # Please see API reference for trim_messages for other ways to specify a token counter.
    token_counter=ChatOpenAI(model="gpt-4o"),
    max_tokens=80,  # <-- token limit
    # The start_on is specified
    # Most chat models expect that chat history starts with either:
    # (1) a HumanMessage or
    # (2) a SystemMessage followed by a HumanMessage
    # start_on="human" makes sure we produce a valid chat history
    start_on="human",
    # Usually, we want to keep the SystemMessage
    # if it's present in the original history.
    # The SystemMessage has special instructions for the model.
    include_system=True,
    strategy="last",
)

for msg in selected_messages:
    msg.pretty_print()
```

**API Reference:**[trim\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html)

```output
================================[1m System Message [0m================================

you're a good assistant, you always respond with a joke.
================================[1m Human Message [0m=================================

why is 42 always the answer?
==================================[1m Ai Message [0m==================================

Because itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!
================================[1m Human Message [0m=================================

What did the cow say?
```

## Modern usage with LangGraph[â€‹](#modern-usage-with-langgraph "Direct link to Modern usage with LangGraph")

The example below shows how to use LangGraph to add simple conversation pre-processing logic.

note

If you want to avoid running the computation on the entire conversation history each time, you can follow the [how-to guide on summarization](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/) that demonstrates how to discard older messages, ensuring they aren't re-processed during later turns.

Details

```python
import uuid

from IPython.display import Image, display
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

# Define a new graph
workflow = StateGraph(state_schema=MessagesState)

# Define a chat model
model = ChatOpenAI()


# Define the function that calls the model
def call_model(state: MessagesState):
    selected_messages = trim_messages(
        state["messages"],
        token_counter=len,  # <-- len will simply count the number of messages rather than tokens
        max_tokens=5,  # <-- allow up to 5 messages.
        strategy="last",
        # Most chat models expect that chat history starts with either:
        # (1) a HumanMessage or
        # (2) a SystemMessage followed by a HumanMessage
        # start_on="human" makes sure we produce a valid chat history
        start_on="human",
        # Usually, we want to keep the SystemMessage
        # if it's present in the original history.
        # The SystemMessage has special instructions for the model.
        include_system=True,
        allow_partial=False,
    )

    response = model.invoke(selected_messages)
    # We return a list, because this will get added to the existing list
    return {"messages": response}


# Define the two nodes we will cycle between
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)


# Adding memory is straight forward in langgraph!
memory = MemorySaver()

app = workflow.compile(
    checkpointer=memory
)


# The thread id is a unique key that identifies
# this particular conversation.
# We'll just generate a random uuid here.
thread_id = uuid.uuid4()
config = {"configurable": {"thread_id": thread_id}}

input_message = HumanMessage(content="hi! I'm bob")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()

# Here, let's confirm that the AI remembers our name!
config = {"configurable": {"thread_id": thread_id}}
input_message = HumanMessage(content="what was my name?")
for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```output
================================[1m Human Message [0m=================================

hi! I'm bob
==================================[1m Ai Message [0m==================================

Hello Bob! How can I assist you today?
================================[1m Human Message [0m=================================

what was my name?
==================================[1m Ai Message [0m==================================

Your name is Bob. How can I help you, Bob?
```

## Usage with a pre-built langgraph agent[â€‹](#usage-with-a-pre-built-langgraph-agent "Direct link to Usage with a pre-built langgraph agent")

This example shows usage of an Agent Executor with a pre-built agent constructed using the [create\_tool\_calling\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) function.

If you are using one of the [old LangChain pre-built agents](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/), you should be able to replace that code with the new [langgraph pre-built agent](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/) which leverages native tool calling capabilities of chat models and will likely work better out of the box.

Details

```python
import uuid

from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    trim_messages,
)
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent


@tool
def get_user_age(name: str) -> str:
    """Use this tool to find the user's age."""
    # This is a placeholder for the actual implementation
    if "bob" in name.lower():
        return "42 years old"
    return "41 years old"


memory = MemorySaver()
model = ChatOpenAI()


def prompt(state) -> list[BaseMessage]:
    """Given the agent state, return a list of messages for the chat model."""
    # We're using the message processor defined above.
    return trim_messages(
        state["messages"],
        token_counter=len,  # <-- len will simply count the number of messages rather than tokens
        max_tokens=5,  # <-- allow up to 5 messages.
        strategy="last",
        # Most chat models expect that chat history starts with either:
        # (1) a HumanMessage or
        # (2) a SystemMessage followed by a HumanMessage
        # start_on="human" makes sure we produce a valid chat history
        start_on="human",
        # Usually, we want to keep the SystemMessage
        # if it's present in the original history.
        # The SystemMessage has special instructions for the model.
        include_system=True,
        allow_partial=False,
    )



app = create_react_agent(
    model,
    tools=[get_user_age],
    checkpointer=memory,
    prompt=prompt,
)

# The thread id is a unique key that identifies
# this particular conversation.
# We'll just generate a random uuid here.
thread_id = uuid.uuid4()
config = {"configurable": {"thread_id": thread_id}}

# Tell the AI that our name is Bob, and ask it to use a tool to confirm
# that it's capable of working like an agent.
input_message = HumanMessage(content="hi! I'm bob. What is my age?")

for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()

# Confirm that the chat bot has access to previous conversation
# and can respond to the user saying that the user's name is Bob.
input_message = HumanMessage(content="do you remember my name?")

for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [trim\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```output
================================[1m Human Message [0m=================================

hi! I'm bob. What is my age?
==================================[1m Ai Message [0m==================================
Tool Calls:
  get_user_age (call_jsMvoIFv970DhqqLCJDzPKsp)
 Call ID: call_jsMvoIFv970DhqqLCJDzPKsp
  Args:
    name: bob
=================================[1m Tool Message [0m=================================
Name: get_user_age

42 years old
==================================[1m Ai Message [0m==================================

Bob, you are 42 years old.
================================[1m Human Message [0m=================================

do you remember my name?
==================================[1m Ai Message [0m==================================

Yes, your name is Bob.
```

## LCEL: Add a preprocessing step[â€‹](#lcel-add-a-preprocessing-step "Direct link to LCEL: Add a preprocessing step")

The simplest way to add complex conversation management is by introducing a pre-processing step in front of the chat model and pass the full conversation history to the pre-processing step.

This approach is conceptually simple and will work in many situations; for example, if using a [RunnableWithMessageHistory](/docs/how_to/message_history/) instead of wrapping the chat model, wrap the chat model with the pre-processor.

The obvious downside of this approach is that latency starts to increase as the conversation history grows because of two reasons:

1. As the conversation gets longer, more data may need to be fetched from whatever store your'e using to store the conversation history (if not storing it in memory).
2. The pre-processing logic will end up doing a lot of redundant computation, repeating computation from previous steps of the conversation.

caution

If you want to use a chat model's tool calling capabilities, remember to bind the tools to the model before adding the history pre-processing step to it!

Details

```python
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    trim_messages,
)
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

model = ChatOpenAI()


@tool
def what_did_the_cow_say() -> str:
    """Check to see what the cow said."""
    return "foo"


message_processor = trim_messages(  # Returns a Runnable if no messages are provided
    token_counter=len,  # <-- len will simply count the number of messages rather than tokens
    max_tokens=5,  # <-- allow up to 5 messages.
    strategy="last",
    # The start_on is specified
    # to make sure we do not generate a sequence where
    # a ToolMessage that contains the result of a tool invocation
    # appears before the AIMessage that requested a tool invocation
    # as this will cause some chat models to raise an error.
    start_on=("human", "ai"),
    include_system=True,  # <-- Keep the system message
    allow_partial=False,
)

# Note that we bind tools to the model first!
model_with_tools = model.bind_tools([what_did_the_cow_say])

model_with_preprocessor = message_processor | model_with_tools

full_history = [
    SystemMessage("you're a good assistant, you always respond with a joke."),
    HumanMessage("i wonder why it's called langchain"),
    AIMessage(
        'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
    ),
    HumanMessage("and who is harrison chasing anyways"),
    AIMessage(
        "Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"
    ),
    HumanMessage("why is 42 always the answer?"),
    AIMessage(
        "Because itâ€™s the only number thatâ€™s constantly right, even when it doesnâ€™t add up!"
    ),
    HumanMessage("What did the cow say?"),
]


# We pass it explicity to the model_with_preprocesor for illustrative purposes.
# If you're using `RunnableWithMessageHistory` the history will be automatically
# read from the source the you configure.
model_with_preprocessor.invoke(full_history).pretty_print()
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [trim\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
==================================[1m Ai Message [0m==================================
Tool Calls:
  what_did_the_cow_say (call_urHTB5CShhcKz37QiVzNBlIS)
 Call ID: call_urHTB5CShhcKz37QiVzNBlIS
  Args:
```

If you need to implement more efficient logic and want to use `RunnableWithMessageHistory` for now the way to achieve this is to subclass from [BaseChatMessageHistory](https://python.langchain.com/api_reference/core/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html) and define appropriate logic for `add_messages` (that doesn't simply append the history, but instead re-writes it).

Unless you have a good reason to implement this solution, you should instead use LangGraph.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

Explore persistence with LangGraph:

- [LangGraph quickstart tutorial](https://langchain-ai.github.io/langgraph/tutorials/introduction/)
- [How to add persistence ("memory") to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)
- [How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/)
- [How to add summary of the conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)

Add persistence with simple LCEL (favor langgraph for more complex use cases):

- [How to add message history](/docs/how_to/message_history/)

Working with message history:

- [How to trim messages](/docs/how_to/trim_messages/)
- [How to filter messages](/docs/how_to/filter_messages/)
- [How to merge message runs](/docs/how_to/merge_message_runs/)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb)

* * *


- [Set up](#set-up)
- [Legacy usage with LLMChain / Conversation Chain](#legacy-usage-with-llmchain--conversation-chain)
- [Reimplementing ConversationBufferWindowMemory logic](#reimplementing-conversationbufferwindowmemory-logic)
- [Reimplementing ConversationTokenBufferMemory logic](#reimplementing-conversationtokenbuffermemory-logic)
- [Modern usage with LangGraph](#modern-usage-with-langgraph)
- [Usage with a pre-built langgraph agent](#usage-with-a-pre-built-langgraph-agent)
- [LCEL: Add a preprocessing step](#lcel-add-a-preprocessing-step)
- [Next steps](#next-steps)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_memory/index.mdx)

# How to migrate to LangGraph memory

As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into their LangChain application.

- Users that rely on `RunnableWithMessageHistory` or `BaseChatMessageHistory` do **not** need to make any changes, but are encouraged to consider using LangGraph for more complex use cases.
- Users that rely on deprecated memory abstractions from LangChain 0.0.x should follow this guide to upgrade to the new LangGraph persistence feature in LangChain 0.3.x.

## Why use LangGraph for memory?[â€‹](#why-use-langgraph-for-memory "Direct link to Why use LangGraph for memory?")

The main advantages of persistence in LangGraph are:

- Built-in support for multiple users and conversations, which is a typical requirement for real-world conversational AI applications.
- Ability to save and resume complex conversations at any point. This helps with:
  
  - Error recovery
  - Allowing human intervention in AI workflows
  - Exploring different conversation paths ("time travel")
- Full compatibility with both traditional [language models](/docs/concepts/text_llms/) and modern [chat models](/docs/concepts/chat_models/). Early memory implementations in LangChain weren't designed for newer chat model APIs, causing issues with features like tool-calling. LangGraph memory can persist any custom state.
- Highly customizable, allowing you to fully control how memory works and use different storage backends.

## Evolution of memory in LangChain[â€‹](#evolution-of-memory-in-langchain "Direct link to Evolution of memory in LangChain")

The concept of memory has evolved significantly in LangChain since its initial release.

### LangChain 0.0.x memory[â€‹](#langchain-00x-memory "Direct link to LangChain 0.0.x memory")

Broadly speaking, LangChain 0.0.x memory was used to handle three main use cases:

| Use Case                             | Example                                                                                                                           |
|--------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|
| Managing conversation history        | Keep only the last `n` turns of the conversation between the user and the AI.                                                     |
| Extraction of structured information | Extract structured information from the conversation history, such as a list of facts learned about the user.                     |
| Composite memory implementations     | Combine multiple memory sources, e.g., a list of known facts about the user along with facts learned during a given conversation. |

While the LangChain 0.0.x memory abstractions were useful, they were limited in their capabilities and not well suited for real-world conversational AI applications. These memory abstractions lacked built-in support for multi-user, multi-conversation scenarios, which are essential for practical conversational AI systems.

Most of these implementations have been officially deprecated in LangChain 0.3.x in favor of LangGraph persistence.

### RunnableWithMessageHistory and BaseChatMessageHistory[â€‹](#runnablewithmessagehistory-and-basechatmessagehistory "Direct link to RunnableWithMessageHistory and BaseChatMessageHistory")

note

Please see [How to use BaseChatMessageHistory with LangGraph](/docs/versions/migrating_memory/chat_history/), if you would like to use `BaseChatMessageHistory` (with or without `RunnableWithMessageHistory`) in LangGraph.

As of LangChain v0.1, we started recommending that users rely primarily on [BaseChatMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#langchain_core.runnables.history.RunnableWithMessageHistory). `BaseChatMessageHistory` serves as a simple persistence for storing and retrieving messages in a conversation.

At that time, the only option for orchestrating LangChain chains was via [LCEL](https://python.langchain.com/docs/how_to/#langchain-expression-language-lcel). To incorporate memory with `LCEL`, users had to use the [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html#langchain_core.runnables.history.RunnableWithMessageHistory) interface. While sufficient for basic chat applications, many users found the API unintuitive and challenging to use.

As of LangChain v0.3, we recommend that **new** code takes advantage of LangGraph for both orchestration and persistence:

- Orchestration: In LangGraph, users define [graphs](https://langchain-ai.github.io/langgraph/concepts/low_level/) that specify the flow of the application. This allows users to keep using `LCEL` within individual nodes when `LCEL` is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.
- Persistence: Users can rely on LangGraph's [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to store and retrieve data. LangGraph persistence is extremely flexible and can support a much wider range of use cases than the `RunnableWithMessageHistory` interface.

important

If you have been using `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do not need to make any changes. We do not plan on deprecating either functionality in the near future. This functionality is sufficient for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.

## Migrations[â€‹](#migrations "Direct link to Migrations")

Prerequisites

These guides assume some familiarity with the following concepts:

- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [v0.0.x Memory](https://python.langchain.com/v0.1/docs/modules/memory/)
- [How to add persistence ("memory") to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)

### 1. Managing conversation history[â€‹](#1-managing-conversation-history "Direct link to 1. Managing conversation history")

The goal of managing conversation history is to store and retrieve the history in a way that is optimal for a chat model to use.

Often this involves trimming and / or summarizing the conversation history to keep the most relevant parts of the conversation while having the conversation fit inside the context window of the chat model.

Memory classes that fall into this category include:

| Memory Type                       | How to Migrate                                                                                         | Description                                                                                                                                                                                                         |
|-----------------------------------|--------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `ConversationBufferMemory`        | [Link to Migration Guide](/docs/versions/migrating_memory/conversation_buffer_memory/)                 | A basic memory implementation that simply stores the conversation history.                                                                                                                                          |
| `ConversationStringBufferMemory`  | [Link to Migration Guide](/docs/versions/migrating_memory/conversation_buffer_memory/)                 | A special case of `ConversationBufferMemory` designed for LLMs and no longer relevant.                                                                                                                              |
| `ConversationBufferWindowMemory`  | [Link to Migration Guide](/docs/versions/migrating_memory/conversation_buffer_window_memory/)          | Keeps the last `n` turns of the conversation. Drops the oldest turn when the buffer is full.                                                                                                                        |
| `ConversationTokenBufferMemory`   | [Link to Migration Guide](/docs/versions/migrating_memory/conversation_buffer_window_memory/)          | Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.                                                   |
| `ConversationSummaryMemory`       | [Link to Migration Guide](/docs/versions/migrating_memory/conversation_summary_memory/)                | Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history.                                              |
| `ConversationSummaryBufferMemory` | [Link to Migration Guide](/docs/versions/migrating_memory/conversation_summary_memory/)                | Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |
| `VectorStoreRetrieverMemory`      | See related [long-term memory agent tutorial](/docs/versions/migrating_memory/long_term_memory_agent/) | Stores the conversation history in a vector store and retrieves the most relevant parts of past conversation based on the input.                                                                                    |

### 2. Extraction of structured information from the conversation history[â€‹](#2-extraction-of-structured-information-from-the-conversation-history "Direct link to 2. Extraction of structured information from the conversation history")

Please see [long-term memory agent tutorial](/docs/versions/migrating_memory/long_term_memory_agent/) implements an agent that can extract structured information from the conversation history.

Memory classes that fall into this category include:

| Memory Type                | Description                                                                                                                                                                                                       |
|----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `BaseEntityStore`          | An abstract interface that resembles a key-value store. It was used for storing structured information learned during the conversation. The information had to be represented as a dictionary of key-value pairs. |
| `ConversationEntityMemory` | Combines the ability to summarize the conversation while extracting structured information from the conversation history.                                                                                         |

And specific backend implementations of abstractions:

| Memory Type               | Description                                                                                              |
|---------------------------|----------------------------------------------------------------------------------------------------------|
| `InMemoryEntityStore`     | An implementation of `BaseEntityStore` that stores the information in the literal computer memory (RAM). |
| `RedisEntityStore`        | A specific implementation of `BaseEntityStore` that uses Redis as the backend.                           |
| `SQLiteEntityStore`       | A specific implementation of `BaseEntityStore` that uses SQLite as the backend.                          |
| `UpstashRedisEntityStore` | A specific implementation of `BaseEntityStore` that uses Upstash as the backend.                         |

These abstractions have received limited development since their initial release. This is because they generally require significant customization for a specific application to be effective, making them less widely used than the conversation history management abstractions.

For this reason, there are no migration guides for these abstractions. If you're struggling to migrate an application that relies on these abstractions, please:

1. Please review this [Long-term memory agent tutorial](/docs/versions/migrating_memory/long_term_memory_agent/) which should provide a good starting point for how to extract structured information from the conversation history.
2. If you're still struggling, please open an issue on the LangChain GitHub repository, explain your use case, and we'll try to provide more guidance on how to migrate these abstractions.

The general strategy for extracting structured information from the conversation history is to use a chat model with tool calling capabilities to extract structured information from the conversation history. The extracted information can then be saved into an appropriate data structure (e.g., a dictionary), and information from it can be retrieved and added into the prompt as needed.

### 3. Implementations that provide composite logic on top of one or more memory implementations[â€‹](#3-implementations-that-provide-composite-logic-on-top-of-one-or-more-memory-implementations "Direct link to 3. Implementations that provide composite logic on top of one or more memory implementations")

Memory classes that fall into this category include:

| Memory Type            | Description                                                                                                                    |
|------------------------|--------------------------------------------------------------------------------------------------------------------------------|
| `CombinedMemory`       | This abstraction accepted a list of `BaseMemory` and fetched relevant memory information from each of them based on the input. |
| `SimpleMemory`         | Used to add read-only hard-coded context. Users can simply write this information into the prompt.                             |
| `ReadOnlySharedMemory` | Provided a read-only view of an existing `BaseMemory` implementation.                                                          |

These implementations did not seem to be used widely or provide significant value. Users should be able to re-implement these without too much difficulty in custom code.

## Related Resources[â€‹](#related-resources "Direct link to Related Resources")

Explore persistence with LangGraph:

- [LangGraph quickstart tutorial](https://langchain-ai.github.io/langgraph/tutorials/introduction/)
- [How to add persistence ("memory") to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)
- [How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/)
- [How to add summary of the conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)

Add persistence with simple LCEL (favor langgraph for more complex use cases):

- [How to add message history](https://python.langchain.com/docs/how_to/message_history/)

Working with message history:

- [How to trim messages](https://python.langchain.com/docs/how_to/trim_messages)
- [How to filter messages](https://python.langchain.com/docs/how_to/filter_messages/)
- [How to merge message runs](https://python.langchain.com/docs/how_to/merge_message_runs/)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_memory/index.mdx)

* * *


- [Why use LangGraph for memory?](#why-use-langgraph-for-memory)
- [Evolution of memory in LangChain](#evolution-of-memory-in-langchain)
  
  - [LangChain 0.0.x memory](#langchain-00x-memory)
  - [RunnableWithMessageHistory and BaseChatMessageHistory](#runnablewithmessagehistory-and-basechatmessagehistory)
- [Migrations](#migrations)
  
  - [1. Managing conversation history](#1-managing-conversation-history)
  - [2. Extraction of structured information from the conversation history](#2-extraction-of-structured-information-from-the-conversation-history)
  - [3. Implementations that provide composite logic on top of one or more memory implementations](#3-implementations-that-provide-composite-logic-on-top-of-one-or-more-memory-implementations)
- [Related Resources](#related-resources)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/rag.mdx)

# Retrieval augmented generation (RAG)

Prerequisites

- [Retrieval](/docs/concepts/retrieval/)

## Overview[â€‹](#overview "Direct link to Overview")

Retrieval Augmented Generation (RAG) is a powerful technique that enhances [language models](/docs/concepts/chat_models/) by combining them with external knowledge bases. RAG addresses [a key limitation of models](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise): models rely on fixed training datasets, which can lead to outdated or incomplete information. When given a query, RAG systems first search a knowledge base for relevant information. The system then incorporates this retrieved information into the model's prompt. The model uses the provided context to generate a response to the query. By bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.

## Key concepts[â€‹](#key-concepts "Direct link to Key concepts")

![Conceptual Overview](/assets/images/rag_concepts-4499b260d1053838a3e361fb54f376ec.png)

(1) **Retrieval system**: Retrieve relevant information from a knowledge base.

(2) **Adding external knowledge**: Pass retrieved information to a model.

## Retrieval system[â€‹](#retrieval-system "Direct link to Retrieval system")

Model's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training. This limits their ability to answer questions about current events, or to provide specific domain knowledge. To address this, there are various knowledge injection techniques like [fine-tuning](https://hamel.dev/blog/posts/fine_tuning_valuable.html) or continued pre-training. Both are [costly](https://www.glean.com/blog/how-to-build-an-ai-assistant-for-the-enterprise) and often [poorly suited](https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts) for factual retrieval. Using a retrieval system offers several advantages:

- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.
- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in specific domains.
- **Reduced hallucination**: Grounding responses in retrieved facts helps minimize false or invented information.
- **Cost-effective knowledge integration**: RAG offers a more efficient alternative to expensive model fine-tuning.

Further reading

See our conceptual guide on [retrieval](/docs/concepts/retrieval/).

## Adding external knowledge[â€‹](#adding-external-knowledge "Direct link to Adding external knowledge")

With a retrieval system in place, we need to pass knowledge from this system to the model. A RAG pipeline typically achieves this following these steps:

- Receive an input query.
- Use the retrieval system to search for relevant information based on the query.
- Incorporate the retrieved information into the prompt sent to the LLM.
- Generate a response that leverages the retrieved context.

As an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# Define a system prompt that tells the model how to use the retrieved context
system_prompt = """You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. 
Use three sentences maximum and keep the answer concise.
Context: {context}:"""
    
# Define a question
question = """What are the main components of an LLM-powered autonomous agent system?"""

# Retrieve relevant documents
docs = retriever.invoke(question)

# Combine the documents into a single string
docs_text = "".join(d.page_content for d in docs)

# Populate the system prompt with the retrieved context
system_prompt_fmt = system_prompt.format(context=docs_text)

# Create a model
model = ChatOpenAI(model="gpt-4o", temperature=0) 

# Generate a response
questions = model.invoke([SystemMessage(content=system_prompt_fmt),
                          HumanMessage(content=question)])
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

Further reading

RAG a deep area with many possible optimization and design choices:

- See [this excellent blog](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval?utm_source=profile&utm_medium=reader2) from Cameron Wolfe for a comprehensive overview and history of RAG.
- See our [RAG how-to guides](/docs/how_to/#qa-with-rag).
- See our RAG [tutorials](/docs/tutorials/).
- See our RAG from Scratch course, with [code](https://github.com/langchain-ai/rag-from-scratch) and [video playlist](https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x).
- Also, see our RAG from Scratch course [on Freecodecamp](https://youtu.be/sVcwVQRHIc8?feature=shared).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/rag.mdx)

* * *


- [Overview](#overview)
- [Key concepts](#key-concepts)
- [Retrieval system](#retrieval-system)
- [Adding external knowledge](#adding-external-knowledge)









# How to convert tools to OpenAI Functions

This notebook goes over how to use LangChain [tools](/docs/concepts/tools/) as OpenAI functions.

```python
%pip install -qU langchain-community langchain-openai
```

```python
from langchain_community.tools import MoveFileTool
from langchain_core.messages import HumanMessage
from langchain_core.utils.function_calling import convert_to_openai_function
from langchain_openai import ChatOpenAI
```

**API Reference:**[MoveFileTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.file_management.move.MoveFileTool.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [convert\_to\_openai\_function](https://python.langchain.com/api_reference/core/utils/langchain_core.utils.function_calling.convert_to_openai_function.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```python
model = ChatOpenAI(model="gpt-3.5-turbo")
```

```python
tools = [MoveFileTool()]
functions = [convert_to_openai_function(t) for t in tools]
```

```python
functions[0]
```

```output
{'name': 'move_file',
 'description': 'Move or rename a file from one location to another',
 'parameters': {'type': 'object',
  'properties': {'source_path': {'description': 'Path of the file to move',
    'type': 'string'},
   'destination_path': {'description': 'New path for the moved file',
    'type': 'string'}},
  'required': ['source_path', 'destination_path']}}
```

```python
message = model.invoke(
    [HumanMessage(content="move file foo to bar")], functions=functions
)
```

```python
message
```

```output
AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\n  "source_path": "foo",\n  "destination_path": "bar"\n}', 'name': 'move_file'}})
```

```python
message.additional_kwargs["function_call"]
```

```output
{'name': 'move_file',
 'arguments': '{\n  "source_path": "foo",\n  "destination_path": "bar"\n}'}
```

With OpenAI chat models we can also automatically bind and convert function-like objects with `bind_functions`

```python
model_with_functions = model.bind_functions(tools)
model_with_functions.invoke([HumanMessage(content="move file foo to bar")])
```

```output
AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\n  "source_path": "foo",\n  "destination_path": "bar"\n}', 'name': 'move_file'}})
```

Or we can use the update OpenAI API that uses `tools` and `tool_choice` instead of `functions` and `function_call` by using `ChatOpenAI.bind_tools`:

```python
model_with_tools = model.bind_tools(tools)
model_with_tools.invoke([HumanMessage(content="move file foo to bar")])
```

```output
AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_btkY3xV71cEVAOHnNa5qwo44', 'function': {'arguments': '{\n  "source_path": "foo",\n  "destination_path": "bar"\n}', 'name': 'move_file'}, 'type': 'function'}]})
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tools_as_openai_functions.ipynb)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_custom.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_custom.ipynb)

# How to create a custom Document Loader

## Overview[â€‹](#overview "Direct link to Overview")

Applications based on LLMs frequently entail extracting data from databases or files, like PDFs, and converting it into a format that LLMs can utilize. In LangChain, this usually involves creating Document objects, which encapsulate the extracted text (`page_content`) along with metadataâ€”a dictionary containing details about the document, such as the author's name or the date of publication.

`Document` objects are often formatted into prompts that are fed into an LLM, allowing the LLM to use the information in the `Document` to generate a desired response (e.g., summarizing the document). `Documents` can be either used immediately or indexed into a vectorstore for future retrieval and use.

The main abstractions for [Document Loading](/docs/concepts/document_loaders/) are:

| Component      | Description                                                                  |
|----------------|------------------------------------------------------------------------------|
| Document       | Contains `text` and `metadata`                                               |
| BaseLoader     | Use to convert raw data into `Documents`                                     |
| Blob           | A representation of binary data that's located either in a file or in memory |
| BaseBlobParser | Logic to parse a `Blob` to yield `Document` objects                          |

This guide will demonstrate how to write custom document loading and file parsing logic; specifically, we'll see how to:

1. Create a standard document Loader by sub-classing from `BaseLoader`.
2. Create a parser using `BaseBlobParser` and use it in conjunction with `Blob` and `BlobLoaders`. This is useful primarily when working with files.

## Standard Document Loader[â€‹](#standard-document-loader "Direct link to Standard Document Loader")

A document loader can be implemented by sub-classing from a `BaseLoader` which provides a standard interface for loading documents.

### Interface[â€‹](#interface "Direct link to Interface")

| Method Name | Explanation                                                                                                                         |
|-------------|-------------------------------------------------------------------------------------------------------------------------------------|
| lazy\_load  | Used to load documents one by one **lazily**. Use for production code.                                                              |
| alazy\_load | Async variant of `lazy_load`                                                                                                        |
| load        | Used to load all the documents into memory **eagerly**. Use for prototyping or interactive work.                                    |
| aload       | Used to load all the documents into memory **eagerly**. Use for prototyping or interactive work. **Added in 2024-04 to LangChain.** |

- The `load` methods is a convenience method meant solely for prototyping work -- it just invokes `list(self.lazy_load())`.
- The `alazy_load` has a default implementation that will delegate to `lazy_load`. If you're using async, we recommend overriding the default implementation and providing a native async implementation.

important

When implementing a document loader do **NOT** provide parameters via the `lazy_load` or `alazy_load` methods.

All configuration is expected to be passed through the initializer (**init**). This was a design choice made by LangChain to make sure that once a document loader has been instantiated it has all the information needed to load documents.

### Implementation[â€‹](#implementation "Direct link to Implementation")

Let's create an example of a standard document loader that loads a file and creates a document from each line in the file.

```python
from typing import AsyncIterator, Iterator

from langchain_core.document_loaders import BaseLoader
from langchain_core.documents import Document


class CustomDocumentLoader(BaseLoader):
    """An example document loader that reads a file line by line."""

    def __init__(self, file_path: str) -> None:
        """Initialize the loader with a file path.

        Args:
            file_path: The path to the file to load.
        """
        self.file_path = file_path

    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments
        """A lazy loader that reads a file line by line.

        When you're implementing lazy load methods, you should use a generator
        to yield documents one by one.
        """
        with open(self.file_path, encoding="utf-8") as f:
            line_number = 0
            for line in f:
                yield Document(
                    page_content=line,
                    metadata={"line_number": line_number, "source": self.file_path},
                )
                line_number += 1

    # alazy_load is OPTIONAL.
    # If you leave out the implementation, a default implementation which delegates to lazy_load will be used!
    async def alazy_load(
        self,
    ) -> AsyncIterator[Document]:  # <-- Does not take any arguments
        """An async lazy loader that reads a file line by line."""
        # Requires aiofiles (install with pip)
        # https://github.com/Tinche/aiofiles
        import aiofiles

        async with aiofiles.open(self.file_path, encoding="utf-8") as f:
            line_number = 0
            async for line in f:
                yield Document(
                    page_content=line,
                    metadata={"line_number": line_number, "source": self.file_path},
                )
                line_number += 1
```

**API Reference:**[BaseLoader](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

### Test ðŸ§ª[â€‹](#test- "Direct link to Test ðŸ§ª")

To test out the document loader, we need a file with some quality content.

```python
with open("./meow.txt", "w", encoding="utf-8") as f:
    quality_content = "meow meowðŸ± \n meow meowðŸ± \n meowðŸ˜»ðŸ˜»"
    f.write(quality_content)

loader = CustomDocumentLoader("./meow.txt")
```

```python
## Test out the lazy load interface
for doc in loader.lazy_load():
    print()
    print(type(doc))
    print(doc)
```

```output

<class 'langchain_core.documents.base.Document'>
page_content='meow meowðŸ± \n' metadata={'line_number': 0, 'source': './meow.txt'}

<class 'langchain_core.documents.base.Document'>
page_content=' meow meowðŸ± \n' metadata={'line_number': 1, 'source': './meow.txt'}

<class 'langchain_core.documents.base.Document'>
page_content=' meowðŸ˜»ðŸ˜»' metadata={'line_number': 2, 'source': './meow.txt'}
```

```python
## Test out the async implementation
async for doc in loader.alazy_load():
    print()
    print(type(doc))
    print(doc)
```

```output

<class 'langchain_core.documents.base.Document'>
page_content='meow meowðŸ± \n' metadata={'line_number': 0, 'source': './meow.txt'}

<class 'langchain_core.documents.base.Document'>
page_content=' meow meowðŸ± \n' metadata={'line_number': 1, 'source': './meow.txt'}

<class 'langchain_core.documents.base.Document'>
page_content=' meowðŸ˜»ðŸ˜»' metadata={'line_number': 2, 'source': './meow.txt'}
```

tip

`load()` can be helpful in an interactive environment such as a jupyter notebook.

Avoid using it for production code since eager loading assumes that all the content can fit into memory, which is not always the case, especially for enterprise data.

```python
loader.load()
```

```output
[Document(page_content='meow meowðŸ± \n', metadata={'line_number': 0, 'source': './meow.txt'}),
 Document(page_content=' meow meowðŸ± \n', metadata={'line_number': 1, 'source': './meow.txt'}),
 Document(page_content=' meowðŸ˜»ðŸ˜»', metadata={'line_number': 2, 'source': './meow.txt'})]
```

## Working with Files[â€‹](#working-with-files "Direct link to Working with Files")

Many document loaders involve parsing files. The difference between such loaders usually stems from how the file is parsed, rather than how the file is loaded. For example, you can use `open` to read the binary content of either a PDF or a markdown file, but you need different parsing logic to convert that binary data into text.

As a result, it can be helpful to decouple the parsing logic from the loading logic, which makes it easier to re-use a given parser regardless of how the data was loaded.

### BaseBlobParser[â€‹](#baseblobparser "Direct link to BaseBlobParser")

A `BaseBlobParser` is an interface that accepts a `blob` and outputs a list of `Document` objects. A `blob` is a representation of data that lives either in memory or in a file. LangChain python has a `Blob` primitive which is inspired by the [Blob WebAPI spec](https://developer.mozilla.org/en-US/docs/Web/API/Blob).

```python
from langchain_core.document_loaders import BaseBlobParser, Blob


class MyParser(BaseBlobParser):
    """A simple parser that creates a document from each line."""

    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
        """Parse a blob into a document line by line."""
        line_number = 0
        with blob.as_bytes_io() as f:
            for line in f:
                line_number += 1
                yield Document(
                    page_content=line,
                    metadata={"line_number": line_number, "source": blob.source},
                )
```

**API Reference:**[BaseBlobParser](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseBlobParser.html) | [Blob](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Blob.html)

```python
blob = Blob.from_path("./meow.txt")
parser = MyParser()
```

```python
list(parser.lazy_parse(blob))
```

```output
[Document(page_content='meow meowðŸ± \n', metadata={'line_number': 1, 'source': './meow.txt'}),
 Document(page_content=' meow meowðŸ± \n', metadata={'line_number': 2, 'source': './meow.txt'}),
 Document(page_content=' meowðŸ˜»ðŸ˜»', metadata={'line_number': 3, 'source': './meow.txt'})]
```

Using the **blob** API also allows one to load content directly from memory without having to read it from a file!

```python
blob = Blob(data=b"some data from memory\nmeow")
list(parser.lazy_parse(blob))
```

```output
[Document(page_content='some data from memory\n', metadata={'line_number': 1, 'source': None}),
 Document(page_content='meow', metadata={'line_number': 2, 'source': None})]
```

### Blob[â€‹](#blob "Direct link to Blob")

Let's take a quick look through some of the Blob API.

```python
blob = Blob.from_path("./meow.txt", metadata={"foo": "bar"})
```

```python
blob.encoding
```

```output
'utf-8'
```

```python
blob.as_bytes()
```

```output
b'meow meow\xf0\x9f\x90\xb1 \n meow meow\xf0\x9f\x90\xb1 \n meow\xf0\x9f\x98\xbb\xf0\x9f\x98\xbb'
```

```python
blob.as_string()
```

```output
'meow meowðŸ± \n meow meowðŸ± \n meowðŸ˜»ðŸ˜»'
```

```python
blob.as_bytes_io()
```

```output
<contextlib._GeneratorContextManager at 0x743f34324450>
```

```python
blob.metadata
```

```output
{'foo': 'bar'}
```

```python
blob.source
```

```output
'./meow.txt'
```

### Blob Loaders[â€‹](#blob-loaders "Direct link to Blob Loaders")

While a parser encapsulates the logic needed to parse binary data into documents, *blob loaders* encapsulate the logic that's necessary to load blobs from a given storage location.

At the moment, `LangChain` only supports `FileSystemBlobLoader`.

You can use the `FileSystemBlobLoader` to load blobs and then use the parser to parse them.

```python
from langchain_community.document_loaders.blob_loaders import FileSystemBlobLoader

blob_loader = FileSystemBlobLoader(path=".", glob="*.mdx", show_progress=True)
```

**API Reference:**[FileSystemBlobLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.blob_loaders.file_system.FileSystemBlobLoader.html)

```python
parser = MyParser()
for blob in blob_loader.yield_blobs():
    for doc in parser.lazy_parse(blob):
        print(doc)
        break
```

```output
  0%|          | 0/8 [00:00<?, ?it/s]
```

```output
page_content='# Microsoft Office\n' metadata={'line_number': 1, 'source': 'office_file.mdx'}
page_content='# Markdown\n' metadata={'line_number': 1, 'source': 'markdown.mdx'}
page_content='# JSON\n' metadata={'line_number': 1, 'source': 'json.mdx'}
page_content='---\n' metadata={'line_number': 1, 'source': 'pdf.mdx'}
page_content='---\n' metadata={'line_number': 1, 'source': 'index.mdx'}
page_content='# File Directory\n' metadata={'line_number': 1, 'source': 'file_directory.mdx'}
page_content='# CSV\n' metadata={'line_number': 1, 'source': 'csv.mdx'}
page_content='# HTML\n' metadata={'line_number': 1, 'source': 'html.mdx'}
```

### Generic Loader[â€‹](#generic-loader "Direct link to Generic Loader")

LangChain has a `GenericLoader` abstraction which composes a `BlobLoader` with a `BaseBlobParser`.

`GenericLoader` is meant to provide standardized classmethods that make it easy to use existing `BlobLoader` implementations. At the moment, only the `FileSystemBlobLoader` is supported.

```python
from langchain_community.document_loaders.generic import GenericLoader

loader = GenericLoader.from_filesystem(
    path=".", glob="*.mdx", show_progress=True, parser=MyParser()
)

for idx, doc in enumerate(loader.lazy_load()):
    if idx < 5:
        print(doc)

print("... output truncated for demo purposes")
```

**API Reference:**[GenericLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.generic.GenericLoader.html)

```output
  0%|          | 0/8 [00:00<?, ?it/s]
```

```output
page_content='# Microsoft Office\n' metadata={'line_number': 1, 'source': 'office_file.mdx'}
page_content='\n' metadata={'line_number': 2, 'source': 'office_file.mdx'}
page_content='>[The Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\n' metadata={'line_number': 3, 'source': 'office_file.mdx'}
page_content='\n' metadata={'line_number': 4, 'source': 'office_file.mdx'}
page_content='This covers how to load commonly used file formats including `DOCX`, `XLSX` and `PPTX` documents into a document format that we can use downstream.\n' metadata={'line_number': 5, 'source': 'office_file.mdx'}
... output truncated for demo purposes
```

#### Custom Generic Loader[â€‹](#custom-generic-loader "Direct link to Custom Generic Loader")

If you really like creating classes, you can sub-class and create a class to encapsulate the logic together.

You can sub-class from this class to load content using an existing loader.

```python
from typing import Any


class MyCustomLoader(GenericLoader):
    @staticmethod
    def get_parser(**kwargs: Any) -> BaseBlobParser:
        """Override this method to associate a default parser with the class."""
        return MyParser()
```

```python
loader = MyCustomLoader.from_filesystem(path=".", glob="*.mdx", show_progress=True)

for idx, doc in enumerate(loader.lazy_load()):
    if idx < 5:
        print(doc)

print("... output truncated for demo purposes")
```

```output
  0%|          | 0/8 [00:00<?, ?it/s]
```

```output
page_content='# Microsoft Office\n' metadata={'line_number': 1, 'source': 'office_file.mdx'}
page_content='\n' metadata={'line_number': 2, 'source': 'office_file.mdx'}
page_content='>[The Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\n' metadata={'line_number': 3, 'source': 'office_file.mdx'}
page_content='\n' metadata={'line_number': 4, 'source': 'office_file.mdx'}
page_content='This covers how to load commonly used file formats including `DOCX`, `XLSX` and `PPTX` documents into a document format that we can use downstream.\n' metadata={'line_number': 5, 'source': 'office_file.mdx'}
... output truncated for demo purposes
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/document_loader_custom.ipynb)

* * *


- [Overview](#overview)
- [Standard Document Loader](#standard-document-loader)
  
  - [Interface](#interface)
  - [Implementation](#implementation)
  - [Test ðŸ§ª](#test-)
- [Working with Files](#working-with-files)
  
  - [BaseBlobParser](#baseblobparser)
  - [Blob](#blob)
  - [Blob Loaders](#blob-loaders)
  - [Generic Loader](#generic-loader)








the AI Agent Conference by langchain

13-14 may 2025

San Francisco

![Interrupt logo](https://cdn.prod.website-files.com/676ff521bb2339da86ae92b0/679b67454e8fc78026b7e490_Logo_new-1.svg)

Exploring at the Edge of AI Agents

Interrupt is *the* conference for AI agent builders. Whether you're a

- Software Engineer
- ML Engineer
- Data Scientist
- Engineering Leader
- Researcher
- Product Builder

youâ€™ll learn practical know-how and jumpstart your creativity to build impactful AIÂ applications.

Mark your calendar

May 13-14, 2025  
The Midway  
San Francisco

see who's speaking

[view agenda](/agenda)

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae92b0/67a0b814116fb2658607e2ff_Sphere-v4-1400.webp)

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae92b0/6798ea284cc03f3e7dffb85a_title.svg)

![Harrison Chase](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/6798ec7e3320cbd832b86a79_679267781687e0db0354214c_67926245569a4f501289939c_1.webp)

### Harrison Chase

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/6798ec7e3320cbd832b86a76_679267781687e0db03542142_6792625706c45c2ad10023fd_langchain.svg)

Co-Founder &amp; CEO at LangChain

![Adam D'Angelo](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/6798ec7d11a1f6dc1e780641_679267767c06c79bd5fd14cd_6792630a1687e0db034eca63_3.jpeg)

### Adam D'Angelo

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/6798ec7e11a1f6dc1e780646_679267767c06c79bd5fd14ca_67926310eed2ca4b5110c6dd_quora.svg)

Co-Founder &amp; CEO at Quora

![Andrew Ng](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/679c0e82261aa243fd44f9af_andrew.jpg)

### Andrew Ng

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/679c1c04238f198321c58f37_depelearning2.png)

Founder at DeepLearning.AI &amp; Co-Founder at Coursera

![Michele Catasta](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/6798ec7e10ce798347297171_67926776dc03be65bff0f21a_679262bca2044eba70cf201b_2.jpeg)

### Michele Catasta

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/6798ec7e10ce798347297168_67926776dc03be65bff0f20a_679262cf70ead57174bd69a8_replit.svg)

President at Replit

![Russell Kaplan](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67a930effae2ab371f0fa318_russell2.png)

### Russell Kaplan

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/679c1d1b6bc1c2287ece3dbc_cognition.png)

President at Cognition

![Carlos Pereira](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df4daf0490a55a5fd9d667_carlos%20bw.jpg)

### Carlos Pereira

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d42649ba94846ccbacd864_Cisco_CustomerExperience_Stack_forDark%201.svg)

Fellow &amp; Chief Architect at Cisco

![Ben Kus](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67f451366ca5182de3fe9fa1_ben%20kus%20bw.png)

### Ben Kus

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d426446a82abeeb4e4785d_box_logo.svg.svg)

CTO at Box

![Ben Liebald](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67c76c2293eaf964535db9cf_ben.png)

### Ben Liebald

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67c76c2d3a8344e0c4a962a6_harveytransp.png)

VP of Engineering at Harvey

![Eno Reyes](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/679c11786f3789cd98383509_Eno_Reyes%20%281%29%20copy.jpg)

### Eno Reyes

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/679d429a26fe65a206be41cf_factory.png)

Founder &amp; CTO at Factory

![Shreya Shankar](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/679ffbfb248ec737003afeb9_shreya.jpg)

### Shreya Shankar

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/679ffc8ddb1725ac54314fdc_berkeley.png)

AI Researcher at UC Berkeley

![Sourabh Shirhatti](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67b77f67260d6d30b19fd878_sourabh.jpg)

### Sourabh Shirhatti

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67b7818fccafc7dd21da4ea2_uberlogo.png)

Lead Product Manager, Developer Platform at Uber

![Matas Rastenis](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67b782203e13baef02916810_matas2.jpg)

### Matas Rastenis

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67b7818fccafc7dd21da4ea2_uberlogo.png)

Senior Software Engineer, Developer Platform at Uber

![Pedro Vicente-Valdez](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d20201525cd1a12074a5f9_pedro.png)

### Pedro Vicente-Valdez

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d202ac181a79513823bfa7_blackrock.png)

Principal AI Engineer at BlackRock

![Brennan Rosales](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d202d7ef836d7859d0d429_brennan.png)

### Brennan Rosales

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d202ac181a79513823bfa7_blackrock.png)

AI Engineering Lead at BlackRock

![Assaf Elovic](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df45c1f04087548ae53320_Assaf%20Elovic%20Headshot.jpg)

### Assaf Elovic

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df47b160541df37b4f8bbf_monday.png)

Head of AI at Monday

![Sayantan Mukhopadhyay](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df49527152b7b46ae204f1_tan%20bw.jpg)

### Sayantan Mukhopadhyay

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df498d22d52b417bd97d90_nubank.png)

General Manager at Nubank

![Zheng Xue](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df4c2b291803a3b112ac2c_Zheng%20b%26w.png)

### Zheng Xue

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df4c6db4a355642ab0f459_jpm.png)

Executive Director at JP Morgan Chase

![David Odomirok](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df4c84f04087548ae886c4_david%20o%20bw.jpg)

### David Odomirok

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df4c6db4a355642ab0f459_jpm.png)

Executive Director at JP Morgan Chase

![Sherwood Callaway](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/679c2235934bba3b70ca60b5_sherwood.jpg)

### Sherwood Callaway

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/679c21bd6bf9be51f01925b3_11x.png)

Engineering Lead at 11x

![Keith Fearon](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/679c29e8cfabaf8dd0f6c8f4_keith.jpg)

### Keith Fearon

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/679c21bd6bf9be51f01925b3_11x.png)

Head of Growth at 11x

![Connor Heggie](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df60ee3cc2ac460a0ad7c1_connor%20bw.jpg)

### Connor Heggie

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df61ae60541df37b5d484f_unify.png)

Co-founder &amp; CTO at Unify

![Kunal Rai](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df631631ad3adc06ecdc56_kunal%20bw.jpg)

### Kunal Rai

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df61ae60541df37b5d484f_unify.png)

Software Engineer at Unify

![David Tag](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df63d8686c0fa6488183cc_david%20bw.jpg)

### David Tag

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67df6442675c714ef86c0453_linkedin.png)

Staff Software Engineer at Linkedin

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae92b0/679a48a43ed0bfb8b06a2f08_tickets.svg)

### Tickets are officially sold out!

If you're a LangChain Enterprise customer and still hoping to attend, reach out to your representative.

If you have an access code, you may enter it [here](/secret-tickets).

Want to be the first to get the recordings from Interrupt and hear when tickets for the next Interrupt go on sale? Â Fill out the form.

SUBMIT

Thank you!  
Your submission has been received!

Oops! Something went wrong while submitting the form.

Becomeâ€¨a sponsor

Interested in sponsoring?  
Reach out to us atâ€¨

[sponsorships@langchain.dev](mailto:sponsorships@langchain.dev?subject=Interrupt%20sponsor)

Got questions?

[visit the FAQ](/faq)

presenting sponsor

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d42649ba94846ccbacd864_Cisco_CustomerExperience_Stack_forDark%201.svg)](https://www.cisco.com/site/us/en/services/modern-data-center/index.html?CCID=cc005911&DTID=eivtotr001480&OID=srwsas032775)

Booth sponsors

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d4263ea8952d765fd0d160_Arcade_Logo_Black%201.svg)](http://arcade.dev/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d426446a82abeeb4e4785d_box_logo.svg.svg)](https://www.box.com/home)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d42631d6ed82e473ca1c3a_cloudflare_logo.svg.svg)](https://www.cloudflare.com/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d4262211220f7da05fe7d0_coinbase_logo.svg.svg)](https://www.coinbase.com/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d42614fab660a1a589075c_copilotkit.svg)](https://www.copilotkit.ai/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d425b9952e85ba7f99b772_datastax.svg)](https://www.datastax.com/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d425fd7c599270fb6486fd_daytona.svg)](http://daytona.io/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d425f8d623902b4efd9f21_jetbrains_logo.svg.svg)](https://www.jetbrains.com/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d4261a9327ccd9cf477538_kong.svg)](https://konghq.com/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d42627c7c9e886d6088908_mongodb_logo.svg.svg)](https://www.mongodb.com/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d42675492de8db7af71a7d_Simplification.svg)](https://www.paymanai.com/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d426371d93414ba0a8bc92_redis.svg)](https://redis.io/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d4260e81a078cb7dab9f85_stytch.svg)](https://stytch.com/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d42607c77645f4d165b6e2_tavily.svg)](https://tavily.com/)

[![](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67eb953d1aa6a8772d458f30_uipath_logo.svg)](https://www.uipath.com/)

Event sponsors

[![DEShawCo](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d425f2f43558b9950552cb_deshaw.svg)](https://www.deshaw.com/)

[![Focused](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d425c06eda7a1fa45074a0_focused.svg)](http://focused.io/)

[![Hawken](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67e50d3f23542d75aa9b8afa_hawkenai%202-white.svg)](https://hawken.ai)

[![Neo4j](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d425eb1b8b11d8ff066a96_neo4.svg)](https://neo4j.com/)

[![Pinecone](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67d42cdc720b0eb481571745_pinecone.svg)](https://www.pinecone.io/)

[![Qubika](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67e64a48846657b518b8205f_Qubika_Logotype_White_Rgb.svg)](https://qubika.com/)

[![Riza](https://cdn.prod.website-files.com/676ff521bb2339da86ae9326/67e64a1b4f78d0dbc9d50908_Frame%2099724.svg)](https://riza.io/)

the AI Agent Conference by langchain

13-14 may 2025

San Francisco

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae92b0/679b67454e8fc78026b7e490_Logo_new-1.svg)

[get Tickets](/#tickets)

[Event terms](https://www.langchain.com/event-terms)

![](https://cdn.prod.website-files.com/676ff521bb2339da86ae92b0/6798ea284cc03f3e7dffb860_sphere.webp)![](https://cdn.prod.website-files.com/676ff521bb2339da86ae92b0/6798ea284cc03f3e7dffb874_gradients-shadow.webp)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/lcel_cheatsheet.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/lcel_cheatsheet.ipynb)

# LangChain Expression Language Cheatsheet

This is a quick reference for all the most important [LCEL](/docs/concepts/lcel/) primitives. For more advanced usage see the [LCEL how-to guides](/docs/how_to/#langchain-expression-language-lcel) and the [full API reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html).

### Invoke a runnable[â€‹](#invoke-a-runnable "Direct link to Invoke a runnable")

#### [Runnable.invoke()](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.invoke) / [Runnable.ainvoke()](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.ainvoke)[â€‹](#runnableinvoke--runnableainvoke "Direct link to runnableinvoke--runnableainvoke")

```python
from langchain_core.runnables import RunnableLambda

runnable = RunnableLambda(lambda x: str(x))
runnable.invoke(5)

# Async variant:
# await runnable.ainvoke(5)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```output
'5'
```

### Batch a runnable[â€‹](#batch-a-runnable "Direct link to Batch a runnable")

#### [Runnable.batch()](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.batch) / [Runnable.abatch()](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.abatch)[â€‹](#runnablebatch--runnableabatch "Direct link to runnablebatch--runnableabatch")

```python
from langchain_core.runnables import RunnableLambda

runnable = RunnableLambda(lambda x: str(x))
runnable.batch([7, 8, 9])

# Async variant:
# await runnable.abatch([7, 8, 9])
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```output
['7', '8', '9']
```

### Stream a runnable[â€‹](#stream-a-runnable "Direct link to Stream a runnable")

#### [Runnable.stream()](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) / [Runnable.astream()](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream)[â€‹](#runnablestream--runnableastream "Direct link to runnablestream--runnableastream")

```python
from langchain_core.runnables import RunnableLambda


def func(x):
    for y in x:
        yield str(y)


runnable = RunnableLambda(func)

for chunk in runnable.stream(range(5)):
    print(chunk)

# Async variant:
# async for chunk in await runnable.astream(range(5)):
#     print(chunk)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```output
0
1
2
3
4
```

### Compose runnables[â€‹](#compose-runnables "Direct link to Compose runnables")

#### Pipe operator `|`[â€‹](#pipe-operator- "Direct link to pipe-operator-")

```python
from langchain_core.runnables import RunnableLambda

runnable1 = RunnableLambda(lambda x: {"foo": x})
runnable2 = RunnableLambda(lambda x: [x] * 2)

chain = runnable1 | runnable2

chain.invoke(2)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```output
[{'foo': 2}, {'foo': 2}]
```

### Invoke runnables in parallel[â€‹](#invoke-runnables-in-parallel "Direct link to Invoke runnables in parallel")

#### [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)[â€‹](#runnableparallel "Direct link to runnableparallel")

```python
from langchain_core.runnables import RunnableLambda, RunnableParallel

runnable1 = RunnableLambda(lambda x: {"foo": x})
runnable2 = RunnableLambda(lambda x: [x] * 2)

chain = RunnableParallel(first=runnable1, second=runnable2)

chain.invoke(2)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)

```output
{'first': {'foo': 2}, 'second': [2, 2]}
```

### Turn any function into a runnable[â€‹](#turn-any-function-into-a-runnable "Direct link to Turn any function into a runnable")

#### [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)[â€‹](#runnablelambda "Direct link to runnablelambda")

```python
from langchain_core.runnables import RunnableLambda


def func(x):
    return x + 5


runnable = RunnableLambda(func)
runnable.invoke(2)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```output
7
```

### Merge input and output dicts[â€‹](#merge-input-and-output-dicts "Direct link to Merge input and output dicts")

#### [RunnablePassthrough.assign](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)[â€‹](#runnablepassthroughassign "Direct link to runnablepassthroughassign")

```python
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

runnable1 = RunnableLambda(lambda x: x["foo"] + 7)

chain = RunnablePassthrough.assign(bar=runnable1)

chain.invoke({"foo": 10})
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)

```output
{'foo': 10, 'bar': 17}
```

### Include input dict in output dict[â€‹](#include-input-dict-in-output-dict "Direct link to Include input dict in output dict")

#### [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)[â€‹](#runnablepassthrough "Direct link to runnablepassthrough")

```python
from langchain_core.runnables import (
    RunnableLambda,
    RunnableParallel,
    RunnablePassthrough,
)

runnable1 = RunnableLambda(lambda x: x["foo"] + 7)

chain = RunnableParallel(bar=runnable1, baz=RunnablePassthrough())

chain.invoke({"foo": 10})
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)

```output
{'bar': 17, 'baz': {'foo': 10}}
```

### Add default invocation args[â€‹](#add-default-invocation-args "Direct link to Add default invocation args")

#### [Runnable.bind](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.bind)[â€‹](#runnablebind "Direct link to runnablebind")

```python
from typing import Optional

from langchain_core.runnables import RunnableLambda


def func(main_arg: dict, other_arg: Optional[str] = None) -> dict:
    if other_arg:
        return {**main_arg, **{"foo": other_arg}}
    return main_arg


runnable1 = RunnableLambda(func)
bound_runnable1 = runnable1.bind(other_arg="bye")

bound_runnable1.invoke({"bar": "hello"})
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```output
{'bar': 'hello', 'foo': 'bye'}
```

### Add fallbacks[â€‹](#add-fallbacks "Direct link to Add fallbacks")

#### [Runnable.with\_fallbacks](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_fallbacks)[â€‹](#runnablewith_fallbacks "Direct link to runnablewith_fallbacks")

```python
from langchain_core.runnables import RunnableLambda

runnable1 = RunnableLambda(lambda x: x + "foo")
runnable2 = RunnableLambda(lambda x: str(x) + "foo")

chain = runnable1.with_fallbacks([runnable2])

chain.invoke(5)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```output
'5foo'
```

### Add retries[â€‹](#add-retries "Direct link to Add retries")

#### [Runnable.with\_retry](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_retry)[â€‹](#runnablewith_retry "Direct link to runnablewith_retry")

```python
from langchain_core.runnables import RunnableLambda

counter = -1


def func(x):
    global counter
    counter += 1
    print(f"attempt with {counter=}")
    return x / counter


chain = RunnableLambda(func).with_retry(stop_after_attempt=2)

chain.invoke(2)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```output
attempt with counter=0
attempt with counter=1
```

```output
2.0
```

### Configure runnable execution[â€‹](#configure-runnable-execution "Direct link to Configure runnable execution")

#### [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html)[â€‹](#runnableconfig "Direct link to runnableconfig")

```python
from langchain_core.runnables import RunnableLambda, RunnableParallel

runnable1 = RunnableLambda(lambda x: {"foo": x})
runnable2 = RunnableLambda(lambda x: [x] * 2)
runnable3 = RunnableLambda(lambda x: str(x))

chain = RunnableParallel(first=runnable1, second=runnable2, third=runnable3)

chain.invoke(7, config={"max_concurrency": 2})
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)

```output
{'first': {'foo': 7}, 'second': [7, 7], 'third': '7'}
```

### Add default config to runnable[â€‹](#add-default-config-to-runnable "Direct link to Add default config to runnable")

#### [Runnable.with\_config](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_config)[â€‹](#runnablewith_config "Direct link to runnablewith_config")

```python
from langchain_core.runnables import RunnableLambda, RunnableParallel

runnable1 = RunnableLambda(lambda x: {"foo": x})
runnable2 = RunnableLambda(lambda x: [x] * 2)
runnable3 = RunnableLambda(lambda x: str(x))

chain = RunnableParallel(first=runnable1, second=runnable2, third=runnable3)
configured_chain = chain.with_config(max_concurrency=2)

chain.invoke(7)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)

```output
{'first': {'foo': 7}, 'second': [7, 7], 'third': '7'}
```

### Make runnable attributes configurable[â€‹](#make-runnable-attributes-configurable "Direct link to Make runnable attributes configurable")

#### [Runnable.with\_configurable\_fields](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable.configurable_fields)[â€‹](#runnablewith_configurable_fields "Direct link to runnablewith_configurable_fields")

```python
from typing import Any, Optional

from langchain_core.runnables import (
    ConfigurableField,
    RunnableConfig,
    RunnableSerializable,
)


class FooRunnable(RunnableSerializable[dict, dict]):
    output_key: str

    def invoke(
        self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any
    ) -> list:
        return self._call_with_config(self.subtract_seven, input, config, **kwargs)

    def subtract_seven(self, input: dict) -> dict:
        return {self.output_key: input["foo"] - 7}


runnable1 = FooRunnable(output_key="bar")
configurable_runnable1 = runnable1.configurable_fields(
    output_key=ConfigurableField(id="output_key")
)

configurable_runnable1.invoke(
    {"foo": 10}, config={"configurable": {"output_key": "not bar"}}
)
```

**API Reference:**[ConfigurableField](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [RunnableSerializable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSerializable.html)

```output
{'not bar': 3}
```

```python
configurable_runnable1.invoke({"foo": 10})
```

```output
{'bar': 3}
```

### Make chain components configurable[â€‹](#make-chain-components-configurable "Direct link to Make chain components configurable")

#### [Runnable.with\_configurable\_alternatives](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSerializable.html#langchain_core.runnables.base.RunnableSerializable.configurable_alternatives)[â€‹](#runnablewith_configurable_alternatives "Direct link to runnablewith_configurable_alternatives")

```python
from typing import Any, Optional

from langchain_core.runnables import RunnableConfig, RunnableLambda, RunnableParallel


class ListRunnable(RunnableSerializable[Any, list]):
    def invoke(
        self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any
    ) -> list:
        return self._call_with_config(self.listify, input, config, **kwargs)

    def listify(self, input: Any) -> list:
        return [input]


class StrRunnable(RunnableSerializable[Any, str]):
    def invoke(
        self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any
    ) -> list:
        return self._call_with_config(self.strify, input, config, **kwargs)

    def strify(self, input: Any) -> str:
        return str(input)


runnable1 = RunnableLambda(lambda x: {"foo": x})

configurable_runnable = ListRunnable().configurable_alternatives(
    ConfigurableField(id="second_step"), default_key="list", string=StrRunnable()
)
chain = runnable1 | configurable_runnable

chain.invoke(7, config={"configurable": {"second_step": "string"}})
```

**API Reference:**[RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)

```output
"{'foo': 7}"
```

```python
chain.invoke(7)
```

```output
[{'foo': 7}]
```

### Build a chain dynamically based on input[â€‹](#build-a-chain-dynamically-based-on-input "Direct link to Build a chain dynamically based on input")

```python
from langchain_core.runnables import RunnableLambda, RunnableParallel

runnable1 = RunnableLambda(lambda x: {"foo": x})
runnable2 = RunnableLambda(lambda x: [x] * 2)

chain = RunnableLambda(lambda x: runnable1 if x > 6 else runnable2)

chain.invoke(7)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)

```output
{'foo': 7}
```

```python
chain.invoke(5)
```

```output
[5, 5]
```

### Generate a stream of events[â€‹](#generate-a-stream-of-events "Direct link to Generate a stream of events")

#### [Runnable.astream\_events](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream_events)[â€‹](#runnableastream_events "Direct link to runnableastream_events")

```python
# | echo: false

import nest_asyncio

nest_asyncio.apply()
```

```python
from langchain_core.runnables import RunnableLambda, RunnableParallel

runnable1 = RunnableLambda(lambda x: {"foo": x}, name="first")


async def func(x):
    for _ in range(5):
        yield x


runnable2 = RunnableLambda(func, name="second")

chain = runnable1 | runnable2

async for event in chain.astream_events("bar", version="v2"):
    print(f"event={event['event']} | name={event['name']} | data={event['data']}")
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)

```output
event=on_chain_start | name=RunnableSequence | data={'input': 'bar'}
event=on_chain_start | name=first | data={}
event=on_chain_stream | name=first | data={'chunk': {'foo': 'bar'}}
event=on_chain_start | name=second | data={}
event=on_chain_end | name=first | data={'output': {'foo': 'bar'}, 'input': 'bar'}
event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}
event=on_chain_end | name=second | data={'output': {'foo': 'bar'}, 'input': {'foo': 'bar'}}
event=on_chain_end | name=RunnableSequence | data={'output': {'foo': 'bar'}}
```

### Yield batched outputs as they complete[â€‹](#yield-batched-outputs-as-they-complete "Direct link to Yield batched outputs as they complete")

#### [Runnable.batch\_as\_completed](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.batch_as_completed) / [Runnable.abatch\_as\_completed](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.abatch_as_completed)[â€‹](#runnablebatch_as_completed--runnableabatch_as_completed "Direct link to runnablebatch_as_completed--runnableabatch_as_completed")

```python
import time

from langchain_core.runnables import RunnableLambda, RunnableParallel

runnable1 = RunnableLambda(lambda x: time.sleep(x) or print(f"slept {x}"))

for idx, result in runnable1.batch_as_completed([5, 1]):
    print(idx, result)

# Async variant:
# async for idx, result in runnable1.abatch_as_completed([5, 1]):
#     print(idx, result)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)

```output
slept 1
1 None
slept 5
0 None
```

### Return subset of output dict[â€‹](#return-subset-of-output-dict "Direct link to Return subset of output dict")

#### [Runnable.pick](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.pick)[â€‹](#runnablepick "Direct link to runnablepick")

```python
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

runnable1 = RunnableLambda(lambda x: x["baz"] + 5)
chain = RunnablePassthrough.assign(foo=runnable1).pick(["foo", "bar"])

chain.invoke({"bar": "hi", "baz": 2})
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)

```output
{'foo': 7, 'bar': 'hi'}
```

### Declaratively make a batched version of a runnable[â€‹](#declaratively-make-a-batched-version-of-a-runnable "Direct link to Declaratively make a batched version of a runnable")

#### [Runnable.map](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.map)[â€‹](#runnablemap "Direct link to runnablemap")

```python
from langchain_core.runnables import RunnableLambda

runnable1 = RunnableLambda(lambda x: list(range(x)))
runnable2 = RunnableLambda(lambda x: x + 5)

chain = runnable1 | runnable2.map()

chain.invoke(3)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```output
[5, 6, 7]
```

### Get a graph representation of a runnable[â€‹](#get-a-graph-representation-of-a-runnable "Direct link to Get a graph representation of a runnable")

#### [Runnable.get\_graph](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.get_graph)[â€‹](#runnableget_graph "Direct link to runnableget_graph")

```python
from langchain_core.runnables import RunnableLambda, RunnableParallel

runnable1 = RunnableLambda(lambda x: {"foo": x})
runnable2 = RunnableLambda(lambda x: [x] * 2)
runnable3 = RunnableLambda(lambda x: str(x))

chain = runnable1 | RunnableParallel(second=runnable2, third=runnable3)

chain.get_graph().print_ascii()
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)

```output
                             +-------------+                              
                             | LambdaInput |                              
                             +-------------+                              
                                    *                                     
                                    *                                     
                                    *                                     
                    +------------------------------+                      
                    | Lambda(lambda x: {'foo': x}) |                      
                    +------------------------------+                      
                                    *                                     
                                    *                                     
                                    *                                     
                     +-----------------------------+                      
                     | Parallel<second,third>Input |                      
                     +-----------------------------+                      
                        ****                  ***                         
                    ****                         ****                     
                  **                                 **                   
+---------------------------+               +--------------------------+  
| Lambda(lambda x: [x] * 2) |               | Lambda(lambda x: str(x)) |  
+---------------------------+               +--------------------------+  
                        ****                  ***                         
                            ****          ****                            
                                **      **                                
                    +------------------------------+                      
                    | Parallel<second,third>Output |                      
                    +------------------------------+
```

### Get all prompts in a chain[â€‹](#get-all-prompts-in-a-chain "Direct link to Get all prompts in a chain")

#### [Runnable.get\_prompts](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.get_prompts)[â€‹](#runnableget_prompts "Direct link to runnableget_prompts")

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda

prompt1 = ChatPromptTemplate.from_messages(
    [("system", "good ai"), ("human", "{input}")]
)
prompt2 = ChatPromptTemplate.from_messages(
    [
        ("system", "really good ai"),
        ("human", "{input}"),
        ("ai", "{ai_output}"),
        ("human", "{input2}"),
    ]
)
fake_llm = RunnableLambda(lambda prompt: "i am good ai")
chain = prompt1.assign(ai_output=fake_llm) | prompt2 | fake_llm

for i, prompt in enumerate(chain.get_prompts()):
    print(f"**prompt {i=}**\n")
    print(prompt.pretty_repr())
    print("\n" * 3)
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```output
**prompt i=0**

================================ System Message ================================

good ai

================================ Human Message =================================

{input}




**prompt i=1**

================================ System Message ================================

really good ai

================================ Human Message =================================

{input}

================================== AI Message ==================================

{ai_output}

================================ Human Message =================================

{input2}
```

### Add lifecycle listeners[â€‹](#add-lifecycle-listeners "Direct link to Add lifecycle listeners")

#### [Runnable.with\_listeners](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_listeners)[â€‹](#runnablewith_listeners "Direct link to runnablewith_listeners")

```python
import time

from langchain_core.runnables import RunnableLambda
from langchain_core.tracers.schemas import Run


def on_start(run_obj: Run):
    print("start_time:", run_obj.start_time)


def on_end(run_obj: Run):
    print("end_time:", run_obj.end_time)


runnable1 = RunnableLambda(lambda x: time.sleep(x))
chain = runnable1.with_listeners(on_start=on_start, on_end=on_end)
chain.invoke(2)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [Run](https://python.langchain.com/api_reference/langsmith/run_trees/langsmith.run_trees.Run.html)

```output
start_time: 2024-05-17 23:04:00.951065+00:00
end_time: 2024-05-17 23:04:02.958765+00:00
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/lcel_cheatsheet.ipynb)

* * *


- [Invoke a runnable](#invoke-a-runnable)
- [Batch a runnable](#batch-a-runnable)
- [Stream a runnable](#stream-a-runnable)
- [Compose runnables](#compose-runnables)
- [Invoke runnables in parallel](#invoke-runnables-in-parallel)
- [Turn any function into a runnable](#turn-any-function-into-a-runnable)
- [Merge input and output dicts](#merge-input-and-output-dicts)
- [Include input dict in output dict](#include-input-dict-in-output-dict)
- [Add default invocation args](#add-default-invocation-args)
- [Add fallbacks](#add-fallbacks)
- [Add retries](#add-retries)
- [Configure runnable execution](#configure-runnable-execution)
- [Add default config to runnable](#add-default-config-to-runnable)
- [Make runnable attributes configurable](#make-runnable-attributes-configurable)
- [Make chain components configurable](#make-chain-components-configurable)
- [Build a chain dynamically based on input](#build-a-chain-dynamically-based-on-input)
- [Generate a stream of events](#generate-a-stream-of-events)
- [Yield batched outputs as they complete](#yield-batched-outputs-as-they-complete)
- [Return subset of output dict](#return-subset-of-output-dict)
- [Declaratively make a batched version of a runnable](#declaratively-make-a-batched-version-of-a-runnable)
- [Get a graph representation of a runnable](#get-a-graph-representation-of-a-runnable)
- [Get all prompts in a chain](#get-all-prompts-in-a-chain)
- [Add lifecycle listeners](#add-lifecycle-listeners)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_markdown.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_markdown.ipynb)

# How to load Markdown

[Markdown](https://en.wikipedia.org/wiki/Markdown) is a lightweight markup language for creating formatted text using a plain-text editor.

Here we cover how to load `Markdown` documents into LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects that we can use downstream.

We will cover:

- Basic usage;
- Parsing of Markdown into elements such as titles, list items, and text.

LangChain implements an [UnstructuredMarkdownLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader.html) object which requires the [Unstructured](https://docs.unstructured.io/welcome/) package. First we install it:

```python
%pip install "unstructured[md]" nltk
```

Basic usage will ingest a Markdown file to a single document. Here we demonstrate on LangChain's readme:

```python
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_core.documents import Document

markdown_path = "../../../README.md"
loader = UnstructuredMarkdownLoader(markdown_path)

data = loader.load()
assert len(data) == 1
assert isinstance(data[0], Document)
readme_content = data[0].page_content
print(readme_content[:250])
```

**API Reference:**[UnstructuredMarkdownLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

```output
ðŸ¦œï¸ðŸ”— LangChain

âš¡ Build context-aware reasoning applications âš¡

Looking for the JS/TS library? Check out LangChain.js.

To help you ship LangChain apps to production faster, check out LangSmith. 
LangSmith is a unified developer platform for building,
```

## Retain Elements[â€‹](#retain-elements "Direct link to Retain Elements")

Under the hood, Unstructured creates different "elements" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying `mode="elements"`.

```python
loader = UnstructuredMarkdownLoader(markdown_path, mode="elements")

data = loader.load()
print(f"Number of documents: {len(data)}\n")

for document in data[:2]:
    print(f"{document}\n")
```

```output
Number of documents: 66

page_content='ðŸ¦œï¸ðŸ”— LangChain' metadata={'source': '../../../README.md', 'category_depth': 0, 'last_modified': '2024-06-28T15:20:01', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../..', 'filename': 'README.md', 'category': 'Title'}

page_content='âš¡ Build context-aware reasoning applications âš¡' metadata={'source': '../../../README.md', 'last_modified': '2024-06-28T15:20:01', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '../../..', 'filename': 'README.md', 'category': 'NarrativeText'}
```

Note that in this case we recover three distinct element types:

```python
print(set(document.metadata["category"] for document in data))
```

```output
{'ListItem', 'NarrativeText', 'Title'}
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/document_loader_markdown.ipynb)

* * *


- [Retain Elements](#retain-elements)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/serialization.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/serialization.ipynb)

# How to save and load LangChain objects

LangChain classes implement standard methods for serialization. Serializing LangChain objects using these methods confer some advantages:

- Secrets, such as API keys, are separated from other parameters and can be loaded back to the object on de-serialization;
- De-serialization is kept compatible across package versions, so objects that were serialized with one version of LangChain can be properly de-serialized with another.

To save and load LangChain objects using this system, use the `dumpd`, `dumps`, `load`, and `loads` functions in the [load module](https://python.langchain.com/api_reference/core/load.html) of `langchain-core`. These functions support JSON and JSON-serializable objects.

All LangChain objects that inherit from [Serializable](https://python.langchain.com/api_reference/core/load/langchain_core.load.serializable.Serializable.html) are JSON-serializable. Examples include [messages](https://python.langchain.com/api_reference//python/core_api_reference.html#module-langchain_core.messages), [document objects](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) (e.g., as returned from [retrievers](/docs/concepts/retrievers/)), and most [Runnables](/docs/concepts/lcel/), such as chat models, retrievers, and [chains](/docs/how_to/sequence/) implemented with the LangChain Expression Language.

Below we walk through an example with a simple [LLM chain](/docs/tutorials/llm_chain/).

caution

De-serialization using `load` and `loads` can instantiate any serializable LangChain object. Only use this feature with trusted inputs!

De-serialization is a beta feature and is subject to change.

```python
from langchain_core.load import dumpd, dumps, load, loads
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Translate the following into {language}:"),
        ("user", "{text}"),
    ],
)

llm = ChatOpenAI(model="gpt-4o-mini", api_key="llm-api-key")

chain = prompt | llm
```

**API Reference:**[dumpd](https://python.langchain.com/api_reference/core/load/langchain_core.load.dump.dumpd.html) | [dumps](https://python.langchain.com/api_reference/core/load/langchain_core.load.dump.dumps.html) | [load](https://python.langchain.com/api_reference/core/load/langchain_core.load.load.load.html) | [loads](https://python.langchain.com/api_reference/core/load/langchain_core.load.load.loads.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

## Saving objects[â€‹](#saving-objects "Direct link to Saving objects")

### To json[â€‹](#to-json "Direct link to To json")

```python
string_representation = dumps(chain, pretty=True)
print(string_representation[:500])
```

```output
{
  "lc": 1,
  "type": "constructor",
  "id": [
    "langchain",
    "schema",
    "runnable",
    "RunnableSequence"
  ],
  "kwargs": {
    "first": {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain",
        "prompts",
        "chat",
        "ChatPromptTemplate"
      ],
      "kwargs": {
        "input_variables": [
          "language",
          "text"
        ],
        "messages": [
          {
            "lc": 1,
            "type": "constructor",
```

### To a json-serializable Python dict[â€‹](#to-a-json-serializable-python-dict "Direct link to To a json-serializable Python dict")

```python
dict_representation = dumpd(chain)

print(type(dict_representation))
```

```output
<class 'dict'>
```

### To disk[â€‹](#to-disk "Direct link to To disk")

```python
import json

with open("/tmp/chain.json", "w") as fp:
    json.dump(string_representation, fp)
```

Note that the API key is withheld from the serialized representations. Parameters that are considered secret are specified by the `.lc_secrets` attribute of the LangChain object:

```python
chain.last.lc_secrets
```

```output
{'openai_api_key': 'OPENAI_API_KEY'}
```

## Loading objects[â€‹](#loading-objects "Direct link to Loading objects")

Specifying `secrets_map` in `load` and `loads` will load the corresponding secrets onto the de-serialized LangChain object.

### From string[â€‹](#from-string "Direct link to From string")

```python
chain = loads(string_representation, secrets_map={"OPENAI_API_KEY": "llm-api-key"})
```

### From dict[â€‹](#from-dict "Direct link to From dict")

```python
chain = load(dict_representation, secrets_map={"OPENAI_API_KEY": "llm-api-key"})
```

### From disk[â€‹](#from-disk "Direct link to From disk")

```python
with open("/tmp/chain.json", "r") as fp:
    chain = loads(json.load(fp), secrets_map={"OPENAI_API_KEY": "llm-api-key"})
```

Note that we recover the API key specified at the start of the guide:

```python
chain.last.openai_api_key.get_secret_value()
```

```output
'llm-api-key'
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/serialization.ipynb)

* * *


- [Saving objects](#saving-objects)
  
  - [To json](#to-json)
  - [To a json-serializable Python dict](#to-a-json-serializable-python-dict)
  - [To disk](#to-disk)
- [Loading objects](#loading-objects)
  
  - [From string](#from-string)
  - [From dict](#from-dict)
  - [From disk](#from-disk)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/chat_models.mdx)

# Chat models

## Overview[â€‹](#overview "Direct link to Overview")

Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.

Modern LLMs are typically accessed through a chat model interface that takes a list of [messages](/docs/concepts/messages/) as input and returns a [message](/docs/concepts/messages/) as output.

The newest generation of chat models offer additional capabilities:

- [Tool calling](/docs/concepts/tool_calling/): Many popular chat models offer a native [tool calling](/docs/concepts/tool_calling/) API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.
- [Structured output](/docs/concepts/structured_outputs/): A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.
- [Multimodality](/docs/concepts/multimodality/): The ability to work with data other than text; for example, images, audio, and video.

## Features[â€‹](#features "Direct link to Features")

LangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.

- Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/integrations/chat/) for an up-to-date list of supported models.
- Use either LangChain's [messages](/docs/concepts/messages/) format or OpenAI format.
- Standard [tool calling API](/docs/concepts/tool_calling/): standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.
- Standard API for [structuring outputs](/docs/concepts/structured_outputs/#structured-output-method) via the `with_structured_output` method.
- Provides support for [async programming](/docs/concepts/async/), [efficient batching](/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich streaming API](/docs/concepts/streaming/).
- Integration with [LangSmith](https://docs.smith.langchain.com) for monitoring and debugging production-grade applications based on LLMs.
- Additional features like standardized [token usage](/docs/concepts/messages/#aimessage), [rate limiting](#rate-limiting), [caching](#caching) and more.

## Integrations[â€‹](#integrations "Direct link to Integrations")

LangChain has many chat model integrations that allow you to use a wide variety of models from different providers.

These integrations are one of two types:

1. **Official models**: These are models that are officially supported by LangChain and/or model provider. You can find these models in the `langchain-<provider>` packages.
2. **Community models**: There are models that are mostly contributed and supported by the community. You can find these models in the `langchain-community` package.

LangChain chat models are named with a convention that prefixes "Chat" to their class names (e.g., `ChatOllama`, `ChatAnthropic`, `ChatOpenAI`, etc.).

Please review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.

note

Models that do **not** include the prefix "Chat" in their name or include "LLM" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.

## Interface[â€‹](#interface "Direct link to Interface")

LangChain chat models implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface. Because `BaseChatModel` also implements the [Runnable Interface](/docs/concepts/runnables/), chat models support a [standard streaming interface](/docs/concepts/streaming/), [async programming](/docs/concepts/async/), optimized [batching](/docs/concepts/runnables/#optimized-parallel-execution-batch), and more. Please see the [Runnable Interface](/docs/concepts/runnables/) for more details.

Many of the key methods of chat models operate on [messages](/docs/concepts/messages/) as input and return messages as output.

Chat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the [standard parameters](#standard-parameters) section for more details.

note

In documentation, we will often use the terms "LLM" and "Chat Model" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface.

However, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models are typically named without the "Chat" prefix (e.g., `Ollama`, `Anthropic`, `OpenAI`, etc.). These models implement the [BaseLLM](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.BaseLLM.html#langchain_core.language_models.llms.BaseLLM) interface and may be named with the "LLM" suffix (e.g., `OllamaLLM`, `AnthropicLLM`, `OpenAILLM`, etc.). Generally, users should not use these models.

### Key methods[â€‹](#key-methods "Direct link to Key methods")

The key methods of a chat model are:

1. **invoke**: The primary method for interacting with a chat model. It takes a list of [messages](/docs/concepts/messages/) as input and returns a list of messages as output.
2. **stream**: A method that allows you to stream the output of a chat model as it is generated.
3. **batch**: A method that allows you to batch multiple requests to a chat model together for more efficient processing.
4. **bind\_tools**: A method that allows you to bind a tool to a chat model for use in the model's execution context.
5. **with\_structured\_output**: A wrapper around the `invoke` method for models that natively support [structured output](/docs/concepts/structured_outputs/).

Other important methods can be found in the [BaseChatModel API Reference](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html).

### Inputs and outputs[â€‹](#inputs-and-outputs "Direct link to Inputs and outputs")

Modern LLMs are typically accessed through a chat model interface that takes [messages](/docs/concepts/messages/) as input and returns [messages](/docs/concepts/messages/) as output. Messages are typically associated with a role (e.g., "system", "human", "assistant") and one or more content blocks that contain text or potentially multimodal data (e.g., images, audio, video).

LangChain supports two message formats to interact with chat models:

1. **LangChain Message Format**: LangChain's own message format, which is used by default and is used internally by LangChain.
2. **OpenAI's Message Format**: OpenAI's message format.

### Standard parameters[â€‹](#standard-parameters "Direct link to Standard parameters")

Many chat models have standardized parameters that can be used to configure the model:

| Parameter      | Description                                                                                                                                                                                                                                                                                                   |
|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `model`        | The name or identifier of the specific AI model you want to use (e.g., `"gpt-3.5-turbo"` or `"gpt-4"`).                                                                                                                                                                                                       |
| `temperature`  | Controls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.                                                                                                                           |
| `timeout`      | The maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesnâ€™t hang indefinitely.                                                                                                                                                              |
| `max_tokens`   | Limits the total number of tokens (words and punctuation) in the response. This controls how long the output can be.                                                                                                                                                                                          |
| `stop`         | Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.                                                                                                                                             |
| `max_retries`  | The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.                                                                                                                                                                       |
| `api_key`      | The API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model.                                                                                                                                                                             |
| `base_url`     | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.                                                                                                                                                         |
| `rate_limiter` | An optional [BaseRateLimiter](https://python.langchain.com/api_reference/core/rate_limiters/langchain_core.rate_limiters.BaseRateLimiter.html#langchain_core.rate_limiters.BaseRateLimiter) to space out requests to avoid exceeding rate limits. See [rate-limiting](#rate-limiting) below for more details. |

Some important things to note:

- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max\_tokens can't be supported on these.
- Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. `langchain-openai`, `langchain-anthropic`, etc.), they're not enforced on models in `langchain-community`.

Chat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model head to the their respective [API reference](https://python.langchain.com/api_reference/) for that model.

## Tool calling[â€‹](#tool-calling "Direct link to Tool calling")

Chat models can call [tools](/docs/concepts/tools/) to perform tasks such as fetching data from a database, making API requests, or running custom code. Please see the [tool calling](/docs/concepts/tool_calling/) guide for more information.

## Structured outputs[â€‹](#structured-outputs "Direct link to Structured outputs")

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely useful for information extraction tasks. Please read more about the technique in the [structured outputs](/docs/concepts/structured_outputs/) guide.

## Multimodality[â€‹](#multimodality "Direct link to Multimodality")

Large Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as [multimodality](/docs/concepts/multimodality/).

Currently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.

## Context window[â€‹](#context-window "Direct link to Context window")

A chat model's context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.

If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).

The size of the input is measured in [tokens](/docs/concepts/tokens/) which are the unit of processing that the model uses.

## Advanced topics[â€‹](#advanced-topics "Direct link to Advanced topics")

### Rate-limiting[â€‹](#rate-limiting "Direct link to Rate-limiting")

Many chat model providers impose a limit on the number of requests that can be made in a given time period.

If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

You have a few options to deal with rate limits:

1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.
2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information.
3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.

### Caching[â€‹](#caching "Direct link to Caching")

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.

The reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?

An alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.

A semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models/) to convert text to a vector representation), and it's not guaranteed to capture the meaning of the input accurately.

However, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.

Please see the [how to cache chat model responses](/docs/how_to/chat_model_caching/) guide for more details.

## Related resources[â€‹](#related-resources "Direct link to Related resources")

- How-to guides on using chat models: [how-to guides](/docs/how_to/#chat-models).
- List of supported chat models: [chat model integrations](/docs/integrations/chat/).

### Conceptual guides[â€‹](#conceptual-guides "Direct link to Conceptual guides")

- [Messages](/docs/concepts/messages/)
- [Tool calling](/docs/concepts/tool_calling/)
- [Multimodality](/docs/concepts/multimodality/)
- [Structured outputs](/docs/concepts/structured_outputs/)
- [Tokens](/docs/concepts/tokens/)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/chat_models.mdx)

* * *


- [Overview](#overview)
- [Features](#features)
- [Integrations](#integrations)
- [Interface](#interface)
  
  - [Key methods](#key-methods)
  - [Inputs and outputs](#inputs-and-outputs)
  - [Standard parameters](#standard-parameters)
- [Tool calling](#tool-calling)
- [Structured outputs](#structured-outputs)
- [Multimodality](#multimodality)
- [Context window](#context-window)
- [Advanced topics](#advanced-topics)
  
  - [Rate-limiting](#rate-limiting)
  - [Caching](#caching)
- [Related resources](#related-resources)
  
  - [Conceptual guides](#conceptual-guides)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/llm_math_chain.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/llm_math_chain.ipynb)

# Migrating from LLMMathChain

[`LLMMathChain`](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm_math.base.LLMMathChain.html) enabled the evaluation of mathematical expressions generated by a LLM. Instructions for generating the expressions were formatted into the prompt, and the expressions were parsed out of the string response before evaluation using the [numexpr](https://numexpr.readthedocs.io/en/latest/user_guide.html) library.

This is more naturally achieved via [tool calling](/docs/concepts/tool_calling/). We can equip a chat model with a simple calculator tool leveraging `numexpr` and construct a simple chain around it using [LangGraph](https://langchain-ai.github.io/langgraph/). Some advantages of this approach include:

- Leverage tool-calling capabilities of chat models that have been fine-tuned for this purpose;
- Reduce parsing errors from extracting expression from a string LLM response;
- Delegation of instructions to [message roles](/docs/concepts/messages/) (e.g., chat models can understand what a `ToolMessage` represents without the need for additional prompting);
- Support for streaming, both of individual tokens and chain steps.

```python
%pip install --upgrade --quiet numexpr
```

```python
import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

## Legacy[â€‹](#legacy "Direct link to Legacy")

Details

```python
from langchain.chains import LLMMathChain
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

chain = LLMMathChain.from_llm(llm)

chain.invoke("What is 551368 divided by 82?")
```

**API Reference:**[LLMMathChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm_math.base.LLMMathChain.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
{'question': 'What is 551368 divided by 82?', 'answer': 'Answer: 6724.0'}
```

## LangGraph[â€‹](#langgraph "Direct link to LangGraph")

Details

```python
import math
from typing import Annotated, Sequence

import numexpr
from langchain_core.messages import BaseMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt.tool_node import ToolNode
from typing_extensions import TypedDict


@tool
def calculator(expression: str) -> str:
    """Calculate expression using Python's numexpr library.

    Expression should be a single line mathematical expression
    that solves the problem.

    Examples:
        "37593 * 67" for "37593 times 67"
        "37593**(1/5)" for "37593^(1/5)"
    """
    local_dict = {"pi": math.pi, "e": math.e}
    return str(
        numexpr.evaluate(
            expression.strip(),
            global_dict={},  # restrict access to globals
            local_dict=local_dict,  # add common mathematical functions
        )
    )


llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
tools = [calculator]
llm_with_tools = llm.bind_tools(tools, tool_choice="any")


class ChainState(TypedDict):
    """LangGraph state."""

    messages: Annotated[Sequence[BaseMessage], add_messages]


async def acall_chain(state: ChainState, config: RunnableConfig):
    last_message = state["messages"][-1]
    response = await llm_with_tools.ainvoke(state["messages"], config)
    return {"messages": [response]}


async def acall_model(state: ChainState, config: RunnableConfig):
    response = await llm.ainvoke(state["messages"], config)
    return {"messages": [response]}


graph_builder = StateGraph(ChainState)
graph_builder.add_node("call_tool", acall_chain)
graph_builder.add_node("execute_tool", ToolNode(tools))
graph_builder.add_node("call_model", acall_model)
graph_builder.set_entry_point("call_tool")
graph_builder.add_edge("call_tool", "execute_tool")
graph_builder.add_edge("execute_tool", "call_model")
graph_builder.add_edge("call_model", END)
chain = graph_builder.compile()
```

**API Reference:**[BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [add\_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)

```python
# Visualize chain:

from IPython.display import Image

Image(chain.get_graph().draw_mermaid_png())
```

![](data:image/jpg;base64,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)

```python
# Stream chain steps:

example_query = "What is 551368 divided by 82"

events = chain.astream(
    {"messages": [("user", example_query)]},
    stream_mode="values",
)
async for event in events:
    event["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

What is 551368 divided by 82
==================================[1m Ai Message [0m==================================
Tool Calls:
  calculator (call_1ic3gjuII0Aq9vxlSYiwvjSb)
 Call ID: call_1ic3gjuII0Aq9vxlSYiwvjSb
  Args:
    expression: 551368 / 82
=================================[1m Tool Message [0m=================================
Name: calculator

6724.0
==================================[1m Ai Message [0m==================================

551368 divided by 82 equals 6724.
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

See guides for building and working with tools [here](/docs/how_to/#tools).

Check out the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) for detail on building with LangGraph.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_chains/llm_math_chain.ipynb)

* * *


- [Legacy](#legacy)
- [LangGraph](#langgraph)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_memory/chat_history.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_memory/chat_history.ipynb)

# How to use BaseChatMessageHistory with LangGraph

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat History](/docs/concepts/chat_history/)
- [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html)
- [LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/)
- [Memory](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/#memory)

We recommend that new LangChain applications take advantage of the [built-in LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to implement memory.

In some situations, users may need to keep using an existing persistence solution for chat message history.

Here, we will show how to use [LangChain chat message histories](https://python.langchain.com/docs/integrations/memory/) (implementations of [BaseChatMessageHistory](https://python.langchain.com/api_reference/core/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html)) with LangGraph.

## Set up[â€‹](#set-up "Direct link to Set up")

```python
%%capture --no-stderr
%pip install --upgrade --quiet langchain-anthropic langgraph
```

```python
import os
from getpass import getpass

if "ANTHROPIC_API_KEY" not in os.environ:
    os.environ["ANTHROPIC_API_KEY"] = getpass()
```

## ChatMessageHistory[â€‹](#chatmessagehistory "Direct link to ChatMessageHistory")

A message history needs to be parameterized by a conversation ID or maybe by the 2-tuple of (user ID, conversation ID).

Many of the [LangChain chat message histories](https://python.langchain.com/docs/integrations/memory/) will have either a `session_id` or some `namespace` to allow keeping track of different conversations. Please refer to the specific implementations to check how it is parameterized.

The built-in `InMemoryChatMessageHistory` does not contains such a parameterization, so we'll create a dictionary to keep track of the message histories.

```python
import uuid

from langchain_core.chat_history import InMemoryChatMessageHistory

chats_by_session_id = {}


def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:
    chat_history = chats_by_session_id.get(session_id)
    if chat_history is None:
        chat_history = InMemoryChatMessageHistory()
        chats_by_session_id[session_id] = chat_history
    return chat_history
```

**API Reference:**[InMemoryChatMessageHistory](https://python.langchain.com/api_reference/core/chat_history/langchain_core.chat_history.InMemoryChatMessageHistory.html)

## Use with LangGraph[â€‹](#use-with-langgraph "Direct link to Use with LangGraph")

Next, we'll set up a basic chat bot using LangGraph. If you're not familiar with LangGraph, you should look at the following [Quick Start Tutorial](https://langchain-ai.github.io/langgraph/tutorials/introduction/).

We'll create a [LangGraph node](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes) for the chat model, and manually manage the conversation history, taking into account the conversation ID passed as part of the RunnableConfig.

The conversation ID can be passed as either part of the RunnableConfig (as we'll do here), or as part of the [graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#state).

```python
import uuid

from langchain_anthropic import ChatAnthropic
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import START, MessagesState, StateGraph

# Define a new graph
builder = StateGraph(state_schema=MessagesState)

# Define a chat model
model = ChatAnthropic(model="claude-3-haiku-20240307")


# Define the function that calls the model
def call_model(state: MessagesState, config: RunnableConfig) -> list[BaseMessage]:
    # Make sure that config is populated with the session id
    if "configurable" not in config or "session_id" not in config["configurable"]:
        raise ValueError(
            "Make sure that the config includes the following information: {'configurable': {'session_id': 'some_value'}}"
        )
    # Fetch the history of messages and append to it any new messages.
    chat_history = get_chat_history(config["configurable"]["session_id"])
    messages = list(chat_history.messages) + state["messages"]
    ai_message = model.invoke(messages)
    # Finally, update the chat message history to include
    # the new input message from the user together with the
    # response from the model.
    chat_history.add_messages(state["messages"] + [ai_message])
    return {"messages": ai_message}


# Define the two nodes we will cycle between
builder.add_edge(START, "model")
builder.add_node("model", call_model)

graph = builder.compile()

# Here, we'll create a unique session ID to identify the conversation
session_id = uuid.uuid4()
config = {"configurable": {"session_id": session_id}}

input_message = HumanMessage(content="hi! I'm bob")
for event in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()

# Here, let's confirm that the AI remembers our name!
input_message = HumanMessage(content="what was my name?")
for event in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```output
================================[1m Human Message [0m=================================

hi! I'm bob
==================================[1m Ai Message [0m==================================

Hello Bob! It's nice to meet you. I'm Claude, an AI assistant created by Anthropic. How are you doing today?
================================[1m Human Message [0m=================================

what was my name?
==================================[1m Ai Message [0m==================================

You introduced yourself as Bob when you said "hi! I'm bob".
```

tip

This also supports streaming LLM content token by token if using langgraph &gt;= 0.2.28.

```python
from langchain_core.messages import AIMessageChunk

first = True

for msg, metadata in graph.stream(
    {"messages": input_message}, config, stream_mode="messages"
):
    if msg.content and not isinstance(msg, HumanMessage):
        print(msg.content, end="|", flush=True)
```

**API Reference:**[AIMessageChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html)

```output
You| sai|d your| name was Bob.|
```

## Using With RunnableWithMessageHistory[â€‹](#using-with-runnablewithmessagehistory "Direct link to Using With RunnableWithMessageHistory")

This how-to guide used the `messages` and `add_messages` interface of `BaseChatMessageHistory` directly.

Alternatively, you can use [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html), as [LCEL](/docs/concepts/lcel/) can be used inside any [LangGraph node](https://langchain-ai.github.io/langgraph/concepts/low_level/#nodes).

To do that replace the following code:

```python
def call_model(state: MessagesState, config: RunnableConfig) -> list[BaseMessage]:
    # Make sure that config is populated with the session id
    if "configurable" not in config or "session_id" not in config["configurable"]:
        raise ValueError(
            "You make sure that the config includes the following information: {'configurable': {'session_id': 'some_value'}}"
        )
    # Fetch the history of messages and append to it any new messages.
    chat_history = get_chat_history(config["configurable"]["session_id"])
    messages = list(chat_history.messages) + state["messages"]
    ai_message = model.invoke(messages)
    # Finally, update the chat message history to include
    # the new input message from the user together with the
    # response from the model.
    chat_history.add_messages(state["messages"] + [ai_message])
    # hilight-end
    return {"messages": ai_message}
```

With the corresponding instance of `RunnableWithMessageHistory` defined in your current application.

```python
runnable = RunnableWithMessageHistory(...) # From existing code

def call_model(state: MessagesState, config: RunnableConfig) -> list[BaseMessage]:
    # RunnableWithMessageHistory takes care of reading the message history
    # and updating it with the new human message and ai response.
    ai_message = runnable.invoke(state['messages'], config)
    return {
        "messages": ai_message
    }
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_memory/chat_history.ipynb)

* * *


- [Set up](#set-up)
- [ChatMessageHistory](#chatmessagehistory)
- [Use with LangGraph](#use-with-langgraph)
- [Using With RunnableWithMessageHistory](#using-with-runnablewithmessagehistory)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/inspect.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/inspect.ipynb)

# How to inspect runnables

Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel/)
- [Chaining runnables](/docs/how_to/sequence/)

Once you create a runnable with [LangChain Expression Language](/docs/concepts/lcel/), you may often want to inspect it to get a better sense for what is going on. This notebook covers some methods for doing so.

This guide shows some ways you can programmatically introspect the internal steps of chains. If you are instead interested in debugging issues in your chain, see [this section](/docs/how_to/debugging/) instead.

First, let's create an example chain. We will create one that does retrieval:

```python
%pip install -qU langchain langchain-openai faiss-cpu tiktoken
```

```python
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

vectorstore = FAISS.from_texts(
    ["harrison worked at kensho"], embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()

template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

model = ChatOpenAI()

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)
```

**API Reference:**[FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

## Get a graph[â€‹](#get-a-graph "Direct link to Get a graph")

You can use the `get_graph()` method to get a graph representation of the runnable:

```python
chain.get_graph()
```

## Print a graph[â€‹](#print-a-graph "Direct link to Print a graph")

While that is not super legible, you can use the `print_ascii()` method to show that graph in a way that's easier to understand:

```python
chain.get_graph().print_ascii()
```

```output
           +---------------------------------+         
           | Parallel<context,question>Input |         
           +---------------------------------+         
                    **               **                
                 ***                   ***             
               **                         **           
+----------------------+              +-------------+  
| VectorStoreRetriever |              | Passthrough |  
+----------------------+              +-------------+  
                    **               **                
                      ***         ***                  
                         **     **                     
           +----------------------------------+        
           | Parallel<context,question>Output |        
           +----------------------------------+        
                             *                         
                             *                         
                             *                         
                  +--------------------+               
                  | ChatPromptTemplate |               
                  +--------------------+               
                             *                         
                             *                         
                             *                         
                      +------------+                   
                      | ChatOpenAI |                   
                      +------------+                   
                             *                         
                             *                         
                             *                         
                   +-----------------+                 
                   | StrOutputParser |                 
                   +-----------------+                 
                             *                         
                             *                         
                             *                         
                +-----------------------+              
                | StrOutputParserOutput |              
                +-----------------------+
```

## Get the prompts[â€‹](#get-the-prompts "Direct link to Get the prompts")

You may want to see just the prompts that are used in a chain with the `get_prompts()` method:

```python
chain.get_prompts()
```

```output
[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n'))])]
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to introspect your composed LCEL chains.

Next, check out the other how-to guides on runnables in this section, or the related how-to guide on [debugging your chains](/docs/how_to/debugging/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/inspect.ipynb)

* * *


- [Get a graph](#get-a-graph)
- [Print a graph](#print-a-graph)
- [Get the prompts](#get-the-prompts)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/streaming.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/streaming.ipynb)

# How to stream runnables

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models/)
- [LangChain Expression Language](/docs/concepts/lcel/)
- [Output parsers](/docs/concepts/output_parsers/)

Streaming is critical in making applications based on LLMs feel responsive to end-users.

Important LangChain primitives like [chat models](/docs/concepts/chat_models/), [output parsers](/docs/concepts/output_parsers/), [prompts](/docs/concepts/prompt_templates/), [retrievers](/docs/concepts/retrievers/), and [agents](/docs/concepts/agents/) implement the LangChain [Runnable Interface](/docs/concepts/runnables/).

This interface provides two general approaches to stream content:

1. sync `stream` and async `astream`: a **default implementation** of streaming that streams the **final output** from the chain.
2. async `astream_events` and async `astream_log`: these provide a way to stream both **intermediate steps** and **final output** from the chain.

Let's take a look at both approaches, and try to understand how to use them.

info

For a higher-level overview of streaming techniques in LangChain, see [this section of the conceptual guide](/docs/concepts/streaming/).

## Using Stream[â€‹](#using-stream "Direct link to Using Stream")

All `Runnable` objects implement a sync method called `stream` and an async variant called `astream`.

These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.

Streaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.

The complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.

The best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!

### LLMs and Chat Models[â€‹](#llms-and-chat-models "Direct link to LLMs and Chat Models")

Large language models and their chat variants are the primary bottleneck in LLM based apps.

Large language models can take **several seconds** to generate a complete response to a query. This is far slower than the **~200-300 ms** threshold at which an application feels responsive to an end user.

The key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model **token by token**.

We will show examples of streaming using a chat model. Choose one from the options below:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4o-mini", model_provider="openai")
```

Let's start with the sync `stream` API:

```python
chunks = []
for chunk in model.stream("what color is the sky?"):
    chunks.append(chunk)
    print(chunk.content, end="|", flush=True)
```

```output
The| sky| appears| blue| during| the| day|.|
```

Alternatively, if you're working in an async environment, you may consider using the async `astream` API:

```python
chunks = []
async for chunk in model.astream("what color is the sky?"):
    chunks.append(chunk)
    print(chunk.content, end="|", flush=True)
```

```output
The| sky| appears| blue| during| the| day|.|
```

Let's inspect one of the chunks

```python
chunks[0]
```

```output
AIMessageChunk(content='The', id='run-b36bea64-5511-4d7a-b6a3-a07b3db0c8e7')
```

We got back something called an `AIMessageChunk`. This chunk represents a part of an `AIMessage`.

Message chunks are additive by design -- one can simply add them up to get the state of the response so far!

```python
chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]
```

```output
AIMessageChunk(content='The sky appears blue during', id='run-b36bea64-5511-4d7a-b6a3-a07b3db0c8e7')
```

### Chains[â€‹](#chains "Direct link to Chains")

Virtually all LLM applications involve more steps than just a call to a language model.

Let's build a simple chain using `LangChain Expression Language` (`LCEL`) that combines a prompt, model and a parser and verify that streaming works.

We will use [`StrOutputParser`](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) to parse the output from the model. This is a simple parser that extracts the `content` field from an `AIMessageChunk`, giving us the `token` returned by the model.

tip

LCEL is a *declarative* way to specify a "program" by chainining together different LangChain primitives. Chains created using LCEL benefit from an automatic implementation of `stream` and `astream` allowing streaming of the final output. In fact, chains created with LCEL implement the entire standard Runnable interface.

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for chunk in chain.astream({"topic": "parrot"}):
    print(chunk, end="|", flush=True)
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
Here|'s| a| joke| about| a| par|rot|:|

A man| goes| to| a| pet| shop| to| buy| a| par|rot|.| The| shop| owner| shows| him| two| stunning| pa|rr|ots| with| beautiful| pl|um|age|.|

"|There|'s| a| talking| par|rot| an|d a| non|-|talking| par|rot|,"| the| owner| says|.| "|The| talking| par|rot| costs| $|100|,| an|d the| non|-|talking| par|rot| is| $|20|."|

The| man| says|,| "|I|'ll| take| the| non|-|talking| par|rot| at| $|20|."|

He| pays| an|d leaves| with| the| par|rot|.| As| he|'s| walking| down| the| street|,| the| par|rot| looks| up| at| him| an|d says|,| "|You| know|,| you| really| are| a| stupi|d man|!"|

The| man| is| stun|ne|d an|d looks| at| the| par|rot| in| dis|bel|ief|.| The| par|rot| continues|,| "|Yes|,| you| got| r|ippe|d off| big| time|!| I| can| talk| just| as| well| as| that| other| par|rot|,| an|d you| only| pai|d $|20| |for| me|!"|
```

Note that we're getting streaming output even though we're using `parser` at the end of the chain above. The `parser` operates on each streaming chunk individidually. Many of the [LCEL primitives](/docs/how_to/#langchain-expression-language-lcel) also support this kind of transform-style passthrough streaming, which can be very convenient when constructing apps.

Custom functions can be [designed to return generators](/docs/how_to/functions/#streaming), which are able to operate on streams.

Certain runnables, like [prompt templates](/docs/how_to/#prompt-templates) and [chat models](/docs/how_to/#chat-models), cannot process individual chunks and instead aggregate all previous steps. Such runnables can interrupt the streaming process.

note

The LangChain Expression language allows you to separate the construction of a chain from the mode in which it is used (e.g., sync/async, batch/streaming etc.). If this is not relevant to what you're building, you can also rely on a standard **imperative** programming approach by caling `invoke`, `batch` or `stream` on each component individually, assigning the results to variables and then using them downstream as you see fit.

### Working with Input Streams[â€‹](#working-with-input-streams "Direct link to Working with Input Streams")

What if you wanted to stream JSON from the output as it was being generated?

If you were to rely on `json.loads` to parse the partial json, the parsing would fail as the partial json wouldn't be valid json.

You'd likely be at a complete loss of what to do and claim that it wasn't possible to stream JSON.

Well, turns out there is a way to do it -- the parser needs to operate on the **input stream**, and attempt to "auto-complete" the partial json into a valid state.

Let's see such a parser in action to understand what this means.

```python
from langchain_core.output_parsers import JsonOutputParser

chain = (
    model | JsonOutputParser()
)  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models
async for text in chain.astream(
    "output a list of the countries france, spain and japan and their populations in JSON format. "
    'Use a dict with an outer key of "countries" which contains a list of countries. '
    "Each country should have the key `name` and `population`"
):
    print(text, flush=True)
```

**API Reference:**[JsonOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html)

```output
{}
{'countries': []}
{'countries': [{}]}
{'countries': [{'name': ''}]}
{'countries': [{'name': 'France'}]}
{'countries': [{'name': 'France', 'population': 67}]}
{'countries': [{'name': 'France', 'population': 67413}]}
{'countries': [{'name': 'France', 'population': 67413000}]}
{'countries': [{'name': 'France', 'population': 67413000}, {}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': ''}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': ''}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan'}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125584}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125584000}]}
```

Now, let's **break** streaming. We'll use the previous example and append an extraction function at the end that extracts the country names from the finalized JSON.

warning

Any steps in the chain that operate on **finalized inputs** rather than on **input streams** can break streaming functionality via `stream` or `astream`.

tip

Later, we will discuss the `astream_events` API which streams results from intermediate steps. This API will stream results from intermediate steps even if the chain contains steps that only operate on **finalized inputs**.

```python
from langchain_core.output_parsers import (
    JsonOutputParser,
)


# A function that operates on finalized inputs
# rather than on an input_stream
def _extract_country_names(inputs):
    """A function that does not operates on input streams and breaks streaming."""
    if not isinstance(inputs, dict):
        return ""

    if "countries" not in inputs:
        return ""

    countries = inputs["countries"]

    if not isinstance(countries, list):
        return ""

    country_names = [
        country.get("name") for country in countries if isinstance(country, dict)
    ]
    return country_names


chain = model | JsonOutputParser() | _extract_country_names

async for text in chain.astream(
    "output a list of the countries france, spain and japan and their populations in JSON format. "
    'Use a dict with an outer key of "countries" which contains a list of countries. '
    "Each country should have the key `name` and `population`"
):
    print(text, end="|", flush=True)
```

**API Reference:**[JsonOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html)

```output
['France', 'Spain', 'Japan']|
```

#### Generator Functions[â€‹](#generator-functions "Direct link to Generator Functions")

Let's fix the streaming using a generator function that can operate on the **input stream**.

tip

A generator function (a function that uses `yield`) allows writing code that operates on **input streams**

```python
from langchain_core.output_parsers import JsonOutputParser


async def _extract_country_names_streaming(input_stream):
    """A function that operates on input streams."""
    country_names_so_far = set()

    async for input in input_stream:
        if not isinstance(input, dict):
            continue

        if "countries" not in input:
            continue

        countries = input["countries"]

        if not isinstance(countries, list):
            continue

        for country in countries:
            name = country.get("name")
            if not name:
                continue
            if name not in country_names_so_far:
                yield name
                country_names_so_far.add(name)


chain = model | JsonOutputParser() | _extract_country_names_streaming

async for text in chain.astream(
    "output a list of the countries france, spain and japan and their populations in JSON format. "
    'Use a dict with an outer key of "countries" which contains a list of countries. '
    "Each country should have the key `name` and `population`",
):
    print(text, end="|", flush=True)
```

**API Reference:**[JsonOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html)

```output
France|Spain|Japan|
```

note

Because the code above is relying on JSON auto-completion, you may see partial names of countries (e.g., `Sp` and `Spain`), which is not what one would want for an extraction result!

We're focusing on streaming concepts, not necessarily the results of the chains.

### Non-streaming components[â€‹](#non-streaming-components "Direct link to Non-streaming components")

Some built-in components like Retrievers do not offer any `streaming`. What happens if we try to `stream` them? ðŸ¤¨

```python
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings

template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

vectorstore = FAISS.from_texts(
    ["harrison worked at kensho", "harrison likes spicy food"],
    embedding=OpenAIEmbeddings(),
)
retriever = vectorstore.as_retriever()

chunks = [chunk for chunk in retriever.stream("where did harrison work?")]
chunks
```

**API Reference:**[FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```output
[[Document(page_content='harrison worked at kensho'),
  Document(page_content='harrison likes spicy food')]]
```

Stream just yielded the final result from that component.

This is OK ðŸ¥¹! Not all components have to implement streaming -- in some cases streaming is either unnecessary, difficult or just doesn't make sense.

tip

An LCEL chain constructed using non-streaming components, will still be able to stream in a lot of cases, with streaming of partial output starting after the last non-streaming step in the chain.

```python
retrieval_chain = (
    {
        "context": retriever.with_config(run_name="Docs"),
        "question": RunnablePassthrough(),
    }
    | prompt
    | model
    | StrOutputParser()
)
```

```python
for chunk in retrieval_chain.stream(
    "Where did harrison work? " "Write 3 made up sentences about this place."
):
    print(chunk, end="|", flush=True)
```

```output
Base|d on| the| given| context|,| Harrison| worke|d at| K|ens|ho|.|

Here| are| |3| |made| up| sentences| about| this| place|:|

1|.| K|ens|ho| was| a| cutting|-|edge| technology| company| known| for| its| innovative| solutions| in| artificial| intelligence| an|d data| analytics|.|

2|.| The| modern| office| space| at| K|ens|ho| feature|d open| floor| plans|,| collaborative| work|sp|aces|,| an|d a| vib|rant| atmosphere| that| fos|tere|d creativity| an|d team|work|.|

3|.| With| its| prime| location| in| the| heart| of| the| city|,| K|ens|ho| attracte|d top| talent| from| aroun|d the| worl|d,| creating| a| diverse| an|d dynamic| work| environment|.|
```

Now that we've seen how `stream` and `astream` work, let's venture into the world of streaming events. ðŸžï¸

## Using Stream Events[â€‹](#using-stream-events "Direct link to Using Stream Events")

Event Streaming is a **beta** API. This API may change a bit based on feedback.

note

This guide demonstrates the `V2` API and requires langchain-core &gt;= 0.2. For the `V1` API compatible with older versions of LangChain, see [here](https://python.langchain.com/v0.1/docs/expression_language/streaming/#using-stream-events).

```python
import langchain_core

langchain_core.__version__
```

For the `astream_events` API to work properly:

- Use `async` throughout the code to the extent possible (e.g., async tools etc)
- Propagate callbacks if defining custom functions / runnables
- Whenever using runnables without LCEL, make sure to call `.astream()` on LLMs rather than `.ainvoke` to force the LLM to stream tokens.
- Let us know if anything doesn't work as expected! :)

### Event Reference[â€‹](#event-reference "Direct link to Event Reference")

Below is a reference table that shows some events that might be emitted by the various Runnable objects.

note

When streaming is implemented properly, the inputs to a runnable will not be known until after the input stream has been entirely consumed. This means that `inputs` will often be included only for `end` events and rather than for `start` events.

| event                   | name              | chunk                           | input                                           | output                                           |
|-------------------------|-------------------|---------------------------------|-------------------------------------------------|--------------------------------------------------|
| on\_chat\_model\_start  | \[model name]     |                                 | {"messages": \[\[SystemMessage, HumanMessage]]} |                                                  |
| on\_chat\_model\_stream | \[model name]     | AIMessageChunk(content="hello") |                                                 |                                                  |
| on\_chat\_model\_end    | \[model name]     |                                 | {"messages": \[\[SystemMessage, HumanMessage]]} | AIMessageChunk(content="hello world")            |
| on\_llm\_start          | \[model name]     |                                 | {'input': 'hello'}                              |                                                  |
| on\_llm\_stream         | \[model name]     | 'Hello'                         |                                                 |                                                  |
| on\_llm\_end            | \[model name]     |                                 | 'Hello human!'                                  |                                                  |
| on\_chain\_start        | format\_docs      |                                 |                                                 |                                                  |
| on\_chain\_stream       | format\_docs      | "hello world!, goodbye world!"  |                                                 |                                                  |
| on\_chain\_end          | format\_docs      |                                 | \[Document(...)]                                | "hello world!, goodbye world!"                   |
| on\_tool\_start         | some\_tool        |                                 | {"x": 1, "y": "2"}                              |                                                  |
| on\_tool\_end           | some\_tool        |                                 |                                                 | {"x": 1, "y": "2"}                               |
| on\_retriever\_start    | \[retriever name] |                                 | {"query": "hello"}                              |                                                  |
| on\_retriever\_end      | \[retriever name] |                                 | {"query": "hello"}                              | \[Document(...), ..]                             |
| on\_prompt\_start       | \[template\_name] |                                 | {"question": "hello"}                           |                                                  |
| on\_prompt\_end         | \[template\_name] |                                 | {"question": "hello"}                           | ChatPromptValue(messages: \[SystemMessage, ...]) |

### Chat Model[â€‹](#chat-model "Direct link to Chat Model")

Let's start off by looking at the events produced by a chat model.

```python
events = []
async for event in model.astream_events("hello"):
    events.append(event)
```

note

For `langchain-core<0.3.37`, set the `version` kwarg explicitly (e.g., `model.astream_events("hello", version="v2")`).

Let's take a look at the few of the start event and a few of the end events.

```python
events[:3]
```

```output
[{'event': 'on_chat_model_start',
  'data': {'input': 'hello'},
  'name': 'ChatAnthropic',
  'tags': [],
  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',
  'metadata': {'ls_provider': 'anthropic',
   'ls_model_name': 'claude-3-sonnet-20240229',
   'ls_model_type': 'chat',
   'ls_temperature': 0.0,
   'ls_max_tokens': 1024},
  'parent_ids': []},
 {'event': 'on_chat_model_stream',
  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',
  'name': 'ChatAnthropic',
  'tags': [],
  'metadata': {'ls_provider': 'anthropic',
   'ls_model_name': 'claude-3-sonnet-20240229',
   'ls_model_type': 'chat',
   'ls_temperature': 0.0,
   'ls_max_tokens': 1024},
  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66', usage_metadata={'input_tokens': 8, 'output_tokens': 4, 'total_tokens': 12, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},
  'parent_ids': []},
 {'event': 'on_chat_model_stream',
  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',
  'name': 'ChatAnthropic',
  'tags': [],
  'metadata': {'ls_provider': 'anthropic',
   'ls_model_name': 'claude-3-sonnet-20240229',
   'ls_model_type': 'chat',
   'ls_temperature': 0.0,
   'ls_max_tokens': 1024},
  'data': {'chunk': AIMessageChunk(content='Hello! How can', additional_kwargs={}, response_metadata={}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66')},
  'parent_ids': []}]
```

```python
events[-2:]
```

```output
[{'event': 'on_chat_model_stream',
  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',
  'name': 'ChatAnthropic',
  'tags': [],
  'metadata': {'ls_provider': 'anthropic',
   'ls_model_name': 'claude-3-sonnet-20240229',
   'ls_model_type': 'chat',
   'ls_temperature': 0.0,
   'ls_max_tokens': 1024},
  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66', usage_metadata={'input_tokens': 0, 'output_tokens': 12, 'total_tokens': 12, 'input_token_details': {}})},
  'parent_ids': []},
 {'event': 'on_chat_model_end',
  'data': {'output': AIMessageChunk(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66', usage_metadata={'input_tokens': 8, 'output_tokens': 16, 'total_tokens': 24, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},
  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',
  'name': 'ChatAnthropic',
  'tags': [],
  'metadata': {'ls_provider': 'anthropic',
   'ls_model_name': 'claude-3-sonnet-20240229',
   'ls_model_type': 'chat',
   'ls_temperature': 0.0,
   'ls_max_tokens': 1024},
  'parent_ids': []}]
```

### Chain[â€‹](#chain "Direct link to Chain")

Let's revisit the example chain that parsed streaming JSON to explore the streaming events API.

```python
chain = (
    model | JsonOutputParser()
)  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models

events = [
    event
    async for event in chain.astream_events(
        "output a list of the countries france, spain and japan and their populations in JSON format. "
        'Use a dict with an outer key of "countries" which contains a list of countries. '
        "Each country should have the key `name` and `population`",
    )
]
```

If you examine at the first few events, you'll notice that there are **3** different start events rather than **2** start events.

The three start events correspond to:

1. The chain (model + parser)
2. The model
3. The parser

```python
events[:3]
```

```output
[{'event': 'on_chain_start',
  'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`'},
  'name': 'RunnableSequence',
  'tags': [],
  'run_id': '4765006b-16e2-4b1d-a523-edd9fd64cb92',
  'metadata': {}},
 {'event': 'on_chat_model_start',
  'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`')]]}},
  'name': 'ChatAnthropic',
  'tags': ['seq:step:1'],
  'run_id': '0320c234-7b52-4a14-ae4e-5f100949e589',
  'metadata': {}},
 {'event': 'on_chat_model_stream',
  'data': {'chunk': AIMessageChunk(content='{', id='run-0320c234-7b52-4a14-ae4e-5f100949e589')},
  'run_id': '0320c234-7b52-4a14-ae4e-5f100949e589',
  'name': 'ChatAnthropic',
  'tags': ['seq:step:1'],
  'metadata': {}}]
```

What do you think you'd see if you looked at the last 3 events? what about the middle?

Let's use this API to take output the stream events from the model and the parser. We're ignoring start events, end events and events from the chain.

```python
num_events = 0

async for event in chain.astream_events(
    "output a list of the countries france, spain and japan and their populations in JSON format. "
    'Use a dict with an outer key of "countries" which contains a list of countries. '
    "Each country should have the key `name` and `population`",
):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(
            f"Chat model chunk: {repr(event['data']['chunk'].content)}",
            flush=True,
        )
    if kind == "on_parser_stream":
        print(f"Parser chunk: {event['data']['chunk']}", flush=True)
    num_events += 1
    if num_events > 30:
        # Truncate the output
        print("...")
        break
```

```output
Chat model chunk: ''
Chat model chunk: '{'
Parser chunk: {}
Chat model chunk: '\n  "countries'
Chat model chunk: '": [\n    '
Parser chunk: {'countries': []}
Chat model chunk: '{\n      "'
Parser chunk: {'countries': [{}]}
Chat model chunk: 'name": "France'
Parser chunk: {'countries': [{'name': 'France'}]}
Chat model chunk: '",\n      "'
Chat model chunk: 'population": 67'
Parser chunk: {'countries': [{'name': 'France', 'population': 67}]}
Chat model chunk: '413'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413}]}
Chat model chunk: '000\n    },'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}]}
Chat model chunk: '\n    {'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {}]}
Chat model chunk: '\n      "name":'
...
```

Because both the model and the parser support streaming, we see streaming events from both components in real time! Kind of cool isn't it? ðŸ¦œ

### Filtering Events[â€‹](#filtering-events "Direct link to Filtering Events")

Because this API produces so many events, it is useful to be able to filter on events.

You can filter by either component `name`, component `tags` or component `type`.

#### By Name[â€‹](#by-name "Direct link to By Name")

```python
chain = model.with_config({"run_name": "model"}) | JsonOutputParser().with_config(
    {"run_name": "my_parser"}
)

max_events = 0
async for event in chain.astream_events(
    "output a list of the countries france, spain and japan and their populations in JSON format. "
    'Use a dict with an outer key of "countries" which contains a list of countries. '
    "Each country should have the key `name` and `population`",
    include_names=["my_parser"],
):
    print(event)
    max_events += 1
    if max_events > 10:
        # Truncate output
        print("...")
        break
```

```output
{'event': 'on_parser_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'my_parser', 'tags': ['seq:step:2'], 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'metadata': {}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
...
```

#### By Type[â€‹](#by-type "Direct link to By Type")

```python
chain = model.with_config({"run_name": "model"}) | JsonOutputParser().with_config(
    {"run_name": "my_parser"}
)

max_events = 0
async for event in chain.astream_events(
    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`',
    include_types=["chat_model"],
):
    print(event)
    max_events += 1
    if max_events > 10:
        # Truncate output
        print("...")
        break
```

```output
{'event': 'on_chat_model_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'model', 'tags': ['seq:step:1'], 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c', usage_metadata={'input_tokens': 56, 'output_tokens': 1, 'total_tokens': 57, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\n  "countries', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='": [\n    ', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{\n      "', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='name": "France', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='",\n      "', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='population": 67', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='413', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='000\n    },', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
...
```

#### By Tags[â€‹](#by-tags "Direct link to By Tags")

caution

Tags are inherited by child components of a given runnable.

If you're using tags to filter, make sure that this is what you want.

```python
chain = (model | JsonOutputParser()).with_config({"tags": ["my_chain"]})

max_events = 0
async for event in chain.astream_events(
    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`',
    include_tags=["my_chain"],
):
    print(event)
    max_events += 1
    if max_events > 10:
        # Truncate output
        print("...")
        break
```

```output
{'event': 'on_chain_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'RunnableSequence', 'tags': ['my_chain'], 'run_id': '58d1302e-36ce-4df7-a3cb-47cb73d57e44', 'metadata': {}, 'parent_ids': []}
{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`', additional_kwargs={}, response_metadata={})]]}}, 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b', usage_metadata={'input_tokens': 56, 'output_tokens': 1, 'total_tokens': 57, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_parser_start', 'data': {}, 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'run_id': '75604c84-e1e6-494a-8b2a-950f45d932e8', 'metadata': {}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b')}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_parser_stream', 'run_id': '75604c84-e1e6-494a-8b2a-950f45d932e8', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_chain_stream', 'run_id': '58d1302e-36ce-4df7-a3cb-47cb73d57e44', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\n  "countries', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b')}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='": [\n    ', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b')}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_parser_stream', 'run_id': '75604c84-e1e6-494a-8b2a-950f45d932e8', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_chain_stream', 'run_id': '58d1302e-36ce-4df7-a3cb-47cb73d57e44', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': []}
...
```

### Non-streaming components[â€‹](#non-streaming-components-1 "Direct link to Non-streaming components")

Remember how some components don't stream well because they don't operate on **input streams**?

While such components can break streaming of the final output when using `astream`, `astream_events` will still yield streaming events from intermediate steps that support streaming!

```python
# Function that does not support streaming.
# It operates on the finalizes inputs rather than
# operating on the input stream.
def _extract_country_names(inputs):
    """A function that does not operates on input streams and breaks streaming."""
    if not isinstance(inputs, dict):
        return ""

    if "countries" not in inputs:
        return ""

    countries = inputs["countries"]

    if not isinstance(countries, list):
        return ""

    country_names = [
        country.get("name") for country in countries if isinstance(country, dict)
    ]
    return country_names


chain = (
    model | JsonOutputParser() | _extract_country_names
)  # This parser only works with OpenAI right now
```

As expected, the `astream` API doesn't work correctly because `_extract_country_names` doesn't operate on streams.

```python
async for chunk in chain.astream(
    "output a list of the countries france, spain and japan and their populations in JSON format. "
    'Use a dict with an outer key of "countries" which contains a list of countries. '
    "Each country should have the key `name` and `population`",
):
    print(chunk, flush=True)
```

```output
['France', 'Spain', 'Japan']
```

Now, let's confirm that with astream\_events we're still seeing streaming output from the model and the parser.

```python
num_events = 0

async for event in chain.astream_events(
    "output a list of the countries france, spain and japan and their populations in JSON format. "
    'Use a dict with an outer key of "countries" which contains a list of countries. '
    "Each country should have the key `name` and `population`",
):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(
            f"Chat model chunk: {repr(event['data']['chunk'].content)}",
            flush=True,
        )
    if kind == "on_parser_stream":
        print(f"Parser chunk: {event['data']['chunk']}", flush=True)
    num_events += 1
    if num_events > 30:
        # Truncate the output
        print("...")
        break
```

```output
Chat model chunk: ''
Chat model chunk: '{'
Parser chunk: {}
Chat model chunk: '\n  "countries'
Chat model chunk: '": [\n    '
Parser chunk: {'countries': []}
Chat model chunk: '{\n      "'
Parser chunk: {'countries': [{}]}
Chat model chunk: 'name": "France'
Parser chunk: {'countries': [{'name': 'France'}]}
Chat model chunk: '",\n      "'
Chat model chunk: 'population": 67'
Parser chunk: {'countries': [{'name': 'France', 'population': 67}]}
Chat model chunk: '413'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413}]}
Chat model chunk: '000\n    },'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}]}
Chat model chunk: '\n    {'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {}]}
Chat model chunk: '\n      "name":'
Chat model chunk: ' "Spain",'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}
Chat model chunk: '\n      "population":'
Chat model chunk: ' 47'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}
Chat model chunk: '351'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351}]}
...
```

### Propagating Callbacks[â€‹](#propagating-callbacks "Direct link to Propagating Callbacks")

caution

If you're using invoking runnables inside your tools, you need to propagate callbacks to the runnable; otherwise, no stream events will be generated.

note

When using `RunnableLambdas` or `@chain` decorator, callbacks are propagated automatically behind the scenes.

```python
from langchain_core.runnables import RunnableLambda
from langchain_core.tools import tool


def reverse_word(word: str):
    return word[::-1]


reverse_word = RunnableLambda(reverse_word)


@tool
def bad_tool(word: str):
    """Custom tool that doesn't propagate callbacks."""
    return reverse_word.invoke(word)


async for event in bad_tool.astream_events("hello"):
    print(event)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

```output
{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'bad_tool', 'tags': [], 'run_id': 'ea900472-a8f7-425d-b627-facdef936ee8', 'metadata': {}}
{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '77b01284-0515-48f4-8d7c-eb27c1882f86', 'metadata': {}}
{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '77b01284-0515-48f4-8d7c-eb27c1882f86', 'name': 'reverse_word', 'tags': [], 'metadata': {}}
{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'ea900472-a8f7-425d-b627-facdef936ee8', 'name': 'bad_tool', 'tags': [], 'metadata': {}}
```

Here's a re-implementation that does propagate callbacks correctly. You'll notice that now we're getting events from the `reverse_word` runnable as well.

```python
@tool
def correct_tool(word: str, callbacks):
    """A tool that correctly propagates callbacks."""
    return reverse_word.invoke(word, {"callbacks": callbacks})


async for event in correct_tool.astream_events("hello"):
    print(event)
```

```output
{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'correct_tool', 'tags': [], 'run_id': 'd5ea83b9-9278-49cc-9f1d-aa302d671040', 'metadata': {}}
{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '44dafbf4-2f87-412b-ae0e-9f71713810df', 'metadata': {}}
{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '44dafbf4-2f87-412b-ae0e-9f71713810df', 'name': 'reverse_word', 'tags': [], 'metadata': {}}
{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'd5ea83b9-9278-49cc-9f1d-aa302d671040', 'name': 'correct_tool', 'tags': [], 'metadata': {}}
```

If you're invoking runnables from within Runnable Lambdas or `@chains`, then callbacks will be passed automatically on your behalf.

```python
from langchain_core.runnables import RunnableLambda


async def reverse_and_double(word: str):
    return await reverse_word.ainvoke(word) * 2


reverse_and_double = RunnableLambda(reverse_and_double)

await reverse_and_double.ainvoke("1234")

async for event in reverse_and_double.astream_events("1234"):
    print(event)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```output
{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'metadata': {}}
{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': '5cf26fc8-840b-4642-98ed-623dda28707a', 'metadata': {}}
{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': '5cf26fc8-840b-4642-98ed-623dda28707a', 'name': 'reverse_word', 'tags': [], 'metadata': {}}
{'event': 'on_chain_stream', 'data': {'chunk': '43214321'}, 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}
{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}
```

And with the `@chain` decorator:

```python
from langchain_core.runnables import chain


@chain
async def reverse_and_double(word: str):
    return await reverse_word.ainvoke(word) * 2


await reverse_and_double.ainvoke("1234")

async for event in reverse_and_double.astream_events("1234"):
    print(event)
```

**API Reference:**[chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)

```output
{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'metadata': {}}
{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': '64fc99f0-5d7d-442b-b4f5-4537129f67d1', 'metadata': {}}
{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': '64fc99f0-5d7d-442b-b4f5-4537129f67d1', 'name': 'reverse_word', 'tags': [], 'metadata': {}}
{'event': 'on_chain_stream', 'data': {'chunk': '43214321'}, 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}
{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

Now you've learned some ways to stream both final outputs and internal steps with LangChain.

To learn more, check out the other how-to guides in this section, or the [conceptual guide on Langchain Expression Language](/docs/concepts/lcel/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/streaming.ipynb)

* * *


- [Using Stream](#using-stream)
  
  - [LLMs and Chat Models](#llms-and-chat-models)
  - [Chains](#chains)
  - [Working with Input Streams](#working-with-input-streams)
  - [Non-streaming components](#non-streaming-components)
- [Using Stream Events](#using-stream-events)
  
  - [Event Reference](#event-reference)
  - [Chat Model](#chat-model)
  - [Chain](#chain)
  - [Filtering Events](#filtering-events)
  - [Non-streaming components](#non-streaming-components-1)
  - [Propagating Callbacks](#propagating-callbacks)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/sql_prompting.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/sql_prompting.ipynb)

# How to better prompt when doing SQL question-answering

In this guide we'll go over prompting strategies to improve SQL query generation using [create\_sql\_query\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.sql_database.query.create_sql_query_chain.html). We'll largely focus on methods for getting relevant database-specific information in your prompt.

We will cover:

- How the dialect of the LangChain [SQLDatabase](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.sql_database.SQLDatabase.html) impacts the prompt of the chain;
- How to format schema information into the prompt using `SQLDatabase.get_context`;
- How to build and select [few-shot examples](/docs/concepts/few_shot_prompting/) to assist the model.

## Setup[â€‹](#setup "Direct link to Setup")

First, get required packages and set environment variables:

```python
%pip install --upgrade --quiet  langchain langchain-community langchain-experimental langchain-openai
```

```python
# Uncomment the below to use LangSmith. Not required.
# import os
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
# os.environ["LANGSMITH_TRACING"] = "true"
```

The below example will use a SQLite connection with Chinook database. Follow [these installation steps](https://database.guide/2-sample-databases-sqlite/) to create `Chinook.db` in the same directory as this notebook:

- Save [this file](https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql) as `Chinook_Sqlite.sql`
- Run `sqlite3 Chinook.db`
- Run `.read Chinook_Sqlite.sql`
- Test `SELECT * FROM Artist LIMIT 10;`

Now, `Chinook.db` is in our directory and we can interface with it using the SQLAlchemy-driven `SQLDatabase` class:

```python
from langchain_community.utilities import SQLDatabase

db = SQLDatabase.from_uri("sqlite:///Chinook.db", sample_rows_in_table_info=3)
print(db.dialect)
print(db.get_usable_table_names())
print(db.run("SELECT * FROM Artist LIMIT 10;"))
```

**API Reference:**[SQLDatabase](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.sql_database.SQLDatabase.html)

```output
sqlite
['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']
[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'AntÃ´nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]
```

## Dialect-specific prompting[â€‹](#dialect-specific-prompting "Direct link to Dialect-specific prompting")

One of the simplest things we can do is make our prompt specific to the SQL dialect we're using. When using the built-in [create\_sql\_query\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.sql_database.query.create_sql_query_chain.html) and [SQLDatabase](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.sql_database.SQLDatabase.html), this is handled for you for any of the following dialects:

```python
from langchain.chains.sql_database.prompt import SQL_PROMPTS

list(SQL_PROMPTS)
```

```output
['crate',
 'duckdb',
 'googlesql',
 'mssql',
 'mysql',
 'mariadb',
 'oracle',
 'postgresql',
 'sqlite',
 'clickhouse',
 'prestodb']
```

For example, using our current DB we can see that we'll get a SQLite-specific prompt.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
from langchain.chains import create_sql_query_chain

chain = create_sql_query_chain(llm, db)
chain.get_prompts()[0].pretty_print()
```

**API Reference:**[create\_sql\_query\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.sql_database.query.create_sql_query_chain.html)

```output
You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.
Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.
Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (") to denote them as delimited identifiers.
Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.
Pay attention to use date('now') function to get the current date, if the question involves "today".

Use the following format:

Question: Question here
SQLQuery: SQL Query to run
SQLResult: Result of the SQLQuery
Answer: Final answer here

Only use the following tables:
[33;1m[1;3m{table_info}[0m

Question: [33;1m[1;3m{input}[0m
```

## Table definitions and example rows[â€‹](#table-definitions-and-example-rows "Direct link to Table definitions and example rows")

In most SQL chains, we'll need to feed the model at least part of the database schema. Without this it won't be able to write valid queries. Our database comes with some convenience methods to give us the relevant context. Specifically, we can get the table names, their schemas, and a sample of rows from each table.

Here we will use `SQLDatabase.get_context`, which provides available tables and their schemas:

```python
context = db.get_context()
print(list(context))
print(context["table_info"])
```

```output
['table_info', 'table_names']

CREATE TABLE "Album" (
	"AlbumId" INTEGER NOT NULL, 
	"Title" NVARCHAR(160) NOT NULL, 
	"ArtistId" INTEGER NOT NULL, 
	PRIMARY KEY ("AlbumId"), 
	FOREIGN KEY("ArtistId") REFERENCES "Artist" ("ArtistId")
)

/*
3 rows from Album table:
AlbumId	Title	ArtistId
1	For Those About To Rock We Salute You	1
2	Balls to the Wall	2
3	Restless and Wild	2
*/


CREATE TABLE "Artist" (
	"ArtistId" INTEGER NOT NULL, 
	"Name" NVARCHAR(120), 
	PRIMARY KEY ("ArtistId")
)

/*
3 rows from Artist table:
ArtistId	Name
1	AC/DC
2	Accept
3	Aerosmith
*/


CREATE TABLE "Customer" (
	"CustomerId" INTEGER NOT NULL, 
	"FirstName" NVARCHAR(40) NOT NULL, 
	"LastName" NVARCHAR(20) NOT NULL, 
	"Company" NVARCHAR(80), 
	"Address" NVARCHAR(70), 
	"City" NVARCHAR(40), 
	"State" NVARCHAR(40), 
	"Country" NVARCHAR(40), 
	"PostalCode" NVARCHAR(10), 
	"Phone" NVARCHAR(24), 
	"Fax" NVARCHAR(24), 
	"Email" NVARCHAR(60) NOT NULL, 
	"SupportRepId" INTEGER, 
	PRIMARY KEY ("CustomerId"), 
	FOREIGN KEY("SupportRepId") REFERENCES "Employee" ("EmployeeId")
)

/*
3 rows from Customer table:
CustomerId	FirstName	LastName	Company	Address	City	State	Country	PostalCode	Phone	Fax	Email	SupportRepId
1	LuÃ­s	GonÃ§alves	Embraer - Empresa Brasileira de AeronÃ¡utica S.A.	Av. Brigadeiro Faria Lima, 2170	SÃ£o JosÃ© dos Campos	SP	Brazil	12227-000	+55 (12) 3923-5555	+55 (12) 3923-5566	luisg@embraer.com.br	3
2	Leonie	KÃ¶hler	None	Theodor-Heuss-StraÃŸe 34	Stuttgart	None	Germany	70174	+49 0711 2842222	None	leonekohler@surfeu.de	5
3	FranÃ§ois	Tremblay	None	1498 rue BÃ©langer	MontrÃ©al	QC	Canada	H2G 1A7	+1 (514) 721-4711	None	ftremblay@gmail.com	3
*/


CREATE TABLE "Employee" (
	"EmployeeId" INTEGER NOT NULL, 
	"LastName" NVARCHAR(20) NOT NULL, 
	"FirstName" NVARCHAR(20) NOT NULL, 
	"Title" NVARCHAR(30), 
	"ReportsTo" INTEGER, 
	"BirthDate" DATETIME, 
	"HireDate" DATETIME, 
	"Address" NVARCHAR(70), 
	"City" NVARCHAR(40), 
	"State" NVARCHAR(40), 
	"Country" NVARCHAR(40), 
	"PostalCode" NVARCHAR(10), 
	"Phone" NVARCHAR(24), 
	"Fax" NVARCHAR(24), 
	"Email" NVARCHAR(60), 
	PRIMARY KEY ("EmployeeId"), 
	FOREIGN KEY("ReportsTo") REFERENCES "Employee" ("EmployeeId")
)

/*
3 rows from Employee table:
EmployeeId	LastName	FirstName	Title	ReportsTo	BirthDate	HireDate	Address	City	State	Country	PostalCode	Phone	Fax	Email
1	Adams	Andrew	General Manager	None	1962-02-18 00:00:00	2002-08-14 00:00:00	11120 Jasper Ave NW	Edmonton	AB	Canada	T5K 2N1	+1 (780) 428-9482	+1 (780) 428-3457	andrew@chinookcorp.com
2	Edwards	Nancy	Sales Manager	1	1958-12-08 00:00:00	2002-05-01 00:00:00	825 8 Ave SW	Calgary	AB	Canada	T2P 2T3	+1 (403) 262-3443	+1 (403) 262-3322	nancy@chinookcorp.com
3	Peacock	Jane	Sales Support Agent	2	1973-08-29 00:00:00	2002-04-01 00:00:00	1111 6 Ave SW	Calgary	AB	Canada	T2P 5M5	+1 (403) 262-3443	+1 (403) 262-6712	jane@chinookcorp.com
*/


CREATE TABLE "Genre" (
	"GenreId" INTEGER NOT NULL, 
	"Name" NVARCHAR(120), 
	PRIMARY KEY ("GenreId")
)

/*
3 rows from Genre table:
GenreId	Name
1	Rock
2	Jazz
3	Metal
*/


CREATE TABLE "Invoice" (
	"InvoiceId" INTEGER NOT NULL, 
	"CustomerId" INTEGER NOT NULL, 
	"InvoiceDate" DATETIME NOT NULL, 
	"BillingAddress" NVARCHAR(70), 
	"BillingCity" NVARCHAR(40), 
	"BillingState" NVARCHAR(40), 
	"BillingCountry" NVARCHAR(40), 
	"BillingPostalCode" NVARCHAR(10), 
	"Total" NUMERIC(10, 2) NOT NULL, 
	PRIMARY KEY ("InvoiceId"), 
	FOREIGN KEY("CustomerId") REFERENCES "Customer" ("CustomerId")
)

/*
3 rows from Invoice table:
InvoiceId	CustomerId	InvoiceDate	BillingAddress	BillingCity	BillingState	BillingCountry	BillingPostalCode	Total
1	2	2021-01-01 00:00:00	Theodor-Heuss-StraÃŸe 34	Stuttgart	None	Germany	70174	1.98
2	4	2021-01-02 00:00:00	UllevÃ¥lsveien 14	Oslo	None	Norway	0171	3.96
3	8	2021-01-03 00:00:00	GrÃ©trystraat 63	Brussels	None	Belgium	1000	5.94
*/


CREATE TABLE "InvoiceLine" (
	"InvoiceLineId" INTEGER NOT NULL, 
	"InvoiceId" INTEGER NOT NULL, 
	"TrackId" INTEGER NOT NULL, 
	"UnitPrice" NUMERIC(10, 2) NOT NULL, 
	"Quantity" INTEGER NOT NULL, 
	PRIMARY KEY ("InvoiceLineId"), 
	FOREIGN KEY("TrackId") REFERENCES "Track" ("TrackId"), 
	FOREIGN KEY("InvoiceId") REFERENCES "Invoice" ("InvoiceId")
)

/*
3 rows from InvoiceLine table:
InvoiceLineId	InvoiceId	TrackId	UnitPrice	Quantity
1	1	2	0.99	1
2	1	4	0.99	1
3	2	6	0.99	1
*/


CREATE TABLE "MediaType" (
	"MediaTypeId" INTEGER NOT NULL, 
	"Name" NVARCHAR(120), 
	PRIMARY KEY ("MediaTypeId")
)

/*
3 rows from MediaType table:
MediaTypeId	Name
1	MPEG audio file
2	Protected AAC audio file
3	Protected MPEG-4 video file
*/


CREATE TABLE "Playlist" (
	"PlaylistId" INTEGER NOT NULL, 
	"Name" NVARCHAR(120), 
	PRIMARY KEY ("PlaylistId")
)

/*
3 rows from Playlist table:
PlaylistId	Name
1	Music
2	Movies
3	TV Shows
*/


CREATE TABLE "PlaylistTrack" (
	"PlaylistId" INTEGER NOT NULL, 
	"TrackId" INTEGER NOT NULL, 
	PRIMARY KEY ("PlaylistId", "TrackId"), 
	FOREIGN KEY("TrackId") REFERENCES "Track" ("TrackId"), 
	FOREIGN KEY("PlaylistId") REFERENCES "Playlist" ("PlaylistId")
)

/*
3 rows from PlaylistTrack table:
PlaylistId	TrackId
1	3402
1	3389
1	3390
*/


CREATE TABLE "Track" (
	"TrackId" INTEGER NOT NULL, 
	"Name" NVARCHAR(200) NOT NULL, 
	"AlbumId" INTEGER, 
	"MediaTypeId" INTEGER NOT NULL, 
	"GenreId" INTEGER, 
	"Composer" NVARCHAR(220), 
	"Milliseconds" INTEGER NOT NULL, 
	"Bytes" INTEGER, 
	"UnitPrice" NUMERIC(10, 2) NOT NULL, 
	PRIMARY KEY ("TrackId"), 
	FOREIGN KEY("MediaTypeId") REFERENCES "MediaType" ("MediaTypeId"), 
	FOREIGN KEY("GenreId") REFERENCES "Genre" ("GenreId"), 
	FOREIGN KEY("AlbumId") REFERENCES "Album" ("AlbumId")
)

/*
3 rows from Track table:
TrackId	Name	AlbumId	MediaTypeId	GenreId	Composer	Milliseconds	Bytes	UnitPrice
1	For Those About To Rock (We Salute You)	1	1	1	Angus Young, Malcolm Young, Brian Johnson	343719	11170334	0.99
2	Balls to the Wall	2	2	1	U. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann	342562	5510424	0.99
3	Fast As a Shark	3	2	1	F. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman	230619	3990994	0.99
*/
```

When we don't have too many, or too wide of, tables, we can just insert the entirety of this information in our prompt:

```python
prompt_with_context = chain.get_prompts()[0].partial(table_info=context["table_info"])
print(prompt_with_context.pretty_repr()[:1500])
```

```output
You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.
Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.
Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (") to denote them as delimited identifiers.
Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.
Pay attention to use date('now') function to get the current date, if the question involves "today".

Use the following format:

Question: Question here
SQLQuery: SQL Query to run
SQLResult: Result of the SQLQuery
Answer: Final answer here

Only use the following tables:

CREATE TABLE "Album" (
	"AlbumId" INTEGER NOT NULL, 
	"Title" NVARCHAR(160) NOT NULL, 
	"ArtistId" INTEGER NOT NULL, 
	PRIMARY KEY ("AlbumId"), 
	FOREIGN KEY("ArtistId") REFERENCES "Artist" ("ArtistId")
)

/*
3 rows from Album table:
AlbumId	Title	ArtistId
1	For Those About To Rock We Salute You	1
2	Balls to the Wall	2
3	Restless and Wild	2
*/


CREATE TABLE "Artist" (
	"ArtistId" INTEGER NOT NULL, 
	"Name" NVARCHAR(120)
```

When we do have database schemas that are too large to fit into our model's context window, we'll need to come up with ways of inserting only the relevant table definitions into the prompt based on the user input. For more on this head to the [Many tables, wide tables, high-cardinality feature](/docs/how_to/sql_large_db/) guide.

## Few-shot examples[â€‹](#few-shot-examples "Direct link to Few-shot examples")

Including examples of natural language questions being converted to valid SQL queries against our database in the prompt will often improve model performance, especially for complex queries.

Let's say we have the following examples:

```python
examples = [
    {"input": "List all artists.", "query": "SELECT * FROM Artist;"},
    {
        "input": "Find all albums for the artist 'AC/DC'.",
        "query": "SELECT * FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'AC/DC');",
    },
    {
        "input": "List all tracks in the 'Rock' genre.",
        "query": "SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');",
    },
    {
        "input": "Find the total duration of all tracks.",
        "query": "SELECT SUM(Milliseconds) FROM Track;",
    },
    {
        "input": "List all customers from Canada.",
        "query": "SELECT * FROM Customer WHERE Country = 'Canada';",
    },
    {
        "input": "How many tracks are there in the album with ID 5?",
        "query": "SELECT COUNT(*) FROM Track WHERE AlbumId = 5;",
    },
    {
        "input": "Find the total number of invoices.",
        "query": "SELECT COUNT(*) FROM Invoice;",
    },
    {
        "input": "List all tracks that are longer than 5 minutes.",
        "query": "SELECT * FROM Track WHERE Milliseconds > 300000;",
    },
    {
        "input": "Who are the top 5 customers by total purchase?",
        "query": "SELECT CustomerId, SUM(Total) AS TotalPurchase FROM Invoice GROUP BY CustomerId ORDER BY TotalPurchase DESC LIMIT 5;",
    },
    {
        "input": "Which albums are from the year 2000?",
        "query": "SELECT * FROM Album WHERE strftime('%Y', ReleaseDate) = '2000';",
    },
    {
        "input": "How many employees are there",
        "query": 'SELECT COUNT(*) FROM "Employee"',
    },
]
```

We can create a few-shot prompt with them like so:

```python
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

example_prompt = PromptTemplate.from_template("User input: {input}\nSQL query: {query}")
prompt = FewShotPromptTemplate(
    examples=examples[:5],
    example_prompt=example_prompt,
    prefix="You are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than {top_k} rows.\n\nHere is the relevant table info: {table_info}\n\nBelow are a number of examples of questions and their corresponding SQL queries.",
    suffix="User input: {input}\nSQL query: ",
    input_variables=["input", "top_k", "table_info"],
)
```

**API Reference:**[FewShotPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

```python
print(prompt.format(input="How many artists are there?", top_k=3, table_info="foo"))
```

```output
You are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than 3 rows.

Here is the relevant table info: foo

Below are a number of examples of questions and their corresponding SQL queries.

User input: List all artists.
SQL query: SELECT * FROM Artist;

User input: Find all albums for the artist 'AC/DC'.
SQL query: SELECT * FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'AC/DC');

User input: List all tracks in the 'Rock' genre.
SQL query: SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');

User input: Find the total duration of all tracks.
SQL query: SELECT SUM(Milliseconds) FROM Track;

User input: List all customers from Canada.
SQL query: SELECT * FROM Customer WHERE Country = 'Canada';

User input: How many artists are there?
SQL query:
```

## Dynamic few-shot examples[â€‹](#dynamic-few-shot-examples "Direct link to Dynamic few-shot examples")

If we have enough examples, we may want to only include the most relevant ones in the prompt, either because they don't fit in the model's context window or because the long tail of examples distracts the model. And specifically, given any input we want to include the examples most relevant to that input.

We can do just this using an ExampleSelector. In this case we'll use a [SemanticSimilarityExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector.html), which will store the examples in the vector database of our choosing. At runtime it will perform a similarity search between the input and our examples, and return the most semantically similar ones.

We default to OpenAI embeddings here, but you can swap them out for the model provider of your choice.

```python
from langchain_community.vectorstores import FAISS
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings

example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,
    OpenAIEmbeddings(),
    FAISS,
    k=5,
    input_keys=["input"],
)
```

**API Reference:**[FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [SemanticSimilarityExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```python
example_selector.select_examples({"input": "how many artists are there?"})
```

```output
[{'input': 'List all artists.', 'query': 'SELECT * FROM Artist;'},
 {'input': 'How many employees are there',
  'query': 'SELECT COUNT(*) FROM "Employee"'},
 {'input': 'How many tracks are there in the album with ID 5?',
  'query': 'SELECT COUNT(*) FROM Track WHERE AlbumId = 5;'},
 {'input': 'Which albums are from the year 2000?',
  'query': "SELECT * FROM Album WHERE strftime('%Y', ReleaseDate) = '2000';"},
 {'input': "List all tracks in the 'Rock' genre.",
  'query': "SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');"}]
```

To use it, we can pass the ExampleSelector directly in to our FewShotPromptTemplate:

```python
prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="You are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than {top_k} rows.\n\nHere is the relevant table info: {table_info}\n\nBelow are a number of examples of questions and their corresponding SQL queries.",
    suffix="User input: {input}\nSQL query: ",
    input_variables=["input", "top_k", "table_info"],
)
```

```python
print(prompt.format(input="how many artists are there?", top_k=3, table_info="foo"))
```

```output
You are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than 3 rows.

Here is the relevant table info: foo

Below are a number of examples of questions and their corresponding SQL queries.

User input: List all artists.
SQL query: SELECT * FROM Artist;

User input: How many employees are there
SQL query: SELECT COUNT(*) FROM "Employee"

User input: How many tracks are there in the album with ID 5?
SQL query: SELECT COUNT(*) FROM Track WHERE AlbumId = 5;

User input: Which albums are from the year 2000?
SQL query: SELECT * FROM Album WHERE strftime('%Y', ReleaseDate) = '2000';

User input: List all tracks in the 'Rock' genre.
SQL query: SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');

User input: how many artists are there?
SQL query:
```

Trying it out, we see that the model identifies the relevant table:

```python
chain = create_sql_query_chain(llm, db, prompt)
chain.invoke({"question": "how many artists are there?"})
```

```output
'SELECT COUNT(*) FROM Artist;'
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/sql_prompting.ipynb)

* * *


- [Setup](#setup)
- [Dialect-specific prompting](#dialect-specific-prompting)
- [Table definitions and example rows](#table-definitions-and-example-rows)
- [Few-shot examples](#few-shot-examples)
- [Dynamic few-shot examples](#dynamic-few-shot-examples)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/few_shot_examples.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/few_shot_examples.ipynb)

# How to use few shot examples

Prerequisites

This guide assumes familiarity with the following concepts:

- [Prompt templates](/docs/concepts/prompt_templates/)
- [Example selectors](/docs/concepts/example_selectors/)
- [LLMs](/docs/concepts/text_llms/)
- [Vectorstores](/docs/concepts/vectorstores/)

In this guide, we'll learn how to create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called [few-shotting](/docs/concepts/few_shot_prompting/), and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.

A few-shot prompt template can be constructed from either a set of examples, or from an [Example Selector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.base.BaseExampleSelector.html) class responsible for choosing a subset of examples from the defined set.

This guide will cover few-shotting with string prompt templates. For a guide on few-shotting with chat messages for chat models, see [here](/docs/how_to/few_shot_examples_chat/).

## Create a formatter for the few-shot examples[â€‹](#create-a-formatter-for-the-few-shot-examples "Direct link to Create a formatter for the few-shot examples")

Configure a formatter that will format the few-shot examples into a string. This formatter should be a `PromptTemplate` object.

```python
from langchain_core.prompts import PromptTemplate

example_prompt = PromptTemplate.from_template("Question: {question}\n{answer}")
```

**API Reference:**[PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

## Creating the example set[â€‹](#creating-the-example-set "Direct link to Creating the example set")

Next, we'll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.

```python
examples = [
    {
        "question": "Who lived longer, Muhammad Ali or Alan Turing?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: How old was Muhammad Ali when he died?
Intermediate answer: Muhammad Ali was 74 years old when he died.
Follow up: How old was Alan Turing when he died?
Intermediate answer: Alan Turing was 41 years old when he died.
So the final answer is: Muhammad Ali
""",
    },
    {
        "question": "When was the founder of craigslist born?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: Who was the founder of craigslist?
Intermediate answer: Craigslist was founded by Craig Newmark.
Follow up: When was Craig Newmark born?
Intermediate answer: Craig Newmark was born on December 6, 1952.
So the final answer is: December 6, 1952
""",
    },
    {
        "question": "Who was the maternal grandfather of George Washington?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball
""",
    },
    {
        "question": "Are both the directors of Jaws and Casino Royale from the same country?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: Who is the director of Jaws?
Intermediate Answer: The director of Jaws is Steven Spielberg.
Follow up: Where is Steven Spielberg from?
Intermediate Answer: The United States.
Follow up: Who is the director of Casino Royale?
Intermediate Answer: The director of Casino Royale is Martin Campbell.
Follow up: Where is Martin Campbell from?
Intermediate Answer: New Zealand.
So the final answer is: No
""",
    },
]
```

Let's test the formatting prompt with one of our examples:

```python
print(example_prompt.invoke(examples[0]).to_string())
```

```output
Question: Who lived longer, Muhammad Ali or Alan Turing?

Are follow up questions needed here: Yes.
Follow up: How old was Muhammad Ali when he died?
Intermediate answer: Muhammad Ali was 74 years old when he died.
Follow up: How old was Alan Turing when he died?
Intermediate answer: Alan Turing was 41 years old when he died.
So the final answer is: Muhammad Ali
```

### Pass the examples and formatter to `FewShotPromptTemplate`[â€‹](#pass-the-examples-and-formatter-to-fewshotprompttemplate "Direct link to pass-the-examples-and-formatter-to-fewshotprompttemplate")

Finally, create a [`FewShotPromptTemplate`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html) object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this `FewShotPromptTemplate` is formatted, it formats the passed examples using the `example_prompt`, then and adds them to the final prompt before `suffix`:

```python
from langchain_core.prompts import FewShotPromptTemplate

prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    suffix="Question: {input}",
    input_variables=["input"],
)

print(
    prompt.invoke({"input": "Who was the father of Mary Ball Washington?"}).to_string()
)
```

**API Reference:**[FewShotPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html)

```output
Question: Who lived longer, Muhammad Ali or Alan Turing?

Are follow up questions needed here: Yes.
Follow up: How old was Muhammad Ali when he died?
Intermediate answer: Muhammad Ali was 74 years old when he died.
Follow up: How old was Alan Turing when he died?
Intermediate answer: Alan Turing was 41 years old when he died.
So the final answer is: Muhammad Ali


Question: When was the founder of craigslist born?

Are follow up questions needed here: Yes.
Follow up: Who was the founder of craigslist?
Intermediate answer: Craigslist was founded by Craig Newmark.
Follow up: When was Craig Newmark born?
Intermediate answer: Craig Newmark was born on December 6, 1952.
So the final answer is: December 6, 1952


Question: Who was the maternal grandfather of George Washington?

Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball


Question: Are both the directors of Jaws and Casino Royale from the same country?

Are follow up questions needed here: Yes.
Follow up: Who is the director of Jaws?
Intermediate Answer: The director of Jaws is Steven Spielberg.
Follow up: Where is Steven Spielberg from?
Intermediate Answer: The United States.
Follow up: Who is the director of Casino Royale?
Intermediate Answer: The director of Casino Royale is Martin Campbell.
Follow up: Where is Martin Campbell from?
Intermediate Answer: New Zealand.
So the final answer is: No


Question: Who was the father of Mary Ball Washington?
```

By providing the model with examples like this, we can guide the model to a better response.

## Using an example selector[â€‹](#using-an-example-selector "Direct link to Using an example selector")

We will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the `FewShotPromptTemplate` object, we will feed them into an implementation of `ExampleSelector` called [`SemanticSimilarityExampleSelector`](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector.html) instance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.

To show what it looks like, let's initialize an instance and call it in isolation:

```python
from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings

example_selector = SemanticSimilarityExampleSelector.from_examples(
    # This is the list of examples available to select from.
    examples,
    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.
    Chroma,
    # This is the number of examples to produce.
    k=1,
)

# Select the most similar example to the input.
question = "Who was the father of Mary Ball Washington?"
selected_examples = example_selector.select_examples({"question": question})
print(f"Examples most similar to the input: {question}")
for example in selected_examples:
    print("\n")
    for k, v in example.items():
        print(f"{k}: {v}")
```

**API Reference:**[SemanticSimilarityExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```output
Examples most similar to the input: Who was the father of Mary Ball Washington?


answer: 
Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball

question: Who was the maternal grandfather of George Washington?
```

Now, let's create a `FewShotPromptTemplate` object. This object takes in the example selector and the formatter prompt for the few-shot examples.

```python
prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    suffix="Question: {input}",
    input_variables=["input"],
)

print(
    prompt.invoke({"input": "Who was the father of Mary Ball Washington?"}).to_string()
)
```

```output
Question: Who was the maternal grandfather of George Washington?

Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball


Question: Who was the father of Mary Ball Washington?
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to add few-shot examples to your prompts.

Next, check out the other how-to guides on prompt templates in this section, the related how-to guide on [few shotting with chat models](/docs/how_to/few_shot_examples_chat/), or the other [example selector how-to guides](/docs/how_to/example_selectors/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/few_shot_examples.ipynb)

* * *


- [Create a formatter for the few-shot examples](#create-a-formatter-for-the-few-shot-examples)
- [Creating the example set](#creating-the-example-set)
  
  - [Pass the examples and formatter to `FewShotPromptTemplate`](#pass-the-examples-and-formatter-to-fewshotprompttemplate)
- [Using an example selector](#using-an-example-selector)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/response_metadata.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/response_metadata.ipynb)

# Response metadata

Many model providers include some metadata in their chat generation [responses](/docs/concepts/messages/#aimessage). This metadata can be accessed via the `AIMessage.response_metadata: Dict` attribute. Depending on the model provider and model configuration, this can contain information like [token counts](/docs/how_to/chat_token_usage_tracking/), [logprobs](/docs/how_to/logprobs/), and more.

Here's what the response metadata looks like for a few different providers:

## OpenAI[â€‹](#openai "Direct link to OpenAI")

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
msg = llm.invoke("What's the oldest known example of cuneiform")
msg.response_metadata
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
{'token_usage': {'completion_tokens': 110,
  'prompt_tokens': 16,
  'total_tokens': 126,
  'completion_tokens_details': {'accepted_prediction_tokens': 0,
   'audio_tokens': 0,
   'reasoning_tokens': 0,
   'rejected_prediction_tokens': 0},
  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},
 'model_name': 'gpt-4o-mini-2024-07-18',
 'system_fingerprint': 'fp_b8bc95a0ac',
 'id': 'chatcmpl-BDrISvLar6AqcZngBmhajFZXVc2u9',
 'finish_reason': 'stop',
 'logprobs': None}
```

## Anthropic[â€‹](#anthropic "Direct link to Anthropic")

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-5-sonnet-latest")
msg = llm.invoke("What's the oldest known example of cuneiform")
msg.response_metadata
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)

```output
{'id': 'msg_01JHnvPqgERY7MZwrvfkmq52',
 'model': 'claude-3-5-sonnet-20241022',
 'stop_reason': 'end_turn',
 'stop_sequence': None,
 'usage': {'cache_creation_input_tokens': 0,
  'cache_read_input_tokens': 0,
  'input_tokens': 17,
  'output_tokens': 221},
 'model_name': 'claude-3-5-sonnet-20241022'}
```

## Google VertexAI[â€‹](#google-vertexai "Direct link to Google VertexAI")

```python
from langchain_google_vertexai import ChatVertexAI

llm = ChatVertexAI(model="gemini-1.5-flash-001")
msg = llm.invoke("What's the oldest known example of cuneiform")
msg.response_metadata
```

**API Reference:**[ChatVertexAI](https://python.langchain.com/api_reference/google_vertexai/chat_models/langchain_google_vertexai.chat_models.ChatVertexAI.html)

```output
{'is_blocked': False,
 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH',
   'probability_label': 'NEGLIGIBLE',
   'probability_score': 0.046142578125,
   'blocked': False,
   'severity': 'HARM_SEVERITY_NEGLIGIBLE',
   'severity_score': 0.07275390625},
  {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',
   'probability_label': 'NEGLIGIBLE',
   'probability_score': 0.05419921875,
   'blocked': False,
   'severity': 'HARM_SEVERITY_NEGLIGIBLE',
   'severity_score': 0.03955078125},
  {'category': 'HARM_CATEGORY_HARASSMENT',
   'probability_label': 'NEGLIGIBLE',
   'probability_score': 0.083984375,
   'blocked': False,
   'severity': 'HARM_SEVERITY_NEGLIGIBLE',
   'severity_score': 0.029296875},
  {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
   'probability_label': 'NEGLIGIBLE',
   'probability_score': 0.054931640625,
   'blocked': False,
   'severity': 'HARM_SEVERITY_NEGLIGIBLE',
   'severity_score': 0.03466796875}],
 'usage_metadata': {'prompt_token_count': 10,
  'candidates_token_count': 193,
  'total_token_count': 203,
  'prompt_tokens_details': [{'modality': 1, 'token_count': 10}],
  'candidates_tokens_details': [{'modality': 1, 'token_count': 193}],
  'cached_content_token_count': 0,
  'cache_tokens_details': []},
 'finish_reason': 'STOP',
 'avg_logprobs': -0.5702065976790196,
 'model_name': 'gemini-1.5-flash-001'}
```

## Bedrock (Anthropic)[â€‹](#bedrock-anthropic "Direct link to Bedrock (Anthropic)")

```python
from langchain_aws import ChatBedrockConverse

llm = ChatBedrockConverse(model="anthropic.claude-3-sonnet-20240229-v1:0")
msg = llm.invoke("What's the oldest known example of cuneiform")
msg.response_metadata
```

**API Reference:**[ChatBedrockConverse](https://python.langchain.com/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock_converse.ChatBedrockConverse.html)

```output
{'ResponseMetadata': {'RequestId': 'ea0ac2ad-3ad5-4a49-9647-274a0c73ac31',
  'HTTPStatusCode': 200,
  'HTTPHeaders': {'date': 'Sat, 22 Mar 2025 11:27:46 GMT',
   'content-type': 'application/json',
   'content-length': '1660',
   'connection': 'keep-alive',
   'x-amzn-requestid': 'ea0ac2ad-3ad5-4a49-9647-274a0c73ac31'},
  'RetryAttempts': 0},
 'stopReason': 'end_turn',
 'metrics': {'latencyMs': [11044]}}
```

## MistralAI[â€‹](#mistralai "Direct link to MistralAI")

```python
from langchain_mistralai import ChatMistralAI

llm = ChatMistralAI(model="mistral-small-latest")
msg = llm.invoke([("human", "What's the oldest known example of cuneiform")])
msg.response_metadata
```

**API Reference:**[ChatMistralAI](https://python.langchain.com/api_reference/mistralai/chat_models/langchain_mistralai.chat_models.ChatMistralAI.html)

```output
{'token_usage': {'prompt_tokens': 13,
  'total_tokens': 219,
  'completion_tokens': 206},
 'model_name': 'mistral-small-latest',
 'model': 'mistral-small-latest',
 'finish_reason': 'stop'}
```

## Groq[â€‹](#groq "Direct link to Groq")

```python
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-8b-instant")
msg = llm.invoke("What's the oldest known example of cuneiform")
msg.response_metadata
```

**API Reference:**[ChatGroq](https://python.langchain.com/api_reference/groq/chat_models/langchain_groq.chat_models.ChatGroq.html)

```output
{'token_usage': {'completion_tokens': 184,
  'prompt_tokens': 45,
  'total_tokens': 229,
  'completion_time': 0.245333333,
  'prompt_time': 0.002262803,
  'queue_time': 0.19315161,
  'total_time': 0.247596136},
 'model_name': 'llama-3.1-8b-instant',
 'system_fingerprint': 'fp_a56f6eea01',
 'finish_reason': 'stop',
 'logprobs': None}
```

## FireworksAI[â€‹](#fireworksai "Direct link to FireworksAI")

```python
from langchain_fireworks import ChatFireworks

llm = ChatFireworks(model="accounts/fireworks/models/llama-v3p1-70b-instruct")
msg = llm.invoke("What's the oldest known example of cuneiform")
msg.response_metadata
```

**API Reference:**[ChatFireworks](https://python.langchain.com/api_reference/fireworks/chat_models/langchain_fireworks.chat_models.ChatFireworks.html)

```output
{'token_usage': {'prompt_tokens': 25,
  'total_tokens': 352,
  'completion_tokens': 327},
 'model_name': 'accounts/fireworks/models/llama-v3p1-70b-instruct',
 'system_fingerprint': '',
 'finish_reason': 'stop',
 'logprobs': None}
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/response_metadata.ipynb)

* * *


- [OpenAI](#openai)
- [Anthropic](#anthropic)
- [Google VertexAI](#google-vertexai)
- [Bedrock (Anthropic)](#bedrock-anthropic)
- [MistralAI](#mistralai)
- [Groq](#groq)
- [FireworksAI](#fireworksai)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/graph_constructing.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/graph_constructing.ipynb)

# How to construct knowledge graphs

In this guide we'll go over the basic ways of constructing a knowledge graph based on unstructured text. The constructured graph can then be used as knowledge base in a [RAG](/docs/concepts/rag/) application.

## âš ï¸ Security note âš ï¸[â€‹](#%EF%B8%8F-security-note-%EF%B8%8F "Direct link to âš ï¸ Security note âš ï¸")

Constructing knowledge graphs requires executing write access to the database. There are inherent risks in doing this. Make sure that you verify and validate data before importing it. For more on general security best practices, [see here](/docs/security/).

## Architecture[â€‹](#architecture "Direct link to Architecture")

At a high-level, the steps of constructing a knowledge graph from text are:

1. **Extracting structured information from text**: Model is used to extract structured graph information from text.
2. **Storing into graph database**: Storing the extracted structured graph information into a graph database enables downstream RAG applications

## Setup[â€‹](#setup "Direct link to Setup")

First, get required packages and set environment variables. In this example, we will be using Neo4j graph database.

```python
%pip install --upgrade --quiet  langchain langchain-neo4j langchain-openai langchain-experimental neo4j
```

```output

[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m24.0[0m[39;49m -> [0m[32;49m24.3.1[0m
[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m
Note: you may need to restart the kernel to use updated packages.
```

We default to OpenAI models in this guide.

```python
import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

# Uncomment the below to use LangSmith. Not required.
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
# os.environ["LANGSMITH_TRACING"] = "true"
```

```output
 Â·Â·Â·Â·Â·Â·Â·Â·
```

Next, we need to define Neo4j credentials and connection. Follow [these installation steps](https://neo4j.com/docs/operations-manual/current/installation/) to set up a Neo4j database.

```python
import os

from langchain_neo4j import Neo4jGraph

os.environ["NEO4J_URI"] = "bolt://localhost:7687"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "password"

graph = Neo4jGraph(refresh_schema=False)
```

**API Reference:**[Neo4jGraph](https://python.langchain.com/api_reference/neo4j/graphs/langchain_neo4j.graphs.neo4j_graph.Neo4jGraph.html)

## LLM Graph Transformer[â€‹](#llm-graph-transformer "Direct link to LLM Graph Transformer")

Extracting graph data from text enables the transformation of unstructured information into structured formats, facilitating deeper insights and more efficient navigation through complex relationships and patterns. The `LLMGraphTransformer` converts text documents into structured graph documents by leveraging a LLM to parse and categorize entities and their relationships. The selection of the LLM model significantly influences the output by determining the accuracy and nuance of the extracted graph data.

```python
import os

from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(temperature=0, model_name="gpt-4-turbo")

llm_transformer = LLMGraphTransformer(llm=llm)
```

**API Reference:**[LLMGraphTransformer](https://python.langchain.com/api_reference/experimental/graph_transformers/langchain_experimental.graph_transformers.llm.LLMGraphTransformer.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

Now we can pass in example text and examine the results.

```python
from langchain_core.documents import Document

text = """
Marie Curie, born in 1867, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.
She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.
Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.
She was, in 1906, the first woman to become a professor at the University of Paris.
"""
documents = [Document(page_content=text)]
graph_documents = llm_transformer.convert_to_graph_documents(documents)
print(f"Nodes:{graph_documents[0].nodes}")
print(f"Relationships:{graph_documents[0].relationships}")
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

```output
Nodes:[Node(id='Marie Curie', type='Person', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={})]
Relationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='MARRIED', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='PROFESSOR', properties={})]
```

Examine the following image to better grasp the structure of the generated knowledge graph.

![graph_construction1.png](/assets/images/graph_construction1-2b4d31978d58696d5a6a52ad92ae088f.png)

Note that the graph construction process is non-deterministic since we are using LLM. Therefore, you might get slightly different results on each execution.

Additionally, you have the flexibility to define specific types of nodes and relationships for extraction according to your requirements.

```python
llm_transformer_filtered = LLMGraphTransformer(
    llm=llm,
    allowed_nodes=["Person", "Country", "Organization"],
    allowed_relationships=["NATIONALITY", "LOCATED_IN", "WORKED_AT", "SPOUSE"],
)
graph_documents_filtered = llm_transformer_filtered.convert_to_graph_documents(
    documents
)
print(f"Nodes:{graph_documents_filtered[0].nodes}")
print(f"Relationships:{graph_documents_filtered[0].relationships}")
```

```output
Nodes:[Node(id='Marie Curie', type='Person', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={})]
Relationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='WORKED_AT', properties={})]
```

To define the graph schema more precisely, consider using a three-tuple approach for relationships. In this approach, each tuple consists of three elements: the source node, the relationship type, and the target node.

```python
allowed_relationships = [
    ("Person", "SPOUSE", "Person"),
    ("Person", "NATIONALITY", "Country"),
    ("Person", "WORKED_AT", "Organization"),
]

llm_transformer_tuple = LLMGraphTransformer(
    llm=llm,
    allowed_nodes=["Person", "Country", "Organization"],
    allowed_relationships=allowed_relationships,
)
graph_documents_filtered = llm_transformer_tuple.convert_to_graph_documents(documents)
print(f"Nodes:{graph_documents_filtered[0].nodes}")
print(f"Relationships:{graph_documents_filtered[0].relationships}")
```

```output
Nodes:[Node(id='Marie Curie', type='Person', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={})]
Relationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='WORKED_AT', properties={})]
```

For a better understanding of the generated graph, we can again visualize it.

![graph_construction2.png](/assets/images/graph_construction2-8b43506ae0fb3a006eaa4ba83fea8af5.png)

The `node_properties` parameter enables the extraction of node properties, allowing the creation of a more detailed graph. When set to `True`, LLM autonomously identifies and extracts relevant node properties. Conversely, if `node_properties` is defined as a list of strings, the LLM selectively retrieves only the specified properties from the text.

```python
llm_transformer_props = LLMGraphTransformer(
    llm=llm,
    allowed_nodes=["Person", "Country", "Organization"],
    allowed_relationships=["NATIONALITY", "LOCATED_IN", "WORKED_AT", "SPOUSE"],
    node_properties=["born_year"],
)
graph_documents_props = llm_transformer_props.convert_to_graph_documents(documents)
print(f"Nodes:{graph_documents_props[0].nodes}")
print(f"Relationships:{graph_documents_props[0].relationships}")
```

```output
Nodes:[Node(id='Marie Curie', type='Person', properties={'born_year': '1867'}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={}), Node(id='Poland', type='Country', properties={}), Node(id='France', type='Country', properties={})]
Relationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Poland', type='Country', properties={}), type='NATIONALITY', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='France', type='Country', properties={}), type='NATIONALITY', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='WORKED_AT', properties={})]
```

## Storing to graph database[â€‹](#storing-to-graph-database "Direct link to Storing to graph database")

The generated graph documents can be stored to a graph database using the `add_graph_documents` method.

```python
graph.add_graph_documents(graph_documents_props)
```

Most graph databases support indexes to optimize data import and retrieval. Since we might not know all the node labels in advance, we can handle this by adding a secondary base label to each node using the `baseEntityLabel` parameter.

```python
graph.add_graph_documents(graph_documents, baseEntityLabel=True)
```

Results will look like:

![graph_construction3.png](/assets/images/graph_construction3-86cbbef451d33d8b6fa50c2d79af6103.png)

The final option is to also import the source documents for the extracted nodes and relationships. This approach lets us track which documents each entity appeared in.

```python
graph.add_graph_documents(graph_documents, include_source=True)
```

Graph will have the following structure:

![graph_construction4.png](/assets/images/graph_construction4-e41087302ef4c331c2c95b57467f4c62.png)

In this visualization, the source document is highlighted in blue, with all entities extracted from it connected by `MENTIONS` relationships.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/graph_constructing.ipynb)

* * *


- [âš ï¸ Security note âš ï¸](#%EF%B8%8F-security-note-%EF%B8%8F)
- [Architecture](#architecture)
- [Setup](#setup)
- [LLM Graph Transformer](#llm-graph-transformer)
- [Storing to graph database](#storing-to-graph-database)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/ensemble_retriever.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/ensemble_retriever.ipynb)

# How to combine results from multiple retrievers

The [EnsembleRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.ensemble.EnsembleRetriever.html) supports ensembling of results from multiple [retrievers](/docs/concepts/retrievers/). It is initialized with a list of [BaseRetriever](https://python.langchain.com/api_reference/core/retrievers/langchain_core.retrievers.BaseRetriever.html) objects. EnsembleRetrievers rerank the results of the constituent retrievers based on the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.

By leveraging the strengths of different algorithms, the `EnsembleRetriever` can achieve better performance than any single algorithm.

The most common pattern is to combine a sparse retriever (like BM25) with a dense retriever (like embedding similarity), because their strengths are complementary. It is also known as "hybrid search". The sparse retriever is good at finding relevant documents based on keywords, while the dense retriever is good at finding relevant documents based on semantic similarity.

## Basic usage[â€‹](#basic-usage "Direct link to Basic usage")

Below we demonstrate ensembling of a [BM25Retriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.bm25.BM25Retriever.html) with a retriever derived from the [FAISS vector store](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html).

```python
%pip install --upgrade --quiet  rank_bm25 > /dev/null
```

```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

doc_list_1 = [
    "I like apples",
    "I like oranges",
    "Apples and oranges are fruits",
]

# initialize the bm25 retriever and faiss retriever
bm25_retriever = BM25Retriever.from_texts(
    doc_list_1, metadatas=[{"source": 1}] * len(doc_list_1)
)
bm25_retriever.k = 2

doc_list_2 = [
    "You like apples",
    "You like oranges",
]

embedding = OpenAIEmbeddings()
faiss_vectorstore = FAISS.from_texts(
    doc_list_2, embedding, metadatas=[{"source": 2}] * len(doc_list_2)
)
faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={"k": 2})

# initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]
)
```

**API Reference:**[EnsembleRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.ensemble.EnsembleRetriever.html) | [BM25Retriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.bm25.BM25Retriever.html) | [FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```python
docs = ensemble_retriever.invoke("apples")
docs
```

```output
[Document(page_content='I like apples', metadata={'source': 1}),
 Document(page_content='You like apples', metadata={'source': 2}),
 Document(page_content='Apples and oranges are fruits', metadata={'source': 1}),
 Document(page_content='You like oranges', metadata={'source': 2})]
```

## Runtime Configuration[â€‹](#runtime-configuration "Direct link to Runtime Configuration")

We can also configure the individual retrievers at runtime using [configurable fields](/docs/how_to/configure/). Below we update the "top-k" parameter for the FAISS retriever specifically:

```python
from langchain_core.runnables import ConfigurableField

faiss_retriever = faiss_vectorstore.as_retriever(
    search_kwargs={"k": 2}
).configurable_fields(
    search_kwargs=ConfigurableField(
        id="search_kwargs_faiss",
        name="Search Kwargs",
        description="The search kwargs to use",
    )
)

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]
)
```

**API Reference:**[ConfigurableField](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html)

```python
config = {"configurable": {"search_kwargs_faiss": {"k": 1}}}
docs = ensemble_retriever.invoke("apples", config=config)
docs
```

```output
[Document(page_content='I like apples', metadata={'source': 1}),
 Document(page_content='You like apples', metadata={'source': 2}),
 Document(page_content='Apples and oranges are fruits', metadata={'source': 1})]
```

Notice that this only returns one source from the FAISS retriever, because we pass in the relevant configuration at run time

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/ensemble_retriever.ipynb)

* * *


- [Basic usage](#basic-usage)
- [Runtime Configuration](#runtime-configuration)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_retriever.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_retriever.ipynb)

# How to create a custom Retriever

## Overview[â€‹](#overview "Direct link to Overview")

Many LLM applications involve retrieving information from external data sources using a [Retriever](/docs/concepts/retrievers/).

A retriever is responsible for retrieving a list of relevant [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) to a given user `query`.

The retrieved documents are often formatted into prompts that are fed into an LLM, allowing the LLM to use the information in the to generate an appropriate response (e.g., answering a user question based on a knowledge base).

## Interface[â€‹](#interface "Direct link to Interface")

To create your own retriever, you need to extend the `BaseRetriever` class and implement the following methods:

| Method                     | Description                                | Required/Optional |
|----------------------------|--------------------------------------------|-------------------|
| `_get_relevant_documents`  | Get documents relevant to a query.         | Required          |
| `_aget_relevant_documents` | Implement to provide async native support. | Optional          |

The logic inside of `_get_relevant_documents` can involve arbitrary calls to a database or to the web using requests.

tip

By inherting from `BaseRetriever`, your retriever automatically becomes a LangChain [Runnable](/docs/concepts/runnables/) and will gain the standard `Runnable` functionality out of the box!

info

You can use a `RunnableLambda` or `RunnableGenerator` to implement a retriever.

The main benefit of implementing a retriever as a `BaseRetriever` vs. a `RunnableLambda` (a custom [runnable function](/docs/how_to/functions/)) is that a `BaseRetriever` is a well known LangChain entity so some tooling for monitoring may implement specialized behavior for retrievers. Another difference is that a `BaseRetriever` will behave slightly differently from `RunnableLambda` in some APIs; e.g., the `start` event in `astream_events` API will be `on_retriever_start` instead of `on_chain_start`.

## Example[â€‹](#example "Direct link to Example")

Let's implement a toy retriever that returns all documents whose text contains the text in the user query.

```python
from typing import List

from langchain_core.callbacks import CallbackManagerForRetrieverRun
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever


class ToyRetriever(BaseRetriever):
    """A toy retriever that contains the top k documents that contain the user query.

    This retriever only implements the sync method _get_relevant_documents.

    If the retriever were to involve file access or network access, it could benefit
    from a native async implementation of `_aget_relevant_documents`.

    As usual, with Runnables, there's a default async implementation that's provided
    that delegates to the sync implementation running on another thread.
    """

    documents: List[Document]
    """List of documents to retrieve from."""
    k: int
    """Number of top results to return"""

    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        """Sync implementations for retriever."""
        matching_documents = []
        for document in documents:
            if len(matching_documents) > self.k:
                return matching_documents

            if query.lower() in document.page_content.lower():
                matching_documents.append(document)
        return matching_documents

    # Optional: Provide a more efficient native implementation by overriding
    # _aget_relevant_documents
    # async def _aget_relevant_documents(
    #     self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun
    # ) -> List[Document]:
    #     """Asynchronously get documents relevant to a query.

    #     Args:
    #         query: String to find relevant documents for
    #         run_manager: The callbacks handler to use

    #     Returns:
    #         List of relevant documents
    #     """
```

**API Reference:**[CallbackManagerForRetrieverRun](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.CallbackManagerForRetrieverRun.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [BaseRetriever](https://python.langchain.com/api_reference/core/retrievers/langchain_core.retrievers.BaseRetriever.html)

## Test it ðŸ§ª[â€‹](#test-it- "Direct link to Test it ðŸ§ª")

```python
documents = [
    Document(
        page_content="Dogs are great companions, known for their loyalty and friendliness.",
        metadata={"type": "dog", "trait": "loyalty"},
    ),
    Document(
        page_content="Cats are independent pets that often enjoy their own space.",
        metadata={"type": "cat", "trait": "independence"},
    ),
    Document(
        page_content="Goldfish are popular pets for beginners, requiring relatively simple care.",
        metadata={"type": "fish", "trait": "low maintenance"},
    ),
    Document(
        page_content="Parrots are intelligent birds capable of mimicking human speech.",
        metadata={"type": "bird", "trait": "intelligence"},
    ),
    Document(
        page_content="Rabbits are social animals that need plenty of space to hop around.",
        metadata={"type": "rabbit", "trait": "social"},
    ),
]
retriever = ToyRetriever(documents=documents, k=3)
```

```python
retriever.invoke("that")
```

```output
[Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'}),
 Document(page_content='Rabbits are social animals that need plenty of space to hop around.', metadata={'type': 'rabbit', 'trait': 'social'})]
```

It's a **runnable** so it'll benefit from the standard Runnable Interface! ðŸ¤©

```python
await retriever.ainvoke("that")
```

```output
[Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'}),
 Document(page_content='Rabbits are social animals that need plenty of space to hop around.', metadata={'type': 'rabbit', 'trait': 'social'})]
```

```python
retriever.batch(["dog", "cat"])
```

```output
[[Document(page_content='Dogs are great companions, known for their loyalty and friendliness.', metadata={'type': 'dog', 'trait': 'loyalty'})],
 [Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'})]]
```

```python
async for event in retriever.astream_events("bar", version="v1"):
    print(event)
```

```output
{'event': 'on_retriever_start', 'run_id': 'f96f268d-8383-4921-b175-ca583924d9ff', 'name': 'ToyRetriever', 'tags': [], 'metadata': {}, 'data': {'input': 'bar'}}
{'event': 'on_retriever_stream', 'run_id': 'f96f268d-8383-4921-b175-ca583924d9ff', 'tags': [], 'metadata': {}, 'name': 'ToyRetriever', 'data': {'chunk': []}}
{'event': 'on_retriever_end', 'name': 'ToyRetriever', 'run_id': 'f96f268d-8383-4921-b175-ca583924d9ff', 'tags': [], 'metadata': {}, 'data': {'output': []}}
```

## Contributing[â€‹](#contributing "Direct link to Contributing")

We appreciate contributions of interesting retrievers!

Here's a checklist to help make sure your contribution gets added to LangChain:

Documentation:

- The retriever contains doc-strings for all initialization arguments, as these will be surfaced in the [API Reference](https://python.langchain.com/api_reference/langchain/index.html).
- The class doc-string for the model contains a link to any relevant APIs used for the retriever (e.g., if the retriever is retrieving from wikipedia, it'll be good to link to the wikipedia API!)

Tests:

- Add unit or integration tests to verify that `invoke` and `ainvoke` work.

Optimizations:

If the retriever is connecting to external data sources (e.g., an API or a file), it'll almost certainly benefit from an async native optimization!

- Provide a native async implementation of `_aget_relevant_documents` (used by `ainvoke`)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/custom_retriever.ipynb)

* * *


- [Overview](#overview)
- [Interface](#interface)
- [Example](#example)
- [Test it ðŸ§ª](#test-it-)
- [Contributing](#contributing)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/sequence.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/sequence.ipynb)

# How to chain runnables

Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel/)
- [Prompt templates](/docs/concepts/prompt_templates/)
- [Chat models](/docs/concepts/chat_models/)
- [Output parser](/docs/concepts/output_parsers/)

One point about [LangChain Expression Language](/docs/concepts/lcel/) is that any two runnables can be "chained" together into sequences. The output of the previous runnable's `.invoke()` call is passed as input to the next runnable. This can be done using the pipe operator (`|`), or the more explicit `.pipe()` method, which does the same thing.

The resulting [`RunnableSequence`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html) is itself a runnable, which means it can be invoked, streamed, or further chained just like any other runnable. Advantages of chaining runnables in this way are efficient streaming (the sequence will stream output as soon as it is available), and debugging and tracing with tools like [LangSmith](/docs/how_to/debugging/).

## The pipe operator: `|`[â€‹](#the-pipe-operator- "Direct link to the-pipe-operator-")

To show off how this works, let's go through an example. We'll walk through a common pattern in LangChain: using a [prompt template](/docs/how_to/#prompt-templates) to format input into a [chat model](/docs/how_to/#chat-models), and finally converting the chat message output into a string with an [output parser](/docs/how_to/#output-parsers).

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")

chain = prompt | model | StrOutputParser()
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

Prompts and models are both runnable, and the output type from the prompt call is the same as the input type of the chat model, so we can chain them together. We can then invoke the resulting sequence like any other runnable:

```python
chain.invoke({"topic": "bears"})
```

```output
"Here's a bear joke for you:\n\nWhy did the bear dissolve in water?\nBecause it was a polar bear!"
```

### Coercion[â€‹](#coercion "Direct link to Coercion")

We can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components.

For example, let's say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny.

We would need to be careful with how we format the input into the next chain. In the below example, the dict in the chain is automatically parsed and converted into a [`RunnableParallel`](/docs/how_to/parallel/), which runs all of its values in parallel and returns a dict with the results.

This happens to be the same format the next prompt template expects. Here it is in action:

```python
from langchain_core.output_parsers import StrOutputParser

analysis_prompt = ChatPromptTemplate.from_template("is this a funny joke? {joke}")

composed_chain = {"joke": chain} | analysis_prompt | model | StrOutputParser()

composed_chain.invoke({"topic": "bears"})
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html)

```output
'Haha, that\'s a clever play on words! Using "polar" to imply the bear dissolved or became polar/polarized when put in water. Not the most hilarious joke ever, but it has a cute, groan-worthy pun that makes it mildly amusing. I appreciate a good pun or wordplay joke.'
```

Functions will also be coerced into runnables, so you can add custom logic to your chains too. The below chain results in the same logical flow as before:

```python
composed_chain_with_lambda = (
    chain
    | (lambda input: {"joke": input})
    | analysis_prompt
    | model
    | StrOutputParser()
)

composed_chain_with_lambda.invoke({"topic": "beets"})
```

```output
"Haha, that's a cute and punny joke! I like how it plays on the idea of beets blushing or turning red like someone blushing. Food puns can be quite amusing. While not a total knee-slapper, it's a light-hearted, groan-worthy dad joke that would make me chuckle and shake my head. Simple vegetable humor!"
```

However, keep in mind that using functions like this may interfere with operations like streaming. See [this section](/docs/how_to/functions/) for more information.

## The `.pipe()` method[â€‹](#the-pipe-method "Direct link to the-pipe-method")

We could also compose the same sequence using the `.pipe()` method. Here's what that looks like:

```python
from langchain_core.runnables import RunnableParallel

composed_chain_with_pipe = (
    RunnableParallel({"joke": chain})
    .pipe(analysis_prompt)
    .pipe(model)
    .pipe(StrOutputParser())
)

composed_chain_with_pipe.invoke({"topic": "battlestar galactica"})
```

**API Reference:**[RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)

```output
"I cannot reproduce any copyrighted material verbatim, but I can try to analyze the humor in the joke you provided without quoting it directly.\n\nThe joke plays on the idea that the Cylon raiders, who are the antagonists in the Battlestar Galactica universe, failed to locate the human survivors after attacking their home planets (the Twelve Colonies) due to using an outdated and poorly performing operating system (Windows Vista) for their targeting systems.\n\nThe humor stems from the juxtaposition of a futuristic science fiction setting with a relatable real-world frustration â€“ the use of buggy, slow, or unreliable software or technology. It pokes fun at the perceived inadequacies of Windows Vista, which was widely criticized for its performance issues and other problems when it was released.\n\nBy attributing the Cylons' failure to locate the humans to their use of Vista, the joke creates an amusing and unexpected connection between a fictional advanced race of robots and a familiar technological annoyance experienced by many people in the real world.\n\nOverall, the joke relies on incongruity and relatability to generate humor, but without reproducing any copyrighted material directly."
```

Or the abbreviated:

```python
composed_chain_with_pipe = RunnableParallel({"joke": chain}).pipe(
    analysis_prompt, model, StrOutputParser()
)
```

## Related[â€‹](#related "Direct link to Related")

- [Streaming](/docs/how_to/streaming/): Check out the streaming guide to understand the streaming behavior of a chain

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/sequence.ipynb)

* * *


- [The pipe operator: `|`](#the-pipe-operator-)
  
  - [Coercion](#coercion)
- [The `.pipe()` method](#the-pipe-method)
- [Related](#related)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/tokens.mdx)

# Tokens

Modern large language models (LLMs) are typically based on a transformer architecture that processes a sequence of units known as tokens. Tokens are the fundamental elements that models use to break down input and generate output. In this section, we'll discuss what tokens are and how they are used by language models.

## What is a token?[â€‹](#what-is-a-token "Direct link to What is a token?")

A **token** is the basic unit that a language model reads, processes, and generates. These units can vary based on how the model provider defines them, but in general, they could represent:

- A whole word (e.g., "apple"),
- A part of a word (e.g., "app"),
- Or other linguistic components such as punctuation or spaces.

The way the model tokenizes the input depends on its **tokenizer algorithm**, which converts the input into tokens. Similarly, the modelâ€™s output comes as a stream of tokens, which is then decoded back into human-readable text.

## How tokens work in language models[â€‹](#how-tokens-work-in-language-models "Direct link to How tokens work in language models")

The reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, language models focus on **tokens**, which represent meaningful linguistic units. Here's how the process works:

1. **Input Tokenization**: When you provide a model with a prompt (e.g., "LangChain is cool!"), the tokenizer algorithm splits the text into tokens. For example, the sentence could be tokenized into parts like `["Lang", "Chain", " is", " cool", "!"]`. Note that token boundaries donâ€™t always align with word boundaries. ![](/assets/images/tokenization-10f566ab6774724e63dd99646f69655c.png)
2. **Processing**: The transformer architecture behind these models processes tokens sequentially to predict the next token in a sentence. It does this by analyzing the relationships between tokens, capturing context and meaning from the input.
3. **Output Generation**: The model generates new tokens one by one. These output tokens are then decoded back into human-readable text.

Using tokens instead of raw characters allows the model to focus on linguistically meaningful units, which helps it capture grammar, structure, and context more effectively.

## Tokens donâ€™t have to be text[â€‹](#tokens-dont-have-to-be-text "Direct link to Tokens donâ€™t have to be text")

Although tokens are most commonly used to represent text, they donâ€™t have to be limited to textual data. Tokens can also serve as abstract representations of **multi-modal data**, such as:

- **Images**,
- **Audio**,
- **Video**,
- And other types of data.

At the time of writing, virtually no models support **multi-modal output**, and only a few models can handle **multi-modal inputs** (e.g., text combined with images or audio). However, as advancements in AI continue, we expect **multi-modality** to become much more common. This would allow models to process and generate a broader range of media, significantly expanding the scope of what tokens can represent and how models can interact with diverse types of data.

note

In principle, **anything that can be represented as a sequence of tokens** could be modeled in a similar way. For example, **DNA sequences**â€”which are composed of a series of nucleotides (A, T, C, G)â€”can be tokenized and modeled to capture patterns, make predictions, or generate sequences. This flexibility allows transformer-based models to handle diverse types of sequential data, further broadening their potential applications across various domains, including bioinformatics, signal processing, and other fields that involve structured or unstructured sequences.

Please see the [multimodality](/docs/concepts/multimodality/) section for more information on multi-modal inputs and outputs.

## Why not use characters?[â€‹](#why-not-use-characters "Direct link to Why not use characters?")

Using tokens instead of individual characters makes models both more efficient and better at understanding context and grammar. Tokens represent meaningful units, like whole words or parts of words, allowing models to capture language structure more effectively than by processing raw characters. Token-level processing also reduces the number of units the model has to handle, leading to faster computation.

In contrast, character-level processing would require handling a much larger sequence of input, making it harder for the model to learn relationships and context. Tokens enable models to focus on linguistic meaning, making them more accurate and efficient in generating responses.

## How tokens correspond to text[â€‹](#how-tokens-correspond-to-text "Direct link to How tokens correspond to text")

Please see this post from [OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) for more details on how tokens are counted and how they correspond to text.

According to the OpenAI post, the approximate token counts for English text are as follows:

- 1 token ~= 4 chars in English
- 1 token ~= Â¾ words
- 100 tokens ~= 75 words

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/tokens.mdx)

* * *


- [What is a token?](#what-is-a-token)
- [How tokens work in language models](#how-tokens-work-in-language-models)
- [Tokens donâ€™t have to be text](#tokens-dont-have-to-be-text)
- [Why not use characters?](#why-not-use-characters)
- [How tokens correspond to text](#how-tokens-correspond-to-text)








- [LangSmith](https://docs.smith.langchain.com)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [LangChain Hub](https://smith.langchain.com/hub)
- [LangChain JS/TS](https://js.langchain.com)

[v0.3](#)

- [v0.3](/docs/introduction/)
- [v0.2](https://python.langchain.com/v0.2/docs/introduction)
- [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)

[ðŸ’¬](https://chat.langchain.com)

Search

- [Introduction](/docs/introduction/)
- [Tutorials](/docs/tutorials/)
  
  - [Build a Question Answering application over a Graph Database](/docs/tutorials/graph/)
  - [Tutorials](/docs/tutorials/)
  - [Build a simple LLM application with chat models and prompt templates](/docs/tutorials/llm_chain/)
  - [Build a Chatbot](/docs/tutorials/chatbot/)
  - [Build a Retrieval Augmented Generation (RAG) App: Part 2](/docs/tutorials/qa_chat_history/)
  - [Build an Extraction Chain](/docs/tutorials/extraction/)
  - [Build an Agent](/docs/tutorials/agents/)
  - [Tagging](/docs/tutorials/classification/)
  - [Build a Retrieval Augmented Generation (RAG) App: Part 1](/docs/tutorials/rag/)
  - [Build a semantic search engine](/docs/tutorials/retrievers/)
  - [Build a Question/Answering system over SQL data](/docs/tutorials/sql_qa/)
  - [Summarize Text](/docs/tutorials/summarization/)
- [How-to guides](/docs/how_to/)
  
  - [How-to guides](/docs/how_to/)
  - [How to use tools in a chain](/docs/how_to/tools_chain/)
  - [How to use a vectorstore as a retriever](/docs/how_to/vectorstore_retriever/)
  - [How to add memory to chatbots](/docs/how_to/chatbots_memory/)
  - [How to use example selectors](/docs/how_to/example_selectors/)
  - [How to add a semantic layer over graph database](/docs/how_to/graph_semantic/)
  - [How to invoke runnables in parallel](/docs/how_to/parallel/)
  - [How to stream chat model responses](/docs/how_to/chat_streaming/)
  - [How to add default invocation args to a Runnable](/docs/how_to/binding/)
  - [How to add retrieval to chatbots](/docs/how_to/chatbots_retrieval/)
  - [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)
  - [How to do tool/function calling](/docs/how_to/function_calling/)
  - [How to install LangChain packages](/docs/how_to/installation/)
  - [How to add examples to the prompt for query analysis](/docs/how_to/query_few_shot/)
  - [How to use few shot examples](/docs/how_to/few_shot_examples/)
  - [How to run custom functions](/docs/how_to/functions/)
  - [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)
  - [How to handle cases where no queries are generated](/docs/how_to/query_no_queries/)
  - [How to route between sub-chains](/docs/how_to/routing/)
  - [How to return structured data from a model](/docs/how_to/structured_output/)
  - [How to summarize text through parallelization](/docs/how_to/summarize_map_reduce/)
  - [How to summarize text through iterative refinement](/docs/how_to/summarize_refine/)
  - [How to summarize text in a single LLM call](/docs/how_to/summarize_stuff/)
  - [How to use toolkits](/docs/how_to/toolkits/)
  - [How to add ad-hoc tool calling capability to LLMs and Chat Models](/docs/how_to/tools_prompting/)
  - [Build an Agent with AgentExecutor (Legacy)](/docs/how_to/agent_executor/)
  - [How to construct knowledge graphs](/docs/how_to/graph_constructing/)
  - [How to partially format prompt templates](/docs/how_to/prompts_partial/)
  - [How to handle multiple queries when doing query analysis](/docs/how_to/query_multiple_queries/)
  - [How to use built-in tools and toolkits](/docs/how_to/tools_builtin/)
  - [How to pass through arguments from one step to the next](/docs/how_to/passthrough/)
  - [How to compose prompts together](/docs/how_to/prompts_composition/)
  - [How to handle multiple retrievers when doing query analysis](/docs/how_to/query_multiple_retrievers/)
  - [How to add values to a chain's state](/docs/how_to/assign/)
  - [How to construct filters for query analysis](/docs/how_to/query_constructing_filters/)
  - [How to configure runtime chain internals](/docs/how_to/configure/)
  - [How deal with high cardinality categoricals when doing query analysis](/docs/how_to/query_high_cardinality/)
  - [Custom Document Loader](/docs/how_to/document_loader_custom/)
  - [How to use the MultiQueryRetriever](/docs/how_to/MultiQueryRetriever/)
  - [How to add scores to retriever results](/docs/how_to/add_scores_retriever/)
  - [Caching](/docs/how_to/caching_embeddings/)
  - [How to use callbacks in async environments](/docs/how_to/callbacks_async/)
  - [How to attach callbacks to a runnable](/docs/how_to/callbacks_attach/)
  - [How to propagate callbacks constructor](/docs/how_to/callbacks_constructor/)
  - [How to dispatch custom callback events](/docs/how_to/callbacks_custom_events/)
  - [How to pass callbacks in at runtime](/docs/how_to/callbacks_runtime/)
  - [How to split by character](/docs/how_to/character_text_splitter/)
  - [How to cache chat model responses](/docs/how_to/chat_model_caching/)
  - [How to handle rate limits](/docs/how_to/chat_model_rate_limiting/)
  - [How to init any model in one line](/docs/how_to/chat_models_universal_init/)
  - [How to track token usage in ChatModels](/docs/how_to/chat_token_usage_tracking/)
  - [How to add tools to chatbots](/docs/how_to/chatbots_tools/)
  - [How to split code](/docs/how_to/code_splitter/)
  - [How to do retrieval with contextual compression](/docs/how_to/contextual_compression/)
  - [How to convert Runnables to Tools](/docs/how_to/convert_runnable_to_tool/)
  - [How to create custom callback handlers](/docs/how_to/custom_callbacks/)
  - [How to create a custom chat model class](/docs/how_to/custom_chat_model/)
  - [Custom Embeddings](/docs/how_to/custom_embeddings/)
  - [How to create a custom LLM class](/docs/how_to/custom_llm/)
  - [Custom Retriever](/docs/how_to/custom_retriever/)
  - [How to create tools](/docs/how_to/custom_tools/)
  - [How to debug your LLM apps](/docs/how_to/debugging/)
  - [How to load CSVs](/docs/how_to/document_loader_csv/)
  - [How to load documents from a directory](/docs/how_to/document_loader_directory/)
  - [How to load HTML](/docs/how_to/document_loader_html/)
  - [How to load JSON](/docs/how_to/document_loader_json/)
  - [How to load Markdown](/docs/how_to/document_loader_markdown/)
  - [How to load Microsoft Office files](/docs/how_to/document_loader_office_file/)
  - [How to load PDFs](/docs/how_to/document_loader_pdf/)
  - [How to load web pages](/docs/how_to/document_loader_web/)
  - [How to create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)
  - [Text embedding models](/docs/how_to/embed_text/)
  - [How to combine results from multiple retrievers](/docs/how_to/ensemble_retriever/)
  - [How to select examples from a LangSmith dataset](/docs/how_to/example_selectors_langsmith/)
  - [How to select examples by length](/docs/how_to/example_selectors_length_based/)
  - [How to select examples by maximal marginal relevance (MMR)](/docs/how_to/example_selectors_mmr/)
  - [How to select examples by n-gram overlap](/docs/how_to/example_selectors_ngram/)
  - [How to select examples by similarity](/docs/how_to/example_selectors_similarity/)
  - [How to use reference examples when doing extraction](/docs/how_to/extraction_examples/)
  - [How to handle long text when doing extraction](/docs/how_to/extraction_long_text/)
  - [How to use prompting alone (no tool calling) to do extraction](/docs/how_to/extraction_parse/)
  - [How to add fallbacks to a runnable](/docs/how_to/fallbacks/)
  - [How to filter messages](/docs/how_to/filter_messages/)
  - [Hybrid Search](/docs/how_to/hybrid/)
  - [How to use the LangChain indexing API](/docs/how_to/indexing/)
  - [How to inspect runnables](/docs/how_to/inspect/)
  - [LangChain Expression Language Cheatsheet](/docs/how_to/lcel_cheatsheet/)
  - [How to cache LLM responses](/docs/how_to/llm_caching/)
  - [How to track token usage for LLMs](/docs/how_to/llm_token_usage_tracking/)
  - [Run models locally](/docs/how_to/local_llms/)
  - [How to get log probabilities](/docs/how_to/logprobs/)
  - [How to reorder retrieved results to mitigate the "lost in the middle" effect](/docs/how_to/long_context_reorder/)
  - [How to split Markdown by Headers](/docs/how_to/markdown_header_metadata_splitter/)
  - [How to merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)
  - [How to add message history](/docs/how_to/message_history/)
  - [How to migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent/)
  - [How to retrieve using multiple vectors per document](/docs/how_to/multi_vector/)
  - [How to pass multimodal data to models](/docs/how_to/multimodal_inputs/)
  - [How to use multimodal prompts](/docs/how_to/multimodal_prompts/)
  - [How to create a custom Output Parser](/docs/how_to/output_parser_custom/)
  - [How to use the output-fixing parser](/docs/how_to/output_parser_fixing/)
  - [How to parse JSON output](/docs/how_to/output_parser_json/)
  - [How to retry when a parsing error occurs](/docs/how_to/output_parser_retry/)
  - [How to parse text from message objects](/docs/how_to/output_parser_string/)
  - [How to parse XML output](/docs/how_to/output_parser_xml/)
  - [How to parse YAML output](/docs/how_to/output_parser_yaml/)
  - [How to use the Parent Document Retriever](/docs/how_to/parent_document_retriever/)
  - [How to use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility/)
  - [How to add chat history](/docs/how_to/qa_chat_history_how_to/)
  - [How to get a RAG application to add citations](/docs/how_to/qa_citations/)
  - [How to do per-user retrieval](/docs/how_to/qa_per_user/)
  - [How to get your RAG application to return sources](/docs/how_to/qa_sources/)
  - [How to stream results from your RAG application](/docs/how_to/qa_streaming/)
  - [How to split JSON data](/docs/how_to/recursive_json_splitter/)
  - [How to recursively split text by characters](/docs/how_to/recursive_text_splitter/)
  - [Response metadata](/docs/how_to/response_metadata/)
  - [How to pass runtime secrets to runnables](/docs/how_to/runnable_runtime_secrets/)
  - [How to do "self-querying" retrieval](/docs/how_to/self_query/)
  - [How to split text based on semantic similarity](/docs/how_to/semantic-chunker/)
  - [How to chain runnables](/docs/how_to/sequence/)
  - [How to save and load LangChain objects](/docs/how_to/serialization/)
  - [How to split text by tokens](/docs/how_to/split_by_token/)
  - [How to split HTML](/docs/how_to/split_html/)
  - [How to do question answering over CSVs](/docs/how_to/sql_csv/)
  - [How to deal with large databases when doing SQL question-answering](/docs/how_to/sql_large_db/)
  - [How to better prompt when doing SQL question-answering](/docs/how_to/sql_prompting/)
  - [How to do query validation as part of SQL question-answering](/docs/how_to/sql_query_checking/)
  - [How to stream runnables](/docs/how_to/streaming/)
  - [How to stream responses from an LLM](/docs/how_to/streaming_llm/)
  - [How to use a time-weighted vector store retriever](/docs/how_to/time_weighted_vectorstore/)
  - [How to return artifacts from a tool](/docs/how_to/tool_artifacts/)
  - [How to use chat models to call tools](/docs/how_to/tool_calling/)
  - [How to disable parallel tool calling](/docs/how_to/tool_calling_parallel/)
  - [How to force models to call a tool](/docs/how_to/tool_choice/)
  - [How to access the RunnableConfig from a tool](/docs/how_to/tool_configure/)
  - [How to pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model/)
  - [How to pass run time values to tools](/docs/how_to/tool_runtime/)
  - [How to stream events from a tool](/docs/how_to/tool_stream_events/)
  - [How to stream tool calls](/docs/how_to/tool_streaming/)
  - [How to convert tools to OpenAI Functions](/docs/how_to/tools_as_openai_functions/)
  - [How to handle tool errors](/docs/how_to/tools_error/)
  - [How to use few-shot prompting with tool calling](/docs/how_to/tools_few_shot/)
  - [How to add a human-in-the-loop for tools](/docs/how_to/tools_human/)
  - [How to bind model-specific tools](/docs/how_to/tools_model_specific/)
  - [How to trim messages](/docs/how_to/trim_messages/)
  - [How to create and query vector stores](/docs/how_to/vectorstores/)
- [Conceptual guide](/docs/concepts/)
  
  - [Agents](/docs/concepts/agents/)
  - [Architecture](/docs/concepts/architecture/)
  - [Async programming with langchain](/docs/concepts/async/)
  - [Callbacks](/docs/concepts/callbacks/)
  - [Chat history](/docs/concepts/chat_history/)
  - [Chat models](/docs/concepts/chat_models/)
  - [Document loaders](/docs/concepts/document_loaders/)
  - [Embedding models](/docs/concepts/embedding_models/)
  - [Evaluation](/docs/concepts/evaluation/)
  - [Example selectors](/docs/concepts/example_selectors/)
  - [Few-shot prompting](/docs/concepts/few_shot_prompting/)
  - [Conceptual guide](/docs/concepts/)
  - [Key-value stores](/docs/concepts/key_value_stores/)
  - [LangChain Expression Language (LCEL)](/docs/concepts/lcel/)
  - [Messages](/docs/concepts/messages/)
  - [Multimodality](/docs/concepts/multimodality/)
  - [Output parsers](/docs/concepts/output_parsers/)
  - [Prompt Templates](/docs/concepts/prompt_templates/)
  - [Retrieval augmented generation (RAG)](/docs/concepts/rag/)
  - [Retrieval](/docs/concepts/retrieval/)
  - [Retrievers](/docs/concepts/retrievers/)
  - [Runnable interface](/docs/concepts/runnables/)
  - [Streaming](/docs/concepts/streaming/)
  - [Structured outputs](/docs/concepts/structured_outputs/)
  - [Testing](/docs/concepts/testing/)
  - [String-in, string-out llms](/docs/concepts/text_llms/)
  - [Text splitters](/docs/concepts/text_splitters/)
  - [Tokens](/docs/concepts/tokens/)
  - [Tool calling](/docs/concepts/tool_calling/)
  - [Tools](/docs/concepts/tools/)
  - [Tracing](/docs/concepts/tracing/)
  - [Vector stores](/docs/concepts/vectorstores/)
  - [Why LangChain?](/docs/concepts/why_langchain/)
- Ecosystem
  
  - [ðŸ¦œðŸ› ï¸ LangSmith](https://docs.smith.langchain.com/)
  - [ðŸ¦œðŸ•¸ï¸ LangGraph](https://langchain-ai.github.io/langgraph/)
- Versions
  
  - [v0.3](/docs/versions/v0_3/)
  - [v0.2](/docs/versions/v0_2/overview/)
    
    - [Overview](/docs/versions/v0_2/overview/)
    - [Migration](/docs/versions/v0_2/)
    - [astream\_events v2](/docs/versions/v0_2/migrating_astream_events/)
    - [Changes](/docs/versions/v0_2/deprecations/)
  - [Pydantic compatibility](/docs/how_to/pydantic_compatibility/)
  - [Migrating from v0.0 chains](/docs/versions/migrating_chains/)
    
    - [How to migrate from v0.0 chains](/docs/versions/migrating_chains/)
    - [Migrating from ConstitutionalChain](/docs/versions/migrating_chains/constitutional_chain/)
    - [Migrating from ConversationalChain](/docs/versions/migrating_chains/conversation_chain/)
    - [Migrating from ConversationalRetrievalChain](/docs/versions/migrating_chains/conversation_retrieval_chain/)
    - [Migrating from LLMChain](/docs/versions/migrating_chains/llm_chain/)
    - [Migrating from LLMMathChain](/docs/versions/migrating_chains/llm_math_chain/)
    - [Migrating from LLMRouterChain](/docs/versions/migrating_chains/llm_router_chain/)
    - [Migrating from MapReduceDocumentsChain](/docs/versions/migrating_chains/map_reduce_chain/)
    - [Migrating from MapRerankDocumentsChain](/docs/versions/migrating_chains/map_rerank_docs_chain/)
    - [Migrating from MultiPromptChain](/docs/versions/migrating_chains/multi_prompt_chain/)
    - [Migrating from RefineDocumentsChain](/docs/versions/migrating_chains/refine_docs_chain/)
    - [Migrating from RetrievalQA](/docs/versions/migrating_chains/retrieval_qa/)
    - [Migrating from StuffDocumentsChain](/docs/versions/migrating_chains/stuff_docs_chain/)
  - [Upgrading to LangGraph memory](/docs/versions/migrating_memory/)
    
    - [How to migrate to LangGraph memory](/docs/versions/migrating_memory/)
    - [How to use BaseChatMessageHistory with LangGraph](/docs/versions/migrating_memory/chat_history/)
    - [Migrating off ConversationBufferMemory or ConversationStringBufferMemory](/docs/versions/migrating_memory/conversation_buffer_memory/)
    - [Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory](/docs/versions/migrating_memory/conversation_buffer_window_memory/)
    - [Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory](/docs/versions/migrating_memory/conversation_summary_memory/)
    - [A Long-Term Memory Agent](/docs/versions/migrating_memory/long_term_memory_agent/)
  - [Release policy](/docs/versions/release_policy/)
- [Security Policy](/docs/security/)

<!--THE END-->

- Versions
- v0.2
- Overview


[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/v0_2/overview.mdx)

# Overview

## Whatâ€™s new in LangChain?[â€‹](#whats-new-in-langchain "Direct link to Whatâ€™s new in LangChain?")

The following features have been added during the development of 0.1.x:

- Better streaming support via the [Event Streaming API](https://python.langchain.com/docs/expression_language/streaming/#using-stream-events).
- [Standardized tool calling support](https://blog.langchain.dev/tool-calling-with-langchain/)
- A standardized interface for [structuring output](https://github.com/langchain-ai/langchain/discussions/18154)
- [@chain decorator](https://python.langchain.com/docs/expression_language/how_to/decorator/) to more easily create **RunnableLambdas**
- [https://python.langchain.com/docs/expression\_language/how\_to/inspect/](https://python.langchain.com/docs/expression_language/how_to/inspect/)
- In Python, better async support for many core abstractions (thank you [@cbornet](https://github.com/cbornet)!!)
- Include response metadata in `AIMessage` to make it easy to access raw output from the underlying models
- Tooling to visualize [your runnables](https://python.langchain.com/docs/expression_language/how_to/inspect/) or [your langgraph app](https://github.com/langchain-ai/langgraph/blob/main/examples/visualization.ipynb)
- Interoperability of chat message histories across most providers
- [Over 20+ partner packages in python](https://python.langchain.com/docs/integrations/providers/) for popular integrations

## Whatâ€™s coming to LangChain?[â€‹](#whats-coming-to-langchain "Direct link to Whatâ€™s coming to LangChain?")

- Weâ€™ve been working hard on [langgraph](https://langchain-ai.github.io/langgraph/). We will be building more capabilities on top of it and focusing on making it the go-to framework for agent architectures.
- Vectorstores V2! Weâ€™ll be revisiting our vectorstores abstractions to help improve usability and reliability.
- Better documentation and versioned docs!
- Weâ€™re planning a breaking release (0.3.0) sometime between July-September to [upgrade to full support of Pydantic 2](https://github.com/langchain-ai/langchain/discussions/19339), and will drop support for Pydantic 1 (including objects originating from the `v1` namespace of Pydantic 2).

## What changed?[â€‹](#what-changed "Direct link to What changed?")

Due to the rapidly evolving field, LangChain has also evolved rapidly.

This document serves to outline at a high level what has changed and why.

### TLDR[â€‹](#tldr "Direct link to TLDR")

**As of 0.2.0:**

- This release completes the work that we started with release 0.1.0 by removing the dependency of `langchain` on `langchain-community`.
- `langchain` package no longer requires `langchain-community` . Instead `langchain-community` will now depend on `langchain-core` and `langchain` .
- User code that still relies on deprecated imports from `langchain` will continue to work as long `langchain_community` is installed. These imports will start raising errors in release 0.4.x.

**As of 0.1.0:**

- `langchain` was split into the following component packages: `langchain-core`, `langchain`, `langchain-community`, `langchain-[partner]` to improve the usability of langchain code in production settings. You can read more about it on our [blog](https://blog.langchain.dev/langchain-v0-1-0/).

### Ecosystem organization[â€‹](#ecosystem-organization "Direct link to Ecosystem organization")

By the release of 0.1.0, LangChain had grown to a large ecosystem with many integrations and a large community.

To improve the usability of LangChain in production, we split the single `langchain` package into multiple packages. This allowed us to create a good foundation architecture for the LangChain ecosystem and improve the usability of `langchain` in production.

Here is the high level break down of the Eco-system:

- **langchain-core**: contains core abstractions involving LangChain Runnables, tooling for observability, and base implementations of important abstractions (e.g., Chat Models).
- **langchain:** contains generic code that is built using interfaces defined in `langchain-core`. This package is for code that generalizes well across different implementations of specific interfaces. For example, `create_tool_calling_agent` works across chat models that support [tool calling capabilities](https://blog.langchain.dev/tool-calling-with-langchain/).
- **langchain-community**: community maintained 3rd party integrations. Contains integrations based on interfaces defined in **langchain-core**. Maintained by the LangChain community.
- **Partner Packages (e.g., langchain-\[partner])**: Partner packages are packages dedicated to especially popular integrations (e.g., `langchain-openai`, `langchain-anthropic` etc.). The dedicated packages generally benefit from better reliability and support.
- `langgraph`: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.
- `langserve`: Deploy LangChain chains as REST APIs.

In the 0.1.0 release, `langchain-community` was retained as required a dependency of `langchain`.

This allowed imports of vectorstores, chat models, and other integrations to continue working through `langchain` rather than forcing users to update all of their imports to `langchain-community`.

For the 0.2.0 release, weâ€™re removing the dependency of `langchain` on `langchain-community`. This is something weâ€™ve been planning to do since the 0.1 release because we believe this is the right package architecture.

Old imports will continue to work as long as `langchain-community` is installed. These imports will be removed in the 0.4.0 release.

To understand why we think breaking the dependency of `langchain` on `langchain-community` is best we should understand what each package is meant to do.

`langchain` is meant to contain high-level chains and agent architectures. The logic in these should be specified at the level of abstractions like `ChatModel` and `Retriever`, and should not be specific to any one integration. This has two main benefits:

1. `langchain` is fairly lightweight. Here is the full list of required dependencies (after the split)
   
   ```toml
   python = ">=3.8.1,<4.0"
   langchain-core = "^0.2.0"
   langchain-text-splitters = ">=0.0.1,<0.1"
   langsmith = "^0.1.17"
   pydantic = ">=1,<3"
   SQLAlchemy = ">=1.4,<3"
   requests = "^2"
   PyYAML = ">=5.3"
   numpy = "^1"
   aiohttp = "^3.8.3"
   tenacity = "^8.1.0"
   jsonpatch = "^1.33"
   ```
2. `langchain` chains/agents are largely integration-agnostic, which makes it easy to experiment with different integrations and future-proofs your code should there be issues with one specific integration.

There is also a third less tangible benefit which is that being integration-agnostic forces us to find only those very generic abstractions and architectures which generalize well across integrations. Given how general the abilities of the foundational tech are, and how quickly the space is moving, having generic architectures is a good way of future-proofing your applications.

`langchain-community` is intended to have all integration-specific components that are not yet being maintained in separate `langchain-{partner}` packages. Today this is still the majority of integrations and a lot of code. This code is primarily contributed by the community, while `langchain` is largely written by core maintainers. All of these integrations use optional dependencies and conditional imports, which prevents dependency bloat and conflicts but means compatible dependency versions are not made explicit. Given the volume of integrations in `langchain-community` and the speed at which integrations change, itâ€™s very hard to follow semver versioning, and we currently donâ€™t.

All of which is to say that thereâ€™s no large benefits to `langchain` depending on `langchain-community` and some obvious downsides: the functionality in `langchain` should be integration agnostic anyways, `langchain-community` canâ€™t be properly versioned, and depending on `langchain-community` increases the [vulnerability surface](https://github.com/langchain-ai/langchain/discussions/19083) of `langchain`.

For more context about the reason for the organization please see our blog: [https://blog.langchain.dev/langchain-v0-1-0/](https://blog.langchain.dev/langchain-v0-1-0/)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/v0_2/overview.mdx)

* * *


- [Whatâ€™s new in LangChain?](#whats-new-in-langchain)
- [Whatâ€™s coming to LangChain?](#whats-coming-to-langchain)
- [What changed?](#what-changed)
  
  - [TLDR](#tldr)
  - [Ecosystem organization](#ecosystem-organization)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/qa_streaming.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/qa_streaming.ipynb)

# How to stream results from your RAG application

This guide explains how to stream results from a [RAG](/docs/concepts/rag/) application. It covers streaming tokens from the final output as well as intermediate steps of a chain (e.g., from query re-writing).

We'll work off of the Q&amp;A app with sources we built over the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng in the [RAG tutorial](/docs/tutorials/rag/).

## Setup[â€‹](#setup "Direct link to Setup")

### Dependencies[â€‹](#dependencies "Direct link to Dependencies")

We'll use the following packages:

```python
%pip install --upgrade --quiet  langchain langchain-community langchainhub beautifulsoup4
```

### LangSmith[â€‹](#langsmith "Direct link to LangSmith")

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).

Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:

```python
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

### Components[â€‹](#components "Direct link to Components")

We will need to select three components from LangChain's suite of integrations.

A [chat model](/docs/integrations/chat/):

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

An [embedding model](/docs/integrations/text_embedding/):

Select [embeddings model](/docs/integrations/text_embedding/):

OpenAIâ–¾

[OpenAI](#)

[Azure](#)

[Google](#)

[AWS](#)

[HuggingFace](#)

[Ollama](#)

[Cohere](#)

[MistralAI](#)

[Nomic](#)

[NVIDIA](#)

[Voyage AI](#)

[IBM watsonx](#)

[Fake](#)

```bash
pip install -qU langchain-openai
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
```

And a [vector store](/docs/integrations/vectorstores/):

Select [vector store](/docs/integrations/vectorstores/):

In-memoryâ–¾

[In-memory](#)

[AstraDB](#)

[Chroma](#)

[FAISS](#)

[Milvus](#)

[MongoDB](#)

[PGVector](#)

[Pinecone](#)

[Qdrant](#)

```bash
pip install -qU langchain-core
```

```python
from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)
```

## RAG application[â€‹](#rag-application "Direct link to RAG application")

Let's reconstruct the Q&amp;A app with sources we built over the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng in the [RAG tutorial](/docs/tutorials/rag/).

First we index our documents:

```python
import bs4
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing_extensions import List, TypedDict

# Load and chunk contents of the blog
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

```python
# Index chunks
_ = vector_store.add_documents(documents=all_splits)
```

Next we build the application:

```python
from langchain import hub
from langchain_core.documents import Document
from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict

# Define prompt for question-answering
prompt = hub.pull("rlm/rag-prompt")


# Define state for application
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str


# Define application steps
def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"])
    return {"context": retrieved_docs}


def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}


# Compile application and test
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```python
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

## Streaming final outputs[â€‹](#streaming-final-outputs "Direct link to Streaming final outputs")

LangGraph supports several [streaming modes](https://langchain-ai.github.io/langgraph/how-tos/#streaming), which can be controlled by specifying the `stream_mode` parameter. Setting `stream_mode="messages"` allows us to stream tokens from chat model invocations.

In general there can be multiple chat model invocations in an application (although here there is just one). Below, we filter to only the last step using the name of the corresponding node:

```python
input_message = "What is Task Decomposition?"

for message, metadata in graph.stream(
    {"question": "What is Task Decomposition?"},
    stream_mode="messages",
):
    if metadata["langgraph_node"] == "generate":
        print(message.content, end="|")
```

```output
|Task| De|composition| is| a| technique| used| to| break| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| often| involves| prompting| models| to| "|think| step| by| step|,"| allowing| for| clearer| reasoning| and| better| performance| on| intricate| problems|.| This| can| be| achieved| through| various| methods|,| including| simple| prompts|,| task|-specific| instructions|,| or| human| input|.||
```

## Streaming intermediate steps[â€‹](#streaming-intermediate-steps "Direct link to Streaming intermediate steps")

Other streaming modes will generally stream steps from our invocation-- i.e., state updates from individual nodes. In this case, each node is just appending a new key to the state:

```python
for step in graph.stream(
    {"question": "What is Task Decomposition?"},
    stream_mode="updates",
):
    print(f"{step}\n\n----------------\n")
```

```output
{'retrieve': {'context': [Document(id='5bf5e308-6ccb-4f09-94d2-d0c36b8c9980', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.'), Document(id='d8aed221-7943-414d-8ed7-63c2b0e7523b', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.'), Document(id='bfa87007-02ef-4f81-a008-4522ecea1025', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='6aff7fc0-5c21-4986-9f1e-91e89715d934', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content="(3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.")]}}

----------------

{'generate': {'answer': 'Task Decomposition is the process of breaking down a complex task into smaller, manageable steps to enhance understanding and execution. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think through steps systematically, allowing for better problem-solving. It can be achieved through simple prompting, task-specific instructions, or human input.'}}

----------------
```

For more on streaming with LangGraph, check out its [streaming documentation](https://langchain-ai.github.io/langgraph/how-tos/#streaming). For more information on streaming individual LangChain [Runnables](/docs/concepts/runnables/), refer to [this guide](/docs/how_to/streaming/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/qa_streaming.ipynb)

* * *


- [Setup](#setup)
  
  - [Dependencies](#dependencies)
  - [LangSmith](#langsmith)
  - [Components](#components)
- [RAG application](#rag-application)
- [Streaming final outputs](#streaming-final-outputs)
- [Streaming intermediate steps](#streaming-intermediate-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/tutorials/qa_chat_history.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/qa_chat_history.ipynb)

# Build a Retrieval Augmented Generation (RAG) App: Part 2

In many Q&amp;A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of "memory" of past questions and answers, and some logic for incorporating those into its current thinking.

This is the second part of a multi-part tutorial:

- [Part 1](/docs/tutorials/rag/) introduces RAG and walks through a minimal implementation.
- [Part 2](/docs/tutorials/qa_chat_history/) (this guide) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.

Here we focus on **adding logic for incorporating historical messages.** This involves the management of a [chat history](/docs/concepts/chat_history/).

We will cover two approaches:

1. [Chains](/docs/tutorials/qa_chat_history/#chains), in which we execute at most one retrieval step;
2. [Agents](/docs/tutorials/qa_chat_history/#agents), in which we give an LLM discretion to execute multiple retrieval steps.

note

The methods presented here leverage [tool-calling](/docs/concepts/tool_calling/) capabilities in modern [chat models](/docs/concepts/chat_models/). See [this page](/docs/integrations/chat/) for a table of models supporting tool calling features.

For the external knowledge source, we will use the same [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng from the [Part 1](/docs/tutorials/rag/) of the RAG tutorial.

## Setup[â€‹](#setup "Direct link to Setup")

### Components[â€‹](#components "Direct link to Components")

We will need to select three components from LangChain's suite of integrations.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

Select [embeddings model](/docs/integrations/text_embedding/):

OpenAIâ–¾

[OpenAI](#)

[Azure](#)

[Google](#)

[AWS](#)

[HuggingFace](#)

[Ollama](#)

[Cohere](#)

[MistralAI](#)

[Nomic](#)

[NVIDIA](#)

[Voyage AI](#)

[IBM watsonx](#)

[Fake](#)

```bash
pip install -qU langchain-openai
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
```

Select [vector store](/docs/integrations/vectorstores/):

In-memoryâ–¾

[In-memory](#)

[AstraDB](#)

[Chroma](#)

[FAISS](#)

[Milvus](#)

[MongoDB](#)

[PGVector](#)

[Pinecone](#)

[Qdrant](#)

```bash
pip install -qU langchain-core
```

```python
from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)
```

### Dependencies[â€‹](#dependencies "Direct link to Dependencies")

In addition, we'll use the following packages:

```python
%%capture --no-stderr
%pip install --upgrade --quiet langgraph langchain-community beautifulsoup4
```

### LangSmith[â€‹](#langsmith "Direct link to LangSmith")

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).

Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:

```python
os.environ["LANGSMITH_TRACING"] = "true"
if not os.environ.get("LANGSMITH_API_KEY"):
    os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

## Chains[â€‹](#chains "Direct link to Chains")

Let's first revisit the vector store we built in [Part 1](/docs/tutorials/rag/), which indexes an [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng.

```python
import bs4
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing_extensions import List, TypedDict

# Load and chunk contents of the blog
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

```python
# Index chunks
_ = vector_store.add_documents(documents=all_splits)
```

In the [Part 1](/docs/tutorials/rag/) of the RAG tutorial, we represented the user input, retrieved context, and generated answer as separate keys in the state. Conversational experiences can be naturally represented using a sequence of [messages](/docs/concepts/messages/). In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via [tool messages](/docs/concepts/messages/#toolmessage). This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have

1. User input as a `HumanMessage`;
2. Vector store query as an `AIMessage` with tool calls;
3. Retrieved documents as a `ToolMessage`;
4. Final response as a `AIMessage`.

This model for state is so versatile that LangGraph offers a built-in version for convenience:

```python
from langgraph.graph import MessagesState, StateGraph

graph_builder = StateGraph(MessagesState)
```

**API Reference:**[StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

Leveraging [tool-calling](/docs/concepts/tool_calling/) to interact with a retrieval step has another benefit, which is that the query for the retrieval is generated by our model. This is especially important in a conversational setting, where user queries may require contextualization based on the chat history. For instance, consider the following exchange:

> Human: "What is Task Decomposition?"
> 
> AI: "Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model."
> 
> Human: "What are common ways of doing it?"

In this scenario, a model could generate a query such as `"common approaches to task decomposition"`. Tool-calling facilitates this naturally. As in the [query analysis](/docs/tutorials/rag/#query-analysis) section of the RAG tutorial, this allows a model to rewrite user queries into more effective search queries. It also provides support for direct responses that do not involve a retrieval step (e.g., in response to a generic greeting from the user).

Let's turn our retrieval step into a [tool](/docs/concepts/tools/):

```python
from langchain_core.tools import tool


@tool(response_format="content_and_artifact")
def retrieve(query: str):
    """Retrieve information related to a query."""
    retrieved_docs = vector_store.similarity_search(query, k=2)
    serialized = "\n\n".join(
        (f"Source: {doc.metadata}\n" f"Content: {doc.page_content}")
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

See [this guide](/docs/how_to/custom_tools/) for more detail on creating tools.

Our graph will consist of three nodes:

1. A node that fields the user input, either generating a query for the retriever or responding directly;
2. A node for the retriever tool that executes the retrieval step;
3. A node that generates the final response using the retrieved context.

We build them below. Note that we leverage another pre-built LangGraph component, [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode), that executes the tool and adds the result as a `ToolMessage` to the state.

```python
from langchain_core.messages import SystemMessage
from langgraph.prebuilt import ToolNode


# Step 1: Generate an AIMessage that may include a tool-call to be sent.
def query_or_respond(state: MessagesState):
    """Generate tool call for retrieval or respond."""
    llm_with_tools = llm.bind_tools([retrieve])
    response = llm_with_tools.invoke(state["messages"])
    # MessagesState appends messages to state instead of overwriting
    return {"messages": [response]}


# Step 2: Execute the retrieval.
tools = ToolNode([retrieve])


# Step 3: Generate a response using the retrieved content.
def generate(state: MessagesState):
    """Generate answer."""
    # Get generated ToolMessages
    recent_tool_messages = []
    for message in reversed(state["messages"]):
        if message.type == "tool":
            recent_tool_messages.append(message)
        else:
            break
    tool_messages = recent_tool_messages[::-1]

    # Format into prompt
    docs_content = "\n\n".join(doc.content for doc in tool_messages)
    system_message_content = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you "
        "don't know. Use three sentences maximum and keep the "
        "answer concise."
        "\n\n"
        f"{docs_content}"
    )
    conversation_messages = [
        message
        for message in state["messages"]
        if message.type in ("human", "system")
        or (message.type == "ai" and not message.tool_calls)
    ]
    prompt = [SystemMessage(system_message_content)] + conversation_messages

    # Run
    response = llm.invoke(prompt)
    return {"messages": [response]}
```

**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)

Finally, we compile our application into a single `graph` object. In this case, we are just connecting the steps into a sequence. We also allow the first `query_or_respond` step to "short-circuit" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step

```python
from langgraph.graph import END
from langgraph.prebuilt import ToolNode, tools_condition

graph_builder.add_node(query_or_respond)
graph_builder.add_node(tools)
graph_builder.add_node(generate)

graph_builder.set_entry_point("query_or_respond")
graph_builder.add_conditional_edges(
    "query_or_respond",
    tools_condition,
    {END: END, "tools": "tools"},
)
graph_builder.add_edge("tools", "generate")
graph_builder.add_edge("generate", END)

graph = graph_builder.compile()
```

**API Reference:**[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools\_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)

```python
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

Let's test our application.

Note that it responds appropriately to messages that do not require an additional retrieval step:

```python
input_message = "Hello"

for step in graph.stream(
    {"messages": [{"role": "user", "content": input_message}]},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

Hello
==================================[1m Ai Message [0m==================================

Hello! How can I assist you today?
```

And when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:

```python
input_message = "What is Task Decomposition?"

for step in graph.stream(
    {"messages": [{"role": "user", "content": input_message}]},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

What is Task Decomposition?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_dLjB3rkMoxZZxwUGXi33UBeh)
 Call ID: call_dLjB3rkMoxZZxwUGXi33UBeh
  Args:
    query: Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
==================================[1m Ai Message [0m==================================

Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often involves techniques like Chain of Thought (CoT), which encourages models to think step by step, enhancing performance on complex tasks. This approach allows for a clearer understanding of the task and aids in structuring the problem-solving process.
```

Check out the LangSmith trace [here](https://smith.langchain.com/public/70110399-01d3-4b4b-9139-cbcd4edf9d6d/r).

### Stateful management of chat history[â€‹](#stateful-management-of-chat-history "Direct link to Stateful management of chat history")

note

This section of the tutorial previously used the [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) abstraction. You can access that version of the documentation in the [v0.2 docs](https://python.langchain.com/v0.2/docs/tutorials/chatbot/).

As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.

If your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.

Please see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.

In production, the Q&amp;A application will usually persist the chat history into a database, and be able to read and update it appropriately.

[LangGraph](https://langchain-ai.github.io/langgraph/) implements a built-in [persistence layer](https://langchain-ai.github.io/langgraph/concepts/persistence/), making it ideal for chat applications that support multiple conversational turns.

To manage multiple conversational turns and threads, all we have to do is specify a [checkpointer](https://langchain-ai.github.io/langgraph/concepts/persistence/) when compiling our application. Because the nodes in our graph are appending messages to the state, we will retain a consistent chat history across invocations.

LangGraph comes with a simple in-memory checkpointer, which we use below. See its [documentation](https://langchain-ai.github.io/langgraph/concepts/persistence/) for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).

For a detailed walkthrough of how to manage message history, head to the [How to add message history (memory)](/docs/how_to/message_history/) guide.

```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
graph = graph_builder.compile(checkpointer=memory)

# Specify an ID for the thread
config = {"configurable": {"thread_id": "abc123"}}
```

**API Reference:**[MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver)

We can now invoke similar to before:

```python
input_message = "What is Task Decomposition?"

for step in graph.stream(
    {"messages": [{"role": "user", "content": input_message}]},
    stream_mode="values",
    config=config,
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

What is Task Decomposition?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_JZb6GLD812bW2mQsJ5EJQDnN)
 Call ID: call_JZb6GLD812bW2mQsJ5EJQDnN
  Args:
    query: Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
==================================[1m Ai Message [0m==================================

Task Decomposition is a technique used to break down complicated tasks into smaller, manageable steps. It involves using methods like Chain of Thought (CoT) prompting, which encourages the model to think step by step, enhancing performance on complex tasks. This process helps to clarify the model's reasoning and makes it easier to tackle difficult problems.
```

```python
input_message = "Can you look up some common ways of doing it?"

for step in graph.stream(
    {"messages": [{"role": "user", "content": input_message}]},
    stream_mode="values",
    config=config,
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

Can you look up some common ways of doing it?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_kjRI4Y5cJOiB73yvd7dmb6ux)
 Call ID: call_kjRI4Y5cJOiB73yvd7dmb6ux
  Args:
    query: common methods of task decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.
==================================[1m Ai Message [0m==================================

Common ways of performing Task Decomposition include: (1) using Large Language Models (LLMs) with simple prompts like "Steps for XYZ" or "What are the subgoals for achieving XYZ?", (2) employing task-specific instructions such as "Write a story outline" for specific tasks, and (3) incorporating human inputs to guide the decomposition process.
```

Note that the query generated by the model in the second question incorporates the conversational context.

The [LangSmith](https://smith.langchain.com/public/28e6179f-fc56-45e1-9028-447d76352c14/r) trace is particularly informative here, as we can see exactly what messages are visible to our chat model at each step.

## Agents[â€‹](#agents "Direct link to Agents")

[Agents](/docs/concepts/agents/) leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above "chain", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.

Below we assemble a minimal RAG agent. Using LangGraph's [pre-built ReAct agent constructor](https://langchain-ai.github.io/langgraph/how-tos/#langgraph.prebuilt.chat_agent_executor.create_react_agent), we can do this in one line.

tip

Check out LangGraph's [Agentic RAG](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/) tutorial for more advanced formulations.

```python
from langgraph.prebuilt import create_react_agent

agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

Let's inspect the graph:

```python
display(Image(agent_executor.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

The key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.

Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:

```python
config = {"configurable": {"thread_id": "def234"}}

input_message = (
    "What is the standard method for Task Decomposition?\n\n"
    "Once you get the answer, look up common extensions of that method."
)

for event in agent_executor.stream(
    {"messages": [{"role": "user", "content": input_message}]},
    stream_mode="values",
    config=config,
):
    event["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

What is the standard method for Task Decomposition?

Once you get the answer, look up common extensions of that method.
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_Y3YaIzL71B83Cjqa8d2G0O8N)
 Call ID: call_Y3YaIzL71B83Cjqa8d2G0O8N
  Args:
    query: standard method for Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_2JntP1x4XQMWwgVpYurE12ff)
 Call ID: call_2JntP1x4XQMWwgVpYurE12ff
  Args:
    query: common extensions of Task Decomposition methods
=================================[1m Tool Message [0m=================================
Name: retrieve

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.
==================================[1m Ai Message [0m==================================

The standard method for task decomposition involves using techniques such as Chain of Thought (CoT), where a model is instructed to "think step by step" to break down complex tasks into smaller, more manageable components. This approach enhances model performance by allowing for more thorough reasoning and planning. Task decomposition can be accomplished through various means, including:

1. Simple prompting (e.g., asking for steps to achieve a goal).
2. Task-specific instructions (e.g., asking for a story outline).
3. Human inputs to guide the decomposition process.

### Common Extensions of Task Decomposition Methods:

1. **Tree of Thoughts**: This extension builds on CoT by not only decomposing the problem into thought steps but also generating multiple thoughts at each step, creating a tree structure. The search process can employ breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.

These extensions aim to enhance reasoning capabilities and improve the effectiveness of task decomposition in various contexts.
```

Note that the agent:

1. Generates a query to search for a standard method for task decomposition;
2. Receiving the answer, generates a second query to search for common extensions of it;
3. Having received all necessary context, answers the question.

We can see the full sequence of steps, along with latency and other metadata, in the [LangSmith trace](https://smith.langchain.com/public/48cbd35e-9ac1-49ab-8c09-500d54c06b81/r).

## Next steps[â€‹](#next-steps "Direct link to Next steps")

We've covered the steps to build a basic conversational Q&amp;A application:

- We used chains to build a predictable application that generates at most one query per user input;
- We used agents to build an application that can iterate on a sequence of queries.

To explore different types of retrievers and retrieval strategies, visit the [retrievers](/docs/how_to/#retrievers) section of the how-to guides.

For a detailed walkthrough of LangChain's conversation memory abstractions, visit the [How to add message history (memory)](/docs/how_to/message_history/) guide.

To learn more about agents, check out the [conceptual guide](/docs/concepts/agents/) and LangGraph [agent architectures](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/) page.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/tutorials/qa_chat_history.ipynb)

* * *


- [Setup](#setup)
  
  - [Components](#components)
  - [Dependencies](#dependencies)
  - [LangSmith](#langsmith)
- [Chains](#chains)
  
  - [Stateful management of chat history](#stateful-management-of-chat-history)
- [Agents](#agents)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/local_llms.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/local_llms.ipynb)

# Run models locally

## Use case[â€‹](#use-case "Direct link to Use case")

The popularity of projects like [llama.cpp](https://github.com/ggerganov/llama.cpp), [Ollama](https://github.com/ollama/ollama), [GPT4All](https://github.com/nomic-ai/gpt4all), [llamafile](https://github.com/Mozilla-Ocho/llamafile), and others underscore the demand to run LLMs locally (on your own device).

This has at least two important benefits:

1. `Privacy`: Your data is not sent to a third party, and it is not subject to the terms of service of a commercial service
2. `Cost`: There is no inference fee, which is important for token-intensive applications (e.g., [long-running simulations](https://twitter.com/RLanceMartin/status/1691097659262820352?s=20), summarization)

## Overview[â€‹](#overview "Direct link to Overview")

Running an LLM locally requires a few things:

1. `Open-source LLM`: An open-source LLM that can be freely modified and shared
2. `Inference`: Ability to run this LLM on your device w/ acceptable latency

### Open-source LLMs[â€‹](#open-source-llms "Direct link to Open-source LLMs")

Users can now gain access to a rapidly growing set of [open-source LLMs](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better).

These LLMs can be assessed across at least two dimensions (see figure):

1. `Base model`: What is the base-model and how was it trained?
2. `Fine-tuning approach`: Was the base-model fine-tuned and, if so, what [set of instructions](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms#%C2%A7alpaca-an-instruction-following-llama-model) was used?

![Image description](/assets/images/OSS_LLM_overview-9444c9793c76bd4785a5b0cd020c14ef.png)

The relative performance of these models can be assessed using several leaderboards, including:

1. [LmSys](https://chat.lmsys.org/?arena)
2. [GPT4All](https://gpt4all.io/index.html)
3. [HuggingFace](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)

### Inference[â€‹](#inference "Direct link to Inference")

A few frameworks for this have emerged to support inference of open-source LLMs on various devices:

1. [`llama.cpp`](https://github.com/ggerganov/llama.cpp): C++ implementation of llama inference code with [weight optimization / quantization](https://finbarr.ca/how-is-llama-cpp-possible/)
2. [`gpt4all`](https://docs.gpt4all.io/index.html): Optimized C backend for inference
3. [`Ollama`](https://ollama.ai/): Bundles model weights and environment into an app that runs on device and serves the LLM
4. [`llamafile`](https://github.com/Mozilla-Ocho/llamafile): Bundles model weights and everything needed to run the model in a single file, allowing you to run the LLM locally from this file without any additional installation steps

In general, these frameworks will do a few things:

1. `Quantization`: Reduce the memory footprint of the raw model weights
2. `Efficient implementation for inference`: Support inference on consumer hardware (e.g., CPU or laptop GPU)

In particular, see [this excellent post](https://finbarr.ca/how-is-llama-cpp-possible/) on the importance of quantization.

![Image description](/assets/images/llama-memory-weights-aaccef5df087e993b0f46277500039b6.png)

With less precision, we radically decrease the memory needed to store the LLM in memory.

In addition, we can see the importance of GPU memory bandwidth [sheet](https://docs.google.com/spreadsheets/d/1OehfHHNSn66BP2h3Bxp2NJTVX97icU0GmCXF6pK23H8/edit#gid=0)!

A Mac M2 Max is 5-6x faster than a M1 for inference due to the larger GPU memory bandwidth.

![Image description](/assets/images/llama_t_put-c6f0ea201a6dd508999170325cd6804a.png)

### Formatting prompts[â€‹](#formatting-prompts "Direct link to Formatting prompts")

Some providers have [chat model](/docs/concepts/chat_models/) wrappers that takes care of formatting your input prompt for the specific local model you're using. However, if you are prompting local models with a [text-in/text-out LLM](/docs/concepts/text_llms/) wrapper, you may need to use a prompt tailored for your specific model.

This can [require the inclusion of special tokens](https://huggingface.co/blog/llama2#how-to-prompt-llama-2). [Here's an example for LLaMA 2](https://smith.langchain.com/hub/rlm/rag-prompt-llama).

## Quickstart[â€‹](#quickstart "Direct link to Quickstart")

[`Ollama`](https://ollama.ai/) is one way to easily run inference on macOS.

The instructions [here](https://github.com/jmorganca/ollama?tab=readme-ov-file#ollama) provide details, which we summarize:

- [Download and run](https://ollama.ai/download) the app
- From command line, fetch a model from this [list of options](https://github.com/jmorganca/ollama): e.g., `ollama pull llama3.1:8b`
- When the app is running, all models are automatically served on `localhost:11434`

```python
%pip install -qU langchain_ollama
```

```python
from langchain_ollama import OllamaLLM

llm = OllamaLLM(model="llama3.1:8b")

llm.invoke("The first man on the moon was ...")
```

**API Reference:**[OllamaLLM](https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html)

```output
'...Neil Armstrong!\n\nOn July 20, 1969, Neil Armstrong became the first person to set foot on the lunar surface, famously declaring "That\'s one small step for man, one giant leap for mankind" as he stepped off the lunar module Eagle onto the Moon\'s surface.\n\nWould you like to know more about the Apollo 11 mission or Neil Armstrong\'s achievements?'
```

Stream tokens as they are being generated:

```python
for chunk in llm.stream("The first man on the moon was ..."):
    print(chunk, end="|", flush=True)
```

```````output
...|
``````output
Neil| Armstrong|,| an| American| astronaut|.| He| stepped| out| of| the| lunar| module| Eagle| and| onto| the| surface| of| the| Moon| on| July| |20|,| |196|9|,| famously| declaring|:| "|That|'s| one| small| step| for| man|,| one| giant| leap| for| mankind|."||
```````

Ollama also includes a chat model wrapper that handles formatting conversation turns:

```python
from langchain_ollama import ChatOllama

chat_model = ChatOllama(model="llama3.1:8b")

chat_model.invoke("Who was the first man on the moon?")
```

**API Reference:**[ChatOllama](https://python.langchain.com/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html)

```output
AIMessage(content='The answer is a historic one!\n\nThe first man to walk on the Moon was Neil Armstrong, an American astronaut and commander of the Apollo 11 mission. On July 20, 1969, Armstrong stepped out of the lunar module Eagle onto the surface of the Moon, famously declaring:\n\n"That\'s one small step for man, one giant leap for mankind."\n\nArmstrong was followed by fellow astronaut Edwin "Buzz" Aldrin, who also walked on the Moon during the mission. Michael Collins remained in orbit around the Moon in the command module Columbia.\n\nNeil Armstrong passed away on August 25, 2012, but his legacy as a pioneering astronaut and engineer continues to inspire people around the world!', response_metadata={'model': 'llama3.1:8b', 'created_at': '2024-08-01T00:38:29.176717Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 10681861417, 'load_duration': 34270292, 'prompt_eval_count': 19, 'prompt_eval_duration': 6209448000, 'eval_count': 141, 'eval_duration': 4432022000}, id='run-7bed57c5-7f54-4092-912c-ae49073dcd48-0', usage_metadata={'input_tokens': 19, 'output_tokens': 141, 'total_tokens': 160})
```

## Environment[â€‹](#environment "Direct link to Environment")

Inference speed is a challenge when running models locally (see above).

To minimize latency, it is desirable to run models locally on GPU, which ships with many consumer laptops [e.g., Apple devices](https://www.apple.com/newsroom/2022/06/apple-unveils-m2-with-breakthrough-performance-and-capabilities/).

And even with GPU, the available GPU memory bandwidth (as noted above) is important.

### Running Apple silicon GPU[â€‹](#running-apple-silicon-gpu "Direct link to Running Apple silicon GPU")

`Ollama` and [`llamafile`](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#gpu-support) will automatically utilize the GPU on Apple devices.

Other frameworks require the user to set up the environment to utilize the Apple GPU.

For example, `llama.cpp` python bindings can be configured to use the GPU via [Metal](https://developer.apple.com/metal/).

Metal is a graphics and compute API created by Apple providing near-direct access to the GPU.

See the [`llama.cpp`](/docs/integrations/llms/llamacpp/) setup [here](https://github.com/abetlen/llama-cpp-python/blob/main/docs/install/macos.md) to enable this.

In particular, ensure that conda is using the correct virtual environment that you created (`miniforge3`).

E.g., for me:

```text
conda activate /Users/rlm/miniforge3/envs/llama
```

With the above confirmed, then:

```text
CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dir
```

## LLMs[â€‹](#llms "Direct link to LLMs")

There are various ways to gain access to quantized model weights.

1. [`HuggingFace`](https://huggingface.co/TheBloke) - Many quantized model are available for download and can be run with framework such as [`llama.cpp`](https://github.com/ggerganov/llama.cpp). You can also download models in [`llamafile` format](https://huggingface.co/models?other=llamafile) from HuggingFace.
2. [`gpt4all`](https://gpt4all.io/index.html) - The model explorer offers a leaderboard of metrics and associated quantized models available for download
3. [`Ollama`](https://github.com/jmorganca/ollama) - Several models can be accessed directly via `pull`

### Ollama[â€‹](#ollama "Direct link to Ollama")

With [Ollama](https://github.com/jmorganca/ollama), fetch a model via `ollama pull <model family>:<tag>`:

- E.g., for Llama 2 7b: `ollama pull llama2` will download the most basic version of the model (e.g., smallest # parameters and 4 bit quantization)
- We can also specify a particular version from the [model list](https://github.com/jmorganca/ollama?tab=readme-ov-file#model-library), e.g., `ollama pull llama2:13b`
- See the full set of parameters on the [API reference page](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.ollama.Ollama.html)

```python
llm = OllamaLLM(model="llama2:13b")
llm.invoke("The first man on the moon was ... think step by step")
```

```output
' Sure! Here\'s the answer, broken down step by step:\n\nThe first man on the moon was... Neil Armstrong.\n\nHere\'s how I arrived at that answer:\n\n1. The first manned mission to land on the moon was Apollo 11.\n2. The mission included three astronauts: Neil Armstrong, Edwin "Buzz" Aldrin, and Michael Collins.\n3. Neil Armstrong was the mission commander and the first person to set foot on the moon.\n4. On July 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\'s surface, famously declaring "That\'s one small step for man, one giant leap for mankind."\n\nSo, the first man on the moon was Neil Armstrong!'
```

### Llama.cpp[â€‹](#llamacpp "Direct link to Llama.cpp")

Llama.cpp is compatible with a [broad set of models](https://github.com/ggerganov/llama.cpp).

For example, below we run inference on `llama2-13b` with 4 bit quantization downloaded from [HuggingFace](https://huggingface.co/TheBloke/Llama-2-13B-GGML/tree/main).

As noted above, see the [API reference](https://python.langchain.com/api_reference/langchain/llms/langchain.llms.llamacpp.LlamaCpp.html?highlight=llamacpp#langchain.llms.llamacpp.LlamaCpp) for the full set of parameters.

From the [llama.cpp API reference docs](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.llamacpp.LlamaCpp.html), a few are worth commenting on:

`n_gpu_layers`: number of layers to be loaded into GPU memory

- Value: 1
- Meaning: Only one layer of the model will be loaded into GPU memory (1 is often sufficient).

`n_batch`: number of tokens the model should process in parallel

- Value: n\_batch
- Meaning: It's recommended to choose a value between 1 and n\_ctx (which in this case is set to 2048)

`n_ctx`: Token context window

- Value: 2048
- Meaning: The model will consider a window of 2048 tokens at a time

`f16_kv`: whether the model should use half-precision for the key/value cache

- Value: True
- Meaning: The model will use half-precision, which can be more memory efficient; Metal only supports True.

```python
%env CMAKE_ARGS="-DLLAMA_METAL=on"
%env FORCE_CMAKE=1
%pip install --upgrade --quiet  llama-cpp-python --no-cache-dirclear
```

```python
from langchain_community.llms import LlamaCpp
from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler

llm = LlamaCpp(
    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",
    n_gpu_layers=1,
    n_batch=512,
    n_ctx=2048,
    f16_kv=True,
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
    verbose=True,
)
```

**API Reference:**[LlamaCpp](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.llamacpp.LlamaCpp.html) | [CallbackManager](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.CallbackManager.html) | [StreamingStdOutCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler.html)

The console log will show the below to indicate Metal was enabled properly from steps above:

```text
ggml_metal_init: allocating
ggml_metal_init: using MPS
```

```python
llm.invoke("The first man on the moon was ... Let's think step by step")
```

```````output
Llama.generate: prefix-match hit
``````output
 and use logical reasoning to figure out who the first man on the moon was.

Here are some clues:

1. The first man on the moon was an American.
2. He was part of the Apollo 11 mission.
3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.
4. His last name is Armstrong.

Now, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.
Therefore, the first man on the moon was Neil Armstrong!
``````output

llama_print_timings:        load time =  9623.21 ms
llama_print_timings:      sample time =   143.77 ms /   203 runs   (    0.71 ms per token,  1412.01 tokens per second)
llama_print_timings: prompt eval time =   485.94 ms /     7 tokens (   69.42 ms per token,    14.40 tokens per second)
llama_print_timings:        eval time =  6385.16 ms /   202 runs   (   31.61 ms per token,    31.64 tokens per second)
llama_print_timings:       total time =  7279.28 ms
```````

```output
" and use logical reasoning to figure out who the first man on the moon was.\n\nHere are some clues:\n\n1. The first man on the moon was an American.\n2. He was part of the Apollo 11 mission.\n3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.\n4. His last name is Armstrong.\n\nNow, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.\nTherefore, the first man on the moon was Neil Armstrong!"
```

### GPT4All[â€‹](#gpt4all "Direct link to GPT4All")

We can use model weights downloaded from [GPT4All](/docs/integrations/llms/gpt4all/) model explorer.

Similar to what is shown above, we can run inference and use [the API reference](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.gpt4all.GPT4All.html) to set parameters of interest.

```python
%pip install gpt4all
```

```python
from langchain_community.llms import GPT4All

llm = GPT4All(
    model="/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin"
)
```

**API Reference:**[GPT4All](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.gpt4all.GPT4All.html)

```python
llm.invoke("The first man on the moon was ... Let's think step by step")
```

```output
".\n1) The United States decides to send a manned mission to the moon.2) They choose their best astronauts and train them for this specific mission.3) They build a spacecraft that can take humans to the moon, called the Lunar Module (LM).4) They also create a larger spacecraft, called the Saturn V rocket, which will launch both the LM and the Command Service Module (CSM), which will carry the astronauts into orbit.5) The mission is planned down to the smallest detail: from the trajectory of the rockets to the exact movements of the astronauts during their moon landing.6) On July 16, 1969, the Saturn V rocket launches from Kennedy Space Center in Florida, carrying the Apollo 11 mission crew into space.7) After one and a half orbits around the Earth, the LM separates from the CSM and begins its descent to the moon's surface.8) On July 20, 1969, at 2:56 pm EDT (GMT-4), Neil Armstrong becomes the first man on the moon. He speaks these"
```

### llamafile[â€‹](#llamafile "Direct link to llamafile")

One of the simplest ways to run an LLM locally is using a [llamafile](https://github.com/Mozilla-Ocho/llamafile). All you need to do is:

1. Download a llamafile from [HuggingFace](https://huggingface.co/models?other=llamafile)
2. Make the file executable
3. Run the file

llamafiles bundle model weights and a [specially-compiled](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#technical-details) version of [`llama.cpp`](https://github.com/ggerganov/llama.cpp) into a single file that can run on most computers without any additional dependencies. They also come with an embedded inference server that provides an [API](https://github.com/Mozilla-Ocho/llamafile/blob/main/llama.cpp/server/README.md#api-endpoints) for interacting with your model.

Here's a simple bash script that shows all 3 setup steps:

```bash
# Download a llamafile from HuggingFace
wget https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile

# Make the file executable. On Windows, instead just rename the file to end in ".exe".
chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile

# Start the model server. Listens at http://localhost:8080 by default.
./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser
```

After you run the above setup steps, you can use LangChain to interact with your model:

```python
from langchain_community.llms.llamafile import Llamafile

llm = Llamafile()

llm.invoke("The first man on the moon was ... Let's think step by step.")
```

**API Reference:**[Llamafile](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.llamafile.Llamafile.html)

```output
"\nFirstly, let's imagine the scene where Neil Armstrong stepped onto the moon. This happened in 1969. The first man on the moon was Neil Armstrong. We already know that.\n2nd, let's take a step back. Neil Armstrong didn't have any special powers. He had to land his spacecraft safely on the moon without injuring anyone or causing any damage. If he failed to do this, he would have been killed along with all those people who were on board the spacecraft.\n3rd, let's imagine that Neil Armstrong successfully landed his spacecraft on the moon and made it back to Earth safely. The next step was for him to be hailed as a hero by his people back home. It took years before Neil Armstrong became an American hero.\n4th, let's take another step back. Let's imagine that Neil Armstrong wasn't hailed as a hero, and instead, he was just forgotten. This happened in the 1970s. Neil Armstrong wasn't recognized for his remarkable achievement on the moon until after he died.\n5th, let's take another step back. Let's imagine that Neil Armstrong didn't die in the 1970s and instead, lived to be a hundred years old. This happened in 2036. In the year 2036, Neil Armstrong would have been a centenarian.\nNow, let's think about the present. Neil Armstrong is still alive. He turned 95 years old on July 20th, 2018. If he were to die now, his achievement of becoming the first human being to set foot on the moon would remain an unforgettable moment in history.\nI hope this helps you understand the significance and importance of Neil Armstrong's achievement on the moon!"
```

## Prompts[â€‹](#prompts "Direct link to Prompts")

Some LLMs will benefit from specific prompts.

For example, LLaMA will use [special tokens](https://twitter.com/RLanceMartin/status/1681879318493003776?s=20).

We can use `ConditionalPromptSelector` to set prompt based on the model type.

```python
# Set our LLM
llm = LlamaCpp(
    model_path="/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin",
    n_gpu_layers=1,
    n_batch=512,
    n_ctx=2048,
    f16_kv=True,
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),
    verbose=True,
)
```

Set the associated prompt based upon the model version.

```python
from langchain.chains.prompt_selector import ConditionalPromptSelector
from langchain_core.prompts import PromptTemplate

DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""<<SYS>> \n You are an assistant tasked with improving Google search \
results. \n <</SYS>> \n\n [INST] Generate THREE Google search queries that \
are similar to this question. The output should be a numbered list of questions \
and each should have a question mark at the end: \n\n {question} [/INST]""",
)

DEFAULT_SEARCH_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""You are an assistant tasked with improving Google search \
results. Generate THREE Google search queries that are similar to \
this question. The output should be a numbered list of questions and each \
should have a question mark at the end: {question}""",
)

QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(
    default_prompt=DEFAULT_SEARCH_PROMPT,
    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],
)

prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)
prompt
```

**API Reference:**[ConditionalPromptSelector](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.prompt_selector.ConditionalPromptSelector.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

```output
PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='<<SYS>> \n You are an assistant tasked with improving Google search results. \n <</SYS>> \n\n [INST] Generate THREE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: \n\n {question} [/INST]', template_format='f-string', validate_template=True)
```

```python
# Chain
chain = prompt | llm
question = "What NFL team won the Super Bowl in the year that Justin Bieber was born?"
chain.invoke({"question": question})
```

```````output
  Sure! Here are three similar search queries with a question mark at the end:

1. Which NBA team did LeBron James lead to a championship in the year he was drafted?
2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?
3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?
``````output

llama_print_timings:        load time = 14943.19 ms
llama_print_timings:      sample time =    72.93 ms /   101 runs   (    0.72 ms per token,  1384.87 tokens per second)
llama_print_timings: prompt eval time = 14942.95 ms /    93 tokens (  160.68 ms per token,     6.22 tokens per second)
llama_print_timings:        eval time =  3430.85 ms /   100 runs   (   34.31 ms per token,    29.15 tokens per second)
llama_print_timings:       total time = 18578.26 ms
```````

```output
'  Sure! Here are three similar search queries with a question mark at the end:\n\n1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\n2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\n3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?'
```

We also can use the LangChain Prompt Hub to fetch and / or store prompts that are model specific.

This will work with your [LangSmith API key](https://docs.smith.langchain.com/).

For example, [here](https://smith.langchain.com/hub/rlm/rag-prompt-llama) is a prompt for RAG with LLaMA-specific tokens.

## Use cases[â€‹](#use-cases "Direct link to Use cases")

Given an `llm` created from one of the models above, you can use it for [many use cases](/docs/how_to/#use-cases).

For example, you can implement a [RAG application](/docs/tutorials/rag/) using the chat models demonstrated here.

In general, use cases for local LLMs can be driven by at least two factors:

- `Privacy`: private data (e.g., journals, etc) that a user does not want to share
- `Cost`: text preprocessing (extraction/tagging), summarization, and agent simulations are token-use-intensive tasks

In addition, [here](https://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/) is an overview on fine-tuning, which can utilize open-source LLMs.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/local_llms.ipynb)

* * *


- [Use case](#use-case)
- [Overview](#overview)
  
  - [Open-source LLMs](#open-source-llms)
  - [Inference](#inference)
  - [Formatting prompts](#formatting-prompts)
- [Quickstart](#quickstart)
- [Environment](#environment)
  
  - [Running Apple silicon GPU](#running-apple-silicon-gpu)
- [LLMs](#llms)
  
  - [Ollama](#ollama)
  - [Llama.cpp](#llamacpp)
  - [GPT4All](#gpt4all)
  - [llamafile](#llamafile)
- [Prompts](#prompts)
- [Use cases](#use-cases)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/query_few_shot.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/query_few_shot.ipynb)

# How to add examples to the prompt for query analysis

As our query analysis becomes more complex, the LLM may struggle to understand how exactly it should respond in certain scenarios. In order to improve performance here, we can [add examples](/docs/concepts/few_shot_prompting/) to the prompt to guide the LLM.

Let's take a look at how we can add examples for a LangChain YouTube video query analyzer.

## Setup[â€‹](#setup "Direct link to Setup")

#### Install dependencies[â€‹](#install-dependencies "Direct link to Install dependencies")

```python
# %pip install -qU langchain-core langchain-openai
```

#### Set environment variables[â€‹](#set-environment-variables "Direct link to Set environment variables")

We'll use OpenAI in this example:

```python
import getpass
import os

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass()

# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

## Query schema[â€‹](#query-schema "Direct link to Query schema")

We'll define a query schema that we want our model to output. To make our query analysis a bit more interesting, we'll add a `sub_queries` field that contains more narrow questions derived from the top level question.

```python
from typing import List, Optional

from pydantic import BaseModel, Field

sub_queries_description = """\
If the original question contains multiple distinct sub-questions, \
or if there are more generic questions that would be helpful to answer in \
order to answer the original question, write a list of all relevant sub-questions. \
Make sure this list is comprehensive and covers all parts of the original question. \
It's ok if there's redundancy in the sub-questions. \
Make sure the sub-questions are as narrowly focused as possible."""


class Search(BaseModel):
    """Search over a database of tutorial videos about a software library."""

    query: str = Field(
        ...,
        description="Primary similarity search query applied to video transcripts.",
    )
    sub_queries: List[str] = Field(
        default_factory=list, description=sub_queries_description
    )
    publish_year: Optional[int] = Field(None, description="Year video was published")
```

## Query generation[â€‹](#query-generation "Direct link to Query generation")

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

system = """You are an expert at converting user questions into database queries. \
You have access to a database of tutorial videos about a software library for building LLM-powered applications. \
Given a question, return a list of database queries optimized to retrieve the most relevant results.

If there are acronyms or words you are not familiar with, do not try to rephrase them."""

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        MessagesPlaceholder("examples", optional=True),
        ("human", "{question}"),
    ]
)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_llm = llm.with_structured_output(Search)
query_analyzer = {"question": RunnablePassthrough()} | prompt | structured_llm
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

Let's try out our query analyzer without any examples in the prompt:

```python
query_analyzer.invoke(
    "what's the difference between web voyager and reflection agents? do both use langgraph?"
)
```

```output
Search(query='difference between web voyager and reflection agents', sub_queries=['what is web voyager', 'what are reflection agents', 'do both web voyager and reflection agents use langgraph?'], publish_year=None)
```

## Adding examples and tuning the prompt[â€‹](#adding-examples-and-tuning-the-prompt "Direct link to Adding examples and tuning the prompt")

This works pretty well, but we probably want it to decompose the question even further to separate the queries about Web Voyager and Reflection Agents.

To tune our query generation results, we can add some examples of inputs questions and gold standard output queries to our prompt.

```python
examples = []
```

```python
question = "What's chat langchain, is it a langchain template?"
query = Search(
    query="What is chat langchain and is it a langchain template?",
    sub_queries=["What is chat langchain", "What is a langchain template"],
)
examples.append({"input": question, "tool_calls": [query]})
```

```python
question = "How to build multi-agent system and stream intermediate steps from it"
query = Search(
    query="How to build multi-agent system and stream intermediate steps from it",
    sub_queries=[
        "How to build multi-agent system",
        "How to stream intermediate steps from multi-agent system",
        "How to stream intermediate steps",
    ],
)

examples.append({"input": question, "tool_calls": [query]})
```

```python
question = "LangChain agents vs LangGraph?"
query = Search(
    query="What's the difference between LangChain agents and LangGraph? How do you deploy them?",
    sub_queries=[
        "What are LangChain agents",
        "What is LangGraph",
        "How do you deploy LangChain agents",
        "How do you deploy LangGraph",
    ],
)
examples.append({"input": question, "tool_calls": [query]})
```

Now we need to update our prompt template and chain so that the examples are included in each prompt. Since we're working with OpenAI function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. We'll create a `tool_example_to_messages` helper function to handle this for us:

```python
import uuid
from typing import Dict

from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)


def tool_example_to_messages(example: Dict) -> List[BaseMessage]:
    messages: List[BaseMessage] = [HumanMessage(content=example["input"])]
    openai_tool_calls = []
    for tool_call in example["tool_calls"]:
        openai_tool_calls.append(
            {
                "id": str(uuid.uuid4()),
                "type": "function",
                "function": {
                    "name": tool_call.__class__.__name__,
                    "arguments": tool_call.json(),
                },
            }
        )
    messages.append(
        AIMessage(content="", additional_kwargs={"tool_calls": openai_tool_calls})
    )
    tool_outputs = example.get("tool_outputs") or [
        "You have correctly called this tool."
    ] * len(openai_tool_calls)
    for output, tool_call in zip(tool_outputs, openai_tool_calls):
        messages.append(ToolMessage(content=output, tool_call_id=tool_call["id"]))
    return messages


example_msgs = [msg for ex in examples for msg in tool_example_to_messages(ex)]
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html)

```python
from langchain_core.prompts import MessagesPlaceholder

query_analyzer_with_examples = (
    {"question": RunnablePassthrough()}
    | prompt.partial(examples=example_msgs)
    | structured_llm
)
```

**API Reference:**[MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)

```python
query_analyzer_with_examples.invoke(
    "what's the difference between web voyager and reflection agents? do both use langgraph?"
)
```

```output
Search(query="What's the difference between web voyager and reflection agents? Do both use langgraph?", sub_queries=['What is web voyager', 'What are reflection agents', 'Do web voyager and reflection agents use langgraph?'], publish_year=None)
```

Thanks to our examples we get a slightly more decomposed search query. With some more prompt engineering and tuning of our examples we could improve query generation even more.

You can see that the examples are passed to the model as messages in the [LangSmith trace](https://smith.langchain.com/public/aeaaafce-d2b1-4943-9a61-bc954e8fc6f2/r).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/query_few_shot.ipynb)

* * *


- [Setup](#setup)
- [Query schema](#query-schema)
- [Query generation](#query-generation)
- [Adding examples and tuning the prompt](#adding-examples-and-tuning-the-prompt)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/vectorstores.mdx)

# How to create and query vector stores

info

Head to [Integrations](/docs/integrations/vectorstores/) for documentation on built-in integrations with 3rd-party vector stores.

One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you.

## Get started[â€‹](#get-started "Direct link to Get started")

This guide showcases basic functionality related to vector stores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the [text embedding model interfaces](/docs/how_to/embed_text/) before diving into this.

Before using the vectorstore at all, we need to load some data and initialize an embedding model.

We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.

```python
import os
import getpass

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
```

```python
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader('state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
```

**API Reference:**[TextLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

There are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.

- Chroma
- FAISS
- Lance

This walkthrough uses the `chroma` vector database, which runs on your local machine as a library.

```bash
pip install langchain-chroma
```

```python
from langchain_chroma import Chroma

db = Chroma.from_documents(documents, OpenAIEmbeddings())
```

This walkthrough uses the `FAISS` vector database, which makes use of the Facebook AI Similarity Search (FAISS) library.

```bash
pip install faiss-cpu
```

```python
from langchain_community.vectorstores import FAISS

db = FAISS.from_documents(documents, OpenAIEmbeddings())
```

**API Reference:**[FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html)

This notebook shows how to use functionality related to the LanceDB vector database based on the Lance data format.

```bash
pip install lancedb
```

```python
from langchain_community.vectorstores import LanceDB

import lancedb

db = lancedb.connect("/tmp/lancedb")
table = db.create_table(
    "my_table",
    data=[
        {
            "vector": embeddings.embed_query("Hello World"),
            "text": "Hello World",
            "id": "1",
        }
    ],
    mode="overwrite",
)
db = LanceDB.from_documents(documents, OpenAIEmbeddings())
```

**API Reference:**[LanceDB](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.lancedb.LanceDB.html)

## Similarity search[â€‹](#similarity-search "Direct link to Similarity search")

All vectorstores expose a `similarity_search` method. This will take incoming documents, create an embedding of them, and then find all documents with the most similar embedding.

```python
query = "What did the president say about Ketanji Brown Jackson"
docs = db.similarity_search(query)
print(docs[0].page_content)
```

```output
    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.

    Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.

    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.

    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.
```

### Similarity search by vector[â€‹](#similarity-search-by-vector "Direct link to Similarity search by vector")

It is also possible to do a search for documents similar to a given embedding vector using `similarity_search_by_vector` which accepts an embedding vector as a parameter instead of a string.

```python
embedding_vector = OpenAIEmbeddings().embed_query(query)
docs = db.similarity_search_by_vector(embedding_vector)
print(docs[0].page_content)
```

```output
    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.

    Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.

    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.

    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.
```

## Async Operations[â€‹](#async-operations "Direct link to Async Operations")

Vector stores are usually run as a separate service that requires some IO operations, and therefore they might be called asynchronously. That gives performance benefits as you don't waste time waiting for responses from external services. That might also be important if you work with an asynchronous framework, such as [FastAPI](https://fastapi.tiangolo.com/).

LangChain supports async operation on vector stores. All the methods might be called using their async counterparts, with the prefix `a`, meaning `async`.

```python
docs = await db.asimilarity_search(query)
docs
```

```output
[Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.', metadata={'source': 'state_of_the_union.txt'}),
 Document(page_content='A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since sheâ€™s been nominated, sheâ€™s received a broad range of supportâ€”from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, weâ€™ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWeâ€™ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWeâ€™re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWeâ€™re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.', metadata={'source': 'state_of_the_union.txt'}),
 Document(page_content='And for our LGBTQ+ Americans, letâ€™s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \n\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \n\nWhile it often appears that we never agree, that isnâ€™t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \n\nAnd soon, weâ€™ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \n\nSo tonight Iâ€™m offering a Unity Agenda for the Nation. Four big things we can do together.  \n\nFirst, beat the opioid epidemic.', metadata={'source': 'state_of_the_union.txt'}),
 Document(page_content='Tonight, Iâ€™m announcing a crackdown on these companies overcharging American businesses and consumers. \n\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \n\nThat ends on my watch. \n\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \n\nWeâ€™ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \n\nLetâ€™s pass the Paycheck Fairness Act and paid leave.  \n\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \n\nLetâ€™s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jillâ€”our First Lady who teaches full-timeâ€”calls Americaâ€™s best-kept secret: community colleges.', metadata={'source': 'state_of_the_union.txt'})]
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/vectorstores.mdx)

* * *


- [Get started](#get-started)
- [Similarity search](#similarity-search)
  
  - [Similarity search by vector](#similarity-search-by-vector)
- [Async Operations](#async-operations)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/tools_error.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/tools_error.ipynb)

# How to handle tool errors

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models/)
- [LangChain Tools](/docs/concepts/tools/)
- [How to use a model to call tools](/docs/how_to/tool_calling/)

[Calling tools](/docs/concepts/tool_calling/) with an LLM is generally more reliable than pure prompting, but it isn't perfect. The model may try to call a tool that doesn't exist or fail to return arguments that match the requested schema. Strategies like keeping schemas simple, reducing the number of tools you pass at once, and having good names and descriptions can help mitigate this risk, but aren't foolproof.

This guide covers some ways to build error handling into your chains to mitigate these failure modes.

## Setup[â€‹](#setup "Direct link to Setup")

We'll need to install the following packages:

```python
%pip install --upgrade --quiet langchain-core langchain-openai
```

If you'd like to trace your runs in [LangSmith](https://docs.smith.langchain.com/) uncomment and set the following environment variables:

```python
import getpass
import os

# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

## Chain[â€‹](#chain "Direct link to Chain")

Suppose we have the following (dummy) tool and tool-calling chain. We'll make our tool intentionally convoluted to try and trip up the model.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
# Define tool
from langchain_core.tools import tool


@tool
def complex_tool(int_arg: int, float_arg: float, dict_arg: dict) -> int:
    """Do something complex with a complex tool."""
    return int_arg * float_arg


llm_with_tools = llm.bind_tools(
    [complex_tool],
)

# Define chain
chain = llm_with_tools | (lambda msg: msg.tool_calls[0]["args"]) | complex_tool
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

We can see that when we try to invoke this chain with even a fairly explicit input, the model fails to correctly call the tool (it forgets the `dict_arg` argument).

```python
chain.invoke(
    "use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg"
)
```

```````output
---------------------------------------------------------------------------
``````output
ValidationError                           Traceback (most recent call last)
``````output
Cell In[5], line 1
----> 1 chain.invoke(
      2     "use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg"
      3 )
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2998, in RunnableSequence.invoke(self, input, config, **kwargs)
   2996             input = context.run(step.invoke, input, config, **kwargs)
   2997         else:
-> 2998             input = context.run(step.invoke, input, config)
   2999 # finish the root run
   3000 except BaseException as e:
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:456, in BaseTool.invoke(self, input, config, **kwargs)
    449 def invoke(
    450     self,
    451     input: Union[str, Dict, ToolCall],
    452     config: Optional[RunnableConfig] = None,
    453     **kwargs: Any,
    454 ) -> Any:
    455     tool_input, kwargs = _prep_run_args(input, config, **kwargs)
--> 456     return self.run(tool_input, **kwargs)
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:659, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)
    657 if error_to_raise:
    658     run_manager.on_tool_error(error_to_raise)
--> 659     raise error_to_raise
    660 output = _format_output(content, artifact, tool_call_id, self.name, status)
    661 run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:622, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)
    620 context = copy_context()
    621 context.run(_set_config_context, child_config)
--> 622 tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input)
    623 if signature(self._run).parameters.get("run_manager"):
    624     tool_kwargs["run_manager"] = run_manager
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:545, in BaseTool._to_args_and_kwargs(self, tool_input)
    544 def _to_args_and_kwargs(self, tool_input: Union[str, Dict]) -> Tuple[Tuple, Dict]:
--> 545     tool_input = self._parse_input(tool_input)
    546     # For backwards compatibility, if run_input is a string,
    547     # pass as a positional argument.
    548     if isinstance(tool_input, str):
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:487, in BaseTool._parse_input(self, tool_input)
    485 if input_args is not None:
    486     if issubclass(input_args, BaseModel):
--> 487         result = input_args.model_validate(tool_input)
    488         result_dict = result.model_dump()
    489     elif issubclass(input_args, BaseModelV1):
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/pydantic/main.py:568, in BaseModel.model_validate(cls, obj, strict, from_attributes, context)
    566 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks
    567 __tracebackhide__ = True
--> 568 return cls.__pydantic_validator__.validate_python(
    569     obj, strict=strict, from_attributes=from_attributes, context=context
    570 )
``````output
ValidationError: 1 validation error for complex_toolSchema
dict_arg
  Field required [type=missing, input_value={'int_arg': 5, 'float_arg': 2.1}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.8/v/missing
```````

## Try/except tool call[â€‹](#tryexcept-tool-call "Direct link to Try/except tool call")

The simplest way to more gracefully handle errors is to try/except the tool-calling step and return a helpful message on errors:

```python
from typing import Any

from langchain_core.runnables import Runnable, RunnableConfig


def try_except_tool(tool_args: dict, config: RunnableConfig) -> Runnable:
    try:
        complex_tool.invoke(tool_args, config=config)
    except Exception as e:
        return f"Calling tool with arguments:\n\n{tool_args}\n\nraised the following error:\n\n{type(e)}: {e}"


chain = llm_with_tools | (lambda msg: msg.tool_calls[0]["args"]) | try_except_tool

print(
    chain.invoke(
        "use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg"
    )
)
```

**API Reference:**[Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html)

```output
Calling tool with arguments:

{'int_arg': 5, 'float_arg': 2.1}

raised the following error:

<class 'pydantic_core._pydantic_core.ValidationError'>: 1 validation error for complex_toolSchema
dict_arg
  Field required [type=missing, input_value={'int_arg': 5, 'float_arg': 2.1}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.8/v/missing
```

## Fallbacks[â€‹](#fallbacks "Direct link to Fallbacks")

We can also try to fallback to a better model in the event of a tool invocation error. In this case we'll fall back to an identical chain that uses `gpt-4-1106-preview` instead of `gpt-3.5-turbo`.

```python
chain = llm_with_tools | (lambda msg: msg.tool_calls[0]["args"]) | complex_tool

better_model = ChatOpenAI(model="gpt-4-1106-preview", temperature=0).bind_tools(
    [complex_tool], tool_choice="complex_tool"
)

better_chain = better_model | (lambda msg: msg.tool_calls[0]["args"]) | complex_tool

chain_with_fallback = chain.with_fallbacks([better_chain])

chain_with_fallback.invoke(
    "use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg"
)
```

```output
10.5
```

Looking at the [LangSmith trace](https://smith.langchain.com/public/00e91fc2-e1a4-4b0f-a82e-e6b3119d196c/r) for this chain run, we can see that the first chain call fails as expected and it's the fallback that succeeds.

## Retry with exception[â€‹](#retry-with-exception "Direct link to Retry with exception")

To take things one step further, we can try to automatically re-run the chain with the exception passed in, so that the model may be able to correct its behavior:

```python
from langchain_core.messages import AIMessage, HumanMessage, ToolCall, ToolMessage
from langchain_core.prompts import ChatPromptTemplate


class CustomToolException(Exception):
    """Custom LangChain tool exception."""

    def __init__(self, tool_call: ToolCall, exception: Exception) -> None:
        super().__init__()
        self.tool_call = tool_call
        self.exception = exception


def tool_custom_exception(msg: AIMessage, config: RunnableConfig) -> Runnable:
    try:
        return complex_tool.invoke(msg.tool_calls[0]["args"], config=config)
    except Exception as e:
        raise CustomToolException(msg.tool_calls[0], e)


def exception_to_messages(inputs: dict) -> dict:
    exception = inputs.pop("exception")

    # Add historical messages to the original input, so the model knows that it made a mistake with the last tool call.
    messages = [
        AIMessage(content="", tool_calls=[exception.tool_call]),
        ToolMessage(
            tool_call_id=exception.tool_call["id"], content=str(exception.exception)
        ),
        HumanMessage(
            content="The last tool call raised an exception. Try calling the tool again with corrected arguments. Do not repeat mistakes."
        ),
    ]
    inputs["last_output"] = messages
    return inputs


# We add a last_output MessagesPlaceholder to our prompt which if not passed in doesn't
# affect the prompt at all, but gives us the option to insert an arbitrary list of Messages
# into the prompt if needed. We'll use this on retries to insert the error message.
prompt = ChatPromptTemplate.from_messages(
    [("human", "{input}"), ("placeholder", "{last_output}")]
)
chain = prompt | llm_with_tools | tool_custom_exception

# If the initial chain call fails, we rerun it withe the exception passed in as a message.
self_correcting_chain = chain.with_fallbacks(
    [exception_to_messages | chain], exception_key="exception"
)
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [ToolCall](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolCall.html) | [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```python
self_correcting_chain.invoke(
    {
        "input": "use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg"
    }
)
```

```output
10.5
```

And our chain succeeds! Looking at the [LangSmith trace](https://smith.langchain.com/public/c11e804c-e14f-4059-bd09-64766f999c14/r), we can see that indeed our initial chain still fails, and it's only on retrying that the chain succeeds.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

Now you've seen some strategies how to handle tool calling errors. Next, you can learn more about how to use tools:

- Few shot prompting [with tools](/docs/how_to/tools_few_shot/)
- Stream [tool calls](/docs/how_to/tool_streaming/)
- Pass [runtime values to tools](/docs/how_to/tool_runtime/)

You can also check out some more specific uses of tool calling:

- Getting [structured outputs](/docs/how_to/structured_output/) from models

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tools_error.ipynb)

* * *


- [Setup](#setup)
- [Chain](#chain)
- [Try/except tool call](#tryexcept-tool-call)
- [Fallbacks](#fallbacks)
- [Retry with exception](#retry-with-exception)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/tutorials/classification.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/classification.ipynb)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/use_cases/tagging.ipynb)

# Classify Text into Labels

Tagging means labeling a document with classes such as:

- Sentiment
- Language
- Style (formal, informal etc.)
- Covered topics
- Political tendency

![Image description](/assets/images/tagging-93990e95451d92b715c2b47066384224.png)

## Overview[â€‹](#overview "Direct link to Overview")

Tagging has a few components:

- `function`: Like [extraction](/docs/tutorials/extraction/), tagging uses [functions](https://openai.com/blog/function-calling-and-other-api-updates) to specify how the model should tag a document
- `schema`: defines how we want to tag the document

## Quickstart[â€‹](#quickstart "Direct link to Quickstart")

Let's see a very straightforward example of how we can use OpenAI tool calling for tagging in LangChain. We'll use the [`with_structured_output`](/docs/how_to/structured_output/) method supported by OpenAI models.

```python
pip install --upgrade --quiet langchain-core
```

We'll need to load a [chat model](/docs/integrations/chat/):

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

Let's specify a Pydantic model with a few properties and their expected type in our schema.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

tagging_prompt = ChatPromptTemplate.from_template(
    """
Extract the desired information from the following passage.

Only extract the properties mentioned in the 'Classification' function.

Passage:
{input}
"""
)


class Classification(BaseModel):
    sentiment: str = Field(description="The sentiment of the text")
    aggressiveness: int = Field(
        description="How aggressive the text is on a scale from 1 to 10"
    )
    language: str = Field(description="The language the text is written in")


# LLM
llm = ChatOpenAI(temperature=0, model="gpt-4o-mini").with_structured_output(
    Classification
)
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```python
inp = "Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!"
prompt = tagging_prompt.invoke({"input": inp})
response = llm.invoke(prompt)

response
```

```output
Classification(sentiment='positive', aggressiveness=1, language='Spanish')
```

If we want dictionary output, we can just call `.model_dump()`

```python
inp = "Estoy muy enojado con vos! Te voy a dar tu merecido!"
prompt = tagging_prompt.invoke({"input": inp})
response = llm.invoke(prompt)

response.model_dump()
```

```output
{'sentiment': 'enojado', 'aggressiveness': 8, 'language': 'es'}
```

As we can see in the examples, it correctly interprets what we want.

The results vary so that we may get, for example, sentiments in different languages ('positive', 'enojado' etc.).

We will see how to control these results in the next section.

## Finer control[â€‹](#finer-control "Direct link to Finer control")

Careful schema definition gives us more control over the model's output.

Specifically, we can define:

- Possible values for each property
- Description to make sure that the model understands the property
- Required properties to be returned

Let's redeclare our Pydantic model to control for each of the previously mentioned aspects using enums:

```python
class Classification(BaseModel):
    sentiment: str = Field(..., enum=["happy", "neutral", "sad"])
    aggressiveness: int = Field(
        ...,
        description="describes how aggressive the statement is, the higher the number the more aggressive",
        enum=[1, 2, 3, 4, 5],
    )
    language: str = Field(
        ..., enum=["spanish", "english", "french", "german", "italian"]
    )
```

```python
tagging_prompt = ChatPromptTemplate.from_template(
    """
Extract the desired information from the following passage.

Only extract the properties mentioned in the 'Classification' function.

Passage:
{input}
"""
)

llm = ChatOpenAI(temperature=0, model="gpt-4o-mini").with_structured_output(
    Classification
)
```

Now the answers will be restricted in a way we expect!

```python
inp = "Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!"
prompt = tagging_prompt.invoke({"input": inp})
llm.invoke(prompt)
```

```output
Classification(sentiment='positive', aggressiveness=1, language='Spanish')
```

```python
inp = "Estoy muy enojado con vos! Te voy a dar tu merecido!"
prompt = tagging_prompt.invoke({"input": inp})
llm.invoke(prompt)
```

```output
Classification(sentiment='enojado', aggressiveness=8, language='es')
```

```python
inp = "Weather is ok here, I can go outside without much more than a coat"
prompt = tagging_prompt.invoke({"input": inp})
llm.invoke(prompt)
```

```output
Classification(sentiment='neutral', aggressiveness=1, language='English')
```

The [LangSmith trace](https://smith.langchain.com/public/38294e04-33d8-4c5a-ae92-c2fe68be8332/r) lets us peek under the hood:

![Image description](/assets/images/tagging_trace-de68242b410388c0c3a3b7ca5a95b5ec.png)

### Going deeper[â€‹](#going-deeper "Direct link to Going deeper")

- You can use the [metadata tagger](/docs/integrations/document_transformers/openai_metadata_tagger/) document transformer to extract metadata from a LangChain `Document`.
- This covers the same basic functionality as the tagging chain, only applied to a LangChain `Document`.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/tutorials/classification.ipynb)

* * *


- [Overview](#overview)
- [Quickstart](#quickstart)
- [Finer control](#finer-control)
  
  - [Going deeper](#going-deeper)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/callbacks_async.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/callbacks_async.ipynb)

# How to use callbacks in async environments

Prerequisites

This guide assumes familiarity with the following concepts:

- [Callbacks](/docs/concepts/callbacks/)
- [Custom callback handlers](/docs/how_to/custom_callbacks/)

If you are planning to use the async APIs, it is recommended to use and extend [`AsyncCallbackHandler`](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.AsyncCallbackHandler.html) to avoid blocking the event.

warning

If you use a sync `CallbackHandler` while using an async method to run your LLM / Chain / Tool / Agent, it will still work. However, under the hood, it will be called with [`run_in_executor`](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor) which can cause issues if your `CallbackHandler` is not thread-safe.

danger

If you're on `python<=3.10`, you need to remember to propagate `config` or `callbacks` when invoking other `runnable` from within a `RunnableLambda`, `RunnableGenerator` or `@tool`. If you do not do this, the callbacks will not be propagated to the child runnables being invoked.

```python
import asyncio
from typing import Any, Dict, List

from langchain_anthropic import ChatAnthropic
from langchain_core.callbacks import AsyncCallbackHandler, BaseCallbackHandler
from langchain_core.messages import HumanMessage
from langchain_core.outputs import LLMResult


class MyCustomSyncHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(f"Sync handler being called in a `thread_pool_executor`: token: {token}")


class MyCustomAsyncHandler(AsyncCallbackHandler):
    """Async callback handler that can be used to handle callbacks from langchain."""

    async def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Run when chain starts running."""
        print("zzzz....")
        await asyncio.sleep(0.3)
        class_name = serialized["name"]
        print("Hi! I just woke up. Your llm is starting")

    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Run when chain ends running."""
        print("zzzz....")
        await asyncio.sleep(0.3)
        print("Hi! I just woke up. Your llm is ending")


# To enable streaming, we pass in `streaming=True` to the ChatModel constructor
# Additionally, we pass in a list with our custom handler
chat = ChatAnthropic(
    model="claude-3-sonnet-20240229",
    max_tokens=25,
    streaming=True,
    callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()],
)

await chat.agenerate([[HumanMessage(content="Tell me a joke")]])
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [AsyncCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.AsyncCallbackHandler.html) | [BaseCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [LLMResult](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.llm_result.LLMResult.html)

```output
zzzz....
Hi! I just woke up. Your llm is starting
Sync handler being called in a `thread_pool_executor`: token: Here
Sync handler being called in a `thread_pool_executor`: token: 's
Sync handler being called in a `thread_pool_executor`: token:  a
Sync handler being called in a `thread_pool_executor`: token:  little
Sync handler being called in a `thread_pool_executor`: token:  joke
Sync handler being called in a `thread_pool_executor`: token:  for
Sync handler being called in a `thread_pool_executor`: token:  you
Sync handler being called in a `thread_pool_executor`: token: :
Sync handler being called in a `thread_pool_executor`: token: 

Why
Sync handler being called in a `thread_pool_executor`: token:  can
Sync handler being called in a `thread_pool_executor`: token: 't
Sync handler being called in a `thread_pool_executor`: token:  a
Sync handler being called in a `thread_pool_executor`: token:  bicycle
Sync handler being called in a `thread_pool_executor`: token:  stan
Sync handler being called in a `thread_pool_executor`: token: d up
Sync handler being called in a `thread_pool_executor`: token:  by
Sync handler being called in a `thread_pool_executor`: token:  itself
Sync handler being called in a `thread_pool_executor`: token: ?
Sync handler being called in a `thread_pool_executor`: token:  Because
Sync handler being called in a `thread_pool_executor`: token:  it
Sync handler being called in a `thread_pool_executor`: token: 's
Sync handler being called in a `thread_pool_executor`: token:  two
Sync handler being called in a `thread_pool_executor`: token: -
Sync handler being called in a `thread_pool_executor`: token: tire
zzzz....
Hi! I just woke up. Your llm is ending
```

```output
LLMResult(generations=[[ChatGeneration(text="Here's a little joke for you:\n\nWhy can't a bicycle stand up by itself? Because it's two-tire", message=AIMessage(content="Here's a little joke for you:\n\nWhy can't a bicycle stand up by itself? Because it's two-tire", id='run-8afc89e8-02c0-4522-8480-d96977240bd4-0'))]], llm_output={}, run=[RunInfo(run_id=UUID('8afc89e8-02c0-4522-8480-d96977240bd4'))])
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to create your own custom callback handlers.

Next, check out the other how-to guides in this section, such as [how to attach callbacks to a runnable](/docs/how_to/callbacks_attach/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/callbacks_async.ipynb)

* * *


- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/tools_chain.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/tools_chain.ipynb)

# How to use tools in a chain

In this guide, we will go over the basic ways to create Chains and Agents that call [Tools](/docs/concepts/tools/). Tools can be just about anything â€”Â APIs, functions, databases, etc. Tools allow us to extend the capabilities of a model beyond just outputting text/messages. The key to using models with tools is correctly prompting a model and parsing its response so that it chooses the right tools and provides the right inputs for them.

## Setup[â€‹](#setup "Direct link to Setup")

We'll need to install the following packages for this guide:

```python
%pip install --upgrade --quiet langchain
```

If you'd like to trace your runs in [LangSmith](https://docs.smith.langchain.com/) uncomment and set the following environment variables:

```python
import getpass
import os

# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

## Create a tool[â€‹](#create-a-tool "Direct link to Create a tool")

First, we need to create a tool to call. For this example, we will create a custom tool from a function. For more information on creating custom tools, please see [this guide](/docs/how_to/custom_tools/).

```python
from langchain_core.tools import tool


@tool
def multiply(first_int: int, second_int: int) -> int:
    """Multiply two integers together."""
    return first_int * second_int
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

```python
print(multiply.name)
print(multiply.description)
print(multiply.args)
```

```output
multiply
multiply(first_int: int, second_int: int) -> int - Multiply two integers together.
{'first_int': {'title': 'First Int', 'type': 'integer'}, 'second_int': {'title': 'Second Int', 'type': 'integer'}}
```

```python
multiply.invoke({"first_int": 4, "second_int": 5})
```

```output
20
```

## Chains[â€‹](#chains "Direct link to Chains")

If we know that we only need to use a tool a fixed number of times, we can create a chain for doing so. Let's create a simple chain that just multiplies user-specified numbers.

![chain](/assets/images/tool_chain-3571e7fbc481d648aff93a2630f812ab.svg)

### Tool/function calling[â€‹](#toolfunction-calling "Direct link to Tool/function calling")

One of the most reliable ways to use tools with LLMs is with [tool calling](/docs/concepts/tool_calling/) APIs (also sometimes called function calling). This only works with models that explicitly support tool calling. You can see which models support tool calling [here](/docs/integrations/chat/), and learn more about how to use tool calling in [this guide](/docs/how_to/function_calling/).

First we'll define our model and tools. We'll start with just a single tool, `multiply`.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

We'll use `bind_tools` to pass the definition of our tool in as part of each call to the model, so that the model can invoke the tool when appropriate:

```python
llm_with_tools = llm.bind_tools([multiply])
```

When the model invokes the tool, this will show up in the `AIMessage.tool_calls` attribute of the output:

```python
msg = llm_with_tools.invoke("whats 5 times forty two")
msg.tool_calls
```

```output
[{'name': 'multiply',
  'args': {'first_int': 5, 'second_int': 42},
  'id': 'call_cCP9oA3tRz7HDrjFn1FdmDaG'}]
```

Check out the [LangSmith trace here](https://smith.langchain.com/public/81ff0cbd-e05b-4720-bf61-2c9807edb708/r).

### Invoking the tool[â€‹](#invoking-the-tool "Direct link to Invoking the tool")

Great! We're able to generate tool invocations. But what if we want to actually call the tool? To do so we'll need to pass the generated tool args to our tool. As a simple example we'll just extract the arguments of the first tool\_call:

```python
from operator import itemgetter

chain = llm_with_tools | (lambda x: x.tool_calls[0]["args"]) | multiply
chain.invoke("What's four times 23")
```

```output
92
```

Check out the [LangSmith trace here](https://smith.langchain.com/public/16bbabb9-fc9b-41e5-a33d-487c42df4f85/r).

## Agents[â€‹](#agents "Direct link to Agents")

Chains are great when we know the specific sequence of tool usage needed for any user input. But for certain use cases, how many times we use tools depends on the input. In these cases, we want to let the model itself decide how many times to use tools and in what order. [Agents](/docs/tutorials/agents/) let us do just this.

LangChain comes with a number of built-in agents that are optimized for different use cases. Read about all the [agent types here](/docs/concepts/agents/).

We'll use the [tool calling agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html), which is generally the most reliable kind and the recommended one for most use cases.

![agent](/assets/images/tool_agent-d25fafc271da3ee950ac1fba59cdf490.svg)

```python
from langchain import hub
from langchain.agents import AgentExecutor, create_tool_calling_agent
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create\_tool\_calling\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html)

```python
# Get the prompt to use - can be replaced with any prompt that includes variables "agent_scratchpad" and "input"!
prompt = hub.pull("hwchase17/openai-tools-agent")
prompt.pretty_print()
```

```output
================================[1m System Message [0m================================

You are a helpful assistant

=============================[1m Messages Placeholder [0m=============================

[33;1m[1;3m{chat_history}[0m

================================[1m Human Message [0m=================================

[33;1m[1;3m{input}[0m

=============================[1m Messages Placeholder [0m=============================

[33;1m[1;3m{agent_scratchpad}[0m
```

Agents are also great because they make it easy to use multiple tools.

```python
@tool
def add(first_int: int, second_int: int) -> int:
    "Add two integers."
    return first_int + second_int


@tool
def exponentiate(base: int, exponent: int) -> int:
    "Exponentiate the base to the exponent power."
    return base**exponent


tools = [multiply, add, exponentiate]
```

```python
# Construct the tool calling agent
agent = create_tool_calling_agent(llm, tools, prompt)
```

```python
# Create an agent executor by passing in the agent and tools
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
```

With an agent, we can ask questions that require arbitrarily-many uses of our tools:

```python
agent_executor.invoke(
    {
        "input": "Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result"
    }
)
```

```output


[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `exponentiate` with `{'base': 3, 'exponent': 5}`


[0m[38;5;200m[1;3m243[0m[32;1m[1;3m
Invoking: `add` with `{'first_int': 12, 'second_int': 3}`


[0m[33;1m[1;3m15[0m[32;1m[1;3m
Invoking: `multiply` with `{'first_int': 243, 'second_int': 15}`


[0m[36;1m[1;3m3645[0m[32;1m[1;3m
Invoking: `exponentiate` with `{'base': 405, 'exponent': 2}`


[0m[38;5;200m[1;3m13286025[0m[32;1m[1;3mThe result of taking 3 to the fifth power is 243. 

The sum of twelve and three is 15. 

Multiplying 243 by 15 gives 3645. 

Finally, squaring 3645 gives 13286025.[0m

[1m> Finished chain.[0m
```

```output
{'input': 'Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result',
 'output': 'The result of taking 3 to the fifth power is 243. \n\nThe sum of twelve and three is 15. \n\nMultiplying 243 by 15 gives 3645. \n\nFinally, squaring 3645 gives 13286025.'}
```

Check out the [LangSmith trace here](https://smith.langchain.com/public/eeeb27a4-a2f8-4f06-a3af-9c983f76146c/r).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tools_chain.ipynb)

* * *


- [Setup](#setup)
- [Create a tool](#create-a-tool)
- [Chains](#chains)
  
  - [Tool/function calling](#toolfunction-calling)
  - [Invoking the tool](#invoking-the-tool)
- [Agents](#agents)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/hybrid.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/hybrid.ipynb)

# Hybrid Search

The standard search in LangChain is done by vector similarity. However, a number of [vector store](/docs/integrations/vectorstores/) implementations (Astra DB, ElasticSearch, Neo4J, AzureSearch, Qdrant...) also support more advanced search combining vector similarity search and other search techniques (full-text, BM25, and so on). This is generally referred to as "Hybrid" search.

**Step 1: Make sure the vectorstore you are using supports hybrid search**

At the moment, there is no unified way to perform hybrid search in LangChain. Each vectorstore may have their own way to do it. This is generally exposed as a keyword argument that is passed in during `similarity_search`.

By reading the documentation or source code, figure out whether the vectorstore you are using supports hybrid search, and, if so, how to use it.

**Step 2: Add that parameter as a configurable field for the chain**

This will let you easily call the chain and configure any relevant flags at runtime. See [this documentation](/docs/how_to/configure/) for more information on configuration.

**Step 3: Call the chain with that configurable field**

Now, at runtime you can call this chain with configurable field.

## Code Example[â€‹](#code-example "Direct link to Code Example")

Let's see a concrete example of what this looks like in code. We will use the Cassandra/CQL interface of Astra DB for this example.

Install the following Python package:

```python
!pip install "cassio>=0.1.7"
```

Get the [connection secrets](https://docs.datastax.com/en/astra/astra-db-vector/get-started/quickstart.html).

Initialize cassio:

```python
import cassio

cassio.init(
    database_id="Your database ID",
    token="Your application token",
    keyspace="Your key space",
)
```

Create the Cassandra VectorStore with a standard [index analyzer](https://docs.datastax.com/en/astra/astra-db-vector/cql/use-analyzers-with-cql.html). The index analyzer is needed to enable term matching.

```python
from cassio.table.cql import STANDARD_ANALYZER
from langchain_community.vectorstores import Cassandra
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
vectorstore = Cassandra(
    embedding=embeddings,
    table_name="test_hybrid",
    body_index_options=[STANDARD_ANALYZER],
    session=None,
    keyspace=None,
)

vectorstore.add_texts(
    [
        "In 2023, I visited Paris",
        "In 2022, I visited New York",
        "In 2021, I visited New Orleans",
    ]
)
```

**API Reference:**[Cassandra](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.cassandra.Cassandra.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

If we do a standard similarity search, we get all the documents:

```python
vectorstore.as_retriever().invoke("What city did I visit last?")
```

```output
[Document(page_content='In 2022, I visited New York'),
Document(page_content='In 2023, I visited Paris'),
Document(page_content='In 2021, I visited New Orleans')]
```

The Astra DB vectorstore `body_search` argument can be used to filter the search on the term `new`.

```python
vectorstore.as_retriever(search_kwargs={"body_search": "new"}).invoke(
    "What city did I visit last?"
)
```

```output
[Document(page_content='In 2022, I visited New York'),
Document(page_content='In 2021, I visited New Orleans')]
```

We can now create the chain that we will use to do question-answering over

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import (
    ConfigurableField,
    RunnablePassthrough,
)
from langchain_openai import ChatOpenAI
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ConfigurableField](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

This is basic question-answering chain set up.

```python
template = """Answer the question based only on the following context:
{context}
Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

model = ChatOpenAI()

retriever = vectorstore.as_retriever()
```

Here we mark the retriever as having a configurable field. All vectorstore retrievers have `search_kwargs` as a field. This is just a dictionary, with vectorstore specific fields

```python
configurable_retriever = retriever.configurable_fields(
    search_kwargs=ConfigurableField(
        id="search_kwargs",
        name="Search Kwargs",
        description="The search kwargs to use",
    )
)
```

We can now create the chain using our configurable retriever

```python
chain = (
    {"context": configurable_retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)
```

```python
chain.invoke("What city did I visit last?")
```

```output
Paris
```

We can now invoke the chain with configurable options. `search_kwargs` is the id of the configurable field. The value is the search kwargs to use for Astra DB.

```python
chain.invoke(
    "What city did I visit last?",
    config={"configurable": {"search_kwargs": {"body_search": "new"}}},
)
```

```output
New York
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/hybrid.ipynb)

* * *


- [Code Example](#code-example)









# Evaluation

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications. It involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.

![](/assets/images/langsmith_evaluate-7d48643f3e4c50d77234e13feb95144d.png)

[LangSmith](https://docs.smith.langchain.com/) helps with this process in a few ways:

- It makes it easier to create and curate datasets via its tracing and annotation features
- It provides an evaluation framework that helps you define metrics and run your app against your dataset
- It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code

To learn more, check out [this LangSmith guide](https://docs.smith.langchain.com/concepts/evaluation).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/evaluation.mdx)

* * *









- [LangSmith](https://docs.smith.langchain.com)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [LangChain Hub](https://smith.langchain.com/hub)
- [LangChain JS/TS](https://js.langchain.com)

[v0.3](#)

- [v0.3](/docs/introduction/)
- [v0.2](https://python.langchain.com/v0.2/docs/introduction)
- [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)

[ðŸ’¬](https://chat.langchain.com)

Search

- [Providers](/docs/integrations/providers/)
  
  - [Anthropic](/docs/integrations/providers/anthropic/)
  - [AWS](/docs/integrations/providers/aws/)
  - [Google](/docs/integrations/providers/google/)
  - [Hugging Face](/docs/integrations/providers/huggingface/)
  - [Microsoft](/docs/integrations/providers/microsoft/)
  - [OpenAI](/docs/integrations/providers/openai/)
  - [More](/docs/integrations/providers/all/)
    
    - [Providers](/docs/integrations/providers/)
    - [Abso](/docs/integrations/providers/abso/)
    - [Acreom](/docs/integrations/providers/acreom/)
    - [Activeloop Deep Lake](/docs/integrations/providers/activeloop_deeplake/)
    - [ADS4GPTs](/docs/integrations/providers/ads4gpts/)
    - [Aerospike](/docs/integrations/providers/aerospike/)
    - [AgentQL](/docs/integrations/providers/agentql/)
    - [AI21 Labs](/docs/integrations/providers/ai21/)
    - [Aim](/docs/integrations/providers/aim_tracking/)
    - [AINetwork](/docs/integrations/providers/ainetwork/)
    - [Airbyte](/docs/integrations/providers/airbyte/)
    - [Airtable](/docs/integrations/providers/airtable/)
    - [Alchemy](/docs/integrations/providers/alchemy/)
    - [Aleph Alpha](/docs/integrations/providers/aleph_alpha/)
    - [Alibaba Cloud](/docs/integrations/providers/alibaba_cloud/)
    - [AnalyticDB](/docs/integrations/providers/analyticdb/)
    - [Annoy](/docs/integrations/providers/annoy/)
    - [Anthropic](/docs/integrations/providers/anthropic/)
    - [Anyscale](/docs/integrations/providers/anyscale/)
    - [Apache Software Foundation](/docs/integrations/providers/apache/)
    - [Apache Doris](/docs/integrations/providers/apache_doris/)
    - [Apify](/docs/integrations/providers/apify/)
    - [Apple](/docs/integrations/providers/apple/)
    - [ArangoDB](/docs/integrations/providers/arangodb/)
    - [Arcee](/docs/integrations/providers/arcee/)
    - [ArcGIS](/docs/integrations/providers/arcgis/)
    - [Argilla](/docs/integrations/providers/argilla/)
    - [Arize](/docs/integrations/providers/arize/)
    - [Arthur](/docs/integrations/providers/arthur_tracking/)
    - [Arxiv](/docs/integrations/providers/arxiv/)
    - [Ascend](/docs/integrations/providers/ascend/)
    - [AskNews](/docs/integrations/providers/asknews/)
    - [AssemblyAI](/docs/integrations/providers/assemblyai/)
    - [Astra DB](/docs/integrations/providers/astradb/)
    - [Atlas](/docs/integrations/providers/atlas/)
    - [AwaDB](/docs/integrations/providers/awadb/)
    - [AWS](/docs/integrations/providers/aws/)
    - [AZLyrics](/docs/integrations/providers/azlyrics/)
    - [Azure AI](/docs/integrations/providers/azure_ai/)
    - [BAAI](/docs/integrations/providers/baai/)
    - [Bagel](/docs/integrations/providers/bagel/)
    - [BagelDB](/docs/integrations/providers/bageldb/)
    - [Baichuan](/docs/integrations/providers/baichuan/)
    - [Baidu](/docs/integrations/providers/baidu/)
    - [Banana](/docs/integrations/providers/bananadev/)
    - [Baseten](/docs/integrations/providers/baseten/)
    - [Beam](/docs/integrations/providers/beam/)
    - [Beautiful Soup](/docs/integrations/providers/beautiful_soup/)
    - [BibTeX](/docs/integrations/providers/bibtex/)
    - [BiliBili](/docs/integrations/providers/bilibili/)
    - [Bittensor](/docs/integrations/providers/bittensor/)
    - [Blackboard](/docs/integrations/providers/blackboard/)
    - [bookend.ai](/docs/integrations/providers/bookendai/)
    - [Box](/docs/integrations/providers/box/)
    - [Brave Search](/docs/integrations/providers/brave_search/)
    - [Breebs (Open Knowledge)](/docs/integrations/providers/breebs/)
    - [Browserbase](/docs/integrations/providers/browserbase/)
    - [Browserless](/docs/integrations/providers/browserless/)
    - [ByteDance](/docs/integrations/providers/byte_dance/)
    - [Cassandra](/docs/integrations/providers/cassandra/)
    - [Cerebras](/docs/integrations/providers/cerebras/)
    - [CerebriumAI](/docs/integrations/providers/cerebriumai/)
    - [Chaindesk](/docs/integrations/providers/chaindesk/)
    - [Chroma](/docs/integrations/providers/chroma/)
    - [Clarifai](/docs/integrations/providers/clarifai/)
    - [ClearML](/docs/integrations/providers/clearml_tracking/)
    - [ClickHouse](/docs/integrations/providers/clickhouse/)
    - [ClickUp](/docs/integrations/providers/clickup/)
    - [Cloudflare](/docs/integrations/providers/cloudflare/)
    - [Clova](/docs/integrations/providers/clova/)
    - [CnosDB](/docs/integrations/providers/cnosdb/)
    - [Cognee](/docs/integrations/providers/cognee/)
    - [CogniSwitch](/docs/integrations/providers/cogniswitch/)
    - [Cohere](/docs/integrations/providers/cohere/)
    - [College Confidential](/docs/integrations/providers/college_confidential/)
    - [Comet](/docs/integrations/providers/comet_tracking/)
    - [Confident AI](/docs/integrations/providers/confident/)
    - [Confluence](/docs/integrations/providers/confluence/)
    - [Connery](/docs/integrations/providers/connery/)
    - [Context](/docs/integrations/providers/context/)
    - [Contextual AI](/docs/integrations/providers/contextual/)
    - [Couchbase](/docs/integrations/providers/couchbase/)
    - [Coze](/docs/integrations/providers/coze/)
    - [CrateDB](/docs/integrations/providers/cratedb/)
    - [C Transformers](/docs/integrations/providers/ctransformers/)
    - [CTranslate2](/docs/integrations/providers/ctranslate2/)
    - [Cube](/docs/integrations/providers/cube/)
    - [Dappier](/docs/integrations/providers/dappier/)
    - [DashVector](/docs/integrations/providers/dashvector/)
    - [Databricks](/docs/integrations/providers/databricks/)
    - [Datadog Tracing](/docs/integrations/providers/datadog/)
    - [Datadog Logs](/docs/integrations/providers/datadog_logs/)
    - [DataForSEO](/docs/integrations/providers/dataforseo/)
    - [Dataherald](/docs/integrations/providers/dataherald/)
    - [Dedoc](/docs/integrations/providers/dedoc/)
    - [DeepInfra](/docs/integrations/providers/deepinfra/)
    - [Deeplake](/docs/integrations/providers/deeplake/)
    - [DeepSeek](/docs/integrations/providers/deepseek/)
    - [DeepSparse](/docs/integrations/providers/deepsparse/)
    - [Dell](/docs/integrations/providers/dell/)
    - [Diffbot](/docs/integrations/providers/diffbot/)
    - [DingoDB](/docs/integrations/providers/dingo/)
    - [Discord](/docs/integrations/providers/discord-shikenso/)
    - [Discord (community loader)](/docs/integrations/providers/discord/)
    - [DocArray](/docs/integrations/providers/docarray/)
    - [Docling](/docs/integrations/providers/docling/)
    - [Doctran](/docs/integrations/providers/doctran/)
    - [Docugami](/docs/integrations/providers/docugami/)
    - [Docusaurus](/docs/integrations/providers/docusaurus/)
    - [Dria](/docs/integrations/providers/dria/)
    - [Dropbox](/docs/integrations/providers/dropbox/)
    - [DSPy](/docs/integrations/providers/dspy/)
    - [DuckDB](/docs/integrations/providers/duckdb/)
    - [DuckDuckGo Search](/docs/integrations/providers/duckduckgo_search/)
    - [E2B](/docs/integrations/providers/e2b/)
    - [Eden AI](/docs/integrations/providers/edenai/)
    - [Elasticsearch](/docs/integrations/providers/elasticsearch/)
    - [ElevenLabs](/docs/integrations/providers/elevenlabs/)
    - [Embedchain](/docs/integrations/providers/embedchain/)
    - [Epsilla](/docs/integrations/providers/epsilla/)
    - [Etherscan](/docs/integrations/providers/etherscan/)
    - [Everly AI](/docs/integrations/providers/everlyai/)
    - [EverNote](/docs/integrations/providers/evernote/)
    - [Exa](/docs/integrations/providers/exa_search/)
    - [Facebook - Meta](/docs/integrations/providers/facebook/)
    - [FalkorDB](/docs/integrations/providers/falkordb/)
    - [Fauna](/docs/integrations/providers/fauna/)
    - [Fiddler](/docs/integrations/providers/fiddler/)
    - [Figma](/docs/integrations/providers/figma/)
    - [FireCrawl](/docs/integrations/providers/firecrawl/)
    - [Fireworks AI](/docs/integrations/providers/fireworks/)
    - [Flyte](/docs/integrations/providers/flyte/)
    - [FMP Data (Financial Data Prep)](/docs/integrations/providers/fmp-data/)
    - [Forefront AI](/docs/integrations/providers/forefrontai/)
    - [Friendli AI](/docs/integrations/providers/friendli/)
    - [Smabbler](/docs/integrations/providers/galaxia/)
    - [Geopandas](/docs/integrations/providers/geopandas/)
    - [Git](/docs/integrations/providers/git/)
    - [GitBook](/docs/integrations/providers/gitbook/)
    - [GitHub](/docs/integrations/providers/github/)
    - [GitLab](/docs/integrations/providers/gitlab/)
    - [GOAT](/docs/integrations/providers/goat/)
    - [Golden](/docs/integrations/providers/golden/)
    - [Goodfire](/docs/integrations/providers/goodfire/)
    - [Google](/docs/integrations/providers/google/)
    - [Serper - Google Search API](/docs/integrations/providers/google_serper/)
    - [GooseAI](/docs/integrations/providers/gooseai/)
    - [GPT4All](/docs/integrations/providers/gpt4all/)
    - [Gradient](/docs/integrations/providers/gradient/)
    - [Graph RAG](/docs/integrations/providers/graph_rag/)
    - [Graphsignal](/docs/integrations/providers/graphsignal/)
    - [Grobid](/docs/integrations/providers/grobid/)
    - [Groq](/docs/integrations/providers/groq/)
    - [Gutenberg](/docs/integrations/providers/gutenberg/)
    - [Hacker News](/docs/integrations/providers/hacker_news/)
    - [Hazy Research](/docs/integrations/providers/hazy_research/)
    - [Helicone](/docs/integrations/providers/helicone/)
    - [Hologres](/docs/integrations/providers/hologres/)
    - [HTML to text](/docs/integrations/providers/html2text/)
    - [Huawei](/docs/integrations/providers/huawei/)
    - [Hugging Face](/docs/integrations/providers/huggingface/)
    - [Hyperbrowser](/docs/integrations/providers/hyperbrowser/)
    - [IBM](/docs/integrations/providers/ibm/)
    - [IEIT Systems](/docs/integrations/providers/ieit_systems/)
    - [iFixit](/docs/integrations/providers/ifixit/)
    - [iFlytek](/docs/integrations/providers/iflytek/)
    - [IMSDb](/docs/integrations/providers/imsdb/)
    - [Infinispan VS](/docs/integrations/providers/infinispanvs/)
    - [Infinity](/docs/integrations/providers/infinity/)
    - [Infino](/docs/integrations/providers/infino/)
    - [Intel](/docs/integrations/providers/intel/)
    - [Iugu](/docs/integrations/providers/iugu/)
    - [Jaguar](/docs/integrations/providers/jaguar/)
    - [Javelin AI Gateway](/docs/integrations/providers/javelin_ai_gateway/)
    - [Jenkins](/docs/integrations/providers/jenkins/)
    - [Jina AI](/docs/integrations/providers/jina/)
    - [Johnsnowlabs](/docs/integrations/providers/johnsnowlabs/)
    - [Joplin](/docs/integrations/providers/joplin/)
    - [KDB.AI](/docs/integrations/providers/kdbai/)
    - [Kinetica](/docs/integrations/providers/kinetica/)
    - [KoboldAI](/docs/integrations/providers/koboldai/)
    - [Konko](/docs/integrations/providers/konko/)
    - [KoNLPY](/docs/integrations/providers/konlpy/)
    - [KÃ¹zu](/docs/integrations/providers/kuzu/)
    - [Label Studio](/docs/integrations/providers/labelstudio/)
    - [lakeFS](/docs/integrations/providers/lakefs/)
    - [LanceDB](/docs/integrations/providers/lancedb/)
    - [LangChain Decorators âœ¨](/docs/integrations/providers/langchain_decorators/)
    - [LangFair: Use-Case Level LLM Bias and Fairness Assessments](/docs/integrations/providers/langfair/)
    - [Langfuse ðŸª¢](/docs/integrations/providers/langfuse/)
    - [Lantern](/docs/integrations/providers/lantern/)
    - [Lindorm](/docs/integrations/providers/lindorm/)
    - [Linkup](/docs/integrations/providers/linkup/)
    - [LiteLLM](/docs/integrations/providers/litellm/)
    - [LlamaIndex](/docs/integrations/providers/llama_index/)
    - [Llama.cpp](/docs/integrations/providers/llamacpp/)
    - [LlamaEdge](/docs/integrations/providers/llamaedge/)
    - [llamafile](/docs/integrations/providers/llamafile/)
    - [LLMonitor](/docs/integrations/providers/llmonitor/)
    - [LocalAI](/docs/integrations/providers/localai/)
    - [Log10](/docs/integrations/providers/log10/)
    - [MariaDB](/docs/integrations/providers/mariadb/)
    - [MariTalk](/docs/integrations/providers/maritalk/)
    - [Marqo](/docs/integrations/providers/marqo/)
    - [MediaWikiDump](/docs/integrations/providers/mediawikidump/)
    - [Meilisearch](/docs/integrations/providers/meilisearch/)
    - [Memcached](/docs/integrations/providers/memcached/)
    - [Memgraph](/docs/integrations/providers/memgraph/)
    - [Metal](/docs/integrations/providers/metal/)
    - [Microsoft](/docs/integrations/providers/microsoft/)
    - [Milvus](/docs/integrations/providers/milvus/)
    - [MindsDB](/docs/integrations/providers/mindsdb/)
    - [Minimax](/docs/integrations/providers/minimax/)
    - [MistralAI](/docs/integrations/providers/mistralai/)
    - [MLflow AI Gateway for LLMs](/docs/integrations/providers/mlflow/)
    - [MLflow](/docs/integrations/providers/mlflow_tracking/)
    - [MLX](/docs/integrations/providers/mlx/)
    - [Modal](/docs/integrations/providers/modal/)
    - [ModelScope](/docs/integrations/providers/modelscope/)
    - [Modern Treasury](/docs/integrations/providers/modern_treasury/)
    - [Momento](/docs/integrations/providers/momento/)
    - [MongoDB](/docs/integrations/providers/mongodb/)
    - [MongoDB Atlas](/docs/integrations/providers/mongodb_atlas/)
    - [Motherduck](/docs/integrations/providers/motherduck/)
    - [MotÃ¶rhead](/docs/integrations/providers/motorhead/)
    - [MyScale](/docs/integrations/providers/myscale/)
    - [NAVER](/docs/integrations/providers/naver/)
    - [Neo4j](/docs/integrations/providers/neo4j/)
    - [Netmind](/docs/integrations/providers/netmind/)
    - [Nimble](/docs/integrations/providers/nimble/)
    - [NLPCloud](/docs/integrations/providers/nlpcloud/)
    - [Nomic](/docs/integrations/providers/nomic/)
    - [Notion DB](/docs/integrations/providers/notion/)
    - [Nuclia](/docs/integrations/providers/nuclia/)
    - [NVIDIA](/docs/integrations/providers/nvidia/)
    - [Obsidian](/docs/integrations/providers/obsidian/)
    - [OceanBase](/docs/integrations/providers/oceanbase/)
    - [Oracle Cloud Infrastructure (OCI)](/docs/integrations/providers/oci/)
    - [OctoAI](/docs/integrations/providers/octoai/)
    - [Ollama](/docs/integrations/providers/ollama/)
    - [Ontotext GraphDB](/docs/integrations/providers/ontotext_graphdb/)
    - [OpenAI](/docs/integrations/providers/openai/)
    - [OpenGradient](/docs/integrations/providers/opengradient/)
    - [OpenLLM](/docs/integrations/providers/openllm/)
    - [OpenSearch](/docs/integrations/providers/opensearch/)
    - [OpenWeatherMap](/docs/integrations/providers/openweathermap/)
    - [OracleAI Vector Search](/docs/integrations/providers/oracleai/)
    - [Outline](/docs/integrations/providers/outline/)
    - [Outlines](/docs/integrations/providers/outlines/)
    - [Oxylabs](/docs/integrations/providers/oxylabs/)
    - [Pandas](/docs/integrations/providers/pandas/)
    - [PaymanAI](/docs/integrations/providers/payman-tool/)
    - [Pebblo](/docs/integrations/providers/pebblo/)
    - [Permit](/docs/integrations/providers/permit/)
    - [Perplexity](/docs/integrations/providers/perplexity/)
    - [Petals](/docs/integrations/providers/petals/)
    - [Postgres Embedding](/docs/integrations/providers/pg_embedding/)
    - [PGVector](/docs/integrations/providers/pgvector/)
    - [Pinecone](/docs/integrations/providers/pinecone/)
    - [PipelineAI](/docs/integrations/providers/pipelineai/)
    - [Pipeshift](/docs/integrations/providers/pipeshift/)
    - [Portkey](/docs/integrations/providers/portkey/)
    - [Predibase](/docs/integrations/providers/predibase/)
    - [Prediction Guard](/docs/integrations/providers/predictionguard/)
    - [PremAI](/docs/integrations/providers/premai/)
    - [SWI-Prolog](/docs/integrations/providers/prolog/)
    - [PromptLayer](/docs/integrations/providers/promptlayer/)
    - [Psychic](/docs/integrations/providers/psychic/)
    - [PubMed](/docs/integrations/providers/pubmed/)
    - [PullMd Loader](/docs/integrations/providers/pull-md/)
    - [PygmalionAI](/docs/integrations/providers/pygmalionai/)
    - [PyMuPDF4LLM](/docs/integrations/providers/pymupdf4llm/)
    - [Qdrant](/docs/integrations/providers/qdrant/)
    - [RAGatouille](/docs/integrations/providers/ragatouille/)
    - [rank\_bm25](/docs/integrations/providers/rank_bm25/)
    - [Ray Serve](/docs/integrations/providers/ray_serve/)
    - [Rebuff](/docs/integrations/providers/rebuff/)
    - [Reddit](/docs/integrations/providers/reddit/)
    - [Redis](/docs/integrations/providers/redis/)
    - [Remembrall](/docs/integrations/providers/remembrall/)
    - [Replicate](/docs/integrations/providers/replicate/)
    - [Roam](/docs/integrations/providers/roam/)
    - [Sema4 (fka Robocorp)](/docs/integrations/providers/robocorp/)
    - [Rockset](/docs/integrations/providers/rockset/)
    - [Runhouse](/docs/integrations/providers/runhouse/)
    - [Runpod](/docs/integrations/providers/runpod/)
    - [RWKV-4](/docs/integrations/providers/rwkv/)
    - [Salesforce](/docs/integrations/providers/salesforce/)
    - [Salute Devices](/docs/integrations/providers/salute_devices/)
    - [SambaNova](/docs/integrations/providers/sambanova/)
    - [SAP](/docs/integrations/providers/sap/)
    - [ScrapeGraph AI](/docs/integrations/providers/scrapegraph/)
    - [SearchApi](/docs/integrations/providers/searchapi/)
    - [SearxNG Search API](/docs/integrations/providers/searx/)
    - [SemaDB](/docs/integrations/providers/semadb/)
    - [SerpAPI](/docs/integrations/providers/serpapi/)
    - [Shale Protocol](/docs/integrations/providers/shaleprotocol/)
    - [SingleStore Integration](/docs/integrations/providers/singlestore/)
    - [scikit-learn](/docs/integrations/providers/sklearn/)
    - [Slack](/docs/integrations/providers/slack/)
    - [Snowflake](/docs/integrations/providers/snowflake/)
    - [spaCy](/docs/integrations/providers/spacy/)
    - [Spark](/docs/integrations/providers/spark/)
    - [SparkLLM](/docs/integrations/providers/sparkllm/)
    - [Spreedly](/docs/integrations/providers/spreedly/)
    - [SQLite](/docs/integrations/providers/sqlite/)
    - [Stack Exchange](/docs/integrations/providers/stackexchange/)
    - [StarRocks](/docs/integrations/providers/starrocks/)
    - [StochasticAI](/docs/integrations/providers/stochasticai/)
    - [Streamlit](/docs/integrations/providers/streamlit/)
    - [Stripe](/docs/integrations/providers/stripe/)
    - [Supabase (Postgres)](/docs/integrations/providers/supabase/)
    - [Nebula](/docs/integrations/providers/symblai_nebula/)
    - [Tableau](/docs/integrations/providers/tableau/)
    - [Taiga](/docs/integrations/providers/taiga/)
    - [Tair](/docs/integrations/providers/tair/)
    - [Tavily](/docs/integrations/providers/tavily/)
    - [Telegram](/docs/integrations/providers/telegram/)
    - [Tencent](/docs/integrations/providers/tencent/)
    - [TensorFlow Datasets](/docs/integrations/providers/tensorflow_datasets/)
    - [TiDB](/docs/integrations/providers/tidb/)
    - [TigerGraph](/docs/integrations/providers/tigergraph/)
    - [Tigris](/docs/integrations/providers/tigris/)
    - [Tilores](/docs/integrations/providers/tilores/)
    - [Together AI](/docs/integrations/providers/together/)
    - [2Markdown](/docs/integrations/providers/tomarkdown/)
    - [Transwarp](/docs/integrations/providers/transwarp/)
    - [Trello](/docs/integrations/providers/trello/)
    - [Trubrics](/docs/integrations/providers/trubrics/)
    - [TruLens](/docs/integrations/providers/trulens/)
    - [Twitter](/docs/integrations/providers/twitter/)
    - [Typesense](/docs/integrations/providers/typesense/)
    - [Unstructured](/docs/integrations/providers/unstructured/)
    - [Upstage](/docs/integrations/providers/upstage/)
    - [upstash](/docs/integrations/providers/upstash/)
    - [UpTrain](/docs/integrations/providers/uptrain/)
    - [USearch](/docs/integrations/providers/usearch/)
    - [Valthera](/docs/integrations/providers/valthera/)
    - [VDMS](/docs/integrations/providers/vdms/)
    - [Vearch](/docs/integrations/providers/vearch/)
    - [Vectara](/docs/integrations/providers/vectara/)
    - [Vectorize](/docs/integrations/providers/vectorize/)
    - [Vespa](/docs/integrations/providers/vespa/)
    - [vlite](/docs/integrations/providers/vlite/)
    - [VoyageAI](/docs/integrations/providers/voyageai/)
    - [Weights &amp; Biases](/docs/integrations/providers/wandb/)
    - [Weights &amp; Biases tracing](/docs/integrations/providers/wandb_tracing/)
    - [Weights &amp; Biases tracking](/docs/integrations/providers/wandb_tracking/)
    - [Weather](/docs/integrations/providers/weather/)
    - [Weaviate](/docs/integrations/providers/weaviate/)
    - [WhatsApp](/docs/integrations/providers/whatsapp/)
    - [WhyLabs](/docs/integrations/providers/whylabs_profiling/)
    - [Wikipedia](/docs/integrations/providers/wikipedia/)
    - [Wolfram Alpha](/docs/integrations/providers/wolfram_alpha/)
    - [Writer, Inc.](/docs/integrations/providers/writer/)
    - [xAI](/docs/integrations/providers/xai/)
    - [Xata](/docs/integrations/providers/xata/)
    - [Xorbits Inference (Xinference)](/docs/integrations/providers/xinference/)
    - [Yahoo](/docs/integrations/providers/yahoo/)
    - [Yandex](/docs/integrations/providers/yandex/)
    - [YDB](/docs/integrations/providers/ydb/)
    - [Yeager.ai](/docs/integrations/providers/yeagerai/)
    - [Yellowbrick](/docs/integrations/providers/yellowbrick/)
    - [01.AI](/docs/integrations/providers/yi/)
    - [You](/docs/integrations/providers/you/)
    - [YouTube](/docs/integrations/providers/youtube/)
    - [Zep](/docs/integrations/providers/zep/)
    - [Zhipu AI](/docs/integrations/providers/zhipuai/)
    - [Zilliz](/docs/integrations/providers/zilliz/)
    - [Zotero](/docs/integrations/providers/zotero/)
- [Components](/docs/integrations/components/)
  
  - [Chat models](/docs/integrations/chat/)
    
    - [Chat models](/docs/integrations/chat/)
    - [Abso](/docs/integrations/chat/abso/)
    - [AI21 Labs](/docs/integrations/chat/ai21/)
    - [Alibaba Cloud PAI EAS](/docs/integrations/chat/alibaba_cloud_pai_eas/)
    - [Anthropic](/docs/integrations/chat/anthropic/)
    - [\[Deprecated\] Experimental Anthropic Tools Wrapper](/docs/integrations/chat/anthropic_functions/)
    - [Anyscale](/docs/integrations/chat/anyscale/)
    - [AzureAIChatCompletionsModel](/docs/integrations/chat/azure_ai/)
    - [Azure OpenAI](/docs/integrations/chat/azure_chat_openai/)
    - [Azure ML Endpoint](/docs/integrations/chat/azureml_chat_endpoint/)
    - [Baichuan Chat](/docs/integrations/chat/baichuan/)
    - [Baidu Qianfan](/docs/integrations/chat/baidu_qianfan_endpoint/)
    - [AWS Bedrock](/docs/integrations/chat/bedrock/)
    - [Cerebras](/docs/integrations/chat/cerebras/)
    - [CloudflareWorkersAI](/docs/integrations/chat/cloudflare_workersai/)
    - [Cohere](/docs/integrations/chat/cohere/)
    - [ContextualAI](/docs/integrations/chat/contextual/)
    - [Coze Chat](/docs/integrations/chat/coze/)
    - [Dappier AI](/docs/integrations/chat/dappier/)
    - [Databricks](/docs/integrations/chat/databricks/)
    - [DeepInfra](/docs/integrations/chat/deepinfra/)
    - [DeepSeek](/docs/integrations/chat/deepseek/)
    - [Eden AI](/docs/integrations/chat/edenai/)
    - [Ernie Bot Chat](/docs/integrations/chat/ernie/)
    - [EverlyAI](/docs/integrations/chat/everlyai/)
    - [Fireworks](/docs/integrations/chat/fireworks/)
    - [ChatFriendli](/docs/integrations/chat/friendli/)
    - [GigaChat](/docs/integrations/chat/gigachat/)
    - [Goodfire](/docs/integrations/chat/goodfire/)
    - [Google AI](/docs/integrations/chat/google_generative_ai/)
    - [Google Cloud Vertex AI](/docs/integrations/chat/google_vertex_ai_palm/)
    - [GPTRouter](/docs/integrations/chat/gpt_router/)
    - [Groq](/docs/integrations/chat/groq/)
    - [ChatHuggingFace](/docs/integrations/chat/huggingface/)
    - [IBM watsonx.ai](/docs/integrations/chat/ibm_watsonx/)
    - [JinaChat](/docs/integrations/chat/jinachat/)
    - [Kinetica](/docs/integrations/chat/kinetica/)
    - [Konko](/docs/integrations/chat/konko/)
    - [LiteLLM](/docs/integrations/chat/litellm/)
    - [LiteLLM Router](/docs/integrations/chat/litellm_router/)
    - [Llama 2 Chat](/docs/integrations/chat/llama2_chat/)
    - [Llama API](/docs/integrations/chat/llama_api/)
    - [LlamaEdge](/docs/integrations/chat/llama_edge/)
    - [Llama.cpp](/docs/integrations/chat/llamacpp/)
    - [maritalk](/docs/integrations/chat/maritalk/)
    - [MiniMax](/docs/integrations/chat/minimax/)
    - [MistralAI](/docs/integrations/chat/mistralai/)
    - [MLX](/docs/integrations/chat/mlx/)
    - [ModelScope](/docs/integrations/chat/modelscope_chat_endpoint/)
    - [Moonshot](/docs/integrations/chat/moonshot/)
    - [Naver](/docs/integrations/chat/naver/)
    - [Netmind](/docs/integrations/chat/netmind/)
    - [NVIDIA AI Endpoints](/docs/integrations/chat/nvidia_ai_endpoints/)
    - [ChatOCIModelDeployment](/docs/integrations/chat/oci_data_science/)
    - [OCIGenAI](/docs/integrations/chat/oci_generative_ai/)
    - [ChatOctoAI](/docs/integrations/chat/octoai/)
    - [Ollama](/docs/integrations/chat/ollama/)
    - [OpenAI](/docs/integrations/chat/openai/)
    - [Outlines](/docs/integrations/chat/outlines/)
    - [Perplexity](/docs/integrations/chat/perplexity/)
    - [Pipeshift](/docs/integrations/chat/pipeshift/)
    - [ChatPredictionGuard](/docs/integrations/chat/predictionguard/)
    - [PremAI](/docs/integrations/chat/premai/)
    - [PromptLayer ChatOpenAI](/docs/integrations/chat/promptlayer_chatopenai/)
    - [Qwen QwQ](/docs/integrations/chat/qwq/)
    - [Reka](/docs/integrations/chat/reka/)
    - [RunPod Chat Model](/docs/integrations/chat/runpod/)
    - [SambaNovaCloud](/docs/integrations/chat/sambanova/)
    - [SambaStudio](/docs/integrations/chat/sambastudio/)
    - [ChatSeekrFlow](/docs/integrations/chat/seekrflow/)
    - [Snowflake Cortex](/docs/integrations/chat/snowflake/)
    - [solar](/docs/integrations/chat/solar/)
    - [SparkLLM Chat](/docs/integrations/chat/sparkllm/)
    - [Nebula (Symbl.ai)](/docs/integrations/chat/symblai_nebula/)
    - [Tencent Hunyuan](/docs/integrations/chat/tencent_hunyuan/)
    - [Together](/docs/integrations/chat/together/)
    - [Tongyi Qwen](/docs/integrations/chat/tongyi/)
    - [Upstage](/docs/integrations/chat/upstage/)
    - [vectara](/docs/integrations/chat/vectara/)
    - [vLLM Chat](/docs/integrations/chat/vllm/)
    - [Volc Enging Maas](/docs/integrations/chat/volcengine_maas/)
    - [Chat Writer](/docs/integrations/chat/writer/)
    - [xAI](/docs/integrations/chat/xai/)
    - [Xinference](/docs/integrations/chat/xinference/)
    - [YandexGPT](/docs/integrations/chat/yandex/)
    - [ChatYI](/docs/integrations/chat/yi/)
    - [Yuan2.0](/docs/integrations/chat/yuan2/)
    - [ZHIPU AI](/docs/integrations/chat/zhipuai/)
  - [Retrievers](/docs/integrations/retrievers/)
    
    - [Retrievers](/docs/integrations/retrievers/)
    - [Activeloop Deep Memory](/docs/integrations/retrievers/activeloop/)
    - [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/)
    - [Arcee](/docs/integrations/retrievers/arcee/)
    - [Arxiv](/docs/integrations/retrievers/arxiv/)
    - [AskNews](/docs/integrations/retrievers/asknews/)
    - [Azure AI Search](/docs/integrations/retrievers/azure_ai_search/)
    - [Bedrock (Knowledge Bases)](/docs/integrations/retrievers/bedrock/)
    - [BM25](/docs/integrations/retrievers/bm25/)
    - [Box](/docs/integrations/retrievers/box/)
    - [BREEBS (Open Knowledge)](/docs/integrations/retrievers/breebs/)
    - [Chaindesk](/docs/integrations/retrievers/chaindesk/)
    - [ChatGPT plugin](/docs/integrations/retrievers/chatgpt-plugin/)
    - [Cognee](/docs/integrations/retrievers/cognee/)
    - [Cohere reranker](/docs/integrations/retrievers/cohere-reranker/)
    - [Cohere RAG](/docs/integrations/retrievers/cohere/)
    - [Contextual AI Reranker](/docs/integrations/retrievers/contextual/)
    - [Dappier](/docs/integrations/retrievers/dappier/)
    - [DocArray](/docs/integrations/retrievers/docarray_retriever/)
    - [Dria](/docs/integrations/retrievers/dria_index/)
    - [ElasticSearch BM25](/docs/integrations/retrievers/elastic_search_bm25/)
    - [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/)
    - [Embedchain](/docs/integrations/retrievers/embedchain/)
    - [FlashRank reranker](/docs/integrations/retrievers/flashrank-reranker/)
    - [Fleet AI Context](/docs/integrations/retrievers/fleet_context/)
    - [Galaxia](/docs/integrations/retrievers/galaxia-retriever/)
    - [Google Drive](/docs/integrations/retrievers/google_drive/)
    - [Google Vertex AI Search](/docs/integrations/retrievers/google_vertex_ai_search/)
    - [Graph RAG](/docs/integrations/retrievers/graph_rag/)
    - [IBM watsonx.ai](/docs/integrations/retrievers/ibm_watsonx_ranker/)
    - [JaguarDB Vector Database](/docs/integrations/retrievers/jaguar/)
    - [Kay.ai](/docs/integrations/retrievers/kay/)
    - [Kinetica Vectorstore based Retriever](/docs/integrations/retrievers/kinetica/)
    - [kNN](/docs/integrations/retrievers/knn/)
    - [LinkupSearchRetriever](/docs/integrations/retrievers/linkup_search/)
    - [LLMLingua Document Compressor](/docs/integrations/retrievers/llmlingua/)
    - [LOTR (Merger Retriever)](/docs/integrations/retrievers/merger_retriever/)
    - [Metal](/docs/integrations/retrievers/metal/)
    - [Milvus Hybrid Search](/docs/integrations/retrievers/milvus_hybrid_search/)
    - [NanoPQ (Product Quantization)](/docs/integrations/retrievers/nanopq/)
    - [needle](/docs/integrations/retrievers/needle/)
    - [Nimble](/docs/integrations/retrievers/nimble/)
    - [Outline](/docs/integrations/retrievers/outline/)
    - [Permit](/docs/integrations/retrievers/permit/)
    - [Pinecone Hybrid Search](/docs/integrations/retrievers/pinecone_hybrid_search/)
    - [PubMed](/docs/integrations/retrievers/pubmed/)
    - [Qdrant Sparse Vector](/docs/integrations/retrievers/qdrant-sparse/)
    - [RAGatouille](/docs/integrations/retrievers/ragatouille/)
    - [RePhraseQuery](/docs/integrations/retrievers/re_phrase/)
    - [Rememberizer](/docs/integrations/retrievers/rememberizer/)
    - [SEC filing](/docs/integrations/retrievers/sec_filings/)
    - [Self-querying retrievers](/docs/integrations/retrievers/self_query/)
    - [SVM](/docs/integrations/retrievers/svm/)
    - [TavilySearchAPI](/docs/integrations/retrievers/tavily/)
    - [TF-IDF](/docs/integrations/retrievers/tf_idf/)
    - [\*\*NeuralDB\*\*](/docs/integrations/retrievers/thirdai_neuraldb/)
    - [Vectorize](/docs/integrations/retrievers/vectorize/)
    - [Vespa](/docs/integrations/retrievers/vespa/)
    - [Wikipedia](/docs/integrations/retrievers/wikipedia/)
    - [You.com](/docs/integrations/retrievers/you-retriever/)
    - [Zep Cloud](/docs/integrations/retrievers/zep_cloud_memorystore/)
    - [Zep Open Source](/docs/integrations/retrievers/zep_memorystore/)
    - [Zilliz Cloud Pipeline](/docs/integrations/retrievers/zilliz_cloud_pipeline/)
    - [Zotero](/docs/integrations/retrievers/zotero/)
  - [Tools/Toolkits](/docs/integrations/tools/)
    
    - [Tools](/docs/integrations/tools/)
    - [ADS4GPTs](/docs/integrations/tools/ads4gpts/)
    - [AgentQL](/docs/integrations/tools/agentql/)
    - [AINetwork Toolkit](/docs/integrations/tools/ainetwork/)
    - [Alpha Vantage](/docs/integrations/tools/alpha_vantage/)
    - [Amadeus Toolkit](/docs/integrations/tools/amadeus/)
    - [Apify Actor](/docs/integrations/tools/apify_actors/)
    - [ArXiv](/docs/integrations/tools/arxiv/)
    - [AskNews](/docs/integrations/tools/asknews/)
    - [AWS Lambda](/docs/integrations/tools/awslambda/)
    - [Azure AI Services Toolkit](/docs/integrations/tools/azure_ai_services/)
    - [Azure Cognitive Services Toolkit](/docs/integrations/tools/azure_cognitive_services/)
    - [Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions/)
    - [Shell (bash)](/docs/integrations/tools/bash/)
    - [Bearly Code Interpreter](/docs/integrations/tools/bearly/)
    - [Bing Search](/docs/integrations/tools/bing_search/)
    - [Brave Search](/docs/integrations/tools/brave_search/)
    - [Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database/)
    - [CDP](/docs/integrations/tools/cdp_agentkit/)
    - [ChatGPT Plugins](/docs/integrations/tools/chatgpt_plugins/)
    - [ClickUp Toolkit](/docs/integrations/tools/clickup/)
    - [Cogniswitch Toolkit](/docs/integrations/tools/cogniswitch/)
    - [Connery Toolkit and Tools](/docs/integrations/tools/connery/)
    - [Dall-E Image Generator](/docs/integrations/tools/dalle_image_generator/)
    - [Dappier](/docs/integrations/tools/dappier/)
    - [Databricks Unity Catalog (UC)](/docs/integrations/tools/databricks/)
    - [DataForSEO](/docs/integrations/tools/dataforseo/)
    - [Dataherald](/docs/integrations/tools/dataherald/)
    - [DuckDuckGo Search](/docs/integrations/tools/ddg/)
    - [Discord](/docs/integrations/tools/discord/)
    - [E2B Data Analysis](/docs/integrations/tools/e2b_data_analysis/)
    - [Eden AI](/docs/integrations/tools/edenai_tools/)
    - [ElevenLabs Text2Speech](/docs/integrations/tools/eleven_labs_tts/)
    - [Exa Search](/docs/integrations/tools/exa_search/)
    - [File System](/docs/integrations/tools/filesystem/)
    - [FinancialDatasets Toolkit](/docs/integrations/tools/financial_datasets/)
    - [FMP Data](/docs/integrations/tools/fmp-data/)
    - [Github Toolkit](/docs/integrations/tools/github/)
    - [Gitlab Toolkit](/docs/integrations/tools/gitlab/)
    - [Gmail Toolkit](/docs/integrations/tools/gmail/)
    - [GOAT](/docs/integrations/tools/goat/)
    - [Golden Query](/docs/integrations/tools/golden_query/)
    - [Google Books](/docs/integrations/tools/google_books/)
    - [Google Calendar Toolkit](/docs/integrations/tools/google_calendar/)
    - [Google Cloud Text-to-Speech](/docs/integrations/tools/google_cloud_texttospeech/)
    - [Google Drive](/docs/integrations/tools/google_drive/)
    - [Google Finance](/docs/integrations/tools/google_finance/)
    - [Google Imagen](/docs/integrations/tools/google_imagen/)
    - [Google Jobs](/docs/integrations/tools/google_jobs/)
    - [Google Lens](/docs/integrations/tools/google_lens/)
    - [Google Places](/docs/integrations/tools/google_places/)
    - [Google Scholar](/docs/integrations/tools/google_scholar/)
    - [Google Search](/docs/integrations/tools/google_search/)
    - [Google Serper](/docs/integrations/tools/google_serper/)
    - [Google Trends](/docs/integrations/tools/google_trends/)
    - [Gradio](/docs/integrations/tools/gradio_tools/)
    - [GraphQL](/docs/integrations/tools/graphql/)
    - [HuggingFace Hub Tools](/docs/integrations/tools/huggingface_tools/)
    - [Human as a tool](/docs/integrations/tools/human_tools/)
    - [Hyperbrowser Browser Agent Tools](/docs/integrations/tools/hyperbrowser_browser_agent_tools/)
    - [Hyperbrowser Web Scraping Tools](/docs/integrations/tools/hyperbrowser_web_scraping_tools/)
    - [IBM watsonx.ai](/docs/integrations/tools/ibm_watsonx/)
    - [IFTTT WebHooks](/docs/integrations/tools/ifttt/)
    - [Infobip](/docs/integrations/tools/infobip/)
    - [Ionic Shopping Tool](/docs/integrations/tools/ionic_shopping/)
    - [Jenkins](/docs/integrations/tools/jenkins/)
    - [Jina Search](/docs/integrations/tools/jina_search/)
    - [Jira Toolkit](/docs/integrations/tools/jira/)
    - [JSON Toolkit](/docs/integrations/tools/json/)
    - [Lemon Agent](/docs/integrations/tools/lemonai/)
    - [LinkupSearchTool](/docs/integrations/tools/linkup_search/)
    - [Memgraph](/docs/integrations/tools/memgraph/)
    - [Memorize](/docs/integrations/tools/memorize/)
    - [Mojeek Search](/docs/integrations/tools/mojeek_search/)
    - [MultiOn Toolkit](/docs/integrations/tools/multion/)
    - [NASA Toolkit](/docs/integrations/tools/nasa/)
    - [Naver Search](/docs/integrations/tools/naver_search/)
    - [Nuclia Understanding](/docs/integrations/tools/nuclia/)
    - [NVIDIA Riva: ASR and TTS](/docs/integrations/tools/nvidia_riva/)
    - [Office365 Toolkit](/docs/integrations/tools/office365/)
    - [OpenAPI Toolkit](/docs/integrations/tools/openapi/)
    - [Natural Language API Toolkits](/docs/integrations/tools/openapi_nla/)
    - [OpenGradient](/docs/integrations/tools/opengradient_toolkit/)
    - [OpenWeatherMap](/docs/integrations/tools/openweathermap/)
    - [Oracle AI Vector Search: Generate Summary](/docs/integrations/tools/oracleai/)
    - [Oxylabs](/docs/integrations/tools/oxylabs/)
    - [Pandas Dataframe](/docs/integrations/tools/pandas/)
    - [Passio NutritionAI](/docs/integrations/tools/passio_nutrition_ai/)
    - [PaymanAI](/docs/integrations/tools/payman-tool/)
    - [Permit](/docs/integrations/tools/permit/)
    - [PlayWright Browser Toolkit](/docs/integrations/tools/playwright/)
    - [Polygon IO Toolkit and Tools](/docs/integrations/tools/polygon/)
    - [PowerBI Toolkit](/docs/integrations/tools/powerbi/)
    - [Prolog](/docs/integrations/tools/prolog_tool/)
    - [PubMed](/docs/integrations/tools/pubmed/)
    - [Python REPL](/docs/integrations/tools/python/)
    - [Reddit Search](/docs/integrations/tools/reddit_search/)
    - [Requests Toolkit](/docs/integrations/tools/requests/)
    - [Riza Code Interpreter](/docs/integrations/tools/riza/)
    - [Robocorp Toolkit](/docs/integrations/tools/robocorp/)
    - [Salesforce](/docs/integrations/tools/salesforce/)
    - [SceneXplain](/docs/integrations/tools/sceneXplain/)
    - [ScrapeGraph](/docs/integrations/tools/scrapegraph/)
    - [SearchApi](/docs/integrations/tools/searchapi/)
    - [SearxNG Search](/docs/integrations/tools/searx_search/)
    - [Semantic Scholar API Tool](/docs/integrations/tools/semanticscholar/)
    - [SerpAPI](/docs/integrations/tools/serpapi/)
    - [Slack Toolkit](/docs/integrations/tools/slack/)
    - [Spark SQL Toolkit](/docs/integrations/tools/spark_sql/)
    - [SQLDatabase Toolkit](/docs/integrations/tools/sql_database/)
    - [StackExchange](/docs/integrations/tools/stackexchange/)
    - [Steam Toolkit](/docs/integrations/tools/steam/)
    - [Stripe](/docs/integrations/tools/stripe/)
    - [Tableau](/docs/integrations/tools/tableau/)
    - [Taiga](/docs/integrations/tools/taiga/)
    - [Tavily Extract](/docs/integrations/tools/tavily_extract/)
    - [Tavily Search](/docs/integrations/tools/tavily_search/)
    - [Tilores](/docs/integrations/tools/tilores/)
    - [Twilio](/docs/integrations/tools/twilio/)
    - [Upstage](/docs/integrations/tools/upstage_groundedness_check/)
    - [Valthera](/docs/integrations/tools/valthera/)
    - [Wikidata](/docs/integrations/tools/wikidata/)
    - [Wikipedia](/docs/integrations/tools/wikipedia/)
    - [Wolfram Alpha](/docs/integrations/tools/wolfram_alpha/)
    - [Writer Tools](/docs/integrations/tools/writer/)
    - [Yahoo Finance News](/docs/integrations/tools/yahoo_finance_news/)
    - [You.com Search](/docs/integrations/tools/you/)
    - [YouTube](/docs/integrations/tools/youtube/)
    - [Zapier Natural Language Actions](/docs/integrations/tools/zapier/)
    - [ZenGuard AI](/docs/integrations/tools/zenguard/)
  - [Document loaders](/docs/integrations/document_loaders/)
    
    - [Document loaders](/docs/integrations/document_loaders/)
    - [acreom](/docs/integrations/document_loaders/acreom/)
    - [AgentQLLoader](/docs/integrations/document_loaders/agentql/)
    - [AirbyteLoader](/docs/integrations/document_loaders/airbyte/)
    - [Airbyte CDK (Deprecated)](/docs/integrations/document_loaders/airbyte_cdk/)
    - [Airbyte Gong (Deprecated)](/docs/integrations/document_loaders/airbyte_gong/)
    - [Airbyte Hubspot (Deprecated)](/docs/integrations/document_loaders/airbyte_hubspot/)
    - [Airbyte JSON (Deprecated)](/docs/integrations/document_loaders/airbyte_json/)
    - [Airbyte Salesforce (Deprecated)](/docs/integrations/document_loaders/airbyte_salesforce/)
    - [Airbyte Shopify (Deprecated)](/docs/integrations/document_loaders/airbyte_shopify/)
    - [Airbyte Stripe (Deprecated)](/docs/integrations/document_loaders/airbyte_stripe/)
    - [Airbyte Typeform (Deprecated)](/docs/integrations/document_loaders/airbyte_typeform/)
    - [Airbyte Zendesk Support (Deprecated)](/docs/integrations/document_loaders/airbyte_zendesk_support/)
    - [Airtable](/docs/integrations/document_loaders/airtable/)
    - [Alibaba Cloud MaxCompute](/docs/integrations/document_loaders/alibaba_cloud_maxcompute/)
    - [Amazon Textract](/docs/integrations/document_loaders/amazon_textract/)
    - [Apify Dataset](/docs/integrations/document_loaders/apify_dataset/)
    - [ArcGIS](/docs/integrations/document_loaders/arcgis/)
    - [ArxivLoader](/docs/integrations/document_loaders/arxiv/)
    - [AssemblyAI Audio Transcripts](/docs/integrations/document_loaders/assemblyai/)
    - [AstraDB](/docs/integrations/document_loaders/astradb/)
    - [Async Chromium](/docs/integrations/document_loaders/async_chromium/)
    - [AsyncHtml](/docs/integrations/document_loaders/async_html/)
    - [Athena](/docs/integrations/document_loaders/athena/)
    - [AWS S3 Directory](/docs/integrations/document_loaders/aws_s3_directory/)
    - [AWS S3 File](/docs/integrations/document_loaders/aws_s3_file/)
    - [AZLyrics](/docs/integrations/document_loaders/azlyrics/)
    - [Azure AI Data](/docs/integrations/document_loaders/azure_ai_data/)
    - [Azure Blob Storage Container](/docs/integrations/document_loaders/azure_blob_storage_container/)
    - [Azure Blob Storage File](/docs/integrations/document_loaders/azure_blob_storage_file/)
    - [Azure AI Document Intelligence](/docs/integrations/document_loaders/azure_document_intelligence/)
    - [BibTeX](/docs/integrations/document_loaders/bibtex/)
    - [BiliBili](/docs/integrations/document_loaders/bilibili/)
    - [Blackboard](/docs/integrations/document_loaders/blackboard/)
    - [Blockchain](/docs/integrations/document_loaders/blockchain/)
    - [Box](/docs/integrations/document_loaders/box/)
    - [Brave Search](/docs/integrations/document_loaders/brave_search/)
    - [Browserbase](/docs/integrations/document_loaders/browserbase/)
    - [Browserless](/docs/integrations/document_loaders/browserless/)
    - [BSHTMLLoader](/docs/integrations/document_loaders/bshtml/)
    - [Cassandra](/docs/integrations/document_loaders/cassandra/)
    - [ChatGPT Data](/docs/integrations/document_loaders/chatgpt_loader/)
    - [College Confidential](/docs/integrations/document_loaders/college_confidential/)
    - [Concurrent Loader](/docs/integrations/document_loaders/concurrent/)
    - [Confluence](/docs/integrations/document_loaders/confluence/)
    - [CoNLL-U](/docs/integrations/document_loaders/conll-u/)
    - [Copy Paste](/docs/integrations/document_loaders/copypaste/)
    - [Couchbase](/docs/integrations/document_loaders/couchbase/)
    - [CSV](/docs/integrations/document_loaders/csv/)
    - [Cube Semantic Layer](/docs/integrations/document_loaders/cube_semantic/)
    - [Datadog Logs](/docs/integrations/document_loaders/datadog_logs/)
    - [Dedoc](/docs/integrations/document_loaders/dedoc/)
    - [Diffbot](/docs/integrations/document_loaders/diffbot/)
    - [Discord](/docs/integrations/document_loaders/discord/)
    - [Docling](/docs/integrations/document_loaders/docling/)
    - [Docugami](/docs/integrations/document_loaders/docugami/)
    - [Docusaurus](/docs/integrations/document_loaders/docusaurus/)
    - [Dropbox](/docs/integrations/document_loaders/dropbox/)
    - [DuckDB](/docs/integrations/document_loaders/duckdb/)
    - [Email](/docs/integrations/document_loaders/email/)
    - [EPub](/docs/integrations/document_loaders/epub/)
    - [Etherscan](/docs/integrations/document_loaders/etherscan/)
    - [EverNote](/docs/integrations/document_loaders/evernote/)
    - [example\_data](/docs/integrations/document_loaders/example_data/example/)
    - [Facebook Chat](/docs/integrations/document_loaders/facebook_chat/)
    - [Fauna](/docs/integrations/document_loaders/fauna/)
    - [Figma](/docs/integrations/document_loaders/figma/)
    - [FireCrawl](/docs/integrations/document_loaders/firecrawl/)
    - [Geopandas](/docs/integrations/document_loaders/geopandas/)
    - [Git](/docs/integrations/document_loaders/git/)
    - [GitBook](/docs/integrations/document_loaders/gitbook/)
    - [GitHub](/docs/integrations/document_loaders/github/)
    - [Glue Catalog](/docs/integrations/document_loaders/glue_catalog/)
    - [Google AlloyDB for PostgreSQL](/docs/integrations/document_loaders/google_alloydb/)
    - [Google BigQuery](/docs/integrations/document_loaders/google_bigquery/)
    - [Google Bigtable](/docs/integrations/document_loaders/google_bigtable/)
    - [Google Cloud SQL for SQL server](/docs/integrations/document_loaders/google_cloud_sql_mssql/)
    - [Google Cloud SQL for MySQL](/docs/integrations/document_loaders/google_cloud_sql_mysql/)
    - [Google Cloud SQL for PostgreSQL](/docs/integrations/document_loaders/google_cloud_sql_pg/)
    - [Google Cloud Storage Directory](/docs/integrations/document_loaders/google_cloud_storage_directory/)
    - [Google Cloud Storage File](/docs/integrations/document_loaders/google_cloud_storage_file/)
    - [Google Firestore in Datastore Mode](/docs/integrations/document_loaders/google_datastore/)
    - [Google Drive](/docs/integrations/document_loaders/google_drive/)
    - [Google El Carro for Oracle Workloads](/docs/integrations/document_loaders/google_el_carro/)
    - [Google Firestore (Native Mode)](/docs/integrations/document_loaders/google_firestore/)
    - [Google Memorystore for Redis](/docs/integrations/document_loaders/google_memorystore_redis/)
    - [Google Spanner](/docs/integrations/document_loaders/google_spanner/)
    - [Google Speech-to-Text Audio Transcripts](/docs/integrations/document_loaders/google_speech_to_text/)
    - [Grobid](/docs/integrations/document_loaders/grobid/)
    - [Gutenberg](/docs/integrations/document_loaders/gutenberg/)
    - [Hacker News](/docs/integrations/document_loaders/hacker_news/)
    - [Huawei OBS Directory](/docs/integrations/document_loaders/huawei_obs_directory/)
    - [Huawei OBS File](/docs/integrations/document_loaders/huawei_obs_file/)
    - [HuggingFace dataset](/docs/integrations/document_loaders/hugging_face_dataset/)
    - [HyperbrowserLoader](/docs/integrations/document_loaders/hyperbrowser/)
    - [iFixit](/docs/integrations/document_loaders/ifixit/)
    - [Images](/docs/integrations/document_loaders/image/)
    - [Image captions](/docs/integrations/document_loaders/image_captions/)
    - [IMSDb](/docs/integrations/document_loaders/imsdb/)
    - [Iugu](/docs/integrations/document_loaders/iugu/)
    - [Joplin](/docs/integrations/document_loaders/joplin/)
    - [JSONLoader](/docs/integrations/document_loaders/json/)
    - [Jupyter Notebook](/docs/integrations/document_loaders/jupyter_notebook/)
    - [Kinetica](/docs/integrations/document_loaders/kinetica/)
    - [lakeFS](/docs/integrations/document_loaders/lakefs/)
    - [LangSmith](/docs/integrations/document_loaders/langsmith/)
    - [LarkSuite (FeiShu)](/docs/integrations/document_loaders/larksuite/)
    - [LLM Sherpa](/docs/integrations/document_loaders/llmsherpa/)
    - [Mastodon](/docs/integrations/document_loaders/mastodon/)
    - [MathPixPDFLoader](/docs/integrations/document_loaders/mathpix/)
    - [MediaWiki Dump](/docs/integrations/document_loaders/mediawikidump/)
    - [Merge Documents Loader](/docs/integrations/document_loaders/merge_doc/)
    - [mhtml](/docs/integrations/document_loaders/mhtml/)
    - [Microsoft Excel](/docs/integrations/document_loaders/microsoft_excel/)
    - [Microsoft OneDrive](/docs/integrations/document_loaders/microsoft_onedrive/)
    - [Microsoft OneNote](/docs/integrations/document_loaders/microsoft_onenote/)
    - [Microsoft PowerPoint](/docs/integrations/document_loaders/microsoft_powerpoint/)
    - [Microsoft SharePoint](/docs/integrations/document_loaders/microsoft_sharepoint/)
    - [Microsoft Word](/docs/integrations/document_loaders/microsoft_word/)
    - [Near Blockchain](/docs/integrations/document_loaders/mintbase/)
    - [Modern Treasury](/docs/integrations/document_loaders/modern_treasury/)
    - [MongoDB](/docs/integrations/document_loaders/mongodb/)
    - [Needle Document Loader](/docs/integrations/document_loaders/needle/)
    - [News URL](/docs/integrations/document_loaders/news/)
    - [Notion DB 2/2](/docs/integrations/document_loaders/notion/)
    - [Nuclia](/docs/integrations/document_loaders/nuclia/)
    - [Obsidian](/docs/integrations/document_loaders/obsidian/)
    - [Open Document Format (ODT)](/docs/integrations/document_loaders/odt/)
    - [Open City Data](/docs/integrations/document_loaders/open_city_data/)
    - [Oracle Autonomous Database](/docs/integrations/document_loaders/oracleadb_loader/)
    - [Oracle AI Vector Search: Document Processing](/docs/integrations/document_loaders/oracleai/)
    - [Org-mode](/docs/integrations/document_loaders/org_mode/)
    - [Pandas DataFrame](/docs/integrations/document_loaders/pandas_dataframe/)
    - [parsers](/docs/integrations/document_loaders/parsers/azure_openai_whisper_parser/)
    - [PDFMinerLoader](/docs/integrations/document_loaders/pdfminer/)
    - [PDFPlumber](/docs/integrations/document_loaders/pdfplumber/)
    - [Pebblo Safe DocumentLoader](/docs/integrations/document_loaders/pebblo/)
    - [Polars DataFrame](/docs/integrations/document_loaders/polars_dataframe/)
    - [Dell PowerScale Document Loader](/docs/integrations/document_loaders/powerscale/)
    - [Psychic](/docs/integrations/document_loaders/psychic/)
    - [PubMed](/docs/integrations/document_loaders/pubmed/)
    - [PullMdLoader](/docs/integrations/document_loaders/pull_md/)
    - [PyMuPDFLoader](/docs/integrations/document_loaders/pymupdf/)
    - [PyMuPDF4LLM](/docs/integrations/document_loaders/pymupdf4llm/)
    - [PyPDFDirectoryLoader](/docs/integrations/document_loaders/pypdfdirectory/)
    - [PyPDFium2Loader](/docs/integrations/document_loaders/pypdfium2/)
    - [PyPDFLoader](/docs/integrations/document_loaders/pypdfloader/)
    - [PySpark](/docs/integrations/document_loaders/pyspark_dataframe/)
    - [Quip](/docs/integrations/document_loaders/quip/)
    - [ReadTheDocs Documentation](/docs/integrations/document_loaders/readthedocs_documentation/)
    - [Recursive URL](/docs/integrations/document_loaders/recursive_url/)
    - [Reddit](/docs/integrations/document_loaders/reddit/)
    - [Roam](/docs/integrations/document_loaders/roam/)
    - [Rockset](/docs/integrations/document_loaders/rockset/)
    - [rspace](/docs/integrations/document_loaders/rspace/)
    - [RSS Feeds](/docs/integrations/document_loaders/rss/)
    - [RST](/docs/integrations/document_loaders/rst/)
    - [scrapfly](/docs/integrations/document_loaders/scrapfly/)
    - [ScrapingAnt](/docs/integrations/document_loaders/scrapingant/)
    - [SingleStore](/docs/integrations/document_loaders/singlestore/)
    - [Sitemap](/docs/integrations/document_loaders/sitemap/)
    - [Slack](/docs/integrations/document_loaders/slack/)
    - [Snowflake](/docs/integrations/document_loaders/snowflake/)
    - [Source Code](/docs/integrations/document_loaders/source_code/)
    - [Spider](/docs/integrations/document_loaders/spider/)
    - [Spreedly](/docs/integrations/document_loaders/spreedly/)
    - [Stripe](/docs/integrations/document_loaders/stripe/)
    - [Subtitle](/docs/integrations/document_loaders/subtitle/)
    - [SurrealDB](/docs/integrations/document_loaders/surrealdb/)
    - [Telegram](/docs/integrations/document_loaders/telegram/)
    - [Tencent COS Directory](/docs/integrations/document_loaders/tencent_cos_directory/)
    - [Tencent COS File](/docs/integrations/document_loaders/tencent_cos_file/)
    - [TensorFlow Datasets](/docs/integrations/document_loaders/tensorflow_datasets/)
    - [TiDB](/docs/integrations/document_loaders/tidb/)
    - [2Markdown](/docs/integrations/document_loaders/tomarkdown/)
    - [TOML](/docs/integrations/document_loaders/toml/)
    - [Trello](/docs/integrations/document_loaders/trello/)
    - [TSV](/docs/integrations/document_loaders/tsv/)
    - [Twitter](/docs/integrations/document_loaders/twitter/)
    - [Unstructured](/docs/integrations/document_loaders/unstructured_file/)
    - [UnstructuredMarkdownLoader](/docs/integrations/document_loaders/unstructured_markdown/)
    - [UnstructuredPDFLoader](/docs/integrations/document_loaders/unstructured_pdfloader/)
    - [Upstage](/docs/integrations/document_loaders/upstage/)
    - [URL](/docs/integrations/document_loaders/url/)
    - [Vsdx](/docs/integrations/document_loaders/vsdx/)
    - [Weather](/docs/integrations/document_loaders/weather/)
    - [WebBaseLoader](/docs/integrations/document_loaders/web_base/)
    - [WhatsApp Chat](/docs/integrations/document_loaders/whatsapp_chat/)
    - [Wikipedia](/docs/integrations/document_loaders/wikipedia/)
    - [UnstructuredXMLLoader](/docs/integrations/document_loaders/xml/)
    - [Xorbits Pandas DataFrame](/docs/integrations/document_loaders/xorbits/)
    - [YouTube audio](/docs/integrations/document_loaders/youtube_audio/)
    - [YouTube transcripts](/docs/integrations/document_loaders/youtube_transcript/)
    - [YoutubeLoaderDL](/docs/integrations/document_loaders/yt_dlp/)
    - [Yuque](/docs/integrations/document_loaders/yuque/)
    - [ZeroxPDFLoader](/docs/integrations/document_loaders/zeroxpdfloader/)
  - [Vector stores](/docs/integrations/vectorstores/)
    
    - [Vector stores](/docs/integrations/vectorstores/)
    - [Activeloop Deep Lake](/docs/integrations/vectorstores/activeloop_deeplake/)
    - [Aerospike](/docs/integrations/vectorstores/aerospike/)
    - [Alibaba Cloud OpenSearch](/docs/integrations/vectorstores/alibabacloud_opensearch/)
    - [AnalyticDB](/docs/integrations/vectorstores/analyticdb/)
    - [Annoy](/docs/integrations/vectorstores/annoy/)
    - [Apache Doris](/docs/integrations/vectorstores/apache_doris/)
    - [ApertureDB](/docs/integrations/vectorstores/aperturedb/)
    - [Astra DB Vector Store](/docs/integrations/vectorstores/astradb/)
    - [Atlas](/docs/integrations/vectorstores/atlas/)
    - [AwaDB](/docs/integrations/vectorstores/awadb/)
    - [Azure Cosmos DB Mongo vCore](/docs/integrations/vectorstores/azure_cosmos_db/)
    - [Azure Cosmos DB No SQL](/docs/integrations/vectorstores/azure_cosmos_db_no_sql/)
    - [Azure AI Search](/docs/integrations/vectorstores/azuresearch/)
    - [Bagel](/docs/integrations/vectorstores/bagel/)
    - [BagelDB](/docs/integrations/vectorstores/bageldb/)
    - [Baidu Cloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/baiducloud_vector_search/)
    - [Baidu VectorDB](/docs/integrations/vectorstores/baiduvectordb/)
    - [Apache Cassandra](/docs/integrations/vectorstores/cassandra/)
    - [Chroma](/docs/integrations/vectorstores/chroma/)
    - [Clarifai](/docs/integrations/vectorstores/clarifai/)
    - [ClickHouse](/docs/integrations/vectorstores/clickhouse/)
    - [CloudflareVectorize](/docs/integrations/vectorstores/cloudflare_vectorize/)
    - [Couchbase](/docs/integrations/vectorstores/couchbase/)
    - [DashVector](/docs/integrations/vectorstores/dashvector/)
    - [Databricks](/docs/integrations/vectorstores/databricks_vector_search/)
    - [DingoDB](/docs/integrations/vectorstores/dingo/)
    - [DocArray HnswSearch](/docs/integrations/vectorstores/docarray_hnsw/)
    - [DocArray InMemorySearch](/docs/integrations/vectorstores/docarray_in_memory/)
    - [Amazon Document DB](/docs/integrations/vectorstores/documentdb/)
    - [DuckDB](/docs/integrations/vectorstores/duckdb/)
    - [China Mobile ECloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/ecloud_vector_search/)
    - [Elasticsearch](/docs/integrations/vectorstores/elasticsearch/)
    - [Epsilla](/docs/integrations/vectorstores/epsilla/)
    - [Faiss](/docs/integrations/vectorstores/faiss/)
    - [Faiss (Async)](/docs/integrations/vectorstores/faiss_async/)
    - [FalkorDBVectorStore](/docs/integrations/vectorstores/falkordbvector/)
    - [Google AlloyDB for PostgreSQL](/docs/integrations/vectorstores/google_alloydb/)
    - [Google BigQuery Vector Search](/docs/integrations/vectorstores/google_bigquery_vector_search/)
    - [Google Cloud SQL for MySQL](/docs/integrations/vectorstores/google_cloud_sql_mysql/)
    - [Google Cloud SQL for PostgreSQL](/docs/integrations/vectorstores/google_cloud_sql_pg/)
    - [Firestore](/docs/integrations/vectorstores/google_firestore/)
    - [Google Memorystore for Redis](/docs/integrations/vectorstores/google_memorystore_redis/)
    - [Google Spanner](/docs/integrations/vectorstores/google_spanner/)
    - [Google Vertex AI Feature Store](/docs/integrations/vectorstores/google_vertex_ai_feature_store/)
    - [Google Vertex AI Vector Search](/docs/integrations/vectorstores/google_vertex_ai_vector_search/)
    - [Hippo](/docs/integrations/vectorstores/hippo/)
    - [Hologres](/docs/integrations/vectorstores/hologres/)
    - [Infinispan](/docs/integrations/vectorstores/infinispanvs/)
    - [Jaguar Vector Database](/docs/integrations/vectorstores/jaguar/)
    - [KDB.AI](/docs/integrations/vectorstores/kdbai/)
    - [Kinetica](/docs/integrations/vectorstores/kinetica/)
    - [LanceDB](/docs/integrations/vectorstores/lancedb/)
    - [Lantern](/docs/integrations/vectorstores/lantern/)
    - [Lindorm](/docs/integrations/vectorstores/lindorm/)
    - [LLMRails](/docs/integrations/vectorstores/llm_rails/)
    - [ManticoreSearch VectorStore](/docs/integrations/vectorstores/manticore_search/)
    - [MariaDB](/docs/integrations/vectorstores/mariadb/)
    - [Marqo](/docs/integrations/vectorstores/marqo/)
    - [Meilisearch](/docs/integrations/vectorstores/meilisearch/)
    - [Amazon MemoryDB](/docs/integrations/vectorstores/memorydb/)
    - [Milvus](/docs/integrations/vectorstores/milvus/)
    - [Momento Vector Index (MVI)](/docs/integrations/vectorstores/momento_vector_index/)
    - [MongoDB Atlas](/docs/integrations/vectorstores/mongodb_atlas/)
    - [MyScale](/docs/integrations/vectorstores/myscale/)
    - [Neo4j Vector Index](/docs/integrations/vectorstores/neo4jvector/)
    - [NucliaDB](/docs/integrations/vectorstores/nucliadb/)
    - [Oceanbase](/docs/integrations/vectorstores/oceanbase/)
    - [openGauss](/docs/integrations/vectorstores/opengauss/)
    - [OpenSearch](/docs/integrations/vectorstores/opensearch/)
    - [Oracle AI Vector Search: Vector Store](/docs/integrations/vectorstores/oracle/)
    - [Pathway](/docs/integrations/vectorstores/pathway/)
    - [Postgres Embedding](/docs/integrations/vectorstores/pgembedding/)
    - [PGVecto.rs](/docs/integrations/vectorstores/pgvecto_rs/)
    - [PGVector](/docs/integrations/vectorstores/pgvector/)
    - [Pinecone](/docs/integrations/vectorstores/pinecone/)
    - [Qdrant](/docs/integrations/vectorstores/qdrant/)
    - [Redis](/docs/integrations/vectorstores/redis/)
    - [Relyt](/docs/integrations/vectorstores/relyt/)
    - [Rockset](/docs/integrations/vectorstores/rockset/)
    - [SAP HANA Cloud Vector Engine](/docs/integrations/vectorstores/sap_hanavector/)
    - [ScaNN](/docs/integrations/vectorstores/scann/)
    - [SemaDB](/docs/integrations/vectorstores/semadb/)
    - [SingleStore](/docs/integrations/vectorstores/singlestore/)
    - [scikit-learn](/docs/integrations/vectorstores/sklearn/)
    - [SQLiteVec](/docs/integrations/vectorstores/sqlitevec/)
    - [SQLite-VSS](/docs/integrations/vectorstores/sqlitevss/)
    - [SQLServer](/docs/integrations/vectorstores/sqlserver/)
    - [StarRocks](/docs/integrations/vectorstores/starrocks/)
    - [Supabase (Postgres)](/docs/integrations/vectorstores/supabase/)
    - [SurrealDB](/docs/integrations/vectorstores/surrealdb/)
    - [Tablestore](/docs/integrations/vectorstores/tablestore/)
    - [Tair](/docs/integrations/vectorstores/tair/)
    - [Tencent Cloud VectorDB](/docs/integrations/vectorstores/tencentvectordb/)
    - [ThirdAI NeuralDB](/docs/integrations/vectorstores/thirdai_neuraldb/)
    - [TiDB Vector](/docs/integrations/vectorstores/tidb_vector/)
    - [Tigris](/docs/integrations/vectorstores/tigris/)
    - [TileDB](/docs/integrations/vectorstores/tiledb/)
    - [Timescale Vector (Postgres)](/docs/integrations/vectorstores/timescalevector/)
    - [Typesense](/docs/integrations/vectorstores/typesense/)
    - [Upstash Vector](/docs/integrations/vectorstores/upstash/)
    - [USearch](/docs/integrations/vectorstores/usearch/)
    - [Vald](/docs/integrations/vectorstores/vald/)
    - [VDMS](/docs/integrations/vectorstores/vdms/)
    - [Vearch](/docs/integrations/vectorstores/vearch/)
    - [Vectara](/docs/integrations/vectorstores/vectara/)
    - [Vespa](/docs/integrations/vectorstores/vespa/)
    - [viking DB](/docs/integrations/vectorstores/vikingdb/)
    - [vlite](/docs/integrations/vectorstores/vlite/)
    - [Weaviate](/docs/integrations/vectorstores/weaviate/)
    - [Xata](/docs/integrations/vectorstores/xata/)
    - [YDB](/docs/integrations/vectorstores/ydb/)
    - [Yellowbrick](/docs/integrations/vectorstores/yellowbrick/)
    - [Zep](/docs/integrations/vectorstores/zep/)
    - [Zep Cloud](/docs/integrations/vectorstores/zep_cloud/)
    - [Zilliz](/docs/integrations/vectorstores/zilliz/)
  - [Embedding models](/docs/integrations/text_embedding/)
    
    - [Embedding models](/docs/integrations/text_embedding/)
    - [AI21](/docs/integrations/text_embedding/ai21/)
    - [Aleph Alpha](/docs/integrations/text_embedding/aleph_alpha/)
    - [Anyscale](/docs/integrations/text_embedding/anyscale/)
    - [ascend](/docs/integrations/text_embedding/ascend/)
    - [AwaDB](/docs/integrations/text_embedding/awadb/)
    - [AzureOpenAI](/docs/integrations/text_embedding/azureopenai/)
    - [Baichuan Text Embeddings](/docs/integrations/text_embedding/baichuan/)
    - [Baidu Qianfan](/docs/integrations/text_embedding/baidu_qianfan_endpoint/)
    - [Bedrock](/docs/integrations/text_embedding/bedrock/)
    - [BGE on Hugging Face](/docs/integrations/text_embedding/bge_huggingface/)
    - [Bookend AI](/docs/integrations/text_embedding/bookend/)
    - [Clarifai](/docs/integrations/text_embedding/clarifai/)
    - [Cloudflare Workers AI](/docs/integrations/text_embedding/cloudflare_workersai/)
    - [Clova Embeddings](/docs/integrations/text_embedding/clova/)
    - [Cohere](/docs/integrations/text_embedding/cohere/)
    - [DashScope](/docs/integrations/text_embedding/dashscope/)
    - [Databricks](/docs/integrations/text_embedding/databricks/)
    - [DeepInfra](/docs/integrations/text_embedding/deepinfra/)
    - [EDEN AI](/docs/integrations/text_embedding/edenai/)
    - [Elasticsearch](/docs/integrations/text_embedding/elasticsearch/)
    - [Embaas](/docs/integrations/text_embedding/embaas/)
    - [ERNIE](/docs/integrations/text_embedding/ernie/)
    - [Fake Embeddings](/docs/integrations/text_embedding/fake/)
    - [FastEmbed by Qdrant](/docs/integrations/text_embedding/fastembed/)
    - [Fireworks](/docs/integrations/text_embedding/fireworks/)
    - [GigaChat](/docs/integrations/text_embedding/gigachat/)
    - [Google Generative AI Embeddings](/docs/integrations/text_embedding/google_generative_ai/)
    - [Google Vertex AI](/docs/integrations/text_embedding/google_vertex_ai_palm/)
    - [GPT4All](/docs/integrations/text_embedding/gpt4all/)
    - [Gradient](/docs/integrations/text_embedding/gradient/)
    - [Hugging Face](/docs/integrations/text_embedding/huggingfacehub/)
    - [IBM watsonx.ai](/docs/integrations/text_embedding/ibm_watsonx/)
    - [Infinity](/docs/integrations/text_embedding/infinity/)
    - [Instruct Embeddings on Hugging Face](/docs/integrations/text_embedding/instruct_embeddings/)
    - [IPEX-LLM: Local BGE Embeddings on Intel CPU](/docs/integrations/text_embedding/ipex_llm/)
    - [IPEX-LLM: Local BGE Embeddings on Intel GPU](/docs/integrations/text_embedding/ipex_llm_gpu/)
    - [IntelÂ® Extension for Transformers Quantized Text Embeddings](/docs/integrations/text_embedding/itrex/)
    - [Jina](/docs/integrations/text_embedding/jina/)
    - [John Snow Labs](/docs/integrations/text_embedding/johnsnowlabs_embedding/)
    - [LASER Language-Agnostic SEntence Representations Embeddings by Meta AI](/docs/integrations/text_embedding/laser/)
    - [Lindorm](/docs/integrations/text_embedding/lindorm/)
    - [Llama.cpp](/docs/integrations/text_embedding/llamacpp/)
    - [llamafile](/docs/integrations/text_embedding/llamafile/)
    - [LLMRails](/docs/integrations/text_embedding/llm_rails/)
    - [LocalAI](/docs/integrations/text_embedding/localai/)
    - [MiniMax](/docs/integrations/text_embedding/minimax/)
    - [MistralAI](/docs/integrations/text_embedding/mistralai/)
    - [model2vec](/docs/integrations/text_embedding/model2vec/)
    - [ModelScope](/docs/integrations/text_embedding/modelscope_embedding/)
    - [MosaicML](/docs/integrations/text_embedding/mosaicml/)
    - [Naver](/docs/integrations/text_embedding/naver/)
    - [Netmind](/docs/integrations/text_embedding/netmind/)
    - [NLP Cloud](/docs/integrations/text_embedding/nlp_cloud/)
    - [Nomic](/docs/integrations/text_embedding/nomic/)
    - [NVIDIA NIMs](/docs/integrations/text_embedding/nvidia_ai_endpoints/)
    - [Oracle Cloud Infrastructure Generative AI](/docs/integrations/text_embedding/oci_generative_ai/)
    - [Ollama](/docs/integrations/text_embedding/ollama/)
    - [OpenClip](/docs/integrations/text_embedding/open_clip/)
    - [OpenAI](/docs/integrations/text_embedding/openai/)
    - [OpenVINO](/docs/integrations/text_embedding/openvino/)
    - [Embedding Documents using Optimized and Quantized Embedders](/docs/integrations/text_embedding/optimum_intel/)
    - [Oracle AI Vector Search: Generate Embeddings](/docs/integrations/text_embedding/oracleai/)
    - [OVHcloud](/docs/integrations/text_embedding/ovhcloud/)
    - [Pinecone Embeddings](/docs/integrations/text_embedding/pinecone/)
    - [PredictionGuardEmbeddings](/docs/integrations/text_embedding/predictionguard/)
    - [PremAI](/docs/integrations/text_embedding/premai/)
    - [SageMaker](/docs/integrations/text_embedding/sagemaker-endpoint/)
    - [SambaNovaCloud](/docs/integrations/text_embedding/sambanova/)
    - [SambaStudio](/docs/integrations/text_embedding/sambastudio/)
    - [Self Hosted](/docs/integrations/text_embedding/self-hosted/)
    - [Sentence Transformers on Hugging Face](/docs/integrations/text_embedding/sentence_transformers/)
    - [Solar](/docs/integrations/text_embedding/solar/)
    - [SpaCy](/docs/integrations/text_embedding/spacy_embedding/)
    - [SparkLLM Text Embeddings](/docs/integrations/text_embedding/sparkllm/)
    - [TensorFlow Hub](/docs/integrations/text_embedding/tensorflowhub/)
    - [Text Embeddings Inference](/docs/integrations/text_embedding/text_embeddings_inference/)
    - [TextEmbed - Embedding Inference Server](/docs/integrations/text_embedding/textembed/)
    - [Titan Takeoff](/docs/integrations/text_embedding/titan_takeoff/)
    - [Together AI](/docs/integrations/text_embedding/together/)
    - [Upstage](/docs/integrations/text_embedding/upstage/)
    - [Volc Engine](/docs/integrations/text_embedding/volcengine/)
    - [Voyage AI](/docs/integrations/text_embedding/voyageai/)
    - [Xorbits inference (Xinference)](/docs/integrations/text_embedding/xinference/)
    - [YandexGPT](/docs/integrations/text_embedding/yandex/)
    - [ZhipuAI](/docs/integrations/text_embedding/zhipuai/)
  - [Other](/docs/integrations/llms/)

<!--THE END-->

- Providers


[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/providers/index.mdx)

# Providers

info

If you'd like to write your own integration, see [Extending LangChain](/docs/how_to/#custom). If you'd like to contribute an integration, see [Contributing integrations](/docs/contributing/how_to/integrations/).

LangChain integrates with many providers.

## Integration Packages[â€‹](#integration-packages "Direct link to Integration Packages")

These providers have standalone `langchain-{provider}` packages for improved versioning, dependency management and testing.

| Provider                                                           | Package                                                                                                | Downloads                                                                                                                   | Latest                                                                                                                     | [JS](https://js.langchain.com/docs/integrations/platforms/) |
|--------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|
| [Google VertexAI](/docs/integrations/providers/google/)            | [langchain-google-vertexai](https://python.langchain.com/api_reference/google_vertexai/)               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-google-vertexai?style=flat-square&label=%20&color=blue)        | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-google-vertexai?style=flat-square&label=%20&color=orange)        | âœ…                                                           |
| [OpenAI](/docs/integrations/providers/openai/)                     | [langchain-openai](https://python.langchain.com/api_reference/openai/)                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-openai?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-openai?style=flat-square&label=%20&color=orange)                 | âœ…                                                           |
| [Google Community](/docs/integrations/providers/google/)           | [langchain-google-community](https://python.langchain.com/api_reference/google_community/)             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-google-community?style=flat-square&label=%20&color=blue)       | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-google-community?style=flat-square&label=%20&color=orange)       | âŒ                                                           |
| [AWS](/docs/integrations/providers/aws/)                           | [langchain-aws](https://python.langchain.com/api_reference/aws/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-aws?style=flat-square&label=%20&color=blue)                    | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-aws?style=flat-square&label=%20&color=orange)                    | âœ…                                                           |
| [Anthropic](/docs/integrations/providers/anthropic/)               | [langchain-anthropic](https://python.langchain.com/api_reference/anthropic/)                           | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-anthropic?style=flat-square&label=%20&color=blue)              | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-anthropic?style=flat-square&label=%20&color=orange)              | âœ…                                                           |
| [Google Generative AI](/docs/integrations/providers/google/)       | [langchain-google-genai](https://python.langchain.com/api_reference/google_genai/)                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-google-genai?style=flat-square&label=%20&color=blue)           | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-google-genai?style=flat-square&label=%20&color=orange)           | âœ…                                                           |
| [Ollama](/docs/integrations/providers/ollama/)                     | [langchain-ollama](https://python.langchain.com/api_reference/ollama/)                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-ollama?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-ollama?style=flat-square&label=%20&color=orange)                 | âœ…                                                           |
| [Cohere](/docs/integrations/providers/cohere/)                     | [langchain-cohere](https://python.langchain.com/api_reference/cohere/)                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-cohere?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-cohere?style=flat-square&label=%20&color=orange)                 | âœ…                                                           |
| [Chroma](/docs/integrations/providers/chroma/)                     | [langchain-chroma](https://python.langchain.com/api_reference/chroma/)                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-chroma?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-chroma?style=flat-square&label=%20&color=orange)                 | âŒ                                                           |
| [Groq](/docs/integrations/providers/groq/)                         | [langchain-groq](https://python.langchain.com/api_reference/groq/)                                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-groq?style=flat-square&label=%20&color=blue)                   | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-groq?style=flat-square&label=%20&color=orange)                   | âœ…                                                           |
| [Huggingface](/docs/integrations/providers/huggingface/)           | [langchain-huggingface](https://python.langchain.com/api_reference/huggingface/)                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-huggingface?style=flat-square&label=%20&color=blue)            | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-huggingface?style=flat-square&label=%20&color=orange)            | âŒ                                                           |
| [Postgres](/docs/integrations/providers/pgvector/)                 | [langchain-postgres](https://python.langchain.com/api_reference/postgres/)                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-postgres?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-postgres?style=flat-square&label=%20&color=orange)               | âŒ                                                           |
| [Pinecone](/docs/integrations/providers/pinecone/)                 | [langchain-pinecone](https://python.langchain.com/api_reference/pinecone/)                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-pinecone?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-pinecone?style=flat-square&label=%20&color=orange)               | âœ…                                                           |
| [MistralAI](/docs/integrations/providers/mistralai/)               | [langchain-mistralai](https://python.langchain.com/api_reference/mistralai/)                           | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-mistralai?style=flat-square&label=%20&color=blue)              | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-mistralai?style=flat-square&label=%20&color=orange)              | âœ…                                                           |
| [Fireworks](/docs/integrations/providers/fireworks/)               | [langchain-fireworks](https://python.langchain.com/api_reference/fireworks/)                           | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-fireworks?style=flat-square&label=%20&color=blue)              | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-fireworks?style=flat-square&label=%20&color=orange)              | âŒ                                                           |
| [Milvus](/docs/integrations/providers/milvus/)                     | [langchain-milvus](https://python.langchain.com/api_reference/milvus/)                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-milvus?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-milvus?style=flat-square&label=%20&color=orange)                 | âŒ                                                           |
| [MongoDB](/docs/integrations/providers/mongodb_atlas/)             | [langchain-mongodb](https://python.langchain.com/api_reference/mongodb/)                               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-mongodb?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-mongodb?style=flat-square&label=%20&color=orange)                | âœ…                                                           |
| [Nvidia AI Endpoints](/docs/integrations/providers/nvidia/)        | [langchain-nvidia-ai-endpoints](https://python.langchain.com/api_reference/nvidia_ai_endpoints/)       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-nvidia-ai-endpoints?style=flat-square&label=%20&color=blue)    | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-nvidia-ai-endpoints?style=flat-square&label=%20&color=orange)    | âŒ                                                           |
| [Elasticsearch](/docs/integrations/providers/elasticsearch/)       | [langchain-elasticsearch](https://python.langchain.com/api_reference/elasticsearch/)                   | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-elasticsearch?style=flat-square&label=%20&color=blue)          | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-elasticsearch?style=flat-square&label=%20&color=orange)          | âŒ                                                           |
| [Unstructured](/docs/integrations/providers/unstructured/)         | [langchain-unstructured](https://python.langchain.com/api_reference/unstructured/)                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-unstructured?style=flat-square&label=%20&color=blue)           | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-unstructured?style=flat-square&label=%20&color=orange)           | âŒ                                                           |
| [Qdrant](/docs/integrations/providers/qdrant/)                     | [langchain-qdrant](https://python.langchain.com/api_reference/qdrant/)                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-qdrant?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-qdrant?style=flat-square&label=%20&color=orange)                 | âœ…                                                           |
| [Ibm](/docs/integrations/providers/ibm/)                           | [langchain-ibm](https://python.langchain.com/api_reference/ibm/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-ibm?style=flat-square&label=%20&color=blue)                    | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-ibm?style=flat-square&label=%20&color=orange)                    | âœ…                                                           |
| [Databricks](/docs/integrations/providers/databricks/)             | [databricks-langchain](https://pypi.org/project/databricks-langchain/)                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/databricks-langchain?style=flat-square&label=%20&color=blue)             | ![PyPI - Version](https://img.shields.io/pypi/v/databricks-langchain?style=flat-square&label=%20&color=orange)             | âŒ                                                           |
| [AstraDB](/docs/integrations/providers/astradb/)                   | [langchain-astradb](https://python.langchain.com/api_reference/astradb/)                               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-astradb?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-astradb?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Together](/docs/integrations/providers/together/)                 | [langchain-together](https://python.langchain.com/api_reference/together/)                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-together?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-together?style=flat-square&label=%20&color=orange)               | âŒ                                                           |
| [Deepseek](/docs/integrations/providers/deepseek/)                 | [langchain-deepseek](https://python.langchain.com/api_reference/deepseek/)                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-deepseek?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-deepseek?style=flat-square&label=%20&color=orange)               | âœ…                                                           |
| [Cerebras](/docs/integrations/providers/cerebras/)                 | [langchain-cerebras](https://python.langchain.com/api_reference/cerebras/)                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-cerebras?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-cerebras?style=flat-square&label=%20&color=orange)               | âŒ                                                           |
| [Sambanova](/docs/integrations/providers/sambanova/)               | [langchain-sambanova](https://pypi.org/project/langchain-sambanova/)                                   | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-sambanova?style=flat-square&label=%20&color=blue)              | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-sambanova?style=flat-square&label=%20&color=orange)              | âŒ                                                           |
| [Weaviate](/docs/integrations/providers/weaviate/)                 | [langchain-weaviate](https://python.langchain.com/api_reference/weaviate/)                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-weaviate?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-weaviate?style=flat-square&label=%20&color=orange)               | âœ…                                                           |
| [Neo4J](/docs/integrations/providers/neo4j/)                       | [langchain-neo4j](https://python.langchain.com/api_reference/neo4j/)                                   | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-neo4j?style=flat-square&label=%20&color=blue)                  | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-neo4j?style=flat-square&label=%20&color=orange)                  | âŒ                                                           |
| [XAI](/docs/integrations/providers/xai/)                           | [langchain-xai](https://python.langchain.com/api_reference/xai/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-xai?style=flat-square&label=%20&color=blue)                    | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-xai?style=flat-square&label=%20&color=orange)                    | âŒ                                                           |
| [Redis](/docs/integrations/providers/redis/)                       | [langchain-redis](https://python.langchain.com/api_reference/redis/)                                   | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-redis?style=flat-square&label=%20&color=blue)                  | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-redis?style=flat-square&label=%20&color=orange)                  | âœ…                                                           |
| [VoyageAI](/docs/integrations/providers/voyageai/)                 | [langchain-voyageai](https://python.langchain.com/api_reference/voyageai/)                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-voyageai?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-voyageai?style=flat-square&label=%20&color=orange)               | âŒ                                                           |
| [Azure AI](/docs/integrations/providers/azure_ai/)                 | [langchain-azure-ai](https://python.langchain.com/api_reference/azure_ai/)                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-azure-ai?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-azure-ai?style=flat-square&label=%20&color=orange)               | âœ…                                                           |
| [Upstage](/docs/integrations/providers/upstage/)                   | [langchain-upstage](https://python.langchain.com/api_reference/upstage/)                               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-upstage?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-upstage?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Docling](/docs/integrations/providers/docling/)                   | [langchain-docling](https://pypi.org/project/langchain-docling/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-docling?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-docling?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Nomic](/docs/integrations/providers/nomic/)                       | [langchain-nomic](https://python.langchain.com/api_reference/nomic/)                                   | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-nomic?style=flat-square&label=%20&color=blue)                  | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-nomic?style=flat-square&label=%20&color=orange)                  | âœ…                                                           |
| [Graph RAG](/docs/integrations/providers/graph_rag/)               | [langchain-graph-retriever](https://pypi.org/project/langchain-graph-retriever/)                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-graph-retriever?style=flat-square&label=%20&color=blue)        | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-graph-retriever?style=flat-square&label=%20&color=orange)        | âŒ                                                           |
| [Azure Dynamic Sessions](/docs/integrations/providers/microsoft/)  | [langchain-azure-dynamic-sessions](https://python.langchain.com/api_reference/azure_dynamic_sessions/) | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-azure-dynamic-sessions?style=flat-square&label=%20&color=blue) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-azure-dynamic-sessions?style=flat-square&label=%20&color=orange) | âœ…                                                           |
| [Exa](/docs/integrations/providers/exa_search/)                    | [langchain-exa](https://python.langchain.com/api_reference/exa/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-exa?style=flat-square&label=%20&color=blue)                    | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-exa?style=flat-square&label=%20&color=orange)                    | âœ…                                                           |
| [AI21](/docs/integrations/providers/ai21/)                         | [langchain-ai21](https://python.langchain.com/api_reference/ai21/)                                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-ai21?style=flat-square&label=%20&color=blue)                   | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-ai21?style=flat-square&label=%20&color=orange)                   | âŒ                                                           |
| [Tavily](/docs/integrations/providers/tavily/)                     | [langchain-tavily](https://pypi.org/project/langchain-tavily/)                                         | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-tavily?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-tavily?style=flat-square&label=%20&color=orange)                 | âŒ                                                           |
| [Predictionguard](/docs/integrations/providers/predictionguard/)   | [langchain-predictionguard](https://pypi.org/project/langchain-predictionguard/)                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-predictionguard?style=flat-square&label=%20&color=blue)        | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-predictionguard?style=flat-square&label=%20&color=orange)        | âŒ                                                           |
| [Pymupdf4Llm](/docs/integrations/providers/pymupdf4llm/)           | [langchain-pymupdf4llm](https://pypi.org/project/langchain-pymupdf4llm/)                               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-pymupdf4llm?style=flat-square&label=%20&color=blue)            | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-pymupdf4llm?style=flat-square&label=%20&color=orange)            | âŒ                                                           |
| [Sqlserver](/docs/integrations/providers/microsoft/)               | [langchain-sqlserver](https://python.langchain.com/api_reference/sqlserver/)                           | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-sqlserver?style=flat-square&label=%20&color=blue)              | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-sqlserver?style=flat-square&label=%20&color=orange)              | âŒ                                                           |
| [Snowflake](/docs/integrations/providers/snowflake/)               | [langchain-snowflake](https://python.langchain.com/api_reference/snowflake/)                           | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-snowflake?style=flat-square&label=%20&color=blue)              | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-snowflake?style=flat-square&label=%20&color=orange)              | âŒ                                                           |
| [Sema4](/docs/integrations/providers/robocorp/)                    | [langchain-sema4](https://python.langchain.com/api_reference/sema4/)                                   | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-sema4?style=flat-square&label=%20&color=blue)                  | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-sema4?style=flat-square&label=%20&color=orange)                  | âŒ                                                           |
| [VDMS](/docs/integrations/providers/vdms/)                         | [langchain-vdms](https://pypi.org/project/langchain-vdms/)                                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-vdms?style=flat-square&label=%20&color=blue)                   | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-vdms?style=flat-square&label=%20&color=orange)                   | âŒ                                                           |
| [Prompty](/docs/integrations/providers/microsoft/)                 | [langchain-prompty](https://python.langchain.com/api_reference/prompty/)                               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-prompty?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-prompty?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Scrapegraph](/docs/integrations/providers/scrapegraph/)           | [langchain-scrapegraph](https://pypi.org/project/langchain-scrapegraph/)                               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-scrapegraph?style=flat-square&label=%20&color=blue)            | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-scrapegraph?style=flat-square&label=%20&color=orange)            | âŒ                                                           |
| [LangFair](/docs/integrations/providers/langfair/)                 | [langfair](https://pypi.org/project/langfair/)                                                         | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langfair?style=flat-square&label=%20&color=blue)                         | ![PyPI - Version](https://img.shields.io/pypi/v/langfair?style=flat-square&label=%20&color=orange)                         | âŒ                                                           |
| [ADS4GPTs](/docs/integrations/providers/ads4gpts/)                 | [ads4gpts-langchain](https://pypi.org/project/ads4gpts-langchain/)                                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/ads4gpts-langchain?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/ads4gpts-langchain?style=flat-square&label=%20&color=orange)               | âŒ                                                           |
| [Contextual AI](/docs/integrations/providers/contextual/)          | [langchain-contextual](https://pypi.org/project/langchain-contextual/)                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-contextual?style=flat-square&label=%20&color=blue)             | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-contextual?style=flat-square&label=%20&color=orange)             | âŒ                                                           |
| [Apify](/docs/integrations/providers/apify/)                       | [langchain-apify](https://pypi.org/project/langchain-apify/)                                           | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-apify?style=flat-square&label=%20&color=blue)                  | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-apify?style=flat-square&label=%20&color=orange)                  | âŒ                                                           |
| [Couchbase](/docs/integrations/providers/couchbase/)               | [langchain-couchbase](https://pypi.org/project/langchain-couchbase/)                                   | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-couchbase?style=flat-square&label=%20&color=blue)              | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-couchbase?style=flat-square&label=%20&color=orange)              | âŒ                                                           |
| [MariaDB](/docs/integrations/providers/mariadb/)                   | [langchain-mariadb](https://pypi.org/project/langchain-mariadb/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-mariadb?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-mariadb?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Linkup](/docs/integrations/providers/linkup/)                     | [langchain-linkup](https://pypi.org/project/langchain-linkup/)                                         | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-linkup?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-linkup?style=flat-square&label=%20&color=orange)                 | âŒ                                                           |
| [Box](/docs/integrations/providers/box/)                           | [langchain-box](https://pypi.org/project/langchain-box/)                                               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-box?style=flat-square&label=%20&color=blue)                    | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-box?style=flat-square&label=%20&color=orange)                    | âŒ                                                           |
| [Valthera](/docs/integrations/providers/valthera/)                 | [langchain-valthera](https://pypi.org/project/langchain-valthera/)                                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-valthera?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-valthera?style=flat-square&label=%20&color=orange)               | âŒ                                                           |
| [LocalAI](/docs/integrations/providers/localai/)                   | [langchain-localai](https://pypi.org/project/langchain-localai/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-localai?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-localai?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Writer](/docs/integrations/providers/writer/)                     | [langchain-writer](https://pypi.org/project/langchain-writer/)                                         | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-writer?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-writer?style=flat-square&label=%20&color=orange)                 | âŒ                                                           |
| [Agentql](/docs/integrations/providers/agentql/)                   | [langchain-agentql](https://pypi.org/project/langchain-agentql/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-agentql?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-agentql?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [GOAT SDK](/docs/integrations/providers/goat/)                     | [goat-sdk-adapter-langchain](https://pypi.org/project/goat-sdk-adapter-langchain/)                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/goat-sdk-adapter-langchain?style=flat-square&label=%20&color=blue)       | ![PyPI - Version](https://img.shields.io/pypi/v/goat-sdk-adapter-langchain?style=flat-square&label=%20&color=orange)       | âŒ                                                           |
| [Opengradient](/docs/integrations/providers/opengradient/)         | [langchain-opengradient](https://pypi.org/project/langchain-opengradient/)                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-opengradient?style=flat-square&label=%20&color=blue)           | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-opengradient?style=flat-square&label=%20&color=orange)           | âŒ                                                           |
| [Salesforce](/docs/integrations/providers/salesforce/)             | [langchain-salesforce](https://pypi.org/project/langchain-salesforce/)                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-salesforce?style=flat-square&label=%20&color=blue)             | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-salesforce?style=flat-square&label=%20&color=orange)             | âŒ                                                           |
| [Kuzu](/docs/integrations/providers/kuzu/)                         | [langchain-kuzu](https://pypi.org/project/langchain-kuzu/)                                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-kuzu?style=flat-square&label=%20&color=blue)                   | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-kuzu?style=flat-square&label=%20&color=orange)                   | âŒ                                                           |
| [Goodfire](/docs/integrations/providers/goodfire/)                 | [langchain-goodfire](https://pypi.org/project/langchain-goodfire/)                                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-goodfire?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-goodfire?style=flat-square&label=%20&color=orange)               | âŒ                                                           |
| [Taiga](/docs/integrations/providers/taiga/)                       | [langchain-taiga](https://pypi.org/project/langchain-taiga/)                                           | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-taiga?style=flat-square&label=%20&color=blue)                  | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-taiga?style=flat-square&label=%20&color=orange)                  | âŒ                                                           |
| [Memgraph](/docs/integrations/providers/memgraph/)                 | [langchain-memgraph](https://pypi.org/project/langchain-memgraph/)                                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-memgraph?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-memgraph?style=flat-square&label=%20&color=orange)               | âŒ                                                           |
| [Permit](/docs/integrations/providers/permit/)                     | [langchain-permit](https://pypi.org/project/langchain-permit/)                                         | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-permit?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-permit?style=flat-square&label=%20&color=orange)                 | âŒ                                                           |
| [Dappier](/docs/integrations/providers/dappier/)                   | [langchain-dappier](https://pypi.org/project/langchain-dappier/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-dappier?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-dappier?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Vectara](/docs/integrations/providers/vectara/)                   | [langchain-vectara](https://pypi.org/project/langchain-vectara/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-vectara?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-vectara?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Payman Tool](/docs/integrations/providers/payman-tool/)           | [langchain-payman-tool](https://pypi.org/project/langchain-payman-tool/)                               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-payman-tool?style=flat-square&label=%20&color=blue)            | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-payman-tool?style=flat-square&label=%20&color=orange)            | âŒ                                                           |
| [Hyperbrowser](/docs/integrations/providers/hyperbrowser/)         | [langchain-hyperbrowser](https://pypi.org/project/langchain-hyperbrowser/)                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-hyperbrowser?style=flat-square&label=%20&color=blue)           | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-hyperbrowser?style=flat-square&label=%20&color=orange)           | âŒ                                                           |
| [Prolog](/docs/integrations/providers/prolog/)                     | [langchain-prolog](https://pypi.org/project/langchain-prolog/)                                         | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-prolog?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-prolog?style=flat-square&label=%20&color=orange)                 | âŒ                                                           |
| [Tableau](/docs/integrations/providers/tableau/)                   | [langchain-tableau](https://pypi.org/project/langchain-tableau/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-tableau?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-tableau?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Jenkins](/docs/integrations/providers/jenkins/)                   | [langchain-jenkins](https://pypi.org/project/langchain-jenkins/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-jenkins?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-jenkins?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Abso](/docs/integrations/providers/abso/)                         | [langchain-abso](https://pypi.org/project/langchain-abso/)                                             | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-abso?style=flat-square&label=%20&color=blue)                   | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-abso?style=flat-square&label=%20&color=orange)                   | âŒ                                                           |
| [Xinference](/docs/integrations/providers/xinference/)             | [langchain-xinference](https://pypi.org/project/langchain-xinference/)                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-xinference?style=flat-square&label=%20&color=blue)             | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-xinference?style=flat-square&label=%20&color=orange)             | âŒ                                                           |
| [Deeplake](/docs/integrations/providers/deeplake/)                 | [langchain-deeplake](https://pypi.org/project/langchain-deeplake/)                                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-deeplake?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-deeplake?style=flat-square&label=%20&color=orange)               | âŒ                                                           |
| [Netmind](/docs/integrations/providers/netmind/)                   | [langchain-netmind](https://pypi.org/project/langchain-netmind/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-netmind?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-netmind?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [CrateDB](/docs/integrations/providers/cratedb/)                   | [langchain-cratedb](https://pypi.org/project/langchain-cratedb/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-cratedb?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-cratedb?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Zotero](/docs/integrations/providers/zotero/)                     | [langchain-zotero-retriever](https://pypi.org/project/langchain-zotero-retriever/)                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-zotero-retriever?style=flat-square&label=%20&color=blue)       | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-zotero-retriever?style=flat-square&label=%20&color=orange)       | âŒ                                                           |
| [Nimble](/docs/integrations/providers/nimble/)                     | [langchain-nimble](https://pypi.org/project/langchain-nimble/)                                         | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-nimble?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-nimble?style=flat-square&label=%20&color=orange)                 | âŒ                                                           |
| [PowerScale RAG Connector](/docs/integrations/providers/dell/)     | [powerscale-rag-connector](https://pypi.org/project/powerscale-rag-connector/)                         | ![PyPI - Downloads](https://img.shields.io/pypi/dm/powerscale-rag-connector?style=flat-square&label=%20&color=blue)         | ![PyPI - Version](https://img.shields.io/pypi/v/powerscale-rag-connector?style=flat-square&label=%20&color=orange)         | âŒ                                                           |
| [RunPod](/docs/integrations/providers/runpod/)                     | [langchain-runpod](https://pypi.org/project/langchain-runpod/)                                         | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-runpod?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-runpod?style=flat-square&label=%20&color=orange)                 | âŒ                                                           |
| [Naver](/docs/integrations/providers/naver/)                       | [langchain-naver-community](https://pypi.org/project/langchain-naver-community/)                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-naver-community?style=flat-square&label=%20&color=blue)        | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-naver-community?style=flat-square&label=%20&color=orange)        | âŒ                                                           |
| [FalkorDB](/docs/integrations/providers/falkordb/)                 | [langchain-falkordb](https://pypi.org/project/langchain-falkordb/)                                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-falkordb?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-falkordb?style=flat-square&label=%20&color=orange)               | âŒ                                                           |
| [Discord Shikenso](/docs/integrations/providers/discord-shikenso/) | [langchain-discord-shikenso](https://pypi.org/project/langchain-discord-shikenso/)                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-discord-shikenso?style=flat-square&label=%20&color=blue)       | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-discord-shikenso?style=flat-square&label=%20&color=orange)       | âŒ                                                           |
| [Modelscope](/docs/integrations/providers/modelscope/)             | [langchain-modelscope](https://pypi.org/project/langchain-modelscope/)                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-modelscope?style=flat-square&label=%20&color=blue)             | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-modelscope?style=flat-square&label=%20&color=orange)             | âŒ                                                           |
| [Pull Md](/docs/integrations/providers/pull-md/)                   | [langchain-pull-md](https://pypi.org/project/langchain-pull-md/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-pull-md?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-pull-md?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Cognee](/docs/integrations/providers/cognee/)                     | [langchain-cognee](https://pypi.org/project/langchain-cognee/)                                         | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-cognee?style=flat-square&label=%20&color=blue)                 | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-cognee?style=flat-square&label=%20&color=orange)                 | âŒ                                                           |
| [Fmp Data](/docs/integrations/providers/fmp-data/)                 | [langchain-fmp-data](https://pypi.org/project/langchain-fmp-data/)                                     | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-fmp-data?style=flat-square&label=%20&color=blue)               | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-fmp-data?style=flat-square&label=%20&color=orange)               | âŒ                                                           |
| [Tilores](/docs/integrations/providers/tilores/)                   | [tilores-langchain](https://pypi.org/project/tilores-langchain/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/tilores-langchain?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/tilores-langchain?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Pipeshift](/docs/integrations/providers/pipeshift/)               | [langchain-pipeshift](https://pypi.org/project/langchain-pipeshift/)                                   | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-pipeshift?style=flat-square&label=%20&color=blue)              | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-pipeshift?style=flat-square&label=%20&color=orange)              | âŒ                                                           |
| [Oceanbase](/docs/integrations/providers/oceanbase/)               | [langchain-oceanbase](https://pypi.org/project/langchain-oceanbase/)                                   | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-oceanbase?style=flat-square&label=%20&color=blue)              | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-oceanbase?style=flat-square&label=%20&color=orange)              | âŒ                                                           |
| [Lindorm Integration](/docs/integrations/providers/lindorm/)       | [langchain-lindorm-integration](https://pypi.org/project/langchain-lindorm-integration/)               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-lindorm-integration?style=flat-square&label=%20&color=blue)    | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-lindorm-integration?style=flat-square&label=%20&color=orange)    | âŒ                                                           |
| [Oxylabs](/docs/integrations/providers/oxylabs/)                   | [langchain-oxylabs](https://pypi.org/project/langchain-oxylabs/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-oxylabs?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-oxylabs?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Perplexity](/docs/integrations/providers/perplexity/)             | [langchain-perplexity](https://python.langchain.com/api_reference/perplexity/)                         | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-perplexity?style=flat-square&label=%20&color=blue)             | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-perplexity?style=flat-square&label=%20&color=orange)             | âŒ                                                           |
| [Qwq](/docs/integrations/providers/alibaba_cloud/)                 | [langchain-qwq](https://pypi.org/project/langchain-qwq/)                                               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-qwq?style=flat-square&label=%20&color=blue)                    | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-qwq?style=flat-square&label=%20&color=orange)                    | âŒ                                                           |
| [Litellm](/docs/integrations/providers/litellm/)                   | [langchain-litellm](https://pypi.org/project/langchain-litellm/)                                       | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-litellm?style=flat-square&label=%20&color=blue)                | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-litellm?style=flat-square&label=%20&color=orange)                | âŒ                                                           |
| [Cloudflare](/docs/integrations/providers/cloudflare/)             | [langchain-cloudflare](https://pypi.org/project/langchain-cloudflare/)                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-cloudflare?style=flat-square&label=%20&color=blue)             | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-cloudflare?style=flat-square&label=%20&color=orange)             | âŒ                                                           |
| [YDB](/docs/integrations/providers/ydb/)                           | [langchain-ydb](https://pypi.org/project/langchain-ydb/)                                               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-ydb?style=flat-square&label=%20&color=blue)                    | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-ydb?style=flat-square&label=%20&color=orange)                    | âŒ                                                           |
| [SingleStore](/docs/integrations/providers/singlestore/)           | [langchain-singlestore](https://pypi.org/project/langchain-singlestore/)                               | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-singlestore?style=flat-square&label=%20&color=blue)            | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-singlestore?style=flat-square&label=%20&color=orange)            | âŒ                                                           |
| [Galaxia Retriever](/docs/integrations/providers/galaxia/)         | [langchain-galaxia-retriever](https://pypi.org/project/langchain-galaxia-retriever/)                   | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-galaxia-retriever?style=flat-square&label=%20&color=blue)      | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-galaxia-retriever?style=flat-square&label=%20&color=orange)      | âŒ                                                           |

## All Providers[â€‹](#all-providers "Direct link to All Providers")

Click [here](/docs/integrations/providers/all/) to see all providers. Or search for a provider using the Search field in the top-right corner of the screen.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/integrations/providers/index.mdx)

* * *









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/qa_sources.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/qa_sources.ipynb)

# How to get your RAG application to return sources

Often in [Q&amp;A](/docs/concepts/rag/) applications it's important to show users the sources that were used to generate the answer. The simplest way to do this is for the chain to return the Documents that were retrieved in each generation.

We'll work off of the Q&amp;A app we built over the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng in the [RAG tutorial](/docs/tutorials/rag/).

We will cover two approaches:

1. Using the basic RAG chain covered in [Part 1](/docs/tutorials/rag/) of the RAG tutorial;
2. Using a conversational RAG chain as convered in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial.

We will also show how to structure sources into the model response, such that a model can report what specific sources it used in generating its answer.

## Setup[â€‹](#setup "Direct link to Setup")

### Dependencies[â€‹](#dependencies "Direct link to Dependencies")

We'll use the following packages:

```python
%pip install --upgrade --quiet langchain langchain-community langchainhub beautifulsoup4
```

### LangSmith[â€‹](#langsmith "Direct link to LangSmith")

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).

Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:

```python
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

### Components[â€‹](#components "Direct link to Components")

We will need to select three components from LangChain's suite of integrations.

A [chat model](/docs/integrations/chat/):

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

An [embedding model](/docs/integrations/text_embedding/):

Select [embeddings model](/docs/integrations/text_embedding/):

OpenAIâ–¾

[OpenAI](#)

[Azure](#)

[Google](#)

[AWS](#)

[HuggingFace](#)

[Ollama](#)

[Cohere](#)

[MistralAI](#)

[Nomic](#)

[NVIDIA](#)

[Voyage AI](#)

[IBM watsonx](#)

[Fake](#)

```bash
pip install -qU langchain-openai
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
```

And a [vector store](/docs/integrations/vectorstores/):

Select [vector store](/docs/integrations/vectorstores/):

In-memoryâ–¾

[In-memory](#)

[AstraDB](#)

[Chroma](#)

[FAISS](#)

[Milvus](#)

[MongoDB](#)

[PGVector](#)

[Pinecone](#)

[Qdrant](#)

```bash
pip install -qU langchain-core
```

```python
from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)
```

## RAG application[â€‹](#rag-application "Direct link to RAG application")

Let's reconstruct the Q&amp;A app with sources we built over the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng in the [RAG tutorial](/docs/tutorials/rag/).

First we index our documents:

```python
import bs4
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing_extensions import List, TypedDict

# Load and chunk contents of the blog
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

```python
# Index chunks
_ = vector_store.add_documents(documents=all_splits)
```

Next we build the application:

```python
from langchain import hub
from langchain_core.documents import Document
from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict

# Define prompt for question-answering
prompt = hub.pull("rlm/rag-prompt")


# Define state for application
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str


# Define application steps
def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"])
    return {"context": retrieved_docs}


def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}


# Compile application and test
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```python
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

Because we're tracking the retrieved context in our application's state, it is accessible after invoking the application:

```python
result = graph.invoke({"question": "What is Task Decomposition?"})

print(f'Context: {result["context"]}\n\n')
print(f'Answer: {result["answer"]}')
```

```output
Context: [Document(id='c8471b37-07d8-4d51-856e-4b2c22bca88d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.'), Document(id='acb7eb6f-f252-4353-aec2-f459135354ba', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.'), Document(id='4fae6668-7fec-4237-9b2d-78132f4f3f3f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='3c79dd86-595e-42e8-b64d-404780f9e2d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content="(3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.")]


Answer: Task Decomposition is the process of breaking down a complex task into smaller, manageable steps to facilitate execution. This can be achieved through techniques like Chain of Thought, which encourages step-by-step reasoning, or Tree of Thoughts, which explores multiple reasoning paths for each step. It can be implemented using simple prompts, specific instructions, or human input to effectively tackle the original task.
```

Here, `"context"` contains the sources that the LLM used in generating the response in `"answer"`.

## Structure sources in model response[â€‹](#structure-sources-in-model-response "Direct link to Structure sources in model response")

Up to this point, we've simply propagated the documents returned from the retrieval step through to the final response. But this may not illustrate what subset of information the model relied on when generating its answer. Below, we show how to structure sources into the model response, allowing the model to report what specific context it relied on for its answer.

It is straightforward to extend the above LangGraph implementation. Below, we make a simple change: we use the model's tool-calling features to generate [structured output](/docs/how_to/structured_output/), consisting of an answer and list of sources. The schema for the response is represented in the `AnswerWithSources` TypedDict, below.

```python
from typing import List

from typing_extensions import Annotated, TypedDict


# Desired schema for response
class AnswerWithSources(TypedDict):
    """An answer to the question, with sources."""

    answer: str
    sources: Annotated[
        List[str],
        ...,
        "List of sources (author + year) used to answer the question",
    ]


class State(TypedDict):
    question: str
    context: List[Document]
    answer: AnswerWithSources


def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    structured_llm = llm.with_structured_output(AnswerWithSources)
    response = structured_llm.invoke(messages)
    return {"answer": response}


graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
```

```python
import json

result = graph.invoke({"question": "What is Chain of Thought?"})
print(json.dumps(result["answer"], indent=2))
```

```output
{
  "answer": "Chain of Thought (CoT) is a prompting technique that enhances model performance by instructing it to think step by step, allowing the decomposition of complex tasks into smaller, manageable steps. This method not only aids in task execution but also provides insights into the model's reasoning process. CoT has become a standard approach in improving how language models handle intricate problem-solving tasks.",
  "sources": [
    "Wei et al. 2022"
  ]
}
```

tip

View [LangSmith trace](https://smith.langchain.com/public/51d543f7-bdf6-4d93-9ecd-2fc09bf6d666/r).

## Conversational RAG[â€‹](#conversational-rag "Direct link to Conversational RAG")

[Part 2](/docs/tutorials/qa_chat_history/) of the RAG tutorial implements a different architecture, in which steps in the RAG flow are represented via successive [message](/docs/concepts/messages/) objects. This leverages additional [tool-calling](/docs/concepts/tool_calling/) features of chat models, and more naturally accommodates a "back-and-forth" conversational user experience.

In that tutorial (and below), we propagate the retrieved documents as [artifacts](/docs/how_to/tool_artifacts/) on the tool messages. That makes it easy to pluck out the retrieved documents. Below, we add them as an additional key in the state, for convenience.

Note that we define the response format of the tool as `"content_and_artifact"`:

```python
from langchain_core.tools import tool


@tool(response_format="content_and_artifact")
def retrieve(query: str):
    """Retrieve information related to a query."""
    retrieved_docs = vector_store.similarity_search(query, k=2)
    serialized = "\n\n".join(
        (f"Source: {doc.metadata}\n" f"Content: {doc.page_content}")
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

We can now build and compile the exact same application as in [Part 2](/docs/tutorials/qa_chat_history/) of the RAG tutorial, with two changes:

1. We add a `context` key of the state to store retrieved documents;
2. In the `generate` step, we pluck out the retrieved documents and populate them in the state.

These changes are highlighted below.

```python
from langchain_core.messages import SystemMessage
from langgraph.graph import END, MessagesState, StateGraph
from langgraph.prebuilt import ToolNode, tools_condition


class State(MessagesState):
    context: List[Document]


# Step 1: Generate an AIMessage that may include a tool-call to be sent.
def query_or_respond(state: State):
    """Generate tool call for retrieval or respond."""
    llm_with_tools = llm.bind_tools([retrieve])
    response = llm_with_tools.invoke(state["messages"])
    # MessagesState appends messages to state instead of overwriting
    return {"messages": [response]}


# Step 2: Execute the retrieval.
tools = ToolNode([retrieve])


# Step 3: Generate a response using the retrieved content.
def generate(state: MessagesState):
    """Generate answer."""
    # Get generated ToolMessages
    recent_tool_messages = []
    for message in reversed(state["messages"]):
        if message.type == "tool":
            recent_tool_messages.append(message)
        else:
            break
    tool_messages = recent_tool_messages[::-1]

    # Format into prompt
    docs_content = "\n\n".join(doc.content for doc in tool_messages)
    system_message_content = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you "
        "don't know. Use three sentences maximum and keep the "
        "answer concise."
        "\n\n"
        f"{docs_content}"
    )
    conversation_messages = [
        message
        for message in state["messages"]
        if message.type in ("human", "system")
        or (message.type == "ai" and not message.tool_calls)
    ]
    prompt = [SystemMessage(system_message_content)] + conversation_messages

    # Run
    response = llm.invoke(prompt)
    context = []
    for tool_message in tool_messages:
        context.extend(tool_message.artifact)
    return {"messages": [response], "context": context}
```

**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools\_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)

We can compile the application as before:

```python
graph_builder = StateGraph(MessagesState)

graph_builder.add_node(query_or_respond)
graph_builder.add_node(tools)
graph_builder.add_node(generate)

graph_builder.set_entry_point("query_or_respond")
graph_builder.add_conditional_edges(
    "query_or_respond",
    tools_condition,
    {END: END, "tools": "tools"},
)
graph_builder.add_edge("tools", "generate")
graph_builder.add_edge("generate", END)

graph = graph_builder.compile()
```

```python
display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

Invoking our application, we see that the retrieved [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects are accessible from the application state.

```python
input_message = "What is Task Decomposition?"

for step in graph.stream(
    {"messages": [{"role": "user", "content": input_message}]},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

What is Task Decomposition?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_oA0XZ5hF70X0oW4ccNUFCFxX)
 Call ID: call_oA0XZ5hF70X0oW4ccNUFCFxX
  Args:
    query: Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
==================================[1m Ai Message [0m==================================

Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often utilizes techniques like Chain of Thought (CoT) prompting, which encourages models to think step by step, enhancing performance on complex tasks. This approach helps clarify the model's reasoning and makes it easier to tackle difficult problems.
```

```python
step["context"]
```

```output
[Document(id='c8471b37-07d8-4d51-856e-4b2c22bca88d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.'),
 Document(id='acb7eb6f-f252-4353-aec2-f459135354ba', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.')]
```

tip

Check out the [LangSmith trace](https://smith.langchain.com/public/cc25515d-2e46-44fa-8bb2-b9cb0f451504/r).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/qa_sources.ipynb)

* * *


- [Setup](#setup)
  
  - [Dependencies](#dependencies)
  - [LangSmith](#langsmith)
  - [Components](#components)
- [RAG application](#rag-application)
- [Structure sources in model response](#structure-sources-in-model-response)
- [Conversational RAG](#conversational-rag)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/embed_text.mdx)

# Text embedding models

info

Head to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers.

The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.

Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.

The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former, `.embed_documents`, takes as input multiple texts, while the latter, `.embed_query`, takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself). `.embed_query` will return a list of floats, whereas `.embed_documents` returns a list of lists of floats.

## Get started[â€‹](#get-started "Direct link to Get started")

### Setup[â€‹](#setup "Direct link to Setup")

Select [embeddings model](/docs/integrations/text_embedding/):

OpenAIâ–¾

[OpenAI](#)

[Azure](#)

[Google](#)

[AWS](#)

[HuggingFace](#)

[Ollama](#)

[Cohere](#)

[MistralAI](#)

[Nomic](#)

[NVIDIA](#)

[Voyage AI](#)

[IBM watsonx](#)

[Fake](#)

```bash
pip install -qU langchain-openai
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings_model = OpenAIEmbeddings(model="text-embedding-3-large")
```

### `embed_documents`[â€‹](#embed_documents "Direct link to embed_documents")

#### Embed list of texts[â€‹](#embed-list-of-texts "Direct link to Embed list of texts")

Use `.embed_documents` to embed a list of strings, recovering a list of embeddings:

```python
embeddings = embeddings_model.embed_documents(
    [
        "Hi there!",
        "Oh, hello!",
        "What's your name?",
        "My friends call me World",
        "Hello World!"
    ]
)
len(embeddings), len(embeddings[0])
```

```output
(5, 1536)
```

### `embed_query`[â€‹](#embed_query "Direct link to embed_query")

#### Embed single query[â€‹](#embed-single-query "Direct link to Embed single query")

Use `.embed_query` to embed a single piece of text (e.g., for the purpose of comparing to other embedded pieces of texts).

```python
embedded_query = embeddings_model.embed_query("What was the name mentioned in the conversation?")
embedded_query[:5]
```

```output
[0.0053587136790156364,
 -0.0004999046213924885,
 0.038883671164512634,
 -0.003001077566295862,
 -0.00900818221271038]
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/embed_text.mdx)

* * *


- [Get started](#get-started)
  
  - [Setup](#setup)
  - [`embed_documents`](#embed_documents)
  - [`embed_query`](#embed_query)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/agents.mdx)

# Agents

By themselves, language models can't take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions.

[LangGraph](/docs/concepts/architecture/#langgraph) is an extension of LangChain specifically aimed at creating highly controllable and customizable agents. We recommend that you use LangGraph for building agents.

Please see the following resources for more information:

- LangGraph docs on [common agent architectures](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)
- [Pre-built agents in LangGraph](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

## Legacy agent concept: AgentExecutor[â€‹](#legacy-agent-concept-agentexecutor "Direct link to Legacy agent concept: AgentExecutor")

LangChain previously introduced the `AgentExecutor` as a runtime for agents. While it served as an excellent starting point, its limitations became apparent when dealing with more sophisticated and customized agents. As a result, we're gradually phasing out `AgentExecutor` in favor of more flexible solutions in LangGraph.

### Transitioning from AgentExecutor to langgraph[â€‹](#transitioning-from-agentexecutor-to-langgraph "Direct link to Transitioning from AgentExecutor to langgraph")

If you're currently using `AgentExecutor`, don't worry! We've prepared resources to help you:

1. For those who still need to use `AgentExecutor`, we offer a comprehensive guide on [how to use AgentExecutor](/docs/how_to/agent_executor/).
2. However, we strongly recommend transitioning to LangGraph for improved flexibility and control. To facilitate this transition, we've created a detailed [migration guide](/docs/how_to/migrate_agent/) to help you move from `AgentExecutor` to LangGraph seamlessly.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/agents.mdx)

* * *


- [Legacy agent concept: AgentExecutor](#legacy-agent-concept-agentexecutor)
  
  - [Transitioning from AgentExecutor to langgraph](#transitioning-from-agentexecutor-to-langgraph)









# How to retry when a parsing error occurs

While in some cases it is possible to fix any parsing mistakes by only looking at the output, in other cases it isn't. An example of this is when the output is not just in the incorrect format, but is partially complete. Consider the below example.

```python
from langchain.output_parsers import OutputFixingParser
from langchain_core.exceptions import OutputParserException
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI, OpenAI
from pydantic import BaseModel, Field
```

**API Reference:**[OutputFixingParser](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.fix.OutputFixingParser.html) | [OutputParserException](https://python.langchain.com/api_reference/core/exceptions/langchain_core.exceptions.OutputParserException.html) | [PydanticOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

```python
template = """Based on the user question, provide an Action and Action Input for what step should be taken.
{format_instructions}
Question: {query}
Response:"""


class Action(BaseModel):
    action: str = Field(description="action to take")
    action_input: str = Field(description="input to the action")


parser = PydanticOutputParser(pydantic_object=Action)
```

```python
prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)
```

```python
prompt_value = prompt.format_prompt(query="who is leo di caprios gf?")
```

```python
bad_response = '{"action": "search"}'
```

If we try to parse this response as is, we will get an error:

```python
try:
    parser.parse(bad_response)
except OutputParserException as e:
    print(e)
```

```output
Failed to parse Action from completion {"action": "search"}. Got: 1 validation error for Action
action_input
  Field required [type=missing, input_value={'action': 'search'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE
```

If we try to use the `OutputFixingParser` to fix this error, it will be confused - namely, it doesn't know what to actually put for action input.

```python
fix_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())
```

```python
fix_parser.parse(bad_response)
```

```output
Action(action='search', action_input='input')
```

Instead, we can use the RetryOutputParser, which passes in the prompt (as well as the original output) to try again to get a better response.

```python
from langchain.output_parsers import RetryOutputParser
```

**API Reference:**[RetryOutputParser](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.retry.RetryOutputParser.html)

```python
retry_parser = RetryOutputParser.from_llm(parser=parser, llm=OpenAI(temperature=0))
```

```python
retry_parser.parse_with_prompt(bad_response, prompt_value)
```

```output
Action(action='search', action_input='leo di caprio girlfriend')
```

We can also add the RetryOutputParser easily with a custom chain which transform the raw LLM/ChatModel output into a more workable format.

```python
from langchain_core.runnables import RunnableLambda, RunnableParallel

completion_chain = prompt | OpenAI(temperature=0)

main_chain = RunnableParallel(
    completion=completion_chain, prompt_value=prompt
) | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))


main_chain.invoke({"query": "who is leo di caprios gf?"})
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)

```output
Action(action='search', action_input='leo di caprio girlfriend')
```

Find out api documentation for [RetryOutputParser](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.retry.RetryOutputParser.html#langchain.output_parsers.retry.RetryOutputParser).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/output_parser_retry.ipynb)

* * *










# How to select examples by length

This [example selector](/docs/concepts/example_selectors/) selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.

```python
from langchain_core.example_selectors import LengthBasedExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)
example_selector = LengthBasedExampleSelector(
    # The examples it has available to choose from.
    examples=examples,
    # The PromptTemplate being used to format the examples.
    example_prompt=example_prompt,
    # The maximum length that the formatted examples should be.
    # Length is measured by the get_text_length function below.
    max_length=25,
    # The function used to get the length of a string, which is used
    # to determine which examples to include. It is commented out because
    # it is provided as a default value if none is specified.
    # get_text_length: Callable[[str], int] = lambda x: len(re.split("\n| ", x))
)
dynamic_prompt = FewShotPromptTemplate(
    # We provide an ExampleSelector instead of examples.
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="Give the antonym of every input",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)
```

**API Reference:**[LengthBasedExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.length_based.LengthBasedExampleSelector.html) | [FewShotPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

```python
# An example with small input, so it selects all examples.
print(dynamic_prompt.format(adjective="big"))
```

```output
Give the antonym of every input

Input: happy
Output: sad

Input: tall
Output: short

Input: energetic
Output: lethargic

Input: sunny
Output: gloomy

Input: windy
Output: calm

Input: big
Output:
```

```python
# An example with long input, so it selects only one example.
long_string = "big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else"
print(dynamic_prompt.format(adjective=long_string))
```

```output
Give the antonym of every input

Input: happy
Output: sad

Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else
Output:
```

```python
# You can add an example to an example selector as well.
new_example = {"input": "big", "output": "small"}
dynamic_prompt.example_selector.add_example(new_example)
print(dynamic_prompt.format(adjective="enthusiastic"))
```

```output
Give the antonym of every input

Input: happy
Output: sad

Input: tall
Output: short

Input: energetic
Output: lethargic

Input: sunny
Output: gloomy

Input: windy
Output: calm

Input: big
Output: small

Input: enthusiastic
Output:
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/example_selectors_length_based.ipynb)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/tutorials/rag.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/rag.ipynb)

# Build a Retrieval Augmented Generation (RAG) App: Part 1

One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&amp;A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or [RAG](/docs/concepts/rag/).

This is a multi-part tutorial:

- [Part 1](/docs/tutorials/rag/) (this guide) introduces RAG and walks through a minimal implementation.
- [Part 2](/docs/tutorials/qa_chat_history/) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.

This tutorial will show how to build a simple Q&amp;A application over a text data source. Along the way weâ€™ll go over a typical Q&amp;A architecture and highlight additional resources for more advanced Q&amp;A techniques. Weâ€™ll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity.

If you're already familiar with basic retrieval, you might also be interested in this [high-level overview of different retrieval techniques](/docs/concepts/retrieval/).

**Note**: Here we focus on Q&amp;A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing [question/answering over SQL data](/docs/tutorials/sql_qa/).

## Overview[â€‹](#overview "Direct link to Overview")

A typical RAG application has two main components:

**Indexing**: a pipeline for ingesting data from a source and indexing it. *This usually happens offline.*

**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.

Note: the indexing portion of this tutorial will largely follow the [semantic search tutorial](/docs/tutorials/retrievers/).

The most common full sequence from raw data to answer looks like:

### Indexing[â€‹](#indexing "Direct link to Indexing")

1. **Load**: First we need to load our data. This is done with [Document Loaders](/docs/concepts/document_loaders/).
2. **Split**: [Text splitters](/docs/concepts/text_splitters/) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.
3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](/docs/concepts/vectorstores/) and [Embeddings](/docs/concepts/embedding_models/) model.

![index_diagram](/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)

### Retrieval and generation[â€‹](#retrieval-and-generation "Direct link to Retrieval and generation")

4. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](/docs/concepts/retrievers/).
5. **Generate**: A [ChatModel](/docs/concepts/chat_models/) / [LLM](/docs/concepts/text_llms/) produces an answer using a prompt that includes both the question with the retrieved data

![retrieval_diagram](/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)

Once we've indexed our data, we will use [LangGraph](https://langchain-ai.github.io/langgraph/) as our orchestration framework to implement the retrieval and generation steps.

## Setup[â€‹](#setup "Direct link to Setup")

### Jupyter Notebook[â€‹](#jupyter-notebook "Direct link to Jupyter Notebook")

This and other tutorials are perhaps most conveniently run in a [Jupyter notebooks](https://jupyter.org/). Going through guides in an interactive environment is a great way to better understand them. See [here](https://jupyter.org/install) for instructions on how to install.

### Installation[â€‹](#installation "Direct link to Installation")

This tutorial requires these langchain dependencies:

- Pip
- Conda

```python
%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph
```

```bash
conda install langchain-text-splitters langchain-community langgraph -c conda-forge
```

For more details, see our [Installation guide](/docs/how_to/installation/).

### LangSmith[â€‹](#langsmith "Direct link to LangSmith")

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
```

Or, if in a notebook, you can set them with:

```python
import getpass
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

## Components[â€‹](#components "Direct link to Components")

We will need to select three components from LangChain's suite of integrations.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

Select [embeddings model](/docs/integrations/text_embedding/):

OpenAIâ–¾

[OpenAI](#)

[Azure](#)

[Google](#)

[AWS](#)

[HuggingFace](#)

[Ollama](#)

[Cohere](#)

[MistralAI](#)

[Nomic](#)

[NVIDIA](#)

[Voyage AI](#)

[IBM watsonx](#)

[Fake](#)

```bash
pip install -qU langchain-openai
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
```

Select [vector store](/docs/integrations/vectorstores/):

In-memoryâ–¾

[In-memory](#)

[AstraDB](#)

[Chroma](#)

[FAISS](#)

[Milvus](#)

[MongoDB](#)

[PGVector](#)

[Pinecone](#)

[Qdrant](#)

```bash
pip install -qU langchain-core
```

```python
from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)
```

## Preview[â€‹](#preview "Direct link to Preview")

In this guide weâ€™ll build an app that answers questions about the website's content. The specific website we will use is the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng, which allows us to ask questions about the contents of the post.

We can create a simple indexing pipeline and RAG chain to do this in ~50 lines of code.

```python
import bs4
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict

# Load and chunk contents of the blog
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)

# Index chunks
_ = vector_store.add_documents(documents=all_splits)

# Define prompt for question-answering
prompt = hub.pull("rlm/rag-prompt")


# Define state for application
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str


# Define application steps
def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"])
    return {"context": retrieved_docs}


def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}


# Compile application and test
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```python
response = graph.invoke({"question": "What is Task Decomposition?"})
print(response["answer"])
```

```text
Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate easier execution and understanding. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think step-by-step, allowing them to explore multiple reasoning possibilities. This method enhances performance on complex tasks and provides insight into the model's thinking process.
```

Check out the [LangSmith trace](https://smith.langchain.com/public/65030797-7efa-4356-a7bd-b54b3dc70e17/r).

## Detailed walkthrough[â€‹](#detailed-walkthrough "Direct link to Detailed walkthrough")

Letâ€™s go through the above code step-by-step to really understand whatâ€™s going on.

## 1. Indexing[â€‹](#indexing "Direct link to 1. Indexing")

note

This section is an abbreviated version of the content in the [semantic search tutorial](/docs/tutorials/retrievers/). If you're comfortable with [document loaders](/docs/concepts/document_loaders/), [embeddings](/docs/concepts/embedding_models/), and [vector stores](/docs/concepts/vectorstores/), feel free to skip to the next section on [retrieval and generation](/docs/tutorials/rag/#orchestration).

### Loading documents[â€‹](#loading-documents "Direct link to Loading documents")

We need to first load the blog post contents. We can use [DocumentLoaders](/docs/concepts/document_loaders/) for this, which are objects that load in data from a source and return a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects.

In this case weâ€™ll use the [WebBaseLoader](/docs/integrations/document_loaders/web_base/), which uses `urllib` to load HTML from web URLs and `BeautifulSoup` to parse it to text. We can customize the HTML -&gt; text parsing by passing in parameters into the `BeautifulSoup` parser via `bs_kwargs` (see [BeautifulSoup docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)). In this case only HTML tags with class â€œpost-contentâ€, â€œpost-titleâ€, or â€œpost-headerâ€ are relevant, so weâ€™ll remove all others.

```python
import bs4
from langchain_community.document_loaders import WebBaseLoader

# Only keep post title, headers, and content from the full HTML.
bs4_strainer = bs4.SoupStrainer(class_=("post-title", "post-header", "post-content"))
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs={"parse_only": bs4_strainer},
)
docs = loader.load()

assert len(docs) == 1
print(f"Total characters: {len(docs[0].page_content)}")
```

**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)

```output
Total characters: 43131
```

```python
print(docs[0].page_content[:500])
```

```output


      LLM Powered Autonomous Agents
    
Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng


Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.
Agent System Overview#
In
```

#### Go deeper[â€‹](#go-deeper "Direct link to Go deeper")

`DocumentLoader`: Object that loads data from a source as list of `Documents`.

- [Docs](/docs/how_to/#document-loaders): Detailed documentation on how to use `DocumentLoaders`.
- [Integrations](/docs/integrations/document_loaders/): 160+ integrations to choose from.
- [Interface](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html): API reference for the base interface.

### Splitting documents[â€‹](#splitting-documents "Direct link to Splitting documents")

Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.

To handle this weâ€™ll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.

As in the [semantic search tutorial](/docs/tutorials/retrievers/), we use a [RecursiveCharacterTextSplitter](/docs/how_to/recursive_text_splitter/), which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # chunk size (characters)
    chunk_overlap=200,  # chunk overlap (characters)
    add_start_index=True,  # track index in original document
)
all_splits = text_splitter.split_documents(docs)

print(f"Split blog post into {len(all_splits)} sub-documents.")
```

**API Reference:**[RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

```output
Split blog post into 66 sub-documents.
```

#### Go deeper[â€‹](#go-deeper-1 "Direct link to Go deeper")

`TextSplitter`: Object that splits a list of `Document`s into smaller chunks. Subclass of `DocumentTransformer`s.

- Learn more about splitting text using different methods by reading the [how-to docs](/docs/how_to/#text-splitters)
- [Code (py or js)](/docs/integrations/document_loaders/source_code/)
- [Scientific papers](/docs/integrations/document_loaders/grobid/)
- [Interface](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TextSplitter.html): API reference for the base interface.

`DocumentTransformer`: Object that performs a transformation on a list of `Document` objects.

- [Docs](/docs/how_to/#text-splitters): Detailed documentation on how to use `DocumentTransformers`
- [Integrations](/docs/integrations/document_transformers/)
- [Interface](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.transformers.BaseDocumentTransformer.html): API reference for the base interface.

### Storing documents[â€‹](#storing-documents "Direct link to Storing documents")

Now we need to index our 66 text chunks so that we can search over them at runtime. Following the [semantic search tutorial](/docs/tutorials/retrievers/), our approach is to [embed](/docs/concepts/embedding_models/) the contents of each document split and insert these embeddings into a [vector store](/docs/concepts/vectorstores/). Given an input query, we can then use vector search to retrieve relevant documents.

We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the [start of the tutorial](/docs/tutorials/rag/#components).

```python
document_ids = vector_store.add_documents(documents=all_splits)

print(document_ids[:3])
```

```output
['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']
```

#### Go deeper[â€‹](#go-deeper-2 "Direct link to Go deeper")

`Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings.

- [Docs](/docs/how_to/embed_text/): Detailed documentation on how to use embeddings.
- [Integrations](/docs/integrations/text_embedding/): 30+ integrations to choose from.
- [Interface](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.Embeddings.html): API reference for the base interface.

`VectorStore`: Wrapper around a vector database, used for storing and querying embeddings.

- [Docs](/docs/how_to/vectorstores/): Detailed documentation on how to use vector stores.
- [Integrations](/docs/integrations/vectorstores/): 40+ integrations to choose from.
- [Interface](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html): API reference for the base interface.

This completes the **Indexing** portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.

## 2. Retrieval and Generation[â€‹](#orchestration "Direct link to 2. Retrieval and Generation")

Now letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.

For generation, we will use the chat model selected at the [start of the tutorial](/docs/tutorials/rag/#components).

Weâ€™ll use a prompt for RAG that is checked into the LangChain prompt hub ([here](https://smith.langchain.com/hub/rlm/rag-prompt)).

```python
from langchain import hub

prompt = hub.pull("rlm/rag-prompt")

example_messages = prompt.invoke(
    {"context": "(context goes here)", "question": "(question goes here)"}
).to_messages()

assert len(example_messages) == 1
print(example_messages[0].content)
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html)

```output
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: (question goes here) 
Context: (context goes here) 
Answer:
```

We'll use [LangGraph](https://langchain-ai.github.io/langgraph/) to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:

- We can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.
- We get streamlined deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/).
- LangSmith will automatically trace the steps of our application together.
- We can easily add key features to our application, including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) and [human-in-the-loop approval](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), with minimal code changes.

To use LangGraph, we need to define three things:

1. The state of our application;
2. The nodes of our application (i.e., application steps);
3. The "control flow" of our application (e.g., the ordering of the steps).

#### State:[â€‹](#state "Direct link to State:")

The [state](https://langchain-ai.github.io/langgraph/concepts/low_level/#state) of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a `TypedDict`, but can also be a [Pydantic BaseModel](https://langchain-ai.github.io/langgraph/how-tos/state-model/).

For a simple RAG application, we can just keep track of the input question, retrieved context, and generated answer:

```python
from langchain_core.documents import Document
from typing_extensions import List, TypedDict


class State(TypedDict):
    question: str
    context: List[Document]
    answer: str
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

#### Nodes (application steps)[â€‹](#nodes-application-steps "Direct link to Nodes (application steps)")

Let's start with a simple sequence of two steps: retrieval and generation.

```python
def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"])
    return {"context": retrieved_docs}


def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}
```

Our retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model.

#### Control flow[â€‹](#control-flow "Direct link to Control flow")

Finally, we compile our application into a single `graph` object. In this case, we are just connecting the retrieval and generation steps into a single sequence.

```python
from langgraph.graph import START, StateGraph

graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
```

**API Reference:**[StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

LangGraph also comes with built-in utilities for visualizing the control flow of your application:

```python
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

Do I need to use LangGraph?

LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:

```python
question = "..."

retrieved_docs = vector_store.similarity_search(question)
docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)
prompt = prompt.invoke({"question": question, "context": docs_content})
answer = llm.invoke(prompt)
```

The benefits of LangGraph include:

- Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;
- Automatic support for tracing via [LangSmith](https://docs.smith.langchain.com/) and deployments via [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/);
- Support for persistence, human-in-the-loop, and other features.

Many use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.

#### Usage[â€‹](#usage "Direct link to Usage")

Let's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.

Invoke:

```python
result = graph.invoke({"question": "What is Task Decomposition?"})

print(f'Context: {result["context"]}\n\n')
print(f'Answer: {result["answer"]}')
```

```output
Context: [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content="(3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.")]


Answer: Task decomposition is a technique used to break down complex tasks into smaller, manageable steps, allowing for more efficient problem-solving. This can be achieved through methods like chain of thought prompting or the tree of thoughts approach, which explores multiple reasoning possibilities at each step. It can be initiated through simple prompts, task-specific instructions, or human inputs.
```

Stream steps:

```python
for step in graph.stream(
    {"question": "What is Task Decomposition?"}, stream_mode="updates"
):
    print(f"{step}\n\n----------------\n")
```

```output
{'retrieve': {'context': [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content="(3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.")]}}

----------------

{'generate': {'answer': 'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This technique, often enhanced by methods like Chain of Thought (CoT) or Tree of Thoughts, allows models to reason through tasks systematically and improves performance by clarifying the thought process. It can be achieved through simple prompts, task-specific instructions, or human inputs.'}}

----------------
```

Stream [tokens](/docs/concepts/tokens/):

```python
for message, metadata in graph.stream(
    {"question": "What is Task Decomposition?"}, stream_mode="messages"
):
    print(message.content, end="|")
```

```output
|Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| can| be| achieved| through| techniques| like| Chain| of| Thought| (|Co|T|)| prompting|,| which| encourages| the| model| to| think| step| by| step|,| or| through| more| structured| methods| like| the| Tree| of| Thoughts|.| This| approach| not| only| simplifies| task| execution| but| also| provides| insights| into| the| model|'s| reasoning| process|.||
```

tip

For async invocations, use:

```python
result = await graph.ainvoke(...)
```

and

```python
async for step in graph.astream(...):
```

#### Returning sources[â€‹](#returning-sources "Direct link to Returning sources")

Note that by storing the retrieved context in the state of the graph, we recover sources for the model's generated answer in the `"context"` field of the state. See [this guide](/docs/how_to/qa_sources/) on returning sources for more detail.

#### Go deeper[â€‹](#go-deeper-3 "Direct link to Go deeper")

[Chat models](/docs/concepts/chat_models/) take in a sequence of messages and return a message.

- [Docs](/docs/how_to/#chat-models)
- [Integrations](/docs/integrations/chat/): 25+ integrations to choose from.
- [Interface](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html): API reference for the base interface.

**Customizing the prompt**

As shown above, we can load prompts (e.g., [this RAG prompt](https://smith.langchain.com/hub/rlm/rag-prompt)) from the prompt hub. The prompt can also be easily customized. For example:

```python
from langchain_core.prompts import PromptTemplate

template = """Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Use three sentences maximum and keep the answer as concise as possible.
Always say "thanks for asking!" at the end of the answer.

{context}

Question: {question}

Helpful Answer:"""
custom_rag_prompt = PromptTemplate.from_template(template)
```

**API Reference:**[PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

## Query analysis[â€‹](#query-analysis "Direct link to Query analysis")

So far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:

- In addition to semantic search, we can build in structured filters (e.g., "Find documents since the year 2020.");
- The model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries.

[Query analysis](/docs/concepts/retrieval/#query-analysis) employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let's add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later.

```python
total_documents = len(all_splits)
third = total_documents // 3

for i, document in enumerate(all_splits):
    if i < third:
        document.metadata["section"] = "beginning"
    elif i < 2 * third:
        document.metadata["section"] = "middle"
    else:
        document.metadata["section"] = "end"


all_splits[0].metadata
```

```output
{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',
 'start_index': 8,
 'section': 'beginning'}
```

We will need to update the documents in our vector store. We will use a simple [InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) for this, as we will use some of its specific features (i.e., metadata filtering). Refer to the vector store [integration documentation](/docs/integrations/vectorstores/) for relevant features of your chosen vector store.

```python
from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)
_ = vector_store.add_documents(all_splits)
```

**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html)

Let's next define a schema for our search query. We will use [structured output](/docs/concepts/structured_outputs/) for this purpose. Here we define a query as containing a string query and a document section (either "beginning", "middle", or "end"), but this can be defined however you like.

```python
from typing import Literal

from typing_extensions import Annotated


class Search(TypedDict):
    """Search query."""

    query: Annotated[str, ..., "Search query to run."]
    section: Annotated[
        Literal["beginning", "middle", "end"],
        ...,
        "Section to query.",
    ]
```

Finally, we add a step to our LangGraph application to generate a query from the user's raw input:

```python
class State(TypedDict):
    question: str
    query: Search
    context: List[Document]
    answer: str


def analyze_query(state: State):
    structured_llm = llm.with_structured_output(Search)
    query = structured_llm.invoke(state["question"])
    return {"query": query}


def retrieve(state: State):
    query = state["query"]
    retrieved_docs = vector_store.similarity_search(
        query["query"],
        filter=lambda doc: doc.metadata.get("section") == query["section"],
    )
    return {"context": retrieved_docs}


def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}


graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])
graph_builder.add_edge(START, "analyze_query")
graph = graph_builder.compile()
```

Full Code:

```python
from typing import Literal

import bs4
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.documents import Document
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langgraph.graph import START, StateGraph
from typing_extensions import Annotated, List, TypedDict

# Load and chunk contents of the blog
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)


# Update metadata (illustration purposes)
total_documents = len(all_splits)
third = total_documents // 3

for i, document in enumerate(all_splits):
    if i < third:
        document.metadata["section"] = "beginning"
    elif i < 2 * third:
        document.metadata["section"] = "middle"
    else:
        document.metadata["section"] = "end"


# Index chunks
vector_store = InMemoryVectorStore(embeddings)
_ = vector_store.add_documents(all_splits)


# Define schema for search
class Search(TypedDict):
    """Search query."""

    query: Annotated[str, ..., "Search query to run."]
    section: Annotated[
        Literal["beginning", "middle", "end"],
        ...,
        "Section to query.",
    ]

# Define prompt for question-answering
prompt = hub.pull("rlm/rag-prompt")


# Define state for application
class State(TypedDict):
    question: str
    query: Search
    context: List[Document]
    answer: str


def analyze_query(state: State):
    structured_llm = llm.with_structured_output(Search)
    query = structured_llm.invoke(state["question"])
    return {"query": query}


def retrieve(state: State):
    query = state["query"]
    retrieved_docs = vector_store.similarity_search(
        query["query"],
        filter=lambda doc: doc.metadata.get("section") == query["section"],
    )
    return {"context": retrieved_docs}


def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}


graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])
graph_builder.add_edge(START, "analyze_query")
graph = graph_builder.compile()
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```python
display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

We can test our implementation by specifically asking for context from the end of the post. Note that the model includes different information in its answer.

```python
for step in graph.stream(
    {"question": "What does the end of the post say about Task Decomposition?"},
    stream_mode="updates",
):
    print(f"{step}\n\n----------------\n")
```

```output
{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'end'}}}

----------------

{'retrieve': {'context': [Document(id='d6cef137-e1e8-4ddc-91dc-b62bd33c6020', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39221, 'section': 'end'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n\n\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'), Document(id='d1834ae1-eb6a-43d7-a023-08dfa5028799', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39086, 'section': 'end'}, page_content='}\n]\nChallenges#\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(id='ca7f06e4-2c2e-4788-9a81-2418d82213d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 32942, 'section': 'end'}, page_content='}\n]\nThen after these clarification, the agent moved into the code writing mode with a different system message.\nSystem message:'), Document(id='1fcc2736-30f4-4ef6-90f2-c64af92118cb', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 35127, 'section': 'end'}, page_content='"content": "You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code\'s language, and CODE is the code:\\n\\nFILENAME\\n\`\`\`LANG\\nCODE\\n\`\`\`\\n\\nYou will start with the \\"entrypoint\\" file, then go to the ones that are imported by that file, and so on.\\nPlease')]}}

----------------

{'generate': {'answer': 'The end of the post highlights that task decomposition faces challenges in long-term planning and adapting to unexpected errors. LLMs struggle with adjusting their plans, making them less robust compared to humans who learn from trial and error. This indicates a limitation in effectively exploring the solution space and handling complex tasks.'}}

----------------
```

In both the streamed steps and the [LangSmith trace](https://smith.langchain.com/public/bdbaae61-130c-4338-8b59-9315dfee22a0/r), we can now observe the structured query that was fed into the retrieval step.

Query Analysis is a rich problem with a wide range of approaches. Refer to the [how-to guides](/docs/how_to/#query-analysis) for more examples.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

We've covered the steps to build a basic Q&amp;A app over data:

- Loading data with a [Document Loader](/docs/concepts/document_loaders/)
- Chunking the indexed data with a [Text Splitter](/docs/concepts/text_splitters/) to make it more easily usable by a model
- [Embedding the data](/docs/concepts/embedding_models/) and storing the data in a [vectorstore](/docs/how_to/vectorstores/)
- [Retrieving](/docs/concepts/retrievers/) the previously stored chunks in response to incoming questions
- Generating an answer using the retrieved chunks as context.

In [Part 2](/docs/tutorials/qa_chat_history/) of the tutorial, we will extend the implementation here to accommodate conversation-style interactions and multi-step retrieval processes.

Further reading:

- [Return sources](/docs/how_to/qa_sources/): Learn how to return source documents
- [Streaming](/docs/how_to/streaming/): Learn how to stream outputs and intermediate steps
- [Add chat history](/docs/how_to/message_history/): Learn how to add chat history to your app
- [Retrieval conceptual guide](/docs/concepts/retrieval/): A high-level overview of specific retrieval techniques

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/tutorials/rag.ipynb)

* * *


- [Overview](#overview)
  
  - [Indexing](#indexing)
  - [Retrieval and generation](#retrieval-and-generation)
- [Setup](#setup)
  
  - [Jupyter Notebook](#jupyter-notebook)
  - [Installation](#installation)
  - [LangSmith](#langsmith)
- [Components](#components)
- [Preview](#preview)
- [Detailed walkthrough](#detailed-walkthrough)
- [1. Indexing](#indexing)
  
  - [Loading documents](#loading-documents)
  - [Splitting documents](#splitting-documents)
  - [Storing documents](#storing-documents)
- [2. Retrieval and Generation](#orchestration)
- [Query analysis](#query-analysis)
- [Next steps](#next-steps)








[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_memory/conversation_summary_memory.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_memory/conversation_summary_memory.ipynb)

# Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory

Follow this guide if you're trying to migrate off one of the old memory classes listed below:

| Memory Type                       | Description                                                                                                                                                                                                         |
|-----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `ConversationSummaryMemory`       | Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history.                                              |
| `ConversationSummaryBufferMemory` | Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit. |

Please follow the following [how-to guide on summarization](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/) in LangGraph.

This guide shows how to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_memory/conversation_summary_memory.ipynb)

* * *









- [LangSmith](https://docs.smith.langchain.com)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [LangChain Hub](https://smith.langchain.com/hub)
- [LangChain JS/TS](https://js.langchain.com)

[v0.3](#)

- [v0.3](/docs/introduction/)
- [v0.2](https://python.langchain.com/v0.2/docs/introduction)
- [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)

[ðŸ’¬](https://chat.langchain.com)

Search

- [Welcome Contributors](/docs/contributing/)
- [Tutorials](/docs/contributing/tutorials/)
  
  - [Make your first docs PR](/docs/contributing/tutorials/docs/)
  - [Tutorials](/docs/contributing/tutorials/)
- [How-to guides](/docs/contributing/how_to/)
  
  - [Testing](/docs/contributing/how_to/testing/)
  - [Contribute Code](/docs/contributing/how_to/code/)
  - [Contribute Documentation](/docs/contributing/how_to/documentation/)
  - [How-to Guides](/docs/contributing/how_to/)
  - [Contribute Integrations](/docs/contributing/how_to/integrations/)
- [Reference &amp; FAQ](/docs/contributing/reference/)
  
  - [Repository Structure](/docs/contributing/reference/repo_structure/)
  - [FAQ](/docs/contributing/reference/faq/)
  - [Reference](/docs/contributing/reference/)
  - [Review Process](/docs/contributing/reference/review_process/)

<!--THE END-->

- Welcome Contributors


[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/contributing/index.mdx)

# Welcome Contributors

Hi there! Thank you for your interest in contributing to LangChain. As an open-source project in a fast developing field, we are extremely open to contributions, whether they involve new features, improved infrastructure, better documentation, or bug fixes.

## Tutorials[â€‹](#tutorials "Direct link to Tutorials")

More coming soon! We are working on tutorials to help you make your first contribution to the project.

- [**Make your first docs PR**](/docs/contributing/tutorials/docs/)

## How-to Guides[â€‹](#how-to-guides "Direct link to How-to Guides")

- [**Documentation**](/docs/contributing/how_to/documentation/): Help improve our docs, including this one!
- [**Code**](/docs/contributing/how_to/code/): Help us write code, fix bugs, or improve our infrastructure.
- [**Integrations**](/docs/contributing/how_to/integrations/): Help us integrate with your favorite vendors and tools.
- [**Standard Tests**](/docs/contributing/how_to/integrations/standard_tests/): Ensure your integration passes an expected set of tests.

## Reference[â€‹](#reference "Direct link to Reference")

- [**Repository Structure**](/docs/contributing/reference/repo_structure/): Understand the high level structure of the repository.
- [**Review Process**](/docs/contributing/reference/review_process/): Learn about the review process for pull requests.
- [**Frequently Asked Questions (FAQ)**](/docs/contributing/reference/faq/): Get answers to common questions about contributing.

## Community[â€‹](#community "Direct link to Community")

### ðŸ’­ GitHub Discussions[â€‹](#-github-discussions "Direct link to ðŸ’­ GitHub Discussions")

We have a [discussions](https://github.com/langchain-ai/langchain/discussions) page where users can ask usage questions, discuss design decisions, and propose new features.

If you are able to help answer questions, please do so! This will allow the maintainers to spend more time focused on development and bug fixing.

### ðŸš© GitHub Issues[â€‹](#-github-issues "Direct link to ðŸš© GitHub Issues")

Our [issues](https://github.com/langchain-ai/langchain/issues) page is kept up to date with bugs, improvements, and feature requests.

There is a [taxonomy of labels](https://github.com/langchain-ai/langchain/labels?sort=count-desc) to help with sorting and discovery of issues of interest. Please use these to help organize issues. Check out the [Help Wanted](https://github.com/langchain-ai/langchain/labels/help%20wanted) and [Good First Issue](https://github.com/langchain-ai/langchain/labels/good%20first%20issue) tags for recommendations.

If you start working on an issue, please assign it to yourself.

If you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature. If two issues are related, or blocking, please link them rather than combining them.

We will try to keep these issues as up-to-date as possible, though with the rapid rate of development in this field some may get out of date. If you notice this happening, please let us know.

### ðŸ“¢ Community Slack[â€‹](#-community-slack "Direct link to ðŸ“¢ Community Slack")

We have a [community slack](https://www.langchain.com/join-community) where you can ask questions, get help, and discuss the project with other contributors and users.

### ðŸ™‹ Getting Help[â€‹](#-getting-help "Direct link to ðŸ™‹ Getting Help")

Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please ask in [community slack](https://www.langchain.com/join-community) or open a [discussion on GitHub](https://github.com/langchain-ai/langchain/discussions).

In a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase. If you are finding these difficult (or even just annoying) to work with, feel free to ask in [community slack](https://www.langchain.com/join-community)!

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/contributing/index.mdx)

* * *

- [Community](#community)
  
  - [ðŸ’­ GitHub Discussions](#-github-discussions)
  - [ðŸš© GitHub Issues](#-github-issues)
  - [ðŸ“¢ Community Slack](#-community-slack)
  - [ðŸ™‹ Getting Help](#-getting-help)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/installation.mdx)

# How to install LangChain packages

The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of functionality to install.

## Official release[â€‹](#official-release "Direct link to Official release")

To install the main `langchain` package, run:

- Pip
- Conda

```bash
pip install langchain
```

```bash
conda install langchain -c conda-forge
```

While this package acts as a sane starting point to using LangChain, much of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.

## Ecosystem packages[â€‹](#ecosystem-packages "Direct link to Ecosystem packages")

With the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base classes and abstractions that other packages use. The dependency graph below shows how the different packages are related. A directed arrow indicates that the source package depends on the target package:

![](/assets/images/ecosystem_packages-32943b32657e7a187770c9b585f22a64.png)

When installing a package, you do not need to explicitly install that package's explicit dependencies (such as `langchain-core`). However, you may choose to if you are using a feature only available in a certain version of that dependency. If you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.

### LangChain core[â€‹](#langchain-core "Direct link to LangChain core")

The `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by `langchain`, but can also be used separately. Install with:

```bash
pip install langchain-core
```

### Integration packages[â€‹](#integration-packages "Direct link to Integration packages")

Certain integrations like OpenAI and Anthropic have their own packages. Any integrations that require their own package will be documented as such in the [Integration docs](/docs/integrations/providers/). You can see a list of all integration packages in the [API reference](https://python.langchain.com/api_reference/) under the "Partner libs" dropdown. To install one of these run:

```bash
pip install langchain-openai
```

Any integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:

```bash
pip install langchain-community
```

### LangChain experimental[â€‹](#langchain-experimental "Direct link to LangChain experimental")

The `langchain-experimental` package holds experimental LangChain code, intended for research and experimental uses. Install with:

```bash
pip install langchain-experimental
```

### LangGraph[â€‹](#langgraph "Direct link to LangGraph")

`langgraph` is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it. Install with:

```bash
pip install langgraph
```

### LangServe[â€‹](#langserve "Direct link to LangServe")

LangServe helps developers deploy LangChain runnables and chains as a REST API. LangServe is automatically installed by LangChain CLI. If not using LangChain CLI, install with:

```bash
pip install "langserve[all]"
```

for both client and server dependencies. Or `pip install "langserve[client]"` for client code, and `pip install "langserve[server]"` for server code.

### LangChain CLI[â€‹](#langchain-cli "Direct link to LangChain CLI")

The LangChain CLI is useful for working with LangChain templates and other LangServe projects. Install with:

```bash
pip install langchain-cli
```

### LangSmith SDK[â€‹](#langsmith-sdk "Direct link to LangSmith SDK")

The LangSmith SDK is automatically installed by LangChain. However, it does not depend on `langchain-core`, and can be installed and used independently if desired. If you are not using LangChain, you can install it with:

```bash
pip install langsmith
```

### From source[â€‹](#from-source "Direct link to From source")

If you want to install a package from source, you can do so by cloning the [main LangChain repo](https://github.com/langchain-ai/langchain), enter the directory of the package you want to install `PATH/TO/REPO/langchain/libs/{package}`, and run:

```bash
pip install -e .
```

LangGraph, LangSmith SDK, and certain integration packages live outside the main LangChain repo. You can see [all repos here](https://github.com/langchain-ai).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/installation.mdx)

* * *


- [Official release](#official-release)
- [Ecosystem packages](#ecosystem-packages)
  
  - [LangChain core](#langchain-core)
  - [Integration packages](#integration-packages)
  - [LangChain experimental](#langchain-experimental)
  - [LangGraph](#langgraph)
  - [LangServe](#langserve)
  - [LangChain CLI](#langchain-cli)
  - [LangSmith SDK](#langsmith-sdk)
  - [From source](#from-source)









# How to select examples by maximal marginal relevance (MMR)

The `MaxMarginalRelevanceExampleSelector` selects [examples](/docs/concepts/example_selectors/) based on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples.

```python
from langchain_community.vectorstores import FAISS
from langchain_core.example_selectors import (
    MaxMarginalRelevanceExampleSelector,
    SemanticSimilarityExampleSelector,
)
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain_openai import OpenAIEmbeddings

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]
```

**API Reference:**[FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [MaxMarginalRelevanceExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.semantic_similarity.MaxMarginalRelevanceExampleSelector.html) | [SemanticSimilarityExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector.html) | [FewShotPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```python
example_selector = MaxMarginalRelevanceExampleSelector.from_examples(
    # The list of examples available to select from.
    examples,
    # The embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # The VectorStore class that is used to store the embeddings and do a similarity search over.
    FAISS,
    # The number of examples to produce.
    k=2,
)
mmr_prompt = FewShotPromptTemplate(
    # We provide an ExampleSelector instead of examples.
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="Give the antonym of every input",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)
```

```python
# Input is a feeling, so should select the happy/sad example as the first one
print(mmr_prompt.format(adjective="worried"))
```

```output
Give the antonym of every input

Input: happy
Output: sad

Input: windy
Output: calm

Input: worried
Output:
```

```python
# Let's compare this to what we would just get if we went solely off of similarity,
# by using SemanticSimilarityExampleSelector instead of MaxMarginalRelevanceExampleSelector.
example_selector = SemanticSimilarityExampleSelector.from_examples(
    # The list of examples available to select from.
    examples,
    # The embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # The VectorStore class that is used to store the embeddings and do a similarity search over.
    FAISS,
    # The number of examples to produce.
    k=2,
)
similar_prompt = FewShotPromptTemplate(
    # We provide an ExampleSelector instead of examples.
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="Give the antonym of every input",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)
print(similar_prompt.format(adjective="worried"))
```

```output
Give the antonym of every input

Input: happy
Output: sad

Input: sunny
Output: gloomy

Input: worried
Output:
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/example_selectors_mmr.ipynb)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/parent_document_retriever.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/parent_document_retriever.ipynb)

# How to use the Parent Document Retriever

When splitting documents for [retrieval](/docs/concepts/retrieval/), there are often conflicting desires:

1. You may want to have small documents, so that their embeddings can most accurately reflect their meaning. If too long, then the embeddings can lose meaning.
2. You want to have long enough documents that the context of each chunk is retained.

The `ParentDocumentRetriever` strikes that balance by splitting and storing small chunks of data. During retrieval, it first fetches the small chunks but then looks up the parent ids for those chunks and returns those larger documents.

Note that "parent document" refers to the document that a small chunk originated from. This can either be the whole raw document OR a larger chunk.

```python
from langchain.retrievers import ParentDocumentRetriever
```

**API Reference:**[ParentDocumentRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html)

```python
from langchain.storage import InMemoryStore
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
```

**API Reference:**[InMemoryStore](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.InMemoryStore.html) | [TextLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

```python
loaders = [
    TextLoader("paul_graham_essay.txt"),
    TextLoader("state_of_the_union.txt"),
]
docs = []
for loader in loaders:
    docs.extend(loader.load())
```

## Retrieving full documents[â€‹](#retrieving-full-documents "Direct link to Retrieving full documents")

In this mode, we want to retrieve the full documents. Therefore, we only specify a child [splitter](/docs/concepts/text_splitters/).

```python
# This text splitter is used to create the child documents
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
# The vectorstore to use to index the child chunks
vectorstore = Chroma(
    collection_name="full_documents", embedding_function=OpenAIEmbeddings()
)
# The storage layer for the parent documents
store = InMemoryStore()
retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
)
```

```python
retriever.add_documents(docs, ids=None)
```

This should yield two keys, because we added two documents.

```python
list(store.yield_keys())
```

```output
['9a63376c-58cc-42c9-b0f7-61f0e1a3a688',
 '40091598-e918-4a18-9be0-f46413a95ae4']
```

Let's now call the vector store search functionality - we should see that it returns small chunks (since we're storing the small chunks).

```python
sub_docs = vectorstore.similarity_search("justice breyer")
```

```python
print(sub_docs[0].page_content)
```

```output
Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.
```

Let's now retrieve from the overall retriever. This should return large documents - since it returns the documents where the smaller chunks are located.

```python
retrieved_docs = retriever.invoke("justice breyer")
```

```python
len(retrieved_docs[0].page_content)
```

```output
38540
```

## Retrieving larger chunks[â€‹](#retrieving-larger-chunks "Direct link to Retrieving larger chunks")

Sometimes, the full documents can be too big to want to retrieve them as is. In that case, what we really want to do is to first split the raw documents into larger chunks, and then split it into smaller chunks. We then index the smaller chunks, but on retrieval we retrieve the larger chunks (but still not the full documents).

```python
# This text splitter is used to create the parent documents
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)
# This text splitter is used to create the child documents
# It should create documents smaller than the parent
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)
# The vectorstore to use to index the child chunks
vectorstore = Chroma(
    collection_name="split_parents", embedding_function=OpenAIEmbeddings()
)
# The storage layer for the parent documents
store = InMemoryStore()
```

```python
retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter,
)
```

```python
retriever.add_documents(docs)
```

We can see that there are much more than two documents now - these are the larger chunks.

```python
len(list(store.yield_keys()))
```

```output
66
```

Let's make sure the underlying vector store still retrieves the small chunks.

```python
sub_docs = vectorstore.similarity_search("justice breyer")
```

```python
print(sub_docs[0].page_content)
```

```output
Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.
```

```python
retrieved_docs = retriever.invoke("justice breyer")
```

```python
len(retrieved_docs[0].page_content)
```

```output
1849
```

```python
print(retrieved_docs[0].page_content)
```

```output
In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. 

We cannot let this happen. 

Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. 

Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence. 

A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since sheâ€™s been nominated, sheâ€™s received a broad range of supportâ€”from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. 

And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. 

We can do both. At our border, weâ€™ve installed new technology like cutting-edge scanners to better detect drug smuggling.  

Weâ€™ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  

Weâ€™re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. 

Weâ€™re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/parent_document_retriever.ipynb)

* * *


- [Retrieving full documents](#retrieving-full-documents)
- [Retrieving larger chunks](#retrieving-larger-chunks)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/example_selectors.mdx)

# Example selectors

Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)

## Overview[â€‹](#overview "Direct link to Overview")

One common prompting technique for achieving better performance is to include examples as part of the prompt. This is known as [few-shot prompting](/docs/concepts/few_shot_prompting/).

This gives the [language model](/docs/concepts/chat_models/) concrete examples of how it should behave. Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.

**Example Selectors** are classes responsible for selecting and then formatting examples into prompts.

## Related resources[â€‹](#related-resources "Direct link to Related resources")

- [Example selector how-to guides](/docs/how_to/#example-selectors)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/example_selectors.mdx)

* * *


- [Overview](#overview)
- [Related resources](#related-resources)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/tool_stream_events.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/tool_stream_events.ipynb)

# How to stream events from a tool

Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Tools](/docs/concepts/tools/)
- [Custom tools](/docs/how_to/custom_tools/)
- [Using stream events](/docs/how_to/streaming/#using-stream-events)
- [Accessing RunnableConfig within a custom tool](/docs/how_to/tool_configure/)

If you have [tools](/docs/concepts/tools/) that call [chat models](/docs/concepts/chat_models/), [retrievers](/docs/concepts/retrievers/), or other [runnables](/docs/concepts/runnables/), you may want to access internal events from those runnables or configure them with additional properties. This guide shows you how to manually pass parameters properly so that you can do this using the `astream_events()` method.

Compatibility

LangChain cannot automatically propagate configuration, including callbacks necessary for `astream_events()`, to child runnables if you are running `async` code in `python&lt;=3.10`. This is a common reason why you may fail to see events being emitted from custom runnables or tools.

If you are running python&lt;=3.10, you will need to manually propagate the `RunnableConfig` object to the child runnable in async environments. For an example of how to manually propagate the config, see the implementation of the `bar` RunnableLambda below.

If you are running python&gt;=3.11, the `RunnableConfig` will automatically propagate to child runnables in async environment. However, it is still a good idea to propagate the `RunnableConfig` manually if your code may run in older Python versions.

This guide also requires `langchain-core>=0.2.16`.

Say you have a custom tool that calls a chain that condenses its input by prompting a chat model to return only 10 words, then reversing the output. First, define it in a naive way:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool


@tool
async def special_summarization_tool(long_text: str) -> str:
    """A tool that summarizes input text using advanced techniques."""
    prompt = ChatPromptTemplate.from_template(
        "You are an expert writer. Summarize the following text in 10 words or less:\n\n{long_text}"
    )

    def reverse(x: str):
        return x[::-1]

    chain = prompt | model | StrOutputParser() | reverse
    summary = await chain.ainvoke({"long_text": long_text})
    return summary
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

Invoking the tool directly works just fine:

```python
LONG_TEXT = """
NARRATOR:
(Black screen with text; The sound of buzzing bees can be heard)
According to all known laws of aviation, there is no way a bee should be able to fly. Its wings are too small to get its fat little body off the ground. The bee, of course, flies anyway because bees don't care what humans think is impossible.
BARRY BENSON:
(Barry is picking out a shirt)
Yellow, black. Yellow, black. Yellow, black. Yellow, black. Ooh, black and yellow! Let's shake it up a little.
JANET BENSON:
Barry! Breakfast is ready!
BARRY:
Coming! Hang on a second.
"""

await special_summarization_tool.ainvoke({"long_text": LONG_TEXT})
```

```output
'.yad noitaudarg rof tiftuo sesoohc yrraB ;scisyhp seifed eeB'
```

But if you wanted to access the raw output from the chat model rather than the full tool, you might try to use the [`astream_events()`](/docs/how_to/streaming/#using-stream-events) method and look for an `on_chat_model_end` event. Here's what happens:

```python
stream = special_summarization_tool.astream_events({"long_text": LONG_TEXT})

async for event in stream:
    if event["event"] == "on_chat_model_end":
        # Never triggers in python<=3.10!
        print(event)
```

You'll notice (unless you're running through this guide in `python>=3.11`) that there are no chat model events emitted from the child run!

This is because the example above does not pass the tool's config object into the internal chain. To fix this, redefine your tool to take a special parameter typed as `RunnableConfig` (see [this guide](/docs/how_to/tool_configure/) for more details). You'll also need to pass that parameter through into the internal chain when executing it:

```python
from langchain_core.runnables import RunnableConfig


@tool
async def special_summarization_tool_with_config(
    long_text: str, config: RunnableConfig
) -> str:
    """A tool that summarizes input text using advanced techniques."""
    prompt = ChatPromptTemplate.from_template(
        "You are an expert writer. Summarize the following text in 10 words or less:\n\n{long_text}"
    )

    def reverse(x: str):
        return x[::-1]

    chain = prompt | model | StrOutputParser() | reverse
    # Pass the "config" object as an argument to any executed runnables
    summary = await chain.ainvoke({"long_text": long_text}, config=config)
    return summary
```

**API Reference:**[RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html)

And now try the same `astream_events()` call as before with your new tool:

```python
stream = special_summarization_tool_with_config.astream_events({"long_text": LONG_TEXT})

async for event in stream:
    if event["event"] == "on_chat_model_end":
        print(event)
```

```output
{'event': 'on_chat_model_end', 'data': {'output': AIMessage(content='Bee defies physics; Barry chooses outfit for graduation day.', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-337ac14e-8da8-4c6d-a69f-1573f93b651e', usage_metadata={'input_tokens': 182, 'output_tokens': 19, 'total_tokens': 201, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}), 'input': {'messages': [[HumanMessage(content="You are an expert writer. Summarize the following text in 10 words or less:\n\n\nNARRATOR:\n(Black screen with text; The sound of buzzing bees can be heard)\nAccording to all known laws of aviation, there is no way a bee should be able to fly. Its wings are too small to get its fat little body off the ground. The bee, of course, flies anyway because bees don't care what humans think is impossible.\nBARRY BENSON:\n(Barry is picking out a shirt)\nYellow, black. Yellow, black. Yellow, black. Yellow, black. Ooh, black and yellow! Let's shake it up a little.\nJANET BENSON:\nBarry! Breakfast is ready!\nBARRY:\nComing! Hang on a second.\n", additional_kwargs={}, response_metadata={})]]}}, 'run_id': '337ac14e-8da8-4c6d-a69f-1573f93b651e', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['225beaa6-af73-4c91-b2d3-1afbbb88d53e']}
```

Awesome! This time there's an event emitted.

For streaming, `astream_events()` automatically calls internal runnables in a chain with streaming enabled if possible, so if you wanted to a stream of tokens as they are generated from the chat model, you could simply filter to look for `on_chat_model_stream` events with no other changes:

```python
stream = special_summarization_tool_with_config.astream_events({"long_text": LONG_TEXT})

async for event in stream:
    if event["event"] == "on_chat_model_stream":
        print(event)
```

```output
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', usage_metadata={'input_tokens': 182, 'output_tokens': 2, 'total_tokens': 184, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='Bee', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' defies physics;', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' Barry chooses outfit for', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' graduation day.', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', usage_metadata={'input_tokens': 0, 'output_tokens': 17, 'total_tokens': 17, 'input_token_details': {}})}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now seen how to stream events from within a tool. Next, check out the following guides for more on using tools:

- Pass [runtime values to tools](/docs/how_to/tool_runtime/)
- Pass [tool results back to a model](/docs/how_to/tool_results_pass_to_model/)
- [Dispatch custom callback events](/docs/how_to/callbacks_custom_events/)

You can also check out some more specific uses of tool calling:

- Building [tool-using chains and agents](/docs/how_to/#tools)
- Getting [structured outputs](/docs/how_to/structured_output/) from models

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tool_stream_events.ipynb)

* * *


- [Next steps](#next-steps)









# Output parsers

note

The information here refers to parsers that take a text output from a model try to parse it into a more structured representation. More and more models are supporting function (or tool) calling, which handles this automatically. It is recommended to use function/tool calling rather than output parsing. See documentation for that [here](/docs/concepts/tool_calling/).

`Output parser` is responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks. Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.

LangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:

- **Name**: The name of the output parser
- **Supports Streaming**: Whether the output parser supports streaming.
- **Has Format Instructions**: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.
- **Calls LLM**: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.
- **Input Type**: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.
- **Output Type**: The output type of the object returned by the parser.
- **Description**: Our commentary on this output parser and when to use it.

| Name                                                                                                                                                                                                                                    | Supports Streaming | Has Format Instructions | Calls LLM | Input Type         | Output Type          | Description                                                                                                                                                                                                                                              |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|-------------------------|-----------|--------------------|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Str](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html)                                                                                                         | âœ…                  |                         |           | `str` \| `Message` | String               | Parses texts from message objects. Useful for handling variable formats of message content (e.g., extracting text from content blocks).                                                                                                                  |
| [JSON](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html)                                                                                                         | âœ…                  | âœ…                       |           | `str` \| `Message` | JSON object          | Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.                                    |
| [XML](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.xml.XMLOutputParser.html#langchain_core.output_parsers.xml.XMLOutputParser)                                                          | âœ…                  | âœ…                       |           | `str` \| `Message` | `dict`               | Returns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).                                                                                                                            |
| [CSV](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html#langchain_core.output_parsers.list.CommaSeparatedListOutputParser)                          | âœ…                  | âœ…                       |           | `str` \| `Message` | `List[str]`          | Returns a list of comma separated values.                                                                                                                                                                                                                |
| [OutputFixing](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.fix.OutputFixingParser.html#langchain.output_parsers.fix.OutputFixingParser)                                                |                    |                         | âœ…         | `str` \| `Message` |                      | Wraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.                                                                                              |
| [RetryWithError](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.retry.RetryWithErrorOutputParser.html#langchain.output_parsers.retry.RetryWithErrorOutputParser)                          |                    |                         | âœ…         | `str` \| `Message` |                      | Wraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions. |
| [Pydantic](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html#langchain_core.output_parsers.pydantic.PydanticOutputParser)                                 |                    | âœ…                       |           | `str` \| `Message` | `pydantic.BaseModel` | Takes a user defined Pydantic model and returns data in that format.                                                                                                                                                                                     |
| [YAML](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.yaml.YamlOutputParser.html#langchain.output_parsers.yaml.YamlOutputParser)                                                          |                    | âœ…                       |           | `str` \| `Message` | `pydantic.BaseModel` | Takes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.                                                                                                                                                             |
| [PandasDataFrame](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser.html#langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser) |                    | âœ…                       |           | `str` \| `Message` | `dict`               | Useful for doing operations with pandas DataFrames.                                                                                                                                                                                                      |
| [Enum](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.enum.EnumOutputParser.html#langchain.output_parsers.enum.EnumOutputParser)                                                          |                    | âœ…                       |           | `str` \| `Message` | `Enum`               | Parses response into one of the provided enum values.                                                                                                                                                                                                    |
| [Datetime](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html#langchain.output_parsers.datetime.DatetimeOutputParser)                                      |                    | âœ…                       |           | `str` \| `Message` | `datetime.datetime`  | Parses response into a datetime string.                                                                                                                                                                                                                  |
| [Structured](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html#langchain.output_parsers.structured.StructuredOutputParser)                            |                    | âœ…                       |           | `str` \| `Message` | `Dict[str, str]`     | An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.                                            |

For specifics on how to use output parsers, see the [relevant how-to guides here](/docs/how_to/#output-parsers).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/output_parsers.mdx)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/sql_query_checking.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/sql_query_checking.ipynb)

# How to do query validation as part of SQL question-answering

Perhaps the most error-prone part of any SQL chain or agent is writing valid and safe SQL queries. In this guide we'll go over some strategies for validating our queries and handling invalid queries.

We will cover:

1. Appending a "query validator" step to the query generation;
2. Prompt engineering to reduce the incidence of errors.

## Setup[â€‹](#setup "Direct link to Setup")

First, get required packages and set environment variables:

```python
%pip install --upgrade --quiet  langchain langchain-community langchain-openai
```

```python
# Uncomment the below to use LangSmith. Not required.
# import os
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
# os.environ["LANGSMITH_TRACING"] = "true"
```

The below example will use a SQLite connection with Chinook database. Follow [these installation steps](https://database.guide/2-sample-databases-sqlite/) to create `Chinook.db` in the same directory as this notebook:

- Save [this file](https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql) as `Chinook_Sqlite.sql`
- Run `sqlite3 Chinook.db`
- Run `.read Chinook_Sqlite.sql`
- Test `SELECT * FROM Artist LIMIT 10;`

Now, `Chinook.db` is in our directory and we can interface with it using the SQLAlchemy-driven `SQLDatabase` class:

```python
from langchain_community.utilities import SQLDatabase

db = SQLDatabase.from_uri("sqlite:///Chinook.db")
print(db.dialect)
print(db.get_usable_table_names())
print(db.run("SELECT * FROM Artist LIMIT 10;"))
```

**API Reference:**[SQLDatabase](https://python.langchain.com/api_reference/community/utilities/langchain_community.utilities.sql_database.SQLDatabase.html)

```output
sqlite
['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']
[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'AntÃ´nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]
```

## Query checker[â€‹](#query-checker "Direct link to Query checker")

Perhaps the simplest strategy is to ask the model itself to check the original query for common mistakes. Suppose we have the following SQL query chain:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
from langchain.chains import create_sql_query_chain

chain = create_sql_query_chain(llm, db)
```

**API Reference:**[create\_sql\_query\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.sql_database.query.create_sql_query_chain.html)

And we want to validate its outputs. We can do so by extending the chain with a second prompt and model call:

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

system = """Double check the user's {dialect} query for common mistakes, including:
- Using NOT IN with NULL values
- Using UNION when UNION ALL should have been used
- Using BETWEEN for exclusive ranges
- Data type mismatch in predicates
- Properly quoting identifiers
- Using the correct number of arguments for functions
- Casting to the correct data type
- Using the proper columns for joins

If there are any of the above mistakes, rewrite the query.
If there are no mistakes, just reproduce the original query with no further commentary.

Output the final SQL query only."""
prompt = ChatPromptTemplate.from_messages(
    [("system", system), ("human", "{query}")]
).partial(dialect=db.dialect)
validation_chain = prompt | llm | StrOutputParser()

full_chain = {"query": chain} | validation_chain
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```python
query = full_chain.invoke(
    {
        "question": "What's the average Invoice from an American customer whose Fax is missing since 2003 but before 2010"
    }
)
print(query)
```

```output
SELECT AVG(i.Total) AS AverageInvoice
FROM Invoice i
JOIN Customer c ON i.CustomerId = c.CustomerId
WHERE c.Country = 'USA'
AND c.Fax IS NULL
AND i.InvoiceDate >= '2003-01-01' 
AND i.InvoiceDate < '2010-01-01'
```

Note how we can see both steps of the chain in the [Langsmith trace](https://smith.langchain.com/public/8a743295-a57c-4e4c-8625-bc7e36af9d74/r).

```python
db.run(query)
```

```output
'[(6.632999999999998,)]'
```

The obvious downside of this approach is that we need to make two model calls instead of one to generate our query. To get around this we can try to perform the query generation and query check in a single model invocation:

```python
system = """You are a {dialect} expert. Given an input question, create a syntactically correct {dialect} query to run.
Unless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per {dialect}. You can order the results to return the most informative data in the database.
Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (") to denote them as delimited identifiers.
Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.
Pay attention to use date('now') function to get the current date, if the question involves "today".

Only use the following tables:
{table_info}

Write an initial draft of the query. Then double check the {dialect} query for common mistakes, including:
- Using NOT IN with NULL values
- Using UNION when UNION ALL should have been used
- Using BETWEEN for exclusive ranges
- Data type mismatch in predicates
- Properly quoting identifiers
- Using the correct number of arguments for functions
- Casting to the correct data type
- Using the proper columns for joins

Use format:

First draft: <<FIRST_DRAFT_QUERY>>
Final answer: <<FINAL_ANSWER_QUERY>>
"""
prompt = ChatPromptTemplate.from_messages(
    [("system", system), ("human", "{input}")]
).partial(dialect=db.dialect)


def parse_final_answer(output: str) -> str:
    return output.split("Final answer: ")[1]


chain = create_sql_query_chain(llm, db, prompt=prompt) | parse_final_answer
prompt.pretty_print()
```

```output
================================[1m System Message [0m================================

You are a [33;1m[1;3m{dialect}[0m expert. Given an input question, create a syntactically correct [33;1m[1;3m{dialect}[0m query to run.
Unless the user specifies in the question a specific number of examples to obtain, query for at most [33;1m[1;3m{top_k}[0m results using the LIMIT clause as per [33;1m[1;3m{dialect}[0m. You can order the results to return the most informative data in the database.
Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (") to denote them as delimited identifiers.
Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.
Pay attention to use date('now') function to get the current date, if the question involves "today".

Only use the following tables:
[33;1m[1;3m{table_info}[0m

Write an initial draft of the query. Then double check the [33;1m[1;3m{dialect}[0m query for common mistakes, including:
- Using NOT IN with NULL values
- Using UNION when UNION ALL should have been used
- Using BETWEEN for exclusive ranges
- Data type mismatch in predicates
- Properly quoting identifiers
- Using the correct number of arguments for functions
- Casting to the correct data type
- Using the proper columns for joins

Use format:

First draft: <<FIRST_DRAFT_QUERY>>
Final answer: <<FINAL_ANSWER_QUERY>>


================================[1m Human Message [0m=================================

[33;1m[1;3m{input}[0m
```

```python
query = chain.invoke(
    {
        "question": "What's the average Invoice from an American customer whose Fax is missing since 2003 but before 2010"
    }
)
print(query)
```

```output


SELECT AVG(i."Total") AS "AverageInvoice"
FROM "Invoice" i
JOIN "Customer" c ON i."CustomerId" = c."CustomerId"
WHERE c."Country" = 'USA'
AND c."Fax" IS NULL
AND i."InvoiceDate" BETWEEN '2003-01-01' AND '2010-01-01';
```

```python
db.run(query)
```

```output
'[(6.632999999999998,)]'
```

## Human-in-the-loop[â€‹](#human-in-the-loop "Direct link to Human-in-the-loop")

In some cases our data is sensitive enough that we never want to execute a SQL query without a human approving it first. Head to the [Tool use: Human-in-the-loop](/docs/how_to/tools_human/) page to learn how to add a human-in-the-loop to any tool, chain or agent.

## Error handling[â€‹](#error-handling "Direct link to Error handling")

At some point, the model will make a mistake and craft an invalid SQL query. Or an issue will arise with our database. Or the model API will go down. We'll want to add some error handling behavior to our chains and agents so that we fail gracefully in these situations, and perhaps even automatically recover. To learn about error handling with tools, head to the [Tool use: Error handling](/docs/how_to/tools_error/) page.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/sql_query_checking.ipynb)

* * *


- [Setup](#setup)
- [Query checker](#query-checker)
- [Human-in-the-loop](#human-in-the-loop)
- [Error handling](#error-handling)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_json.mdx)

# How to load JSON

[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attributeâ€“value pairs and arrays (or other serializable values).

[JSON Lines](https://jsonlines.org/) is a file format where each line is a valid JSON value.

LangChain implements a [JSONLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.json_loader.JSONLoader.html) to convert JSON and JSONL data into LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects. It uses a specified [jq schema](https://en.wikipedia.org/wiki/Jq_%28programming_language%29) to parse the JSON files, allowing for the extraction of specific fields into the content and metadata of the LangChain Document.

It uses the `jq` python package. Check out this [manual](https://stedolan.github.io/jq/manual/#Basicfilters) for a detailed documentation of the `jq` syntax.

Here we will demonstrate:

- How to load JSON and JSONL data into the content of a LangChain `Document`;
- How to load JSON and JSONL data into metadata associated with a `Document`.

```python
#!pip install jq
```

```python
from langchain_community.document_loaders import JSONLoader
```

**API Reference:**[JSONLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.json_loader.JSONLoader.html)

```python
import json
from pathlib import Path
from pprint import pprint


file_path='./example_data/facebook_chat.json'
data = json.loads(Path(file_path).read_text())
```

```python
pprint(data)
```

```output
    {'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},
     'is_still_participant': True,
     'joinable_mode': {'link': '', 'mode': 1},
     'magic_words': [],
     'messages': [{'content': 'Bye!',
                   'sender_name': 'User 2',
                   'timestamp_ms': 1675597571851},
                  {'content': 'Oh no worries! Bye',
                   'sender_name': 'User 1',
                   'timestamp_ms': 1675597435669},
                  {'content': 'No Im sorry it was my mistake, the blue one is not '
                              'for sale',
                   'sender_name': 'User 2',
                   'timestamp_ms': 1675596277579},
                  {'content': 'I thought you were selling the blue one!',
                   'sender_name': 'User 1',
                   'timestamp_ms': 1675595140251},
                  {'content': 'Im not interested in this bag. Im interested in the '
                              'blue one!',
                   'sender_name': 'User 1',
                   'timestamp_ms': 1675595109305},
                  {'content': 'Here is $129',
                   'sender_name': 'User 2',
                   'timestamp_ms': 1675595068468},
                  {'photos': [{'creation_timestamp': 1675595059,
                               'uri': 'url_of_some_picture.jpg'}],
                   'sender_name': 'User 2',
                   'timestamp_ms': 1675595060730},
                  {'content': 'Online is at least $100',
                   'sender_name': 'User 2',
                   'timestamp_ms': 1675595045152},
                  {'content': 'How much do you want?',
                   'sender_name': 'User 1',
                   'timestamp_ms': 1675594799696},
                  {'content': 'Goodmorning! $50 is too low.',
                   'sender_name': 'User 2',
                   'timestamp_ms': 1675577876645},
                  {'content': 'Hi! Im interested in your bag. Im offering $50. Let '
                              'me know if you are interested. Thanks!',
                   'sender_name': 'User 1',
                   'timestamp_ms': 1675549022673}],
     'participants': [{'name': 'User 1'}, {'name': 'User 2'}],
     'thread_path': 'inbox/User 1 and User 2 chat',
     'title': 'User 1 and User 2 chat'}
```

## Using `JSONLoader`[â€‹](#using-jsonloader "Direct link to using-jsonloader")

Suppose we are interested in extracting the values under the `content` field within the `messages` key of the JSON data. This can easily be done through the `JSONLoader` as shown below.

### JSON file[â€‹](#json-file "Direct link to JSON file")

```python
loader = JSONLoader(
    file_path='./example_data/facebook_chat.json',
    jq_schema='.messages[].content',
    text_content=False)

data = loader.load()
```

```python
pprint(data)
```

```output
    [Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1}),
     Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2}),
     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3}),
     Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4}),
     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5}),
     Document(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6}),
     Document(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7}),
     Document(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8}),
     Document(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9}),
     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10}),
     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11})]
```

### JSON Lines file[â€‹](#json-lines-file "Direct link to JSON Lines file")

If you want to load documents from a JSON Lines file, you pass `json_lines=True` and specify `jq_schema` to extract `page_content` from a single JSON object.

```python
file_path = './example_data/facebook_chat_messages.jsonl'
pprint(Path(file_path).read_text())
```

```output
    ('{"sender_name": "User 2", "timestamp_ms": 1675597571851, "content": "Bye!"}\n'
     '{"sender_name": "User 1", "timestamp_ms": 1675597435669, "content": "Oh no '
     'worries! Bye"}\n'
     '{"sender_name": "User 2", "timestamp_ms": 1675596277579, "content": "No Im '
     'sorry it was my mistake, the blue one is not for sale"}\n')
```

```python
loader = JSONLoader(
    file_path='./example_data/facebook_chat_messages.jsonl',
    jq_schema='.content',
    text_content=False,
    json_lines=True)

data = loader.load()
```

```python
pprint(data)
```

```output
    [Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 1}),
     Document(page_content='Oh no worries! Bye', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 2}),
     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 3})]
```

Another option is to set `jq_schema='.'` and provide `content_key`:

```python
loader = JSONLoader(
    file_path='./example_data/facebook_chat_messages.jsonl',
    jq_schema='.',
    content_key='sender_name',
    json_lines=True)

data = loader.load()
```

```python
pprint(data)
```

```output
    [Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 1}),
     Document(page_content='User 1', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 2}),
     Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 3})]
```

### JSON file with jq schema `content_key`[â€‹](#json-file-with-jq-schema-content_key "Direct link to json-file-with-jq-schema-content_key")

To load documents from a JSON file using the content\_key within the jq schema, set is\_content\_key\_jq\_parsable=True. Ensure that content\_key is compatible and can be parsed using the jq schema.

```python
file_path = './sample.json'
pprint(Path(file_path).read_text())
```

```outputjson
    {"data": [
        {"attributes": {
            "message": "message1",
            "tags": [
            "tag1"]},
        "id": "1"},
        {"attributes": {
            "message": "message2",
            "tags": [
            "tag2"]},
        "id": "2"}]}
```

```python
loader = JSONLoader(
    file_path=file_path,
    jq_schema=".data[]",
    content_key=".attributes.message",
    is_content_key_jq_parsable=True,
)

data = loader.load()
```

```python
pprint(data)
```

```output
    [Document(page_content='message1', metadata={'source': '/path/to/sample.json', 'seq_num': 1}),
     Document(page_content='message2', metadata={'source': '/path/to/sample.json', 'seq_num': 2})]
```

## Extracting metadata[â€‹](#extracting-metadata "Direct link to Extracting metadata")

Generally, we want to include metadata available in the JSON file into the documents that we create from the content.

The following demonstrates how metadata can be extracted using the `JSONLoader`.

There are some key changes to be noted. In the previous example where we didn't collect the metadata, we managed to directly specify in the schema where the value for the `page_content` can be extracted from.

```text
.messages[].content
```

In the current example, we have to tell the loader to iterate over the records in the `messages` field. The jq\_schema then has to be:

```text
.messages[]
```

This allows us to pass the records (dict) into the `metadata_func` that has to be implemented. The `metadata_func` is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final `Document` object.

Additionally, we now have to explicitly specify in the loader, via the `content_key` argument, the key from the record where the value for the `page_content` needs to be extracted from.

```python
# Define the metadata extraction function.
def metadata_func(record: dict, metadata: dict) -> dict:

    metadata["sender_name"] = record.get("sender_name")
    metadata["timestamp_ms"] = record.get("timestamp_ms")

    return metadata


loader = JSONLoader(
    file_path='./example_data/facebook_chat.json',
    jq_schema='.messages[]',
    content_key="content",
    metadata_func=metadata_func
)

data = loader.load()
```

```python
pprint(data)
```

```output
    [Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}),
     Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}),
     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}),
     Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}),
     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}),
     Document(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}),
     Document(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}),
     Document(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}),
     Document(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}),
     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}),
     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})]
```

Now, you will see that the documents contain the metadata associated with the content we extracted.

## The `metadata_func`[â€‹](#the-metadata_func "Direct link to the-metadata_func")

As shown above, the `metadata_func` accepts the default metadata generated by the `JSONLoader`. This allows full control to the user with respect to how the metadata is formatted.

For example, the default metadata contains the `source` and the `seq_num` keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the `metadata_func` to rename the default keys and use the ones from the JSON data.

The example below shows how we can modify the `source` to only contain information of the file source relative to the `langchain` directory.

```python
# Define the metadata extraction function.
def metadata_func(record: dict, metadata: dict) -> dict:

    metadata["sender_name"] = record.get("sender_name")
    metadata["timestamp_ms"] = record.get("timestamp_ms")

    if "source" in metadata:
        source = metadata["source"].split("/")
        source = source[source.index("langchain"):]
        metadata["source"] = "/".join(source)

    return metadata


loader = JSONLoader(
    file_path='./example_data/facebook_chat.json',
    jq_schema='.messages[]',
    content_key="content",
    metadata_func=metadata_func
)

data = loader.load()
```

```python
pprint(data)
```

```output
    [Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}),
     Document(page_content='Oh no worries! Bye', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}),
     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}),
     Document(page_content='I thought you were selling the blue one!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}),
     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}),
     Document(page_content='Here is $129', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}),
     Document(page_content='', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}),
     Document(page_content='Online is at least $100', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}),
     Document(page_content='How much do you want?', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}),
     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}),
     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})]
```

## Common JSON structures with jq schema[â€‹](#common-json-structures-with-jq-schema "Direct link to Common JSON structures with jq schema")

The list below provides a reference to the possible `jq_schema` the user can use to extract content from the JSON data depending on the structure.

```text
JSON        -> [{"text": ...}, {"text": ...}, {"text": ...}]
jq_schema   -> ".[].text"

JSON        -> {"key": [{"text": ...}, {"text": ...}, {"text": ...}]}
jq_schema   -> ".key[].text"

JSON        -> ["...", "...", "..."]
jq_schema   -> ".[]"
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/document_loader_json.mdx)

* * *


- [Using `JSONLoader`](#using-jsonloader)
  
  - [JSON file](#json-file)
  - [JSON Lines file](#json-lines-file)
  - [JSON file with jq schema `content_key`](#json-file-with-jq-schema-content_key)
- [Extracting metadata](#extracting-metadata)
- [The `metadata_func`](#the-metadata_func)
- [Common JSON structures with jq schema](#common-json-structures-with-jq-schema)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/tools_prompting.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/tools_prompting.ipynb)

# How to add ad-hoc tool calling capability to LLMs and Chat Models

caution

Some models have been fine-tuned for tool calling and provide a dedicated API for tool calling. Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling. Please see the [how to use a chat model to call tools](/docs/how_to/tool_calling/) guide for more information.

Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Tools](/docs/concepts/tools/)
- [Function/tool calling](https://python.langchain.com/docs/concepts/tool_calling)
- [Chat models](/docs/concepts/chat_models/)
- [LLMs](/docs/concepts/text_llms/)

In this guide, we'll see how to add **ad-hoc** tool calling support to a chat model. This is an alternative method to invoke tools if you're using a model that does not natively support [tool calling](/docs/how_to/tool_calling/).

We'll do this by simply writing a prompt that will get the model to invoke the appropriate tools. Here's a diagram of the logic:

![chain](/assets/images/tool_chain-3571e7fbc481d648aff93a2630f812ab.svg)

## Setup[â€‹](#setup "Direct link to Setup")

We'll need to install the following packages:

```python
%pip install --upgrade --quiet langchain langchain-community
```

If you'd like to use LangSmith, uncomment the below:

```python
import getpass
import os
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

You can select any of the given models for this how-to guide. Keep in mind that most of these models already [support native tool calling](/docs/integrations/chat/), so using the prompting strategy shown here doesn't make sense for these models, and instead you should follow the [how to use a chat model to call tools](/docs/how_to/tool_calling/) guide.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4", model_provider="openai")
```

To illustrate the idea, we'll use `phi3` via Ollama, which does **NOT** have native support for tool calling. If you'd like to use `Ollama` as well follow [these instructions](/docs/integrations/chat/ollama/).

```python
from langchain_community.llms import Ollama

model = Ollama(model="phi3")
```

**API Reference:**[Ollama](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.ollama.Ollama.html)

## Create a tool[â€‹](#create-a-tool "Direct link to Create a tool")

First, let's create an `add` and `multiply` tools. For more information on creating custom tools, please see [this guide](/docs/how_to/custom_tools/).

```python
from langchain_core.tools import tool


@tool
def multiply(x: float, y: float) -> float:
    """Multiply two numbers together."""
    return x * y


@tool
def add(x: int, y: int) -> int:
    "Add two numbers."
    return x + y


tools = [multiply, add]

# Let's inspect the tools
for t in tools:
    print("--")
    print(t.name)
    print(t.description)
    print(t.args)
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

```output
--
multiply
Multiply two numbers together.
{'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}
--
add
Add two numbers.
{'x': {'title': 'X', 'type': 'integer'}, 'y': {'title': 'Y', 'type': 'integer'}}
```

```python
multiply.invoke({"x": 4, "y": 5})
```

```output
20.0
```

## Creating our prompt[â€‹](#creating-our-prompt "Direct link to Creating our prompt")

We'll want to write a prompt that specifies the tools the model has access to, the arguments to those tools, and the desired output format of the model. In this case we'll instruct it to output a JSON blob of the form `{"name": "...", "arguments": {...}}`.

```python
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import render_text_description

rendered_tools = render_text_description(tools)
print(rendered_tools)
```

**API Reference:**[JsonOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [render\_text\_description](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.render.render_text_description.html)

```output
multiply(x: float, y: float) -> float - Multiply two numbers together.
add(x: int, y: int) -> int - Add two numbers.
```

```python
system_prompt = f"""\
You are an assistant that has access to the following set of tools. 
Here are the names and descriptions for each tool:

{rendered_tools}

Given the user input, return the name and input of the tool to use. 
Return your response as a JSON blob with 'name' and 'arguments' keys.

The `arguments` should be a dictionary, with keys corresponding 
to the argument names and the values corresponding to the requested values.
"""

prompt = ChatPromptTemplate.from_messages(
    [("system", system_prompt), ("user", "{input}")]
)
```

```python
chain = prompt | model
message = chain.invoke({"input": "what's 3 plus 1132"})

# Let's take a look at the output from the model
# if the model is an LLM (not a chat model), the output will be a string.
if isinstance(message, str):
    print(message)
else:  # Otherwise it's a chat model
    print(message.content)
```

```output
{
    "name": "add",
    "arguments": {
        "x": 3,
        "y": 1132
    }
}
```

## Adding an output parser[â€‹](#adding-an-output-parser "Direct link to Adding an output parser")

We'll use the `JsonOutputParser` for parsing our models output to JSON.

```python
from langchain_core.output_parsers import JsonOutputParser

chain = prompt | model | JsonOutputParser()
chain.invoke({"input": "what's thirteen times 4"})
```

**API Reference:**[JsonOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html)

```output
{'name': 'multiply', 'arguments': {'x': 13.0, 'y': 4.0}}
```

important

ðŸŽ‰ Amazing! ðŸŽ‰ We now instructed our model on how to **request** that a tool be invoked.

Now, let's create some logic to actually run the tool!

## Invoking the tool ðŸƒ[â€‹](#invoking-the-tool- "Direct link to Invoking the tool ðŸƒ")

Now that the model can request that a tool be invoked, we need to write a function that can actually invoke the tool.

The function will select the appropriate tool by name, and pass to it the arguments chosen by the model.

```python
from typing import Any, Dict, Optional, TypedDict

from langchain_core.runnables import RunnableConfig


class ToolCallRequest(TypedDict):
    """A typed dict that shows the inputs into the invoke_tool function."""

    name: str
    arguments: Dict[str, Any]


def invoke_tool(
    tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None
):
    """A function that we can use the perform a tool invocation.

    Args:
        tool_call_request: a dict that contains the keys name and arguments.
            The name must match the name of a tool that exists.
            The arguments are the arguments to that tool.
        config: This is configuration information that LangChain uses that contains
            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.

    Returns:
        output from the requested tool
    """
    tool_name_to_tool = {tool.name: tool for tool in tools}
    name = tool_call_request["name"]
    requested_tool = tool_name_to_tool[name]
    return requested_tool.invoke(tool_call_request["arguments"], config=config)
```

**API Reference:**[RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html)

Let's test this out ðŸ§ª!

```python
invoke_tool({"name": "multiply", "arguments": {"x": 3, "y": 5}})
```

```output
15.0
```

## Let's put it together[â€‹](#lets-put-it-together "Direct link to Let's put it together")

Let's put it together into a chain that creates a calculator with add and multiplication capabilities.

```python
chain = prompt | model | JsonOutputParser() | invoke_tool
chain.invoke({"input": "what's thirteen times 4.14137281"})
```

```output
53.83784653
```

## Returning tool inputs[â€‹](#returning-tool-inputs "Direct link to Returning tool inputs")

It can be helpful to return not only tool outputs but also tool inputs. We can easily do this with LCEL by `RunnablePassthrough.assign`-ing the tool output. This will take whatever the input is to the RunnablePassrthrough components (assumed to be a dictionary) and add a key to it while still passing through everything that's currently in the input:

```python
from langchain_core.runnables import RunnablePassthrough

chain = (
    prompt | model | JsonOutputParser() | RunnablePassthrough.assign(output=invoke_tool)
)
chain.invoke({"input": "what's thirteen times 4.14137281"})
```

**API Reference:**[RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)

```output
{'name': 'multiply',
 'arguments': {'x': 13, 'y': 4.14137281},
 'output': 53.83784653}
```

## What's next?[â€‹](#whats-next "Direct link to What's next?")

This how-to guide shows the "happy path" when the model correctly outputs all the required tool information.

In reality, if you're using more complex tools, you will start encountering errors from the model, especially for models that have not been fine tuned for tool calling and for less capable models.

You will need to be prepared to add strategies to improve the output from the model; e.g.,

1. Provide few shot examples.
2. Add error handling (e.g., catch the exception and feed it back to the LLM to ask it to correct its previous output).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tools_prompting.ipynb)

* * *


- [Setup](#setup)
- [Create a tool](#create-a-tool)
- [Creating our prompt](#creating-our-prompt)
- [Adding an output parser](#adding-an-output-parser)
- [Invoking the tool ðŸƒ](#invoking-the-tool-)
- [Let's put it together](#lets-put-it-together)
- [Returning tool inputs](#returning-tool-inputs)
- [What's next?](#whats-next)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/tutorials/summarization.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/summarization.ipynb)

# Summarize Text

info

This tutorial demonstrates text summarization using built-in chains and [LangGraph](https://langchain-ai.github.io/langgraph/).

A [previous version](https://python.langchain.com/v0.1/docs/use_cases/summarization/) of this page showcased the legacy chains [StuffDocumentsChain](/docs/versions/migrating_chains/stuff_docs_chain/), [MapReduceDocumentsChain](/docs/versions/migrating_chains/map_reduce_chain/), and [RefineDocumentsChain](https://python.langchain.com/docs/versions/migrating_chains/refine_docs_chain/). See [here](/docs/versions/migrating_chains/) for information on using those abstractions and a comparison with the methods demonstrated in this tutorial.

Suppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content.

LLMs are a great tool for this given their proficiency in understanding and synthesizing text.

In the context of [retrieval-augmented generation](/docs/tutorials/rag/), summarizing text can help distill the information in a large number of retrieved documents to provide context for a LLM.

In this walkthrough we'll go over how to summarize content from multiple documents using LLMs.

![Image description](/assets/images/summarization_use_case_1-874f7b2c94f64216f1f967fb5aca7bc1.png)

## Concepts[â€‹](#concepts "Direct link to Concepts")

Concepts we will cover are:

- Using [language models](/docs/concepts/chat_models/).
- Using [document loaders](/docs/concepts/document_loaders/), specifically the [WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) to load content from an HTML webpage.
- Two ways to summarize or otherwise combine documents.
  
  1. [Stuff](/docs/tutorials/summarization/#stuff), which simply concatenates documents into a prompt;
  2. [Map-reduce](/docs/tutorials/summarization/#map-reduce), for larger sets of documents. This splits documents into batches, summarizes those, and then summarizes the summaries.

Shorter, targeted guides on these strategies and others, including [iterative refinement](/docs/how_to/summarize_refine/), can be found in the [how-to guides](/docs/how_to/#summarization).

## Setup[â€‹](#setup "Direct link to Setup")

### Jupyter Notebook[â€‹](#jupyter-notebook "Direct link to Jupyter Notebook")

This guide (and most of the other guides in the documentation) uses [Jupyter notebooks](https://jupyter.org/) and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.

This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See [here](https://jupyter.org/install) for instructions on how to install.

### Installation[â€‹](#installation "Direct link to Installation")

To install LangChain run:

- Pip
- Conda

```bash
pip install langchain
```

```bash
conda install langchain -c conda-forge
```

For more details, see our [Installation guide](/docs/how_to/installation/).

### LangSmith[â€‹](#langsmith "Direct link to LangSmith")

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
```

Or, if in a notebook, you can set them with:

```python
import getpass
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

## Overview[â€‹](#overview "Direct link to Overview")

A central question for building a summarizer is how to pass your documents into the LLM's context window. Two common approaches for this are:

1. `Stuff`: Simply "stuff" all your documents into a single prompt. This is the simplest approach (see [here](/docs/how_to/summarize_stuff/) for more on the `create_stuff_documents_chain` constructor, which is used for this method).
2. `Map-reduce`: Summarize each document on its own in a "map" step and then "reduce" the summaries into a final summary (see [here](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html) for more on the `MapReduceDocumentsChain`, which is used for this method).

Note that map-reduce is especially effective when understanding of a sub-document does not rely on preceding context. For example, when summarizing a corpus of many, shorter documents. In other cases, such as summarizing a novel or body of text with an inherent sequence, [iterative refinement](/docs/how_to/summarize_refine/) may be more effective.

![Image description](/assets/images/summarization_use_case_2-f2a4d5d60980a79140085fb7f8043217.png)

## Setup[â€‹](#setup-1 "Direct link to Setup")

First set environment variables and install packages:

```python
%pip install --upgrade --quiet tiktoken langchain langgraph beautifulsoup4 langchain-community

# Set env var OPENAI_API_KEY or load from a .env file
# import dotenv

# dotenv.load_dotenv()
```

```python
import os

os.environ["LANGSMITH_TRACING"] = "true"
```

First we load in our documents. We will use [WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) to load a blog post:

```python
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")
docs = loader.load()
```

**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)

Let's next select a LLM:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

## Stuff: summarize in a single LLM call[â€‹](#stuff "Direct link to Stuff: summarize in a single LLM call")

We can use [create\_stuff\_documents\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html), especially if using larger context window models such as:

- 128k token OpenAI `gpt-4o`
- 200k token Anthropic `claude-3-5-sonnet-20240620`

The chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM:

```python
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.llm import LLMChain
from langchain_core.prompts import ChatPromptTemplate

# Define prompt
prompt = ChatPromptTemplate.from_messages(
    [("system", "Write a concise summary of the following:\\n\\n{context}")]
)

# Instantiate chain
chain = create_stuff_documents_chain(llm, prompt)

# Invoke chain
result = chain.invoke({"context": docs})
print(result)
```

**API Reference:**[create\_stuff\_documents\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html) | [LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
The article "LLM Powered Autonomous Agents" by Lilian Weng discusses the development and capabilities of autonomous agents powered by large language models (LLMs). It outlines a system architecture that includes three main components: Planning, Memory, and Tool Use. 

1. **Planning** involves task decomposition, where complex tasks are broken down into manageable subgoals, and self-reflection, allowing agents to learn from past actions to improve future performance. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are highlighted for enhancing reasoning and planning.

2. **Memory** is categorized into short-term and long-term memory, with mechanisms for fast retrieval using Maximum Inner Product Search (MIPS) algorithms. This allows agents to retain and recall information effectively.

3. **Tool Use** enables agents to interact with external APIs and tools, enhancing their capabilities beyond the limitations of their training data. Examples include MRKL systems and frameworks like HuggingGPT, which facilitate task planning and execution.

The article also addresses challenges such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It concludes with case studies demonstrating the practical applications of these concepts in scientific discovery and interactive simulations. Overall, the article emphasizes the potential of LLMs as powerful problem solvers in autonomous agent systems.
```

### Streaming[â€‹](#streaming "Direct link to Streaming")

Note that we can also stream the result token-by-token:

```python
for token in chain.stream({"context": docs}):
    print(token, end="|")
```

```output
|The| article| "|LL|M| Powered| Autonomous| Agents|"| by| Lil|ian| W|eng| discusses| the| development| and| capabilities| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| It| outlines| a| system| architecture| that| includes| three| main| components|:| Planning|,| Memory|,| and| Tool| Use|.| 

|1|.| **|Planning|**| involves| task| decomposition|,| where| complex| tasks| are| broken| down| into| manageable| sub|go|als|,| and| self|-ref|lection|,| allowing| agents| to| learn| from| past| actions| to| improve| future| performance|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| (|To|T|)| are| highlighted| for| enhancing| reasoning| and| planning|.

|2|.| **|Memory|**| is| categorized| into| short|-term| and| long|-term| memory|,| with| mechanisms| for| fast| retrieval| using| Maximum| Inner| Product| Search| (|M|IPS|)| algorithms|.| This| allows| agents| to| retain| and| recall| information| effectively|.

|3|.| **|Tool| Use|**| emphasizes| the| integration| of| external| APIs| and| tools| to| extend| the| capabilities| of| L|LM|s|,| enabling| them| to| perform| tasks| beyond| their| inherent| limitations|.| Examples| include| MR|KL| systems| and| frameworks| like| Hug|ging|GPT|,| which| facilitate| task| planning| and| execution|.

|The| article| also| addresses| challenges| such| as| finite| context| length|,| difficulties| in| long|-term| planning|,| and| the| reliability| of| natural| language| interfaces|.| It| concludes| with| case| studies| demonstrating| the| practical| applications| of| L|LM|-powered| agents| in| scientific| discovery| and| interactive| simulations|.| Overall|,| the| piece| illustrates| the| potential| of| L|LM|s| as| general| problem| sol|vers| and| their| evolving| role| in| autonomous| systems|.||
```

### Go deeper[â€‹](#go-deeper "Direct link to Go deeper")

- You can easily customize the prompt.
- You can easily try different LLMs, (e.g., [Claude](/docs/integrations/chat/anthropic/)) via the `llm` parameter.

## Map-Reduce: summarize long texts via parallelization[â€‹](#map-reduce "Direct link to Map-Reduce: summarize long texts via parallelization")

Let's unpack the map reduce approach. For this, we'll first map each document to an individual summary using an LLM. Then we'll reduce or consolidate those summaries into a single global summary.

Note that the map step is typically parallelized over the input documents.

[LangGraph](https://langchain-ai.github.io/langgraph/), built on top of `langchain-core`, supports [map-reduce](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) workflows and is well-suited to this problem:

- LangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;
- LangGraph's [checkpointing](https://langchain-ai.github.io/langgraph/how-tos/persistence/) supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.
- The LangGraph implementation is straightforward to modify and extend, as we will see below.

### Map[â€‹](#map "Direct link to Map")

Let's first define the prompt associated with the map step. We can use the same summarization prompt as in the `stuff` approach, above:

```python
from langchain_core.prompts import ChatPromptTemplate

map_prompt = ChatPromptTemplate.from_messages(
    [("system", "Write a concise summary of the following:\\n\\n{context}")]
)
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

We can also use the Prompt Hub to store and fetch prompts.

This will work with your [LangSmith API key](https://docs.smith.langchain.com/).

For example, see the map prompt [here](https://smith.langchain.com/hub/rlm/map-prompt).

```python
from langchain import hub

map_prompt = hub.pull("rlm/map-prompt")
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html)

### Reduce[â€‹](#reduce "Direct link to Reduce")

We also define a prompt that takes the document mapping results and reduces them into a single output.

```python
# Also available via the hub: `hub.pull("rlm/reduce-prompt")`
reduce_template = """
The following is a set of summaries:
{docs}
Take these and distill it into a final, consolidated summary
of the main themes.
"""

reduce_prompt = ChatPromptTemplate([("human", reduce_template)])
```

### Orchestration via LangGraph[â€‹](#orchestration-via-langgraph "Direct link to Orchestration via LangGraph")

Below we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts.

Map-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model's context window size. Here we implement a recursive "collapsing" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.

First we chunk the blog post into smaller "sub documents" to be mapped:

```python
from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000, chunk_overlap=0
)
split_docs = text_splitter.split_documents(docs)
print(f"Generated {len(split_docs)} documents.")
```

**API Reference:**[CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

```````output
Created a chunk of size 1003, which is longer than the specified 1000
``````output
Generated 14 documents.
```````

Next, we define our graph. Note that we define an artificially low maximum token length of 1,000 tokens to illustrate the "collapsing" step.

```python
import operator
from typing import Annotated, List, Literal, TypedDict

from langchain.chains.combine_documents.reduce import (
    acollapse_docs,
    split_list_of_docs,
)
from langchain_core.documents import Document
from langgraph.constants import Send
from langgraph.graph import END, START, StateGraph

token_max = 1000


def length_function(documents: List[Document]) -> int:
    """Get number of tokens for input contents."""
    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)


# This will be the overall state of the main graph.
# It will contain the input document contents, corresponding
# summaries, and a final summary.
class OverallState(TypedDict):
    # Notice here we use the operator.add
    # This is because we want combine all the summaries we generate
    # from individual nodes back into one list - this is essentially
    # the "reduce" part
    contents: List[str]
    summaries: Annotated[list, operator.add]
    collapsed_summaries: List[Document]
    final_summary: str


# This will be the state of the node that we will "map" all
# documents to in order to generate summaries
class SummaryState(TypedDict):
    content: str


# Here we generate a summary, given a document
async def generate_summary(state: SummaryState):
    prompt = map_prompt.invoke(state["content"])
    response = await llm.ainvoke(prompt)
    return {"summaries": [response.content]}


# Here we define the logic to map out over the documents
# We will use this an edge in the graph
def map_summaries(state: OverallState):
    # We will return a list of `Send` objects
    # Each `Send` object consists of the name of a node in the graph
    # as well as the state to send to that node
    return [
        Send("generate_summary", {"content": content}) for content in state["contents"]
    ]


def collect_summaries(state: OverallState):
    return {
        "collapsed_summaries": [Document(summary) for summary in state["summaries"]]
    }


async def _reduce(input: dict) -> str:
    prompt = reduce_prompt.invoke(input)
    response = await llm.ainvoke(prompt)
    return response.content


# Add node to collapse summaries
async def collapse_summaries(state: OverallState):
    doc_lists = split_list_of_docs(
        state["collapsed_summaries"], length_function, token_max
    )
    results = []
    for doc_list in doc_lists:
        results.append(await acollapse_docs(doc_list, _reduce))

    return {"collapsed_summaries": results}


# This represents a conditional edge in the graph that determines
# if we should collapse the summaries or not
def should_collapse(
    state: OverallState,
) -> Literal["collapse_summaries", "generate_final_summary"]:
    num_tokens = length_function(state["collapsed_summaries"])
    if num_tokens > token_max:
        return "collapse_summaries"
    else:
        return "generate_final_summary"


# Here we will generate the final summary
async def generate_final_summary(state: OverallState):
    response = await _reduce(state["collapsed_summaries"])
    return {"final_summary": response}


# Construct the graph
# Nodes:
graph = StateGraph(OverallState)
graph.add_node("generate_summary", generate_summary)  # same as before
graph.add_node("collect_summaries", collect_summaries)
graph.add_node("collapse_summaries", collapse_summaries)
graph.add_node("generate_final_summary", generate_final_summary)

# Edges:
graph.add_conditional_edges(START, map_summaries, ["generate_summary"])
graph.add_edge("generate_summary", "collect_summaries")
graph.add_conditional_edges("collect_summaries", should_collapse)
graph.add_conditional_edges("collapse_summaries", should_collapse)
graph.add_edge("generate_final_summary", END)

app = graph.compile()
```

**API Reference:**[acollapse\_docs](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.reduce.acollapse_docs.html) | [split\_list\_of\_docs](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.reduce.split_list_of_docs.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [Send](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

LangGraph allows the graph structure to be plotted to help visualize its function:

```python
from IPython.display import Image

Image(app.get_graph().draw_mermaid_png())
```

![](data:image/jpg;base64,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)

When running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.

Note that because we have a loop in the graph, it can be helpful to specify a [recursion\_limit](https://langchain-ai.github.io/langgraph/reference/errors/#langgraph.errors.GraphRecursionError) on its execution. This will raise a specific error when the specified limit is exceeded.

```python
async for step in app.astream(
    {"contents": [doc.page_content for doc in split_docs]},
    {"recursion_limit": 10},
):
    print(list(step.keys()))
```

```output
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['collect_summaries']
['collapse_summaries']
['collapse_summaries']
['generate_final_summary']
```

```python
print(step)
```

```output
{'generate_final_summary': {'final_summary': 'The consolidated summary of the main themes from the provided documents is as follows:\n\n1. **Integration of Large Language Models (LLMs) in Autonomous Agents**: The documents explore the evolving role of LLMs in autonomous systems, emphasizing their enhanced reasoning and acting capabilities through methodologies that incorporate structured planning, memory systems, and tool use.\n\n2. **Core Components of Autonomous Agents**:\n   - **Planning**: Techniques like task decomposition (e.g., Chain of Thought) and external classical planners are utilized to facilitate long-term planning by breaking down complex tasks.\n   - **Memory**: The memory system is divided into short-term (in-context learning) and long-term memory, with parallels drawn between human memory and machine learning to improve agent performance.\n   - **Tool Use**: Agents utilize external APIs and algorithms to enhance problem-solving abilities, exemplified by frameworks like HuggingGPT that manage task workflows.\n\n3. **Neuro-Symbolic Architectures**: The integration of MRKL (Modular Reasoning, Knowledge, and Language) systems combines neural and symbolic expert modules with LLMs, addressing challenges in tasks such as verbal math problem-solving.\n\n4. **Specialized Applications**: Case studies, such as ChemCrow and projects in anticancer drug discovery, demonstrate the advantages of LLMs augmented with expert tools in specialized domains.\n\n5. **Challenges and Limitations**: The documents highlight challenges such as hallucination in model outputs and the finite context length of LLMs, which affects their ability to incorporate historical information and perform self-reflection. Techniques like Chain of Hindsight and Algorithm Distillation are discussed to enhance model performance through iterative learning.\n\n6. **Structured Software Development**: A systematic approach to creating Python software projects is emphasized, focusing on defining core components, managing dependencies, and adhering to best practices for documentation.\n\nOverall, the integration of structured planning, memory systems, and advanced tool use aims to enhance the capabilities of LLM-powered autonomous agents while addressing the challenges and limitations these technologies face in real-world applications.'}}
```

In the corresponding [LangSmith trace](https://smith.langchain.com/public/9d7b1d50-e1d6-44c9-9ab2-eabef621c883/r) we can see the individual LLM calls, grouped under their respective nodes.

### Go deeper[â€‹](#go-deeper-1 "Direct link to Go deeper")

**Customization**

- As shown above, you can customize the LLMs and prompts for map and reduce stages.

**Real-world use-case**

- See [this blog post](https://blog.langchain.dev/llms-to-improve-documentation/) case-study on analyzing user interactions (questions about LangChain documentation)!
- The blog post and associated [repo](https://github.com/mendableai/QA_clustering) also introduce clustering as a means of summarization.
- This opens up another path beyond the `stuff` or `map-reduce` approaches that is worth considering.

![Image description](/assets/images/summarization_use_case_3-896f435bc48194ddaead73043027e16f.png)

## Next steps[â€‹](#next-steps "Direct link to Next steps")

We encourage you to check out the [how-to guides](/docs/how_to/) for more detail on:

- Other summarization strategies, such as [iterative refinement](/docs/how_to/summarize_refine/)
- Built-in [document loaders](/docs/how_to/#document-loaders) and [text-splitters](/docs/how_to/#text-splitters)
- Integrating various combine-document chains into a [RAG application](/docs/tutorials/rag/)
- Incorporating retrieval into a [chatbot](/docs/how_to/chatbots_retrieval/)

and other concepts.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/tutorials/summarization.ipynb)

* * *


- [Concepts](#concepts)
- [Setup](#setup)
  
  - [Jupyter Notebook](#jupyter-notebook)
  - [Installation](#installation)
  - [LangSmith](#langsmith)
- [Overview](#overview)
- [Setup](#setup-1)
- [Stuff: summarize in a single LLM call](#stuff)
  
  - [Streaming](#streaming)
  - [Go deeper](#go-deeper)
- [Map-Reduce: summarize long texts via parallelization](#map-reduce)
  
  - [Map](#map)
  - [Reduce](#reduce)
  - [Orchestration via LangGraph](#orchestration-via-langgraph)
  - [Go deeper](#go-deeper-1)
- [Next steps](#next-steps)








- [LangSmith](https://docs.smith.langchain.com)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [LangChain Hub](https://smith.langchain.com/hub)
- [LangChain JS/TS](https://js.langchain.com)

[v0.3](#)

- [v0.3](/docs/introduction/)
- [v0.2](https://python.langchain.com/v0.2/docs/introduction)
- [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)

[ðŸ’¬](https://chat.langchain.com)

Search

[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/people.mdx)

# People

There are some incredible humans from all over the world who have been instrumental in helping the LangChain community flourish ðŸŒ!

This page highlights a few of those folks who have dedicated their time to the open-source repo in the form of direct contributions and reviews.

## Top reviewers[â€‹](#top-reviewers "Direct link to Top reviewers")

As LangChain has grown, the amount of surface area that maintainers cover has grown as well.

Thank you to the following folks who have gone above and beyond in reviewing incoming PRs ðŸ™!

[![](https://avatars.githubusercontent.com/u/2256422?v=4)](https://github.com/leo-gan)[@leo-gan](https://github.com/leo-gan)

[![](https://avatars.githubusercontent.com/u/11633333?u=e13817e11b3fb8c3d209d747c77a0f0742d11138&v=4)](https://github.com/cbornet)[@cbornet](https://github.com/cbornet)

[![](https://avatars.githubusercontent.com/u/11026406?v=4)](https://github.com/lkuligin)[@lkuligin](https://github.com/lkuligin)

[![](https://avatars.githubusercontent.com/u/289369?u=80655eb5f9a4d03bf1a526b07a67adc6eacccc6b&v=4)](https://github.com/3coins)[@3coins](https://github.com/3coins)

[![](https://avatars.githubusercontent.com/u/67427?v=4)](https://github.com/jexp)[@jexp](https://github.com/jexp)

[![](https://avatars.githubusercontent.com/u/19948365?v=4)](https://github.com/tomasonjo)[@tomasonjo](https://github.com/tomasonjo)

[![](https://avatars.githubusercontent.com/u/48236177?u=757490c6af76be0a8837dd5886991005a23c89c7&v=4)](https://github.com/liugddx)[@liugddx](https://github.com/liugddx)

[![](https://avatars.githubusercontent.com/u/49480?u=4a9b7c8820211aae14da7f72f617d88019a06569&v=4)](https://github.com/joemcelroy)[@joemcelroy](https://github.com/joemcelroy)

[![](https://avatars.githubusercontent.com/u/72488598?u=98dc24a63369cbae14913caff5f379f80f305aab&v=4)](https://github.com/Undertone0809)[@Undertone0809](https://github.com/Undertone0809)

[![](https://avatars.githubusercontent.com/u/44113430?u=34bdaacaeb2880e40fb4b07897c481771c6de544&v=4)](https://github.com/mspronesti)[@mspronesti](https://github.com/mspronesti)

[![](https://avatars.githubusercontent.com/u/8429627?u=d28653fbd93c966ac840f93a05f0ef949495851f&v=4)](https://github.com/JohnNay)[@JohnNay](https://github.com/JohnNay)

[![](https://avatars.githubusercontent.com/u/749277?u=84aeb7b75146a67f8b18b389dc591ba72ef105e4&v=4)](https://github.com/tjaffri)[@tjaffri](https://github.com/tjaffri)

[![](https://avatars.githubusercontent.com/u/6690839?u=e56c2161ddc98c58b01fb82da4076e5400fb1e6d&v=4)](https://github.com/sjwhitmore)[@sjwhitmore](https://github.com/sjwhitmore)

[![](https://avatars.githubusercontent.com/u/13262395?u=430eff10dfbb7d3f27a35f1ea2c9ea6a61067c88&v=4)](https://github.com/holtskinner)[@holtskinner](https://github.com/holtskinner)

[![](https://avatars.githubusercontent.com/u/62768671?u=279f772a5b8325a191a1a8bb623aa40f32a01856&v=4)](https://github.com/skcoirz)[@skcoirz](https://github.com/skcoirz)

[![](https://avatars.githubusercontent.com/u/17039389?u=796226152becf82c4d7fd5cc49a24e58a73ce66f&v=4)](https://github.com/harupy)[@harupy](https://github.com/harupy)

[![](https://avatars.githubusercontent.com/u/20304844?u=f00461bcedad6ba384a4e234a44c906802448b4e&v=4)](https://github.com/tylerhutcherson)[@tylerhutcherson](https://github.com/tylerhutcherson)

[![](https://avatars.githubusercontent.com/u/45242107?u=bf122f1371d59c3ba69a87225255fbd00e894404&v=4)](https://github.com/keenborder786)[@keenborder786](https://github.com/keenborder786)

[![](https://avatars.githubusercontent.com/u/46051506?u=026f5f140e8b7ba4744bf971f9ebdea9ebab67ca&v=4)](https://github.com/Anush008)[@Anush008](https://github.com/Anush008)

[![](https://avatars.githubusercontent.com/u/55082429?v=4)](https://github.com/maang-h)[@maang-h](https://github.com/maang-h)

[![](https://avatars.githubusercontent.com/u/13009163?u=c2b3a11cceaadbc9415f545b971250c9e2b2078b&v=4)](https://github.com/Spartee)[@Spartee](https://github.com/Spartee)

[![](https://avatars.githubusercontent.com/u/17705063?v=4)](https://github.com/Raj725)[@Raj725](https://github.com/Raj725)

[![](https://avatars.githubusercontent.com/u/1635179?u=0631cb84ca580089198114f94d9c27efe730220e&v=4)](https://github.com/MthwRobinson)[@MthwRobinson](https://github.com/MthwRobinson)

[![](https://avatars.githubusercontent.com/u/23314389?u=2014e20e246530fa89bd902fe703b6f9e6ecf833&v=4)](https://github.com/nicoloboschi)[@nicoloboschi](https://github.com/nicoloboschi)

[![](https://avatars.githubusercontent.com/u/2887713?u=7bb198c7d11d29a412dc836818f3da6666f643ee&v=4)](https://github.com/Jibola)[@Jibola](https://github.com/Jibola)

[![](https://avatars.githubusercontent.com/u/5015933?u=80e339672a321cde25f4b484129bbddfefb2356d&v=4)](https://github.com/ShaneHarvey)[@ShaneHarvey](https://github.com/ShaneHarvey)

[![](https://avatars.githubusercontent.com/u/19181718?u=79a9013dea28a7fa654431cd7e89b08dc76434dd&v=4)](https://github.com/sepiatone)[@sepiatone](https://github.com/sepiatone)

[![](https://avatars.githubusercontent.com/u/123224380?v=4)](https://github.com/scadEfUr)[@scadEfUr](https://github.com/scadEfUr)

[![](https://avatars.githubusercontent.com/u/891664?u=722172a0061f68ab22819fa88a354ec973f70a63&v=4)](https://github.com/jeffchuber)[@jeffchuber](https://github.com/jeffchuber)

[![](https://avatars.githubusercontent.com/u/2649301?u=5e688d2b90ddcafd5028a9da292010144cad6d18&v=4)](https://github.com/kacperlukawski)[@kacperlukawski](https://github.com/kacperlukawski)

[![](https://avatars.githubusercontent.com/u/25930426?v=4)](https://github.com/pranjaldoshi96)[@pranjaldoshi96](https://github.com/pranjaldoshi96)

[![](https://avatars.githubusercontent.com/u/2096628?u=2a4822ff8dc6b4f1162c58716d48fdfac08c8601&v=4)](https://github.com/blink1073)[@blink1073](https://github.com/blink1073)

[![](https://avatars.githubusercontent.com/u/31463517?u=635a36cf4e20a25b5bc8cdc3aa27c613fa701cfa&v=4)](https://github.com/B-Step62)[@B-Step62](https://github.com/B-Step62)

[![](https://avatars.githubusercontent.com/u/13749212?u=b58700c3bd236e880223bccba53b7ad0dd4d7003&v=4)](https://github.com/eavanvalkenburg)[@eavanvalkenburg](https://github.com/eavanvalkenburg)

[![](https://avatars.githubusercontent.com/u/1097932?u=0e9c1cc9e2c02469e52963322344af181464bf43&v=4)](https://github.com/gengliangwang)[@gengliangwang](https://github.com/gengliangwang)

[![](https://avatars.githubusercontent.com/u/39497902?u=0c1597698c6f28da87d80ac0de9c8276d5ab63e9&v=4)](https://github.com/dbczumar)[@dbczumar](https://github.com/dbczumar)

[![](https://avatars.githubusercontent.com/u/39283302?u=58e7bdcdc759fa1f823bd5af35641ca5ad2a6d15&v=4)](https://github.com/BenWilson2)[@BenWilson2](https://github.com/BenWilson2)

[![](https://avatars.githubusercontent.com/u/251292?u=a7465aae734d2cbc12d26b885b07d466d969bf0c&v=4)](https://github.com/jmorganca)[@jmorganca](https://github.com/jmorganca)

[![](https://avatars.githubusercontent.com/u/204694?u=c42de41cff108d35269dd2e8fac8977f1f4e471d&v=4)](https://github.com/pprados)[@pprados](https://github.com/pprados)

[![](https://avatars.githubusercontent.com/u/14221764?u=47a1405343b4d92caed3744e82dda1d28d01a251&v=4)](https://github.com/hemidactylus)[@hemidactylus](https://github.com/hemidactylus)

[![](https://avatars.githubusercontent.com/u/82586689?u=f10792bb4d6db272be85086ae5a8159f56d8a64f&v=4)](https://github.com/RafaelXokito)[@RafaelXokito](https://github.com/RafaelXokito)

[![](https://avatars.githubusercontent.com/u/35960?v=4)](https://github.com/bjchambers)[@bjchambers](https://github.com/bjchambers)

[![](https://avatars.githubusercontent.com/u/101075607?v=4)](https://github.com/andychenmagrathea-06e0a82f-34fc-48ca)[@andychenmagrathea-06e0a82f-34fc-48ca](https://github.com/andychenmagrathea-06e0a82f-34fc-48ca)

[![](https://avatars.githubusercontent.com/u/43734688?u=78f139fa940620e301361a58821c9f56128f71d9&v=4)](https://github.com/sam-h-bean)[@sam-h-bean](https://github.com/sam-h-bean)

[![](https://avatars.githubusercontent.com/u/20311743?u=29bf2391ae34297a12a88d813731b0bdf289e4a5&v=4)](https://github.com/nickscamara)[@nickscamara](https://github.com/nickscamara)

[![](https://avatars.githubusercontent.com/u/89161683?u=4a59b199c77215fe3cb8c937797b909061ec49af&v=4)](https://github.com/naveentatikonda)[@naveentatikonda](https://github.com/naveentatikonda)

[![](https://avatars.githubusercontent.com/u/24217337?u=09d0e274f382e264ef578e93b547fb55a5b179fe&v=4)](https://github.com/kylehh)[@kylehh](https://github.com/kylehh)

[![](https://avatars.githubusercontent.com/u/1823547?u=ea9246b84dbc3886d96ba171aabb64d2470c8d60&v=4)](https://github.com/ofermend)[@ofermend](https://github.com/ofermend)

[![](https://avatars.githubusercontent.com/u/6162415?u=637859605152bece4f99fdc13ef5b4f0027b00e1&v=4)](https://github.com/navneet1v)[@navneet1v](https://github.com/navneet1v)

[![](https://avatars.githubusercontent.com/u/144115527?u=b881a61482b25b543dacd217d18fc5b98c38e7a3&v=4)](https://github.com/billytrend-cohere)[@billytrend-cohere](https://github.com/billytrend-cohere)

[![](https://avatars.githubusercontent.com/u/91237924?u=76e7131a2ebbe9ef35061620286d6d06258e7a61&v=4)](https://github.com/openvino-dev-samples)[@openvino-dev-samples](https://github.com/openvino-dev-samples)

[![](https://avatars.githubusercontent.com/u/82044803?u=451c2955f0862cccf64cac30e062570d208d6903&v=4)](https://github.com/serena-ruan)[@serena-ruan](https://github.com/serena-ruan)

[![](https://avatars.githubusercontent.com/u/851520?u=21c6d8ef697fd32a8020d81269e155a24cb081ac&v=4)](https://github.com/maxjakob)[@maxjakob](https://github.com/maxjakob)

## Top recent contributors[â€‹](#top-recent-contributors "Direct link to Top recent contributors")

The list below contains contributors who have had the most PRs merged in the last three months, weighted (imperfectly) by impact.

Thank you all so much for your time and efforts in making LangChain better â¤ï¸!

[![](https://avatars.githubusercontent.com/u/2256422?v=4)](https://github.com/leo-gan)[@leo-gan](https://github.com/leo-gan)

[![](https://avatars.githubusercontent.com/u/11633333?u=e13817e11b3fb8c3d209d747c77a0f0742d11138&v=4)](https://github.com/cbornet)[@cbornet](https://github.com/cbornet)

[![](https://avatars.githubusercontent.com/u/55082429?v=4)](https://github.com/maang-h)[@maang-h](https://github.com/maang-h)

[![](https://avatars.githubusercontent.com/u/46051506?u=026f5f140e8b7ba4744bf971f9ebdea9ebab67ca&v=4)](https://github.com/Anush008)[@Anush008](https://github.com/Anush008)

[![](https://avatars.githubusercontent.com/u/17705063?v=4)](https://github.com/Raj725)[@Raj725](https://github.com/Raj725)

[![](https://avatars.githubusercontent.com/u/24444502?u=324d8acd5a220bfd93b1b46dac1d9d1d82d7d516&v=4)](https://github.com/ZhangShenao)[@ZhangShenao](https://github.com/ZhangShenao)

[![](https://avatars.githubusercontent.com/u/886516?u=9ee4c1643932282abedaf432a5e39c76ca260700&v=4)](https://github.com/shurrey)[@shurrey](https://github.com/shurrey)

[![](https://avatars.githubusercontent.com/u/35960?v=4)](https://github.com/bjchambers)[@bjchambers](https://github.com/bjchambers)

[![](https://avatars.githubusercontent.com/u/19948365?v=4)](https://github.com/tomasonjo)[@tomasonjo](https://github.com/tomasonjo)

[![](https://avatars.githubusercontent.com/u/31463517?u=635a36cf4e20a25b5bc8cdc3aa27c613fa701cfa&v=4)](https://github.com/B-Step62)[@B-Step62](https://github.com/B-Step62)

[![](https://avatars.githubusercontent.com/u/14959173?u=87fcb0013440f648fb263168583695258b6dbf1c&v=4)](https://github.com/jhpiedrahitao)[@jhpiedrahitao](https://github.com/jhpiedrahitao)

[![](https://avatars.githubusercontent.com/u/12782505?u=a3f1c6e7e68b96bb7be08ecd25f74f2396394597&v=4)](https://github.com/nithishr)[@nithishr](https://github.com/nithishr)

[![](https://avatars.githubusercontent.com/u/737181?u=037bd3004f6c333108d7b32a4c8f96f2e8d57e78&v=4)](https://github.com/caseyclements)[@caseyclements](https://github.com/caseyclements)

[![](https://avatars.githubusercontent.com/u/43506685?u=20e4be682f43988bd43b039a801856498e31c764&v=4)](https://github.com/Coniferish)[@Coniferish](https://github.com/Coniferish)

[![](https://avatars.githubusercontent.com/u/17022025?u=ceee62d53f1c06bf9a014096b651ca0c42cfea3b&v=4)](https://github.com/zc277584121)[@zc277584121](https://github.com/zc277584121)

[![](https://avatars.githubusercontent.com/u/82044803?u=451c2955f0862cccf64cac30e062570d208d6903&v=4)](https://github.com/serena-ruan)[@serena-ruan](https://github.com/serena-ruan)

[![](https://avatars.githubusercontent.com/u/45242107?u=bf122f1371d59c3ba69a87225255fbd00e894404&v=4)](https://github.com/keenborder786)[@keenborder786](https://github.com/keenborder786)

[![](https://avatars.githubusercontent.com/u/58721149?v=4)](https://github.com/dristysrivastava)[@dristysrivastava](https://github.com/dristysrivastava)

[![](https://avatars.githubusercontent.com/u/34255899?u=05aba76f1912a56538c8a5141f8135d0e3b1e1bd&v=4)](https://github.com/gbaian10)[@gbaian10](https://github.com/gbaian10)

[![](https://avatars.githubusercontent.com/u/87140293?v=4)](https://github.com/thedavgar)[@thedavgar](https://github.com/thedavgar)

## Core maintainers[â€‹](#core-maintainers "Direct link to Core maintainers")

Hello there ðŸ‘‹!

We're LangChain's core maintainers. If you've spent time in the community, you've probably crossed paths with at least one of us already.

[![](https://avatars.githubusercontent.com/u/22008038?u=8e3d6bbd0adbe02f0bd259c44f2ddb8612f90d88&v=4)](https://github.com/baskaryan)[@baskaryan](https://github.com/baskaryan)

[![](https://avatars.githubusercontent.com/u/26529506?u=528b1df1ba3ba4f21e3e1fb74b12766e5b04c487&v=4)](https://github.com/ccurme)[@ccurme](https://github.com/ccurme)

[![](https://avatars.githubusercontent.com/u/13333726?u=82ebf1e0eb0663ebd49ba66f67a43f51bbf11442&v=4)](https://github.com/hinthornw)[@hinthornw](https://github.com/hinthornw)

[![](https://avatars.githubusercontent.com/u/122662504?u=e88c472fba16a74332c550cc9707fd015738a0da&v=4)](https://github.com/rlancemartin)[@rlancemartin](https://github.com/rlancemartin)

[![](https://avatars.githubusercontent.com/u/56902?u=fdb30e802c68bc338dd9c0820f713e4fdac75db7&v=4)](https://github.com/nfcampos)[@nfcampos](https://github.com/nfcampos)

[![](https://avatars.githubusercontent.com/u/9536492?u=820809d60f4a720a4e1f507a1bf866dfb5f86614&v=4)](https://github.com/agola11)[@agola11](https://github.com/agola11)

[![](https://avatars.githubusercontent.com/u/11986836?u=f4c4f21a82b2af6c9f91e1f1d99ea40062f7a101&v=4)](https://github.com/hwchase17)[@hwchase17](https://github.com/hwchase17)

[![](https://avatars.githubusercontent.com/u/19161700?u=e76bcd472b51c9f07befd2654783d0a381f49005&v=4)](https://github.com/vbarda)[@vbarda](https://github.com/vbarda)

[![](https://avatars.githubusercontent.com/u/9557659?u=44391f1f5f5e3a72acc9772ca30f28bfdcc25fac&v=4)](https://github.com/efriis)[@efriis](https://github.com/efriis)

[![](https://avatars.githubusercontent.com/u/3205522?v=4)](https://github.com/eyurtsev)[@eyurtsev](https://github.com/eyurtsev)

## Top all-time contributors[â€‹](#top-all-time-contributors "Direct link to Top all-time contributors")

And finally, this is an all-time list of all-stars who have made significant contributions to the framework ðŸŒŸ:

[![](https://avatars.githubusercontent.com/u/2256422?v=4)](https://github.com/leo-gan)[@leo-gan](https://github.com/leo-gan)

[![](https://avatars.githubusercontent.com/u/11633333?u=e13817e11b3fb8c3d209d747c77a0f0742d11138&v=4)](https://github.com/cbornet)[@cbornet](https://github.com/cbornet)

[![](https://avatars.githubusercontent.com/u/19948365?v=4)](https://github.com/tomasonjo)[@tomasonjo](https://github.com/tomasonjo)

[![](https://avatars.githubusercontent.com/u/11026406?v=4)](https://github.com/lkuligin)[@lkuligin](https://github.com/lkuligin)

[![](https://avatars.githubusercontent.com/u/55082429?v=4)](https://github.com/maang-h)[@maang-h](https://github.com/maang-h)

[![](https://avatars.githubusercontent.com/u/1635179?u=0631cb84ca580089198114f94d9c27efe730220e&v=4)](https://github.com/MthwRobinson)[@MthwRobinson](https://github.com/MthwRobinson)

[![](https://avatars.githubusercontent.com/u/2649301?u=5e688d2b90ddcafd5028a9da292010144cad6d18&v=4)](https://github.com/kacperlukawski)[@kacperlukawski](https://github.com/kacperlukawski)

[![](https://avatars.githubusercontent.com/u/14221764?u=47a1405343b4d92caed3744e82dda1d28d01a251&v=4)](https://github.com/hemidactylus)[@hemidactylus](https://github.com/hemidactylus)

[![](https://avatars.githubusercontent.com/u/289369?u=80655eb5f9a4d03bf1a526b07a67adc6eacccc6b&v=4)](https://github.com/3coins)[@3coins](https://github.com/3coins)

[![](https://avatars.githubusercontent.com/u/48236177?u=757490c6af76be0a8837dd5886991005a23c89c7&v=4)](https://github.com/liugddx)[@liugddx](https://github.com/liugddx)

[![](https://avatars.githubusercontent.com/u/707699?u=5af157e56c17bb694ed78f27ba313dcb576f00bd&v=4)](https://github.com/timothyasp)[@timothyasp](https://github.com/timothyasp)

[![](https://avatars.githubusercontent.com/u/6690839?u=e56c2161ddc98c58b01fb82da4076e5400fb1e6d&v=4)](https://github.com/sjwhitmore)[@sjwhitmore](https://github.com/sjwhitmore)

[![](https://avatars.githubusercontent.com/u/139469471?u=4f555a1c581194f9957bf95921d3ebec61da0f3c&v=4)](https://github.com/MateuszOssGit)[@MateuszOssGit](https://github.com/MateuszOssGit)

[![](https://avatars.githubusercontent.com/u/45242107?u=bf122f1371d59c3ba69a87225255fbd00e894404&v=4)](https://github.com/keenborder786)[@keenborder786](https://github.com/keenborder786)

[![](https://avatars.githubusercontent.com/u/6439365?u=51c4e9ea28b36473f21524fb68f7b717047e36f9&v=4)](https://github.com/mbchang)[@mbchang](https://github.com/mbchang)

[![](https://avatars.githubusercontent.com/u/131175?u=332fe36f12d9ffe9e4414dc776b381fe801a9c53&v=4)](https://github.com/danielchalef)[@danielchalef](https://github.com/danielchalef)

[![](https://avatars.githubusercontent.com/u/46051506?u=026f5f140e8b7ba4744bf971f9ebdea9ebab67ca&v=4)](https://github.com/Anush008)[@Anush008](https://github.com/Anush008)

[![](https://avatars.githubusercontent.com/u/44113430?u=34bdaacaeb2880e40fb4b07897c481771c6de544&v=4)](https://github.com/mspronesti)[@mspronesti](https://github.com/mspronesti)

[![](https://avatars.githubusercontent.com/u/15604894?u=420ab32f71fa4a6839da653b5a5d97381b087902&v=4)](https://github.com/chyroc)[@chyroc](https://github.com/chyroc)

[![](https://avatars.githubusercontent.com/u/13749212?u=b58700c3bd236e880223bccba53b7ad0dd4d7003&v=4)](https://github.com/eavanvalkenburg)[@eavanvalkenburg](https://github.com/eavanvalkenburg)

[![](https://avatars.githubusercontent.com/u/23517545?u=06757717778f7c2a0a092b78edfc242d356a2b3f&v=4)](https://github.com/shibuiwilliam)[@shibuiwilliam](https://github.com/shibuiwilliam)

[![](https://avatars.githubusercontent.com/u/13262395?u=430eff10dfbb7d3f27a35f1ea2c9ea6a61067c88&v=4)](https://github.com/holtskinner)[@holtskinner](https://github.com/holtskinner)

[![](https://avatars.githubusercontent.com/u/19181718?u=79a9013dea28a7fa654431cd7e89b08dc76434dd&v=4)](https://github.com/sepiatone)[@sepiatone](https://github.com/sepiatone)

[![](https://avatars.githubusercontent.com/u/1823547?u=ea9246b84dbc3886d96ba171aabb64d2470c8d60&v=4)](https://github.com/ofermend)[@ofermend](https://github.com/ofermend)

[![](https://avatars.githubusercontent.com/u/24279597?u=05e329b5fa4f95223f9fbb1daa07118f72e4a071&v=4)](https://github.com/fpingham)[@fpingham](https://github.com/fpingham)

[![](https://avatars.githubusercontent.com/u/204694?u=c42de41cff108d35269dd2e8fac8977f1f4e471d&v=4)](https://github.com/pprados)[@pprados](https://github.com/pprados)

[![](https://avatars.githubusercontent.com/u/10000925?u=7970fa7b01d133adfe533c4311b7963e22dc6766&v=4)](https://github.com/169)[@169](https://github.com/169)

[![](https://avatars.githubusercontent.com/u/749277?u=84aeb7b75146a67f8b18b389dc591ba72ef105e4&v=4)](https://github.com/tjaffri)[@tjaffri](https://github.com/tjaffri)

[![](https://avatars.githubusercontent.com/u/14959173?u=87fcb0013440f648fb263168583695258b6dbf1c&v=4)](https://github.com/jhpiedrahitao)[@jhpiedrahitao](https://github.com/jhpiedrahitao)

[![](https://avatars.githubusercontent.com/u/17705063?v=4)](https://github.com/Raj725)[@Raj725](https://github.com/Raj725)

[![](https://avatars.githubusercontent.com/u/144115527?u=b881a61482b25b543dacd217d18fc5b98c38e7a3&v=4)](https://github.com/billytrend-cohere)[@billytrend-cohere](https://github.com/billytrend-cohere)

[![](https://avatars.githubusercontent.com/u/20311743?u=29bf2391ae34297a12a88d813731b0bdf289e4a5&v=4)](https://github.com/nickscamara)[@nickscamara](https://github.com/nickscamara)

[![](https://avatars.githubusercontent.com/u/851520?u=21c6d8ef697fd32a8020d81269e155a24cb081ac&v=4)](https://github.com/maxjakob)[@maxjakob](https://github.com/maxjakob)

[![](https://avatars.githubusercontent.com/u/142261444?u=23524d34d4d0dfce963a24131a3c28e89daa9fc7&v=4)](https://github.com/maks-operlejn-ds)[@maks-operlejn-ds](https://github.com/maks-operlejn-ds)

[![](https://avatars.githubusercontent.com/u/57520563?v=4)](https://github.com/volodymyr-memsql)[@volodymyr-memsql](https://github.com/volodymyr-memsql)

[![](https://avatars.githubusercontent.com/u/12782505?u=a3f1c6e7e68b96bb7be08ecd25f74f2396394597&v=4)](https://github.com/nithishr)[@nithishr](https://github.com/nithishr)

[![](https://avatars.githubusercontent.com/u/2887713?u=7bb198c7d11d29a412dc836818f3da6666f643ee&v=4)](https://github.com/Jibola)[@Jibola](https://github.com/Jibola)

[![](https://avatars.githubusercontent.com/u/31382824?u=6d105aac7d9644321baf32108937e70878593880&v=4)](https://github.com/Adi8885)[@Adi8885](https://github.com/Adi8885)

[![](https://avatars.githubusercontent.com/u/31463517?u=635a36cf4e20a25b5bc8cdc3aa27c613fa701cfa&v=4)](https://github.com/B-Step62)[@B-Step62](https://github.com/B-Step62)

[![](https://avatars.githubusercontent.com/u/64213648?u=a9a3c39e0277dcb74d102e73511df929d2a1ecc6&v=4)](https://github.com/sergerdn)[@sergerdn](https://github.com/sergerdn)

[![](https://avatars.githubusercontent.com/u/91237924?u=76e7131a2ebbe9ef35061620286d6d06258e7a61&v=4)](https://github.com/openvino-dev-samples)[@openvino-dev-samples](https://github.com/openvino-dev-samples)

[![](https://avatars.githubusercontent.com/u/6519888?u=fe0b0f093e8683bdac4f205b237d2e48d7c755d4&v=4)](https://github.com/averikitsch)[@averikitsch](https://github.com/averikitsch)

[![](https://avatars.githubusercontent.com/u/89161683?u=4a59b199c77215fe3cb8c937797b909061ec49af&v=4)](https://github.com/naveentatikonda)[@naveentatikonda](https://github.com/naveentatikonda)

[![](https://avatars.githubusercontent.com/u/56769451?u=088102b6160822bc68c25a2a5df170080d0b16a2&v=4)](https://github.com/tyumentsev4)[@tyumentsev4](https://github.com/tyumentsev4)

[![](https://avatars.githubusercontent.com/u/40663591?u=d0a44575938f379eb414c15d9bdc0ecf6911f1b8&v=4)](https://github.com/UmerHA)[@UmerHA](https://github.com/UmerHA)

[![](https://avatars.githubusercontent.com/u/84336755?u=35224f42916080bd7add99571a3132f5ef8217b8&v=4)](https://github.com/joshuasundance-swca)[@joshuasundance-swca](https://github.com/joshuasundance-swca)

[![](https://avatars.githubusercontent.com/u/54854336?v=4)](https://github.com/adolkhan)[@adolkhan](https://github.com/adolkhan)

[![](https://avatars.githubusercontent.com/u/8990777?u=9f7c4ab36aa10d7594748fdc9ddba6ff3f0a2f77&v=4)](https://github.com/jamesbraza)[@jamesbraza](https://github.com/jamesbraza)

[![](https://avatars.githubusercontent.com/u/22579106?v=4)](https://github.com/seamusp)[@seamusp](https://github.com/seamusp)

[![](https://avatars.githubusercontent.com/u/63565275?u=14db8a9a8d8164fd4973597296c3e015423706bc&v=4)](https://github.com/michaelfeil)[@michaelfeil](https://github.com/michaelfeil)

[![](https://avatars.githubusercontent.com/u/9318457?u=3dbf765a07fee48e3dd171851b8417c002a41f49&v=4)](https://github.com/rahul-trip)[@rahul-trip](https://github.com/rahul-trip)

[![](https://avatars.githubusercontent.com/u/901795?u=c8cd7391f649623258b5f5ea848550df9407107b&v=4)](https://github.com/virattt)[@virattt](https://github.com/virattt)

[![](https://avatars.githubusercontent.com/u/39553475?u=919fcd626077055164ce97bf6cde0a47c54507de&v=4)](https://github.com/Josephasafg)[@Josephasafg](https://github.com/Josephasafg)

[![](https://avatars.githubusercontent.com/u/210457?u=3f6ac4dcc1ec9f1b98cc62fd7095120da2accbc4&v=4)](https://github.com/blob42)[@blob42](https://github.com/blob42)

[![](https://avatars.githubusercontent.com/u/3690240?v=4)](https://github.com/malandis)[@malandis](https://github.com/malandis)

[![](https://avatars.githubusercontent.com/u/8456706?u=bc28d399a4ef7495eaa1e8a8a7b99dda98217260&v=4)](https://github.com/mpskex)[@mpskex](https://github.com/mpskex)

[![](https://avatars.githubusercontent.com/u/7069390?u=c10e9b05119b96e82f03a807a2392f938a59f4ef&v=4)](https://github.com/davidbuniat)[@davidbuniat](https://github.com/davidbuniat)

[![](https://avatars.githubusercontent.com/u/5787923?u=368596daa7442493d6c26725eb7d0ac5678c7e73&v=4)](https://github.com/ShreyaR)[@ShreyaR](https://github.com/ShreyaR)

[![](https://avatars.githubusercontent.com/u/1296705?v=4)](https://github.com/lalanikarim)[@lalanikarim](https://github.com/lalanikarim)

[![](https://avatars.githubusercontent.com/u/17022025?u=ceee62d53f1c06bf9a014096b651ca0c42cfea3b&v=4)](https://github.com/zc277584121)[@zc277584121](https://github.com/zc277584121)

[![](https://avatars.githubusercontent.com/u/1825679?u=bc5db0325ef2a546c67e1e2ae1f7a0af7afe6803&v=4)](https://github.com/maiqingqiang)[@maiqingqiang](https://github.com/maiqingqiang)

[![](https://avatars.githubusercontent.com/u/20304844?u=f00461bcedad6ba384a4e234a44c906802448b4e&v=4)](https://github.com/tylerhutcherson)[@tylerhutcherson](https://github.com/tylerhutcherson)

[![](https://avatars.githubusercontent.com/u/62768671?u=279f772a5b8325a191a1a8bb623aa40f32a01856&v=4)](https://github.com/skcoirz)[@skcoirz](https://github.com/skcoirz)

[![](https://avatars.githubusercontent.com/u/24444502?u=324d8acd5a220bfd93b1b46dac1d9d1d82d7d516&v=4)](https://github.com/ZhangShenao)[@ZhangShenao](https://github.com/ZhangShenao)

[![](https://avatars.githubusercontent.com/u/17039389?u=796226152becf82c4d7fd5cc49a24e58a73ce66f&v=4)](https://github.com/harupy)[@harupy](https://github.com/harupy)

[![](https://avatars.githubusercontent.com/u/66525873?u=71102c35b5c8d325d34c32a4f9a07b6f97d90836&v=4)](https://github.com/manuel-soria)[@manuel-soria](https://github.com/manuel-soria)

[![](https://avatars.githubusercontent.com/u/94075036?u=b636b7e4d6abff66af96ccae00d539db4735eea1&v=4)](https://github.com/CG80499)[@CG80499](https://github.com/CG80499)

[![](https://avatars.githubusercontent.com/u/60956360?u=5678f015273d23e2cbdacbe172bcf154de0f4f86&v=4)](https://github.com/outday29)[@outday29](https://github.com/outday29)

[![](https://avatars.githubusercontent.com/u/127103098?v=4)](https://github.com/harry-cohere)[@harry-cohere](https://github.com/harry-cohere)

[![](https://avatars.githubusercontent.com/u/1821407?u=0a24b0db8c1a9231ce1c347de92f57341defada2&v=4)](https://github.com/GMartin-dev)[@GMartin-dev](https://github.com/GMartin-dev)

[![](https://avatars.githubusercontent.com/u/15918167?v=4)](https://github.com/ljeagle)[@ljeagle](https://github.com/ljeagle)

[![](https://avatars.githubusercontent.com/u/49480?u=4a9b7c8820211aae14da7f72f617d88019a06569&v=4)](https://github.com/joemcelroy)[@joemcelroy](https://github.com/joemcelroy)

[![](https://avatars.githubusercontent.com/u/10701973?u=866bdbf25a3759626815099ce480e2ffcff520fb&v=4)](https://github.com/IANTHEREAL)[@IANTHEREAL](https://github.com/IANTHEREAL)

[![](https://avatars.githubusercontent.com/u/13748374?u=47b1f523342466ab97dd23e285418c5f5c9820c4&v=4)](https://github.com/wangxuqi)[@wangxuqi](https://github.com/wangxuqi)

[![](https://avatars.githubusercontent.com/u/886516?u=9ee4c1643932282abedaf432a5e39c76ca260700&v=4)](https://github.com/shurrey)[@shurrey](https://github.com/shurrey)

[![](https://avatars.githubusercontent.com/u/2212586?v=4)](https://github.com/mackong)[@mackong](https://github.com/mackong)

[![](https://avatars.githubusercontent.com/u/1097932?u=0e9c1cc9e2c02469e52963322344af181464bf43&v=4)](https://github.com/gengliangwang)[@gengliangwang](https://github.com/gengliangwang)

[![](https://avatars.githubusercontent.com/u/20971593?u=1574196bb286044d23a04aa5aa34203ada8f4309&v=4)](https://github.com/jzluo)[@jzluo](https://github.com/jzluo)

[![](https://avatars.githubusercontent.com/u/58508471?u=74423e863298863bf5c7dd7d1bff0aa106a9cc75&v=4)](https://github.com/Anindyadeep)[@Anindyadeep](https://github.com/Anindyadeep)

[![](https://avatars.githubusercontent.com/u/142883372?u=45481f472f5f89c4d8ca8788617ffac47c5ebd88&v=4)](https://github.com/mateusz-wosinski-ds)[@mateusz-wosinski-ds](https://github.com/mateusz-wosinski-ds)

[![](https://avatars.githubusercontent.com/u/5013466?u=f46f9262437c7f899394561c2f2dcb7e4b669868&v=4)](https://github.com/Jped)[@Jped](https://github.com/Jped)

[![](https://avatars.githubusercontent.com/u/24587702?u=bc1fe15724c747b755a5b3812e802d7cbdd134c2&v=4)](https://github.com/hughcrt)[@hughcrt](https://github.com/hughcrt)

[![](https://avatars.githubusercontent.com/u/62176855?v=4)](https://github.com/cs0lar)[@cs0lar](https://github.com/cs0lar)

[![](https://avatars.githubusercontent.com/u/141953346?u=ede12989daf498a2df632344378a57e4f2b4c317&v=4)](https://github.com/ShorthillsAI)[@ShorthillsAI](https://github.com/ShorthillsAI)

[![](https://avatars.githubusercontent.com/u/35960?v=4)](https://github.com/bjchambers)[@bjchambers](https://github.com/bjchambers)

[![](https://avatars.githubusercontent.com/u/24217337?u=09d0e274f382e264ef578e93b547fb55a5b179fe&v=4)](https://github.com/kylehh)[@kylehh](https://github.com/kylehh)

[![](https://avatars.githubusercontent.com/u/22633385?u=29190f6c8aed91fa9574b064a9995f1e49944acf&v=4)](https://github.com/eltociear)[@eltociear](https://github.com/eltociear)

[![](https://avatars.githubusercontent.com/u/53237856?u=656560c61bb540c9930574037126d2280ef0b4f8&v=4)](https://github.com/jeffvestal)[@jeffvestal](https://github.com/jeffvestal)

[![](https://avatars.githubusercontent.com/u/32310964?u=56cd9386d632a330b8ecb180d7271b3d043c93a3&v=4)](https://github.com/VKudlay)[@VKudlay](https://github.com/VKudlay)

[![](https://avatars.githubusercontent.com/u/25208228?u=a89453c38529259ef0ac9c6fd2a695311a680386&v=4)](https://github.com/conceptofmind)[@conceptofmind](https://github.com/conceptofmind)

[![](https://avatars.githubusercontent.com/u/76683249?u=3f2d197d1391ab3d27cc8be37ac48e6912564204&v=4)](https://github.com/wenngong)[@wenngong](https://github.com/wenngong)

[![](https://avatars.githubusercontent.com/u/154643880?u=3792a3c4581984a90f91ab05f720fd3d7b647d5b&v=4)](https://github.com/raveharpaz)[@raveharpaz](https://github.com/raveharpaz)

[![](https://avatars.githubusercontent.com/u/22171838?u=a7c4ea3fcebeafc5e9857727974bf2a3362dafe4&v=4)](https://github.com/ruoccofabrizio)[@ruoccofabrizio](https://github.com/ruoccofabrizio)

[![](https://avatars.githubusercontent.com/u/14010132?u=7b08fe21105fd9835fe7e7c55a2174f2ec4d0a91&v=4)](https://github.com/aayush3011)[@aayush3011](https://github.com/aayush3011)

[![](https://avatars.githubusercontent.com/u/43506685?u=20e4be682f43988bd43b039a801856498e31c764&v=4)](https://github.com/Coniferish)[@Coniferish](https://github.com/Coniferish)

[![](https://avatars.githubusercontent.com/u/67210837?u=7e6d3db8c71e8fdd631017b8c9f6b83248923007&v=4)](https://github.com/KyrianC)[@KyrianC](https://github.com/KyrianC)

[![](https://avatars.githubusercontent.com/u/49201354?u=adef4744d1abcd52f751d21a30fbe52abddf9b94&v=4)](https://github.com/axiangcoding)[@axiangcoding](https://github.com/axiangcoding)

[![](https://avatars.githubusercontent.com/u/73353463?u=b07dac98e10a359f1a21dc08e61144e3671ca22f&v=4)](https://github.com/hmasdev)[@hmasdev](https://github.com/hmasdev)

[![](https://avatars.githubusercontent.com/u/2464556?u=4d6150c38daf305b43153112d1f2815d287273ea&v=4)](https://github.com/homanp)[@homanp](https://github.com/homanp)

[![](https://avatars.githubusercontent.com/u/10434946?u=517fa2eeadb35b317ec479f813126cbd1c5cc86a&v=4)](https://github.com/yakigac)[@yakigac](https://github.com/yakigac)

[![](https://avatars.githubusercontent.com/u/5001050?u=d5d0c24dc9566cec4b8e3cd376150c05b42c5210&v=4)](https://github.com/HunterGerlach)[@HunterGerlach](https://github.com/HunterGerlach)

[![](https://avatars.githubusercontent.com/u/753206?u=911ac7819a0dcf86bd5fd8ad8e4f986e22b8579b&v=4)](https://github.com/gkorland)[@gkorland](https://github.com/gkorland)

[![](https://avatars.githubusercontent.com/u/730013?v=4)](https://github.com/skozlovf)[@skozlovf](https://github.com/skozlovf)

[![](https://avatars.githubusercontent.com/u/77560236?u=54a3bf63360d61f6571015dd46fa1d03460fbbc9&v=4)](https://github.com/Gordon-BP)[@Gordon-BP](https://github.com/Gordon-BP)

[![](https://avatars.githubusercontent.com/u/18380243?u=746579a015b76842c0994cf04c623e683444fc90&v=4)](https://github.com/kzk-maeda)[@kzk-maeda](https://github.com/kzk-maeda)

[![](https://avatars.githubusercontent.com/u/12809212?u=8c1f0baf8a29f3007e3a51f5cf7b4a8e04c5ca8d&v=4)](https://github.com/parambharat)[@parambharat](https://github.com/parambharat)

[![](https://avatars.githubusercontent.com/u/737181?u=037bd3004f6c333108d7b32a4c8f96f2e8d57e78&v=4)](https://github.com/caseyclements)[@caseyclements](https://github.com/caseyclements)

[![](https://avatars.githubusercontent.com/u/8893086?u=220ec6df446248eeb09a59230c017a2c57bf8e61&v=4)](https://github.com/saginawj)[@saginawj](https://github.com/saginawj)

[![](https://avatars.githubusercontent.com/u/81822489?u=07badfd993685a278b1f929c1500a58837a6621d&v=4)](https://github.com/filip-halt)[@filip-halt](https://github.com/filip-halt)

[![](https://avatars.githubusercontent.com/u/40636930?u=b1f3735dccd19433cc3aad1b673553bf7eb94723&v=4)](https://github.com/zachschillaci27)[@zachschillaci27](https://github.com/zachschillaci27)

[![](https://avatars.githubusercontent.com/u/33070862?v=4)](https://github.com/cwlacewe)[@cwlacewe](https://github.com/cwlacewe)

[![](https://avatars.githubusercontent.com/u/3032459?u=590f1489107c91803bbe75de26cfeeeb77b25f8d&v=4)](https://github.com/nelly-hateva)[@nelly-hateva](https://github.com/nelly-hateva)

[![](https://avatars.githubusercontent.com/u/82044803?u=451c2955f0862cccf64cac30e062570d208d6903&v=4)](https://github.com/serena-ruan)[@serena-ruan](https://github.com/serena-ruan)

[![](https://avatars.githubusercontent.com/u/38650638?u=2b526137f18a7c41934c8da0722f1fedb74c3422&v=4)](https://github.com/wemysschen)[@wemysschen](https://github.com/wemysschen)

[![](https://avatars.githubusercontent.com/u/339166?u=9736b363a1200d66058ff83c58921b19d51c474f&v=4)](https://github.com/alexsherstinsky)[@alexsherstinsky](https://github.com/alexsherstinsky)

[![](https://avatars.githubusercontent.com/u/22759784?v=4)](https://github.com/zanderchase)[@zanderchase](https://github.com/zanderchase)

[![](https://avatars.githubusercontent.com/u/167348611?v=4)](https://github.com/dglogo)[@dglogo](https://github.com/dglogo)

[![](https://avatars.githubusercontent.com/u/5894042?u=e34704516e5f58e932ce098a38747a9be8d614a5&v=4)](https://github.com/danielhjz)[@danielhjz](https://github.com/danielhjz)

[![](https://avatars.githubusercontent.com/u/39944763?u=3074327b189542c2b47bb385b2d81d1e8ccb38e1&v=4)](https://github.com/os1ma)[@os1ma](https://github.com/os1ma)

[![](https://avatars.githubusercontent.com/u/112245?u=c129f9b2439b082cca4a7a322e558fca514bb87d&v=4)](https://github.com/cevian)[@cevian](https://github.com/cevian)

[![](https://avatars.githubusercontent.com/u/1309177?u=6328c998d93a48eba87c6b039783b8a7644c62c3&v=4)](https://github.com/charliermarsh)[@charliermarsh](https://github.com/charliermarsh)

[![](https://avatars.githubusercontent.com/u/63123596?u=ae18d496d5a6ced90d57c147f102f7c5ecf8e63f&v=4)](https://github.com/maximeperrindev)[@maximeperrindev](https://github.com/maximeperrindev)

[![](https://avatars.githubusercontent.com/u/3760?u=1dfde576ef286346afcc2a71eaf1fdb2857fb547&v=4)](https://github.com/bborn)[@bborn](https://github.com/bborn)

[![](https://avatars.githubusercontent.com/u/34462078?u=20243a60ac608142887c14251502c2a975614ba3&v=4)](https://github.com/raghavdixit99)[@raghavdixit99](https://github.com/raghavdixit99)

[![](https://avatars.githubusercontent.com/u/35945268?u=4379ecd5062eea0f6449c520ddde5fe1e3724500&v=4)](https://github.com/junkeon)[@junkeon](https://github.com/junkeon)

[![](https://avatars.githubusercontent.com/u/129657162?u=353d87b0e8d4c628536e2e40a34a7622dc3c18ab&v=4)](https://github.com/jj701)[@jj701](https://github.com/jj701)

[![](https://avatars.githubusercontent.com/u/26039352?v=4)](https://github.com/cauwulixuan)[@cauwulixuan](https://github.com/cauwulixuan)

[![](https://avatars.githubusercontent.com/u/6406557?v=4)](https://github.com/markcusack)[@markcusack](https://github.com/markcusack)

[![](https://avatars.githubusercontent.com/u/24482442?u=d6095b9533599b26d16fe6273d8f513206976a62&v=4)](https://github.com/rohanaggarwal7997)[@rohanaggarwal7997](https://github.com/rohanaggarwal7997)

[![](https://avatars.githubusercontent.com/u/347398?v=4)](https://github.com/delip)[@delip](https://github.com/delip)

[![](https://avatars.githubusercontent.com/u/757060?u=0c7583422d4c2b5572616f9e542e110bf5dd15f7&v=4)](https://github.com/ichernev)[@ichernev](https://github.com/ichernev)

[![](https://avatars.githubusercontent.com/u/5794505?u=2f702037cd4ee069d1a3156d9521ce5dec25249f&v=4)](https://github.com/MartinKolbAtWork)[@MartinKolbAtWork](https://github.com/MartinKolbAtWork)

[![](https://avatars.githubusercontent.com/u/1812592?v=4)](https://github.com/kennethchoe)[@kennethchoe](https://github.com/kennethchoe)

[![](https://avatars.githubusercontent.com/u/70973560?u=1a40b7be391714894999b7412de2e281abad530e&v=4)](https://github.com/amiaxys)[@amiaxys](https://github.com/amiaxys)

[![](https://avatars.githubusercontent.com/u/891664?u=722172a0061f68ab22819fa88a354ec973f70a63&v=4)](https://github.com/jeffchuber)[@jeffchuber](https://github.com/jeffchuber)

[![](https://avatars.githubusercontent.com/u/1995599?v=4)](https://github.com/shane-huang)[@shane-huang](https://github.com/shane-huang)

[![](https://avatars.githubusercontent.com/u/14149230?u=ca710ca2a64391470163ddef6b5ea7633ab26872&v=4)](https://github.com/cbh123)[@cbh123](https://github.com/cbh123)

[![](https://avatars.githubusercontent.com/u/17517367?u=b745b5f2016fbf166a75ce6ec18853c2fe7bbf12&v=4)](https://github.com/sdelgadoc)[@sdelgadoc](https://github.com/sdelgadoc)

[![](https://avatars.githubusercontent.com/u/63742054?u=befe4ae74b906698be965bad482d0e02fc7707ab&v=4)](https://github.com/Nutlope)[@Nutlope](https://github.com/Nutlope)

[![](https://avatars.githubusercontent.com/u/951187?u=e80c215810058f57145042d12360d463e3a53443&v=4)](https://github.com/jirimoravcik)[@jirimoravcik](https://github.com/jirimoravcik)

[![](https://avatars.githubusercontent.com/u/75213811?v=4)](https://github.com/kitrak-rev)[@kitrak-rev](https://github.com/kitrak-rev)

[![](https://avatars.githubusercontent.com/u/3045965?u=3d3c34259d50723955dd92d1de5be21236989356&v=4)](https://github.com/chadj2)[@chadj2](https://github.com/chadj2)

[![](https://avatars.githubusercontent.com/u/1157440?u=2f81a28298c1172e732898a1f8e800342434801d&v=4)](https://github.com/tazarov)[@tazarov](https://github.com/tazarov)

[![](https://avatars.githubusercontent.com/u/57731498?u=fec622b37ca3dc04125144116ad5165f37f85823&v=4)](https://github.com/mattgotteiner)[@mattgotteiner](https://github.com/mattgotteiner)

[![](https://avatars.githubusercontent.com/u/85610855?v=4)](https://github.com/am-kinetica)[@am-kinetica](https://github.com/am-kinetica)

[![](https://avatars.githubusercontent.com/u/139942740?u=fa99ca083ccdc7322c7b24f8a3c001e71be347b4&v=4)](https://github.com/baichuan-assistant)[@baichuan-assistant](https://github.com/baichuan-assistant)

[![](https://avatars.githubusercontent.com/u/22965499?u=36d1ebd75bca4cb50c578fef6faed9357cfef86a&v=4)](https://github.com/sfvaroglu)[@sfvaroglu](https://github.com/sfvaroglu)

[![](https://avatars.githubusercontent.com/u/116604821?u=ec1518c27a7a15f33a138cf0b956ef1758edbaff&v=4)](https://github.com/sfc-gh-jcarroll)[@sfc-gh-jcarroll](https://github.com/sfc-gh-jcarroll)

[![](https://avatars.githubusercontent.com/u/20006225?u=b5c543736384589fcb5b547f0d7700e545cb41ba&v=4)](https://github.com/jeffzwang)[@jeffzwang](https://github.com/jeffzwang)

[![](https://avatars.githubusercontent.com/u/128378696?u=8c818bd39c9cd75b606f3b5b1479787e4e6845d9&v=4)](https://github.com/BeatrixCohere)[@BeatrixCohere](https://github.com/BeatrixCohere)

[![](https://avatars.githubusercontent.com/u/1465768?u=642a7b963f24fd2caaa744eee90a7157322e22db&v=4)](https://github.com/mainred)[@mainred](https://github.com/mainred)

[![](https://avatars.githubusercontent.com/u/57228345?v=4)](https://github.com/CahidArda)[@CahidArda](https://github.com/CahidArda)

[![](https://avatars.githubusercontent.com/u/38215315?u=3985b6a3ecb0e8338c5912ea9e20787152d0ad7a&v=4)](https://github.com/P-E-B)[@P-E-B](https://github.com/P-E-B)

[![](https://avatars.githubusercontent.com/u/43734688?u=78f139fa940620e301361a58821c9f56128f71d9&v=4)](https://github.com/sam-h-bean)[@sam-h-bean](https://github.com/sam-h-bean)

[![](https://avatars.githubusercontent.com/u/60664495?u=ace0011a868848b48cdf9c199110dc8e5be5f433&v=4)](https://github.com/williamdevena)[@williamdevena](https://github.com/williamdevena)

[![](https://avatars.githubusercontent.com/u/31483888?u=55359c6f832dfed3abf0e89ea9842ec88849341d&v=4)](https://github.com/filip-michalsky)[@filip-michalsky](https://github.com/filip-michalsky)

[![](https://avatars.githubusercontent.com/u/3207674?v=4)](https://github.com/k8si)[@k8si](https://github.com/k8si)

[![](https://avatars.githubusercontent.com/u/7287580?u=5fe01002eec3d9df91ce3cef0016916554379efd&v=4)](https://github.com/edwardzjl)[@edwardzjl](https://github.com/edwardzjl)

[![](https://avatars.githubusercontent.com/u/26054637?u=edd1e4f54e91b549f2edb525d43210f4f04d7367&v=4)](https://github.com/paul-paliychuk)[@paul-paliychuk](https://github.com/paul-paliychuk)

[![](https://avatars.githubusercontent.com/u/4133076?u=f3f783e0364abe955dbde6af80445ea27d948fdd&v=4)](https://github.com/gregnr)[@gregnr](https://github.com/gregnr)

[![](https://avatars.githubusercontent.com/u/70665700?u=d7c78b0f3e6c5b1f359d574cd03bdb75bf6bf2da&v=4)](https://github.com/asamant21)[@asamant21](https://github.com/asamant21)

[![](https://avatars.githubusercontent.com/u/12044110?v=4)](https://github.com/sudranga)[@sudranga](https://github.com/sudranga)

[![](https://avatars.githubusercontent.com/u/5168949?v=4)](https://github.com/sseide)[@sseide](https://github.com/sseide)

[![](https://avatars.githubusercontent.com/u/216931?u=a8ca27d75e1765295ea9d23c191d8db834951066&v=4)](https://github.com/scottnath)[@scottnath](https://github.com/scottnath)

[![](https://avatars.githubusercontent.com/u/125713079?u=d42f76da6ffe0be48277c5ebdec4684ff1b38415&v=4)](https://github.com/AI-Bassem)[@AI-Bassem](https://github.com/AI-Bassem)

[![](https://avatars.githubusercontent.com/u/32453863?v=4)](https://github.com/BeautyyuYanli)[@BeautyyuYanli](https://github.com/BeautyyuYanli)

[![](https://avatars.githubusercontent.com/u/1074525?v=4)](https://github.com/gradenr)[@gradenr](https://github.com/gradenr)

[![](https://avatars.githubusercontent.com/u/4787922?u=dd4c7a18d86a6ad56455aa13e66daedbbbcf31b7&v=4)](https://github.com/zhaoshengbo)[@zhaoshengbo](https://github.com/zhaoshengbo)

[![](https://avatars.githubusercontent.com/u/14350521?u=4d5e9bb44d41a1ff30f2efbb2959a21e33644e81&v=4)](https://github.com/hakantekgul)[@hakantekgul](https://github.com/hakantekgul)

[![](https://avatars.githubusercontent.com/u/142571618?v=4)](https://github.com/eryk-dsai)[@eryk-dsai](https://github.com/eryk-dsai)

[![](https://avatars.githubusercontent.com/u/3469711?u=6962798c0280caa0d0260ccb8be1b18fb3ea44b2&v=4)](https://github.com/mrtj)[@mrtj](https://github.com/mrtj)

[![](https://avatars.githubusercontent.com/u/5069448?u=6b0ba426b68777f4935399013b7c2c112635c0df&v=4)](https://github.com/pcliupc)[@pcliupc](https://github.com/pcliupc)

[![](https://avatars.githubusercontent.com/u/36760800?u=12735f9035294180cb0b83446bdf7d8ac1a3fef9&v=4)](https://github.com/alvarobartt)[@alvarobartt](https://github.com/alvarobartt)

[![](https://avatars.githubusercontent.com/u/124558887?u=843f9f9de97097d85d0f685e0916d58196554421&v=4)](https://github.com/rogerserper)[@rogerserper](https://github.com/rogerserper)

[![](https://avatars.githubusercontent.com/u/320302?u=657574cdbadd4bfb4c8ed65f8646d4983d7ca5f0&v=4)](https://github.com/ekzhu)[@ekzhu](https://github.com/ekzhu)

[![](https://avatars.githubusercontent.com/u/139821907?u=f6f9648457adc2c15f407bb06d29089ae7e6f4cf&v=4)](https://github.com/ashleyxuu)[@ashleyxuu](https://github.com/ashleyxuu)

[![](https://avatars.githubusercontent.com/u/4036753?u=c6732c896b41c1ecec917bfae38aa6900585c632&v=4)](https://github.com/bhalder)[@bhalder](https://github.com/bhalder)

[![](https://avatars.githubusercontent.com/u/17904229?u=3c9fa8237a9d29136d3bd1dd2a380ff6dddb5d94&v=4)](https://github.com/ZixinYang)[@ZixinYang](https://github.com/ZixinYang)

[![](https://avatars.githubusercontent.com/u/48101485?u=dcf140777416a7d86a450964fc53ec5b17668603&v=4)](https://github.com/nikhilkjha)[@nikhilkjha](https://github.com/nikhilkjha)

[![](https://avatars.githubusercontent.com/u/43818888?u=0c01fad081c0abd23d2d49ea4496890ffbc22325&v=4)](https://github.com/Dominastorm)[@Dominastorm](https://github.com/Dominastorm)

[![](https://avatars.githubusercontent.com/u/13537446?v=4)](https://github.com/raunakshrivastava7)[@raunakshrivastava7](https://github.com/raunakshrivastava7)

[![](https://avatars.githubusercontent.com/u/121117945?v=4)](https://github.com/rodrigo-f-nogueira)[@rodrigo-f-nogueira](https://github.com/rodrigo-f-nogueira)

[![](https://avatars.githubusercontent.com/u/1585539?u=654a21985c875f78a20eda7e4884e8d64de86fba&v=4)](https://github.com/benjibc)[@benjibc](https://github.com/benjibc)

[![](https://avatars.githubusercontent.com/u/53276514?u=d08fad4653e8d1b89382507a07f6990437730433&v=4)](https://github.com/hoyungcher)[@hoyungcher](https://github.com/hoyungcher)

[![](https://avatars.githubusercontent.com/u/41710527?u=788f651d9933b36523feb431811a6531ecd994f1&v=4)](https://github.com/OwenPendrighElliott)[@OwenPendrighElliott](https://github.com/OwenPendrighElliott)

[![](https://avatars.githubusercontent.com/u/8142467?u=a62a20762c7fd841b470efc0ebdf5e1a01816f87&v=4)](https://github.com/Mikelarg)[@Mikelarg](https://github.com/Mikelarg)

[![](https://avatars.githubusercontent.com/u/10937540?u=fcc094d7dfef2d3778c989def06199d9dc84fb61&v=4)](https://github.com/freemso)[@freemso](https://github.com/freemso)

[![](https://avatars.githubusercontent.com/u/8862797?u=1856f20a3ac7425e75df7860bfd8934278fbdd53&v=4)](https://github.com/netoferraz)[@netoferraz](https://github.com/netoferraz)

[![](https://avatars.githubusercontent.com/u/3625100?u=b219abaae5763632a0edf8d79b46dca035f166a4&v=4)](https://github.com/zizhong)[@zizhong](https://github.com/zizhong)

[![](https://avatars.githubusercontent.com/u/81076998?v=4)](https://github.com/amicus-veritatis)[@amicus-veritatis](https://github.com/amicus-veritatis)

[![](https://avatars.githubusercontent.com/u/18572161?u=a09c7a053aa54cfc62ff8530c81486441215a09c&v=4)](https://github.com/MikeNitsenko)[@MikeNitsenko](https://github.com/MikeNitsenko)

[![](https://avatars.githubusercontent.com/u/7851093?u=ab3c2c9c6ebd0cd1cd3ff2f83f8618ab9b2550ad&v=4)](https://github.com/liangz1)[@liangz1](https://github.com/liangz1)

[![](https://avatars.githubusercontent.com/u/7953259?u=a451fad7ad197a8920651cf89aaf5d950734d0a8&v=4)](https://github.com/mikelambert)[@mikelambert](https://github.com/mikelambert)

[![](https://avatars.githubusercontent.com/u/23314389?u=2014e20e246530fa89bd902fe703b6f9e6ecf833&v=4)](https://github.com/nicoloboschi)[@nicoloboschi](https://github.com/nicoloboschi)

[![](https://avatars.githubusercontent.com/u/136885?u=9a42f56ad8055a03a5ae8a0272e66d1ae4ac083c&v=4)](https://github.com/mkorpela)[@mkorpela](https://github.com/mkorpela)

[![](https://avatars.githubusercontent.com/u/31125281?u=1bc56191c789906c2a11a4183c108b2784609015&v=4)](https://github.com/linancn)[@linancn](https://github.com/linancn)

[![](https://avatars.githubusercontent.com/u/101817?u=39f31ff29d2589046148c6ed1c1c923982d86b1a&v=4)](https://github.com/tsg)[@tsg](https://github.com/tsg)

[![](https://avatars.githubusercontent.com/u/51159628?u=5aec3cf0263e77234dd83f8e6bf4955e39acd472&v=4)](https://github.com/anar2706)[@anar2706](https://github.com/anar2706)

[![](https://avatars.githubusercontent.com/u/79988483?u=7b1cf8516362448115fc68870ad006a37a99d549&v=4)](https://github.com/yifeis7)[@yifeis7](https://github.com/yifeis7)

[![](https://avatars.githubusercontent.com/u/908389?v=4)](https://github.com/whitead)[@whitead](https://github.com/whitead)

[![](https://avatars.githubusercontent.com/u/89472452?u=47bcc0d72d51f2f914a759a0fde9ef3d1c677b98&v=4)](https://github.com/benitoThree)[@benitoThree](https://github.com/benitoThree)

[![](https://avatars.githubusercontent.com/u/3300000?v=4)](https://github.com/ruze00)[@ruze00](https://github.com/ruze00)

[![](https://avatars.githubusercontent.com/u/53417823?v=4)](https://github.com/HeChangHaoGary)[@HeChangHaoGary](https://github.com/HeChangHaoGary)

[![](https://avatars.githubusercontent.com/u/2851934?u=01c0d440fcb7fdb3159a7b641c58b5595028e9bc&v=4)](https://github.com/xiaoyuxee)[@xiaoyuxee](https://github.com/xiaoyuxee)

[![](https://avatars.githubusercontent.com/u/15706966?u=f6dd024f1fc955b7d411eb13ebcae7334b527063&v=4)](https://github.com/jerwelborn)[@jerwelborn](https://github.com/jerwelborn)

[![](https://avatars.githubusercontent.com/u/65446134?u=a292659bc2611825b65a56a7ee6bfe6fdbfa033b&v=4)](https://github.com/vairodp)[@vairodp](https://github.com/vairodp)

[![](https://avatars.githubusercontent.com/u/23406704?u=ac10555099789a8423dbc205ab4257b40aaf3860&v=4)](https://github.com/aletna)[@aletna](https://github.com/aletna)

[![](https://avatars.githubusercontent.com/u/2398765?u=0c438bd074b242c5896334e6da1f0801c2f581e4&v=4)](https://github.com/hsm207)[@hsm207](https://github.com/hsm207)

[![](https://avatars.githubusercontent.com/u/34411969?u=ae4aac513e377777fd6e46980e0e9414cdcd6f96&v=4)](https://github.com/DayuanJiang)[@DayuanJiang](https://github.com/DayuanJiang)

[![](https://avatars.githubusercontent.com/u/7080882?u=f985127fd58fa96b886d591ce104f29f3bd7f81f&v=4)](https://github.com/rigazilla)[@rigazilla](https://github.com/rigazilla)

[![](https://avatars.githubusercontent.com/u/4726889?u=1db838ee4066c26d5c0fa02311c7895c36969fb7&v=4)](https://github.com/apepkuss)[@apepkuss](https://github.com/apepkuss)

[![](https://avatars.githubusercontent.com/u/69025547?u=97202d8501d38ed5015cfb3c40cf0ba2daeb795c&v=4)](https://github.com/gadhagod)[@gadhagod](https://github.com/gadhagod)

[![](https://avatars.githubusercontent.com/u/839799?u=e12646ad6aa244d58cf5fa624dc2933c95acaad9&v=4)](https://github.com/LastMonopoly)[@LastMonopoly](https://github.com/LastMonopoly)

[![](https://avatars.githubusercontent.com/u/91019033?u=30944d2fcb8759eefe2efa26c4d07b218d25ae33&v=4)](https://github.com/matthewdeguzman)[@matthewdeguzman](https://github.com/matthewdeguzman)

[![](https://avatars.githubusercontent.com/u/13414571?u=c5490c987e1bcf8d47d7ecc4dca3812a21713f3a&v=4)](https://github.com/Tokkiu)[@Tokkiu](https://github.com/Tokkiu)

[![](https://avatars.githubusercontent.com/u/100361543?u=f022d60888add75594372c5e8ebb32fc7fdc2794&v=4)](https://github.com/softboyjimbo)[@softboyjimbo](https://github.com/softboyjimbo)

[![](https://avatars.githubusercontent.com/u/56953648?v=4)](https://github.com/Dobiichi-Origami)[@Dobiichi-Origami](https://github.com/Dobiichi-Origami)

[![](https://avatars.githubusercontent.com/u/96572405?u=7784695f37788fb8048f6ce213bf1df3d4713f2d&v=4)](https://github.com/zhanghexian)[@zhanghexian](https://github.com/zhanghexian)

[![](https://avatars.githubusercontent.com/u/117737297?u=0adf0f84cc345cc6e2ca3e4ad3c27a9ca8f53472&v=4)](https://github.com/rajtilakjee)[@rajtilakjee](https://github.com/rajtilakjee)

[![](https://avatars.githubusercontent.com/u/1983160?u=536f2558c6ac33b74a6d89520dcb27ba46954070&v=4)](https://github.com/ashvardanian)[@ashvardanian](https://github.com/ashvardanian)

[![](https://avatars.githubusercontent.com/u/4983896?u=4a0ba92f5b46b0c805a3c4715748f042a8c769a0&v=4)](https://github.com/plv)[@plv](https://github.com/plv)

[![](https://avatars.githubusercontent.com/u/872712?u=c6e76fb451e3a0c1528a8d0e95ef3ed669483690&v=4)](https://github.com/TomTom101)[@TomTom101](https://github.com/TomTom101)

[![](https://avatars.githubusercontent.com/u/47258766?u=4d98445aff6d752476aaf6ec88ec000496db7677&v=4)](https://github.com/lucas-tucker)[@lucas-tucker](https://github.com/lucas-tucker)

[![](https://avatars.githubusercontent.com/u/19825685?u=c9346281a8534aeaf9f112c0f7ca749de5cb8e23&v=4)](https://github.com/JoanFM)[@JoanFM](https://github.com/JoanFM)

[![](https://avatars.githubusercontent.com/u/829644?u=56a7fd939b2d15ed21011497db77ad3f569e8a60&v=4)](https://github.com/mengxr)[@mengxr](https://github.com/mengxr)

[![](https://avatars.githubusercontent.com/u/110841617?u=e473cda5a87ca1dae11082c11db9c1ed1f4c7032&v=4)](https://github.com/erika-cardenas)[@erika-cardenas](https://github.com/erika-cardenas)

[![](https://avatars.githubusercontent.com/u/43986145?u=3d15192e4d6ae36696e49e6c061d29f074f5ba77&v=4)](https://github.com/juliuslipp)[@juliuslipp](https://github.com/juliuslipp)

[![](https://avatars.githubusercontent.com/u/1078320?u=786a976f97c3b9a75bd7467579d77e303d2acc8d&v=4)](https://github.com/pors)[@pors](https://github.com/pors)

[![](https://avatars.githubusercontent.com/u/22906652?u=bee195145bb46c722da707939100f3a5a46fc8b9&v=4)](https://github.com/shivanimodi16)[@shivanimodi16](https://github.com/shivanimodi16)

[![](https://avatars.githubusercontent.com/u/11373553?u=cebc40130d1da9f7ac666a2f6237a3c1148f65ef&v=4)](https://github.com/thomas0809)[@thomas0809](https://github.com/thomas0809)

[![](https://avatars.githubusercontent.com/u/55012400?u=0a53d356ee0f3babed5fd7b3aec73a9e6b1724e6&v=4)](https://github.com/azamiftikhar1000)[@azamiftikhar1000](https://github.com/azamiftikhar1000)

[![](https://avatars.githubusercontent.com/u/135340?v=4)](https://github.com/alecf)[@alecf](https://github.com/alecf)

[![](https://avatars.githubusercontent.com/u/1555858?v=4)](https://github.com/prakul)[@prakul](https://github.com/prakul)

[![](https://avatars.githubusercontent.com/u/54161268?v=4)](https://github.com/Oscilloscope98)[@Oscilloscope98](https://github.com/Oscilloscope98)

[![](https://avatars.githubusercontent.com/u/6756744?u=f576bd2ad9bb2ebfc8d45feb4a49e8add9ae79dc&v=4)](https://github.com/ecneladis)[@ecneladis](https://github.com/ecneladis)

[![](https://avatars.githubusercontent.com/u/72488598?u=98dc24a63369cbae14913caff5f379f80f305aab&v=4)](https://github.com/Undertone0809)[@Undertone0809](https://github.com/Undertone0809)

[![](https://avatars.githubusercontent.com/u/45447813?u=6d1f8b455599848e6cd9c2410ba5f4f02d2d368c&v=4)](https://github.com/hetaoBackend)[@hetaoBackend](https://github.com/hetaoBackend)

[![](https://avatars.githubusercontent.com/u/16384755?v=4)](https://github.com/RichmondAlake)[@RichmondAlake](https://github.com/RichmondAlake)

[![](https://avatars.githubusercontent.com/u/1636116?u=617e8ebbd68598aada3a04642e7801c6b1dda152&v=4)](https://github.com/yackermann)[@yackermann](https://github.com/yackermann)

[![](https://avatars.githubusercontent.com/u/5798036?u=4eba31d63c3818d17fb8f9aa923599ac63ebfea8&v=4)](https://github.com/lesters)[@lesters](https://github.com/lesters)

[![](https://avatars.githubusercontent.com/u/115359769?v=4)](https://github.com/max-arthurai)[@max-arthurai](https://github.com/max-arthurai)

[![](https://avatars.githubusercontent.com/u/98474633?u=32ebf212dfc4d68c87f864c7d5bb9967ac85c96e&v=4)](https://github.com/philipkiely-baseten)[@philipkiely-baseten](https://github.com/philipkiely-baseten)

[![](https://avatars.githubusercontent.com/u/58721149?v=4)](https://github.com/dristysrivastava)[@dristysrivastava](https://github.com/dristysrivastava)

[![](https://avatars.githubusercontent.com/u/45048633?v=4)](https://github.com/schadem)[@schadem](https://github.com/schadem)

[![](https://avatars.githubusercontent.com/u/127325395?u=9e47d0d3ab45b1601c17f258229098681bb78911&v=4)](https://github.com/Aratako)[@Aratako](https://github.com/Aratako)

[![](https://avatars.githubusercontent.com/u/4067380?u=2776e796abeb0dfa8371dd528165ff0d96024a83&v=4)](https://github.com/anubhav94N)[@anubhav94N](https://github.com/anubhav94N)

[![](https://avatars.githubusercontent.com/u/81988348?v=4)](https://github.com/rithwik-db)[@rithwik-db](https://github.com/rithwik-db)

[![](https://avatars.githubusercontent.com/u/50788154?u=f924ef4e8d2b47be96f7a4b4357d17b6fafaea80&v=4)](https://github.com/kartheekyakkala)[@kartheekyakkala](https://github.com/kartheekyakkala)

[![](https://avatars.githubusercontent.com/u/105399924?u=e69e8f1af87a33af3ecbdd5b5d4327c6dc254df6&v=4)](https://github.com/jiayini1119)[@jiayini1119](https://github.com/jiayini1119)

[![](https://avatars.githubusercontent.com/u/11540660?u=efe357bf4cbe05c882528cc3ad78214776b80158&v=4)](https://github.com/shufanhao)[@shufanhao](https://github.com/shufanhao)

[![](https://avatars.githubusercontent.com/u/13724617?v=4)](https://github.com/zcgeng)[@zcgeng](https://github.com/zcgeng)

[![](https://avatars.githubusercontent.com/u/93145909?u=38b3ccf07a613963e9897627f940912128b7a83a&v=4)](https://github.com/ash0ts)[@ash0ts](https://github.com/ash0ts)

[![](https://avatars.githubusercontent.com/u/119620994?u=ac3dfad90764c69144f593023fce93080586702e&v=4)](https://github.com/Honkware)[@Honkware](https://github.com/Honkware)

[![](https://avatars.githubusercontent.com/u/4524535?u=6a41acd9f233fa9e62294d5534d1f2f52faa6b78&v=4)](https://github.com/dwhitena)[@dwhitena](https://github.com/dwhitena)

[![](https://avatars.githubusercontent.com/u/21286981?v=4)](https://github.com/SagarBM396)[@SagarBM396](https://github.com/SagarBM396)

[![](https://avatars.githubusercontent.com/u/88007022?u=1d49b0aa10dcff5b6661b211331334c165c56f28&v=4)](https://github.com/jamie256)[@jamie256](https://github.com/jamie256)

[![](https://avatars.githubusercontent.com/u/2283778?u=0c5a2a583bc77b138b346c5974551ac459059026&v=4)](https://github.com/yanghua)[@yanghua](https://github.com/yanghua)

[![](https://avatars.githubusercontent.com/u/160584887?v=4)](https://github.com/miri-bar)[@miri-bar](https://github.com/miri-bar)

[![](https://avatars.githubusercontent.com/u/62718109?u=ab38af3009ae3adcff49a309580e55bc6f586ba2&v=4)](https://github.com/klein-t)[@klein-t](https://github.com/klein-t)

[![](https://avatars.githubusercontent.com/u/13636019?v=4)](https://github.com/Ayan-Bandyopadhyay)[@Ayan-Bandyopadhyay](https://github.com/Ayan-Bandyopadhyay)

[![](https://avatars.githubusercontent.com/u/27293258?u=3349429e2b89bb75f144bb22c4015d9b676f3fca&v=4)](https://github.com/tugot17)[@tugot17](https://github.com/tugot17)

[![](https://avatars.githubusercontent.com/u/841146?v=4)](https://github.com/DaveDeCaprio)[@DaveDeCaprio](https://github.com/DaveDeCaprio)

[![](https://avatars.githubusercontent.com/u/13009163?u=c2b3a11cceaadbc9415f545b971250c9e2b2078b&v=4)](https://github.com/Spartee)[@Spartee](https://github.com/Spartee)

[![](https://avatars.githubusercontent.com/u/22459070?u=c541f86a16a5b46ae138a7bf1efdce36dd413f24&v=4)](https://github.com/Jflick58)[@Jflick58](https://github.com/Jflick58)

[![](https://avatars.githubusercontent.com/u/20140126?u=d1b9220a46efe488dc3db52e5d92774d85d38dfc&v=4)](https://github.com/JuHyung-Son)[@JuHyung-Son](https://github.com/JuHyung-Son)

[![](https://avatars.githubusercontent.com/u/949393?u=66d8768dc44519c956069acd88cfb1b0dca646f8&v=4)](https://github.com/stewartjarod)[@stewartjarod](https://github.com/stewartjarod)

[![](https://avatars.githubusercontent.com/u/807522?u=03c5551457e21c3c3a2dd8cbbe35eb6183feb3ed&v=4)](https://github.com/mkhludnev)[@mkhludnev](https://github.com/mkhludnev)

[![](https://avatars.githubusercontent.com/u/8279655?v=4)](https://github.com/cxumol)[@cxumol](https://github.com/cxumol)

[![](https://avatars.githubusercontent.com/u/31288628?u=acdfcef703b0d07b69e70e32e20130c05a56a549&v=4)](https://github.com/rihardsgravis)[@rihardsgravis](https://github.com/rihardsgravis)

[![](https://avatars.githubusercontent.com/u/31483498?u=aa8561cc1055386d7753a7f82bf823bbdbae4919&v=4)](https://github.com/kouroshHakha)[@kouroshHakha](https://github.com/kouroshHakha)

[![](https://avatars.githubusercontent.com/u/6432132?v=4)](https://github.com/samnoyes)[@samnoyes](https://github.com/samnoyes)

[![](https://avatars.githubusercontent.com/u/24364830?u=5e3cbf0c6171ba947d65fc9fa6fdff6612ae4af5&v=4)](https://github.com/ByronHsu)[@ByronHsu](https://github.com/ByronHsu)

[![](https://avatars.githubusercontent.com/u/28208564?u=ab938a1030cc6d630609a6d76b1ada65a3009020&v=4)](https://github.com/O-Roma)[@O-Roma](https://github.com/O-Roma)

[![](https://avatars.githubusercontent.com/u/808798?u=8a25786f1b28a0ddf171299eee7c14d9e9f2939b&v=4)](https://github.com/rowillia)[@rowillia](https://github.com/rowillia)

[![](https://avatars.githubusercontent.com/u/13447955?v=4)](https://github.com/lesterpjy)[@lesterpjy](https://github.com/lesterpjy)

[![](https://avatars.githubusercontent.com/u/19216250?u=85921f52a4be080e3529d87d3e3e75bf83847b24&v=4)](https://github.com/junefish)[@junefish](https://github.com/junefish)

[![](https://avatars.githubusercontent.com/u/107998986?u=70520f8a4ad962c0fc2706649ec401b274681927&v=4)](https://github.com/2jimoo)[@2jimoo](https://github.com/2jimoo)

[![](https://avatars.githubusercontent.com/u/55656?u=b9b6aa80966abd617ffed498f3a15b20d3644604&v=4)](https://github.com/petervandenabeele)[@petervandenabeele](https://github.com/petervandenabeele)

[![](https://avatars.githubusercontent.com/u/17451563?v=4)](https://github.com/shahrin014)[@shahrin014](https://github.com/shahrin014)

[![](https://avatars.githubusercontent.com/u/3849275?u=5de71c0b6eaea94c0460c1dc18a1a346168f8720&v=4)](https://github.com/shoelsch)[@shoelsch](https://github.com/shoelsch)

[![](https://avatars.githubusercontent.com/u/45851384?u=c9c158b6040b1fd8ae5543bad513260e157d5892&v=4)](https://github.com/h0rv)[@h0rv](https://github.com/h0rv)

[![](https://avatars.githubusercontent.com/u/18037290?u=dc663a40d4767317b6ab61e88221609e58e9371e&v=4)](https://github.com/asai95)[@asai95](https://github.com/asai95)

[![](https://avatars.githubusercontent.com/u/3195154?u=baa3820b95103662bc2aca01959e41aa651764b5&v=4)](https://github.com/mgoin)[@mgoin](https://github.com/mgoin)

[![](https://avatars.githubusercontent.com/u/23445657?u=84dda94e9330c5538ea94099b5cae699c88586f8&v=4)](https://github.com/Blaizzy)[@Blaizzy](https://github.com/Blaizzy)

[![](https://avatars.githubusercontent.com/u/38002468?u=dd6ba12322fa2ee0d88e83a3773c8abc13ec37af&v=4)](https://github.com/akmhmgc)[@akmhmgc](https://github.com/akmhmgc)

[![](https://avatars.githubusercontent.com/u/4693180?u=8cf781d9099d6e2f2d2caf7612a5c2811ba13ef8&v=4)](https://github.com/gmpetrov)[@gmpetrov](https://github.com/gmpetrov)

[![](https://avatars.githubusercontent.com/u/29749331?u=a7f4d7db2faa6af42af8d43b2737b5547d36154d&v=4)](https://github.com/aarnphm)[@aarnphm](https://github.com/aarnphm)

[![](https://avatars.githubusercontent.com/u/43019056?u=9066bb1f7b39a46309c387650c0ce5b7423f79da&v=4)](https://github.com/aMahanna)[@aMahanna](https://github.com/aMahanna)

[![](https://avatars.githubusercontent.com/u/39014459?u=122f72136fae112def3f40455aec55a5b99920f8&v=4)](https://github.com/hp0404)[@hp0404](https://github.com/hp0404)

[![](https://avatars.githubusercontent.com/u/2098020?u=0e1ecc0cc5eab98d93c0eaa7e210a1de937d95d9&v=4)](https://github.com/liushuaikobe)[@liushuaikobe](https://github.com/liushuaikobe)

[![](https://avatars.githubusercontent.com/u/115371133?u=a032d8cc4a47b9a25bc7a1699a73506bdb752ea2&v=4)](https://github.com/fserv)[@fserv](https://github.com/fserv)

[![](https://avatars.githubusercontent.com/u/5289083?u=d663551cd0b6e74091abd6272c35c9e02e82d6c0&v=4)](https://github.com/seanmavley)[@seanmavley](https://github.com/seanmavley)

[![](https://avatars.githubusercontent.com/u/37284105?u=be61bf8a5cef1060aeeb63a9bdd0a18f2edfe8d1&v=4)](https://github.com/cloudscool)[@cloudscool](https://github.com/cloudscool)

[![](https://avatars.githubusercontent.com/u/243665?u=4f7f2b3bbc666f530bf0e61bf6a4b32f5fcec433&v=4)](https://github.com/Lothiraldan)[@Lothiraldan](https://github.com/Lothiraldan)

[![](https://avatars.githubusercontent.com/u/2106106?u=e59f1d37d627161dc1739d290d1aedfb7348f1ab&v=4)](https://github.com/Ather23)[@Ather23](https://github.com/Ather23)

[![](https://avatars.githubusercontent.com/u/143642606?u=83091119b6b84c82b741298e9c9252161868bae7&v=4)](https://github.com/mogith-pn)[@mogith-pn](https://github.com/mogith-pn)

[![](https://avatars.githubusercontent.com/u/6266815?v=4)](https://github.com/JohnnyDeuss)[@JohnnyDeuss](https://github.com/JohnnyDeuss)

[![](https://avatars.githubusercontent.com/u/2021971?v=4)](https://github.com/eunhye1kim)[@eunhye1kim](https://github.com/eunhye1kim)

[![](https://avatars.githubusercontent.com/u/43149077?u=26d40f875b701db58f54af0441501c12e86dec6f&v=4)](https://github.com/dakinggg)[@dakinggg](https://github.com/dakinggg)

[![](https://avatars.githubusercontent.com/u/32113413?u=069f880e88a96db6ad955e3cc9fc7f9dfcf2beef&v=4)](https://github.com/jackwotherspoon)[@jackwotherspoon](https://github.com/jackwotherspoon)

[![](https://avatars.githubusercontent.com/u/79466298?u=875baabba4c879efd68332024a98fa093bdab2f2&v=4)](https://github.com/klaudialemiec)[@klaudialemiec](https://github.com/klaudialemiec)

[![](https://avatars.githubusercontent.com/u/4492530?u=142efae122e461996caa5cc6d41b9b5f0549c047&v=4)](https://github.com/philippe2803)[@philippe2803](https://github.com/philippe2803)

[![](https://avatars.githubusercontent.com/u/2644049?v=4)](https://github.com/wnleao)[@wnleao](https://github.com/wnleao)

[![](https://avatars.githubusercontent.com/u/160063452?v=4)](https://github.com/fzowl)[@fzowl](https://github.com/fzowl)

[![](https://avatars.githubusercontent.com/u/99611484?u=f421fe8a2917ae3ea24d83f056646055a00d3174&v=4)](https://github.com/kdcokenny)[@kdcokenny](https://github.com/kdcokenny)

[![](https://avatars.githubusercontent.com/u/3761730?u=287b813c1046e929d420e4aee8f4a27f9f816f60&v=4)](https://github.com/qtangs)[@qtangs](https://github.com/qtangs)

[![](https://avatars.githubusercontent.com/u/1651790?u=5a5ea37c495f7787f35172f0f86569daf5a5a65e&v=4)](https://github.com/wey-gu)[@wey-gu](https://github.com/wey-gu)

[![](https://avatars.githubusercontent.com/u/54905519?u=9818cccb258351fd0abec07b4acfb414a0383823&v=4)](https://github.com/Sukitly)[@Sukitly](https://github.com/Sukitly)

[![](https://avatars.githubusercontent.com/u/2951285?u=571c795227b4edbd29f027478346834f83a95076&v=4)](https://github.com/samber)[@samber](https://github.com/samber)

[![](https://avatars.githubusercontent.com/u/601530?u=ab242d6500886c4f8799101543d5b1f7841f1104&v=4)](https://github.com/Atry)[@Atry](https://github.com/Atry)

[![](https://avatars.githubusercontent.com/u/2700370?u=421c7cd75c8f7f1a28e6f6c19a5d587a6d478ed0&v=4)](https://github.com/chosh0615)[@chosh0615](https://github.com/chosh0615)

[![](https://avatars.githubusercontent.com/u/3009596?u=bbc154ae159c938e6e0c4045dc1b7980696b402a&v=4)](https://github.com/avsolatorio)[@avsolatorio](https://github.com/avsolatorio)

[![](https://avatars.githubusercontent.com/u/90301759?v=4)](https://github.com/19374242)[@19374242](https://github.com/19374242)

[![](https://avatars.githubusercontent.com/u/4491983?u=9265a9310ce2fa08b9429dc5d68da5b8677058ba&v=4)](https://github.com/leedotpang)[@leedotpang](https://github.com/leedotpang)

[![](https://avatars.githubusercontent.com/u/39889?u=bd28816c18beaddc4da762d61d842547fdb271d9&v=4)](https://github.com/yarikoptic)[@yarikoptic](https://github.com/yarikoptic)

[![](https://avatars.githubusercontent.com/u/52778543?u=504d8eb452ab2103a86ab469dd793eab49c8a437&v=4)](https://github.com/Jofthomas)[@Jofthomas](https://github.com/Jofthomas)

[![](https://avatars.githubusercontent.com/u/57748216?u=e2029e1262ee9c9d9f5825b2d28952758a628f28&v=4)](https://github.com/marlenezw)[@marlenezw](https://github.com/marlenezw)

[![](https://avatars.githubusercontent.com/u/23070692?u=bc8389d4c965994dee5b8cbadc420f8b4bcd5f0b&v=4)](https://github.com/rancomp)[@rancomp](https://github.com/rancomp)

[![](https://avatars.githubusercontent.com/u/1540803?v=4)](https://github.com/morganda)[@morganda](https://github.com/morganda)

[![](https://avatars.githubusercontent.com/u/1302641?u=643198eed0646ee2e18e22d6b6dab509bf9b2505&v=4)](https://github.com/atroyn)[@atroyn](https://github.com/atroyn)

[![](https://avatars.githubusercontent.com/u/48685774?v=4)](https://github.com/dmenini)[@dmenini](https://github.com/dmenini)

[![](https://avatars.githubusercontent.com/u/987457?u=a0dcd7b2cac59237d1ac2b43ca67a328ea7c437a&v=4)](https://github.com/brotchie)[@brotchie](https://github.com/brotchie)

[![](https://avatars.githubusercontent.com/u/32129522?u=a6fc430ee58b3ebe776dec5fce16b686f81c8e12&v=4)](https://github.com/angeligareta)[@angeligareta](https://github.com/angeligareta)

[![](https://avatars.githubusercontent.com/u/75265893?u=7f11152d07f1719da22084388c09b5fc64ab6c89&v=4)](https://github.com/ovuruska)[@ovuruska](https://github.com/ovuruska)

[![](https://avatars.githubusercontent.com/u/5279578?u=ce483437f50a425eab4b1f6f635ac49159f31576&v=4)](https://github.com/mmajewsk)[@mmajewsk](https://github.com/mmajewsk)

[![](https://avatars.githubusercontent.com/u/15167330?u=2d472fe5c7f09140dc62daac84d45a001b9de94f&v=4)](https://github.com/haydeniw)[@haydeniw](https://github.com/haydeniw)

[![](https://avatars.githubusercontent.com/u/3480154?u=f69c138e15366ba9c15cafd3c753a7ba7da44ad5&v=4)](https://github.com/wangwei1237)[@wangwei1237](https://github.com/wangwei1237)

[![](https://avatars.githubusercontent.com/u/116048415?v=4)](https://github.com/nimimeht)[@nimimeht](https://github.com/nimimeht)

[![](https://avatars.githubusercontent.com/u/5055697?v=4)](https://github.com/alexiri)[@alexiri](https://github.com/alexiri)

[![](https://avatars.githubusercontent.com/u/12781611?v=4)](https://github.com/rjanardhan3)[@rjanardhan3](https://github.com/rjanardhan3)

[![](https://avatars.githubusercontent.com/u/136875?u=611195240df6f68e816214bb865174384b74437e&v=4)](https://github.com/msaelices)[@msaelices](https://github.com/msaelices)

[![](https://avatars.githubusercontent.com/u/21985684?u=96e4830f5dfb5a4a6fcb504fddec997a50b56413&v=4)](https://github.com/SimFG)[@SimFG](https://github.com/SimFG)

[![](https://avatars.githubusercontent.com/u/16047967?v=4)](https://github.com/StankoKuveljic)[@StankoKuveljic](https://github.com/StankoKuveljic)

[![](https://avatars.githubusercontent.com/u/40655746?u=3c10115601fd5b032c3f274e79fd68dc5bb03921&v=4)](https://github.com/quchuyuan)[@quchuyuan](https://github.com/quchuyuan)

[![](https://avatars.githubusercontent.com/u/151817113?v=4)](https://github.com/sirjan-ws-ext)[@sirjan-ws-ext](https://github.com/sirjan-ws-ext)

[![](https://avatars.githubusercontent.com/u/147840?v=4)](https://github.com/anentropic)[@anentropic](https://github.com/anentropic)

[![](https://avatars.githubusercontent.com/u/65639964?u=6a48b9ecb8e188fee4117bffb055afb54566ba97&v=4)](https://github.com/EricLiclair)[@EricLiclair](https://github.com/EricLiclair)

[![](https://avatars.githubusercontent.com/u/23413676?u=b5bef760f9d067457f460d4dd5036f7e5f50d197&v=4)](https://github.com/hsuyuming)[@hsuyuming](https://github.com/hsuyuming)

[![](https://avatars.githubusercontent.com/u/1751809?u=b247b34fa5ccf9bb276ae318d57af47680994600&v=4)](https://github.com/asofter)[@asofter](https://github.com/asofter)

[![](https://avatars.githubusercontent.com/u/16456186?u=b9b30585eb3ddd0c8819bda9694636303c510233&v=4)](https://github.com/ThatsJustCheesy)[@ThatsJustCheesy](https://github.com/ThatsJustCheesy)

[![](https://avatars.githubusercontent.com/u/1621509?u=e54d671ddef5ac7580003427246fc2247964c9ed&v=4)](https://github.com/MacanPN)[@MacanPN](https://github.com/MacanPN)

[![](https://avatars.githubusercontent.com/u/31998003?u=0d91cde56e2c25d8ee7447bc55099e3dad047e99&v=4)](https://github.com/kristapratico)[@kristapratico](https://github.com/kristapratico)

[![](https://avatars.githubusercontent.com/u/7942293?u=6d5e295620df234b697f25d94659ae85d2dd2060&v=4)](https://github.com/imeckr)[@imeckr](https://github.com/imeckr)

[![](https://avatars.githubusercontent.com/u/8013575?u=13891ea430c3534079de533ea11aa92111300d0d&v=4)](https://github.com/christeefy)[@christeefy](https://github.com/christeefy)

[![](https://avatars.githubusercontent.com/u/7935430?v=4)](https://github.com/rc19)[@rc19](https://github.com/rc19)

[![](https://avatars.githubusercontent.com/u/3982077?u=8bbebac42cb84a25c629f83f212b2d099ffa3964&v=4)](https://github.com/anthonychu)[@anthonychu](https://github.com/anthonychu)

[![](https://avatars.githubusercontent.com/u/1664952?u=38196f73e9e69e2cc4f6d2e1207647af87bc440a&v=4)](https://github.com/h3l)[@h3l](https://github.com/h3l)

[![](https://avatars.githubusercontent.com/u/6726111?u=57f5f48085f552366bc8cf19ecd1d4ad0c66cd48&v=4)](https://github.com/JensMadsen)[@JensMadsen](https://github.com/JensMadsen)

[![](https://avatars.githubusercontent.com/u/61808204?v=4)](https://github.com/akiradev0x)[@akiradev0x](https://github.com/akiradev0x)

[![](https://avatars.githubusercontent.com/u/1385510?u=b3066560b9122fc5db6d2e80adf9d5c84113be15&v=4)](https://github.com/JonZeolla)[@JonZeolla](https://github.com/JonZeolla)

[![](https://avatars.githubusercontent.com/u/5136688?u=471ef01a31cc054f84abbe1b9e77ce07b2ac6853&v=4)](https://github.com/mlejva)[@mlejva](https://github.com/mlejva)

[![](https://avatars.githubusercontent.com/u/5564852?u=bb4393ab0f6ea892733e5fa10294207c1cf157f7&v=4)](https://github.com/msetbar)[@msetbar](https://github.com/msetbar)

[![](https://avatars.githubusercontent.com/u/120141355?u=c114874e969ef4e38c54d042fe1b9a69bc634483&v=4)](https://github.com/j-space-b)[@j-space-b](https://github.com/j-space-b)

[![](https://avatars.githubusercontent.com/u/50950969?u=f0c166782c1b8f63eb983383729b5d109d7bed0a&v=4)](https://github.com/chrispy-snps)[@chrispy-snps](https://github.com/chrispy-snps)

[![](https://avatars.githubusercontent.com/u/1863868?u=b00a9408d1433919780ea3248b3fc21258172152&v=4)](https://github.com/amosjyng)[@amosjyng](https://github.com/amosjyng)

[![](https://avatars.githubusercontent.com/u/38786?u=10a7cbcfb424bf45b3858017dc8cffae82adde29&v=4)](https://github.com/ninjapenguin)[@ninjapenguin](https://github.com/ninjapenguin)

[![](https://avatars.githubusercontent.com/u/12752197?u=f4f5d6c5b040422eaa987d0c7f441c65a1266db5&v=4)](https://github.com/dvonthenen)[@dvonthenen](https://github.com/dvonthenen)

[![](https://avatars.githubusercontent.com/u/51022808?u=15abba69b0bbc1e4b03d97769c37a58af637bd81&v=4)](https://github.com/Joffref)[@Joffref](https://github.com/Joffref)

[![](https://avatars.githubusercontent.com/u/56083056?v=4)](https://github.com/HamJaw1432)[@HamJaw1432](https://github.com/HamJaw1432)

[![](https://avatars.githubusercontent.com/u/171019460?v=4)](https://github.com/Anirudh31415926535)[@Anirudh31415926535](https://github.com/Anirudh31415926535)

[![](https://avatars.githubusercontent.com/u/538203?u=b3a13cce34acb23a3ef2808ee54c3461f2fa85bb&v=4)](https://github.com/cristobalcl)[@cristobalcl](https://github.com/cristobalcl)

[![](https://avatars.githubusercontent.com/u/17561003?u=76de0b85da74806eaad024ebc3315201ba49e867&v=4)](https://github.com/krrishdholakia)[@krrishdholakia](https://github.com/krrishdholakia)

[![](https://avatars.githubusercontent.com/u/27777173?u=4490be52549d8b6d2a662f35068b9a0d625b4b66&v=4)](https://github.com/samhita-alla)[@samhita-alla](https://github.com/samhita-alla)

[![](https://avatars.githubusercontent.com/u/3906177?u=3e7cb909eded61c3a35cb0e11336a70d0bc05534&v=4)](https://github.com/ralewis85)[@ralewis85](https://github.com/ralewis85)

[![](https://avatars.githubusercontent.com/u/6785029?v=4)](https://github.com/finnless)[@finnless](https://github.com/finnless)

[![](https://avatars.githubusercontent.com/u/45704090?u=fe471820f7f3939783ddea78efa0ef1f0d86288e&v=4)](https://github.com/felixocker)[@felixocker](https://github.com/felixocker)

[![](https://avatars.githubusercontent.com/u/433221?u=714ae935eadb460e1a7d41d7d29e26c7fed0bbbf&v=4)](https://github.com/brendancol)[@brendancol](https://github.com/brendancol)

[![](https://avatars.githubusercontent.com/u/34255899?u=05aba76f1912a56538c8a5141f8135d0e3b1e1bd&v=4)](https://github.com/gbaian10)[@gbaian10](https://github.com/gbaian10)

[![](https://avatars.githubusercontent.com/u/22055188?u=779840a35ef12f6734b630b1bdedd694132ec68f&v=4)](https://github.com/juliensalinas)[@juliensalinas](https://github.com/juliensalinas)

[![](https://avatars.githubusercontent.com/u/69706702?u=4fe850984b0956793de0a67c7ed9141168942eef&v=4)](https://github.com/muntaqamahmood)[@muntaqamahmood](https://github.com/muntaqamahmood)

[![](https://avatars.githubusercontent.com/u/11441526?u=bbd26dd43cf43212b0b05601ed5aaf29727f5d9f&v=4)](https://github.com/Fei-Wang)[@Fei-Wang](https://github.com/Fei-Wang)

[![](https://avatars.githubusercontent.com/u/45267439?u=d2ad5da7ef06e928644321e7a1cfd16842a897db&v=4)](https://github.com/jupyterjazz)[@jupyterjazz](https://github.com/jupyterjazz)

[![](https://avatars.githubusercontent.com/u/17061663?u=bee0295d999ddb902a98872fac6009bb88950132&v=4)](https://github.com/kooyunmo)[@kooyunmo](https://github.com/kooyunmo)

[![](https://avatars.githubusercontent.com/u/7340008?u=9473b1cdea8b9929771b32f14a28ad702237900c&v=4)](https://github.com/donbr)[@donbr](https://github.com/donbr)

[![](https://avatars.githubusercontent.com/u/22361806?u=c6b2eec689b859aeb182654e5e67936886d860bb&v=4)](https://github.com/jdogmcsteezy)[@jdogmcsteezy](https://github.com/jdogmcsteezy)

[![](https://avatars.githubusercontent.com/u/367522?u=2b439b16d48aaea7f17d1b3b0b24a9cb0b8712ed&v=4)](https://github.com/borisdev)[@borisdev](https://github.com/borisdev)

[![](https://avatars.githubusercontent.com/u/87140293?v=4)](https://github.com/thedavgar)[@thedavgar](https://github.com/thedavgar)

[![](https://avatars.githubusercontent.com/u/14931371?u=2f570f7591396a1ab8b58777746e2412e154fbfa&v=4)](https://github.com/jasonwcfan)[@jasonwcfan](https://github.com/jasonwcfan)

[![](https://avatars.githubusercontent.com/u/46003469?u=4f64d04035d962af0f72d20bffd6ea61635e728e&v=4)](https://github.com/yilmaz-burak)[@yilmaz-burak](https://github.com/yilmaz-burak)

[![](https://avatars.githubusercontent.com/u/8552242?v=4)](https://github.com/yessenzhar)[@yessenzhar](https://github.com/yessenzhar)

[![](https://avatars.githubusercontent.com/u/84070455?v=4)](https://github.com/pjb157)[@pjb157](https://github.com/pjb157)

[![](https://avatars.githubusercontent.com/u/202907?u=a1060b9fd298fd84b1adb7f6874c5c2012e782dc&v=4)](https://github.com/krasserm)[@krasserm](https://github.com/krasserm)

[![](https://avatars.githubusercontent.com/u/8673939?v=4)](https://github.com/NickL77)[@NickL77](https://github.com/NickL77)

[![](https://avatars.githubusercontent.com/u/10400064?u=52b50611d587317f397a96f898753099d65931f1&v=4)](https://github.com/mishushakov)[@mishushakov](https://github.com/mishushakov)

[![](https://avatars.githubusercontent.com/u/1508364?u=e75aca2de6de1a1e57329fc0c6430e1341904318&v=4)](https://github.com/flash1293)[@flash1293](https://github.com/flash1293)

[![](https://avatars.githubusercontent.com/u/6500104?u=c11cdf2671e89749d7d8c01f0d85494cce8d9f84&v=4)](https://github.com/Code-Hex)[@Code-Hex](https://github.com/Code-Hex)

[![](https://avatars.githubusercontent.com/u/22690160?u=50f2d8aa99bd7b12c01df29e8ffe519ed1cff1d5&v=4)](https://github.com/jnis23)[@jnis23](https://github.com/jnis23)

[![](https://avatars.githubusercontent.com/u/36752715?u=5137581b52bcbb8466b394f3ba40f97f9e273f52&v=4)](https://github.com/cgalo5758)[@cgalo5758](https://github.com/cgalo5758)

[![](https://avatars.githubusercontent.com/u/17325195?u=dadc287a6784258704affce9bf91e03e1bb967b4&v=4)](https://github.com/raymond-yuan)[@raymond-yuan](https://github.com/raymond-yuan)

[![](https://avatars.githubusercontent.com/u/5621432?u=bc4551beb3e89dda87d2f475b8652aacb20dabc8&v=4)](https://github.com/sunishsheth2009)[@sunishsheth2009](https://github.com/sunishsheth2009)

[![](https://avatars.githubusercontent.com/u/101966044?v=4)](https://github.com/klae01)[@klae01](https://github.com/klae01)

[![](https://avatars.githubusercontent.com/u/38317983?u=b169467874aeaf478132e46998ca895accfc008e&v=4)](https://github.com/LunarECL)[@LunarECL](https://github.com/LunarECL)

[![](https://avatars.githubusercontent.com/u/12080578?v=4)](https://github.com/whiskyboy)[@whiskyboy](https://github.com/whiskyboy)

[![](https://avatars.githubusercontent.com/u/66191792?v=4)](https://github.com/yuskhan)[@yuskhan](https://github.com/yuskhan)

[![](https://avatars.githubusercontent.com/u/62583018?v=4)](https://github.com/akashAD98)[@akashAD98](https://github.com/akashAD98)

[![](https://avatars.githubusercontent.com/u/45953733?u=b907b96d62f8cb2e75f3bba4f137d296d0d8a87f&v=4)](https://github.com/Shrined)[@Shrined](https://github.com/Shrined)

[![](https://avatars.githubusercontent.com/u/17435126?u=62bec61ef256194a3bb3ab238ab71d1792decd08&v=4)](https://github.com/DavidLMS)[@DavidLMS](https://github.com/DavidLMS)

[![](https://avatars.githubusercontent.com/u/4956442?u=fee6c76ff991cc9c12c4d703a1ad007e7634f58e&v=4)](https://github.com/rmkraus)[@rmkraus](https://github.com/rmkraus)

[![](https://avatars.githubusercontent.com/u/20266953?u=32853a0ed47a83525f3f21b4baf63891e0e3de15&v=4)](https://github.com/rawwar)[@rawwar](https://github.com/rawwar)

[![](https://avatars.githubusercontent.com/u/413669?u=25b5563194493db00c227a98e23f460adb13c9ea&v=4)](https://github.com/pmcfadin)[@pmcfadin](https://github.com/pmcfadin)

[![](https://avatars.githubusercontent.com/u/25740077?u=1c3b2b59a52f332dc22ef1787f2cdc67dc9fea5e&v=4)](https://github.com/tricktreat)[@tricktreat](https://github.com/tricktreat)

[![](https://avatars.githubusercontent.com/u/6334158?u=1d02d8cc173b20c7d18e11ac20a6f40081025fc3&v=4)](https://github.com/fzliu)[@fzliu](https://github.com/fzliu)

[![](https://avatars.githubusercontent.com/u/15992114?u=39c8ea0ffb9f48cec04f9b473f2801327e716ba1&v=4)](https://github.com/dongreenberg)[@dongreenberg](https://github.com/dongreenberg)

[![](https://avatars.githubusercontent.com/u/54540938?u=77dbfd10b709e203865f99668a4c79db04a69661&v=4)](https://github.com/aledelunap)[@aledelunap](https://github.com/aledelunap)

[![](https://avatars.githubusercontent.com/u/1155052?v=4)](https://github.com/stonekim)[@stonekim](https://github.com/stonekim)

[![](https://avatars.githubusercontent.com/u/62909360?v=4)](https://github.com/chip-davis)[@chip-davis](https://github.com/chip-davis)

[![](https://avatars.githubusercontent.com/u/6690727?u=d5742c8e658fe211a8987d9716838c34122485d0&v=4)](https://github.com/tonyabracadabra)[@tonyabracadabra](https://github.com/tonyabracadabra)

[![](https://avatars.githubusercontent.com/u/2857712?u=6809bef8bf07c46b39cd2fcd6027ed86e76372cd&v=4)](https://github.com/machulav)[@machulav](https://github.com/machulav)

[![](https://avatars.githubusercontent.com/u/12604876?u=a441926ef7f4dbc48fc3a1511f3ae5cb4279c464&v=4)](https://github.com/shauryr)[@shauryr](https://github.com/shauryr)

[![](https://avatars.githubusercontent.com/u/42373772?v=4)](https://github.com/PawelFaron)[@PawelFaron](https://github.com/PawelFaron)

[![](https://avatars.githubusercontent.com/u/104267837?u=762d6b00291c68379d66260d7b644942e3bab891&v=4)](https://github.com/lvliang-intel)[@lvliang-intel](https://github.com/lvliang-intel)

[![](https://avatars.githubusercontent.com/u/1909351?u=26bf36601ef34e0aa8a846ff3c98eb077987e882&v=4)](https://github.com/balvisio)[@balvisio](https://github.com/balvisio)

[![](https://avatars.githubusercontent.com/u/8972416?u=8cef7c30a819e5157bece1f1e06a50beab52845f&v=4)](https://github.com/xinqiu)[@xinqiu](https://github.com/xinqiu)

[![](https://avatars.githubusercontent.com/u/30035387?u=38717fe5778531ee96e5fc6e4a350668b5024d1c&v=4)](https://github.com/MikeMcGarry)[@MikeMcGarry](https://github.com/MikeMcGarry)

[![](https://avatars.githubusercontent.com/u/20807672?u=f2efe9788ce26442bb3319da1a56081d64c359e5&v=4)](https://github.com/robcaulk)[@robcaulk](https://github.com/robcaulk)

[![](https://avatars.githubusercontent.com/u/37783831?u=5697294c9a0c5bcca4df1aafd22cf8ab64081f2f&v=4)](https://github.com/jagilley)[@jagilley](https://github.com/jagilley)

[![](https://avatars.githubusercontent.com/u/35005448?u=4b6efd3d2dcdc2acde843cff4183b59087f35a9b&v=4)](https://github.com/prrao87)[@prrao87](https://github.com/prrao87)

[![](https://avatars.githubusercontent.com/u/31956487?u=4693ce4d533d97386b62851f6790881306cb88bc&v=4)](https://github.com/lujingxuansc)[@lujingxuansc](https://github.com/lujingxuansc)

[![](https://avatars.githubusercontent.com/u/15329913?u=d6a01e3a63eb3ef04e5917f994fc2f809f28dd13&v=4)](https://github.com/mplachter)[@mplachter](https://github.com/mplachter)

[![](https://avatars.githubusercontent.com/u/46458320?u=f752991f6c37b213ad11fdae5bf7820aa59b93d0&v=4)](https://github.com/jvelezmagic)[@jvelezmagic](https://github.com/jvelezmagic)

[![](https://avatars.githubusercontent.com/u/50772274?u=5d63cb1b53e5702ea3dd12f865c3b9b252f37a02&v=4)](https://github.com/patrickloeber)[@patrickloeber](https://github.com/patrickloeber)

[![](https://avatars.githubusercontent.com/u/16231195?u=cb98dd7c537280ed31b53108f31286bd50989aea&v=4)](https://github.com/trancethehuman)[@trancethehuman](https://github.com/trancethehuman)

[![](https://avatars.githubusercontent.com/u/68764?v=4)](https://github.com/vadimgu)[@vadimgu](https://github.com/vadimgu)

[![](https://avatars.githubusercontent.com/u/146365078?v=4)](https://github.com/hulitaitai)[@hulitaitai](https://github.com/hulitaitai)

[![](https://avatars.githubusercontent.com/u/6885889?u=0b15031859ad908eb11af83878000ab09bed5609&v=4)](https://github.com/cjcjameson)[@cjcjameson](https://github.com/cjcjameson)

[![](https://avatars.githubusercontent.com/u/69208727?u=132c8ca18143866b79253a6fcbc10f58984f61ab&v=4)](https://github.com/aymeric-roucher)[@aymeric-roucher](https://github.com/aymeric-roucher)

[![](https://avatars.githubusercontent.com/u/24295927?u=27eee7ea85bd7dfd9e918245b96de8c757f5a620&v=4)](https://github.com/Sandy247)[@Sandy247](https://github.com/Sandy247)

[![](https://avatars.githubusercontent.com/u/3887295?u=55c8b3263df68b67f9b465c1758c78898f8b163b&v=4)](https://github.com/zoltan-fedor)[@zoltan-fedor](https://github.com/zoltan-fedor)

[![](https://avatars.githubusercontent.com/u/19657350?u=9847c9919a636e9d7022803e829ffd80008cb2d3&v=4)](https://github.com/berkedilekoglu)[@berkedilekoglu](https://github.com/berkedilekoglu)

[![](https://avatars.githubusercontent.com/u/141281053?u=e3ff32e9ae51ff0cca84b482fc1e6c80c28ab0c6&v=4)](https://github.com/rodrigo-clickup)[@rodrigo-clickup](https://github.com/rodrigo-clickup)

[![](https://avatars.githubusercontent.com/u/35718120?u=af59f3ac14a23d1f2e09942415ac07c10f3a3d05&v=4)](https://github.com/numb3r3)[@numb3r3](https://github.com/numb3r3)

[![](https://avatars.githubusercontent.com/u/42609308?u=3f7f530d338e33205815639ad3dfe7c244455728&v=4)](https://github.com/svdeepak99)[@svdeepak99](https://github.com/svdeepak99)

[![](https://avatars.githubusercontent.com/u/97558871?v=4)](https://github.com/ZyeG)[@ZyeG](https://github.com/ZyeG)

[![](https://avatars.githubusercontent.com/u/28337009?u=47c7e4318c5369bbc9f9cb719e7671336c83f15a&v=4)](https://github.com/itok01)[@itok01](https://github.com/itok01)

[![](https://avatars.githubusercontent.com/u/30483654?u=95e2c59c64c99e4ba77cffb8b2c180f7b44c6a74&v=4)](https://github.com/NoahStapp)[@NoahStapp](https://github.com/NoahStapp)

[![](https://avatars.githubusercontent.com/u/709022?v=4)](https://github.com/tconkling)[@tconkling](https://github.com/tconkling)

[![](https://avatars.githubusercontent.com/u/8368470?u=1b7aebda11db89d56b90ff89f9b108e3cd8bffe5&v=4)](https://github.com/thehapyone)[@thehapyone](https://github.com/thehapyone)

[![](https://avatars.githubusercontent.com/u/986859?u=54d240cfd5355bb0cfdaf4ac0a9589963ae9ccab&v=4)](https://github.com/toshish)[@toshish](https://github.com/toshish)

[![](https://avatars.githubusercontent.com/u/1087039?u=4439c00ef507bef0a99d82cdec33d6d0ed53d67c&v=4)](https://github.com/dremeika)[@dremeika](https://github.com/dremeika)

[![](https://avatars.githubusercontent.com/u/49049296?u=26427e6e1aa0a8ac20cc10594664b59a017f5287&v=4)](https://github.com/mingkang111)[@mingkang111](https://github.com/mingkang111)

[![](https://avatars.githubusercontent.com/u/13622183?u=c23256501191447d645cc03c1f6bc83282ef1498&v=4)](https://github.com/liaokongVFX)[@liaokongVFX](https://github.com/liaokongVFX)

[![](https://avatars.githubusercontent.com/u/36044389?u=e669016609aeb3e08e4f2a50f4faa163d633c073&v=4)](https://github.com/0xRaduan)[@0xRaduan](https://github.com/0xRaduan)

[![](https://avatars.githubusercontent.com/u/127370261?v=4)](https://github.com/apeng-singlestore)[@apeng-singlestore](https://github.com/apeng-singlestore)

[![](https://avatars.githubusercontent.com/u/252377?v=4)](https://github.com/jeffkit)[@jeffkit](https://github.com/jeffkit)

[![](https://avatars.githubusercontent.com/u/158216624?v=4)](https://github.com/xsai9101)[@xsai9101](https://github.com/xsai9101)

[![](https://avatars.githubusercontent.com/u/38943595?v=4)](https://github.com/issam9)[@issam9](https://github.com/issam9)

[![](https://avatars.githubusercontent.com/u/131272471?v=4)](https://github.com/CogniJT)[@CogniJT](https://github.com/CogniJT)

[![](https://avatars.githubusercontent.com/u/87355704?u=e98091da04c6bfe9af8d982938556832f03fb1fb&v=4)](https://github.com/ivyas21)[@ivyas21](https://github.com/ivyas21)

[![](https://avatars.githubusercontent.com/u/90619575?u=a99d480b1238cfdb2dabcd2fe60d1110518049d9&v=4)](https://github.com/florian-morel22)[@florian-morel22](https://github.com/florian-morel22)

[![](https://avatars.githubusercontent.com/u/16679979?u=ddfe3499c7008b4d7c5cf55677a8f6ef77c7abc1&v=4)](https://github.com/gdj0nes)[@gdj0nes](https://github.com/gdj0nes)

[![](https://avatars.githubusercontent.com/u/22898443?u=4e6aceb9132747788c4b6aca6c16027ee1109b01&v=4)](https://github.com/sdan)[@sdan](https://github.com/sdan)

[![](https://avatars.githubusercontent.com/u/16283396?v=4)](https://github.com/samching)[@samching](https://github.com/samching)

[![](https://avatars.githubusercontent.com/u/306671?u=27f910f1bdcdf18622fcccc138274be885cf1058&v=4)](https://github.com/lukestanley)[@lukestanley](https://github.com/lukestanley)

[![](https://avatars.githubusercontent.com/u/63134180?v=4)](https://github.com/IlyaKIS1)[@IlyaKIS1](https://github.com/IlyaKIS1)

[![](https://avatars.githubusercontent.com/u/4432788?u=6883ca123ef6ea5c06b6353183e4f92574b4e152&v=4)](https://github.com/dosuken123)[@dosuken123](https://github.com/dosuken123)

[![](https://avatars.githubusercontent.com/u/356014?u=e354dc99055acba9e834508f635e3e2d754bb30b&v=4)](https://github.com/wietsevenema)[@wietsevenema](https://github.com/wietsevenema)

[![](https://avatars.githubusercontent.com/u/157405112?u=f34aa80161ad2eab0db9255661f4bd7d685cbd0c&v=4)](https://github.com/gustavo-yt)[@gustavo-yt](https://github.com/gustavo-yt)

[![](https://avatars.githubusercontent.com/u/55749660?u=ce02778e563e11675fc7f4a7701d25a95296b437&v=4)](https://github.com/alexander1999-hub)[@alexander1999-hub](https://github.com/alexander1999-hub)

[![](https://avatars.githubusercontent.com/u/93204286?u=4b965586800fef342c6235fec47e9185b8ec1f81&v=4)](https://github.com/jonathanalgar)[@jonathanalgar](https://github.com/jonathanalgar)

[![](https://avatars.githubusercontent.com/u/28803103?u=c0b795ec14b5536f0e757faf1eca1c1900d1ef3c&v=4)](https://github.com/vsxd)[@vsxd](https://github.com/vsxd)

[![](https://avatars.githubusercontent.com/u/89905406?v=4)](https://github.com/amirai21)[@amirai21](https://github.com/amirai21)

[![](https://avatars.githubusercontent.com/u/17221195?u=6182ec534d25d1c9ffe1667bd78ea28fd0eea4c8&v=4)](https://github.com/var77)[@var77](https://github.com/var77)

[![](https://avatars.githubusercontent.com/u/54343137?u=0b69859aa8f8e5145d6fda66985a5c8a82c77524&v=4)](https://github.com/L-cloud)[@L-cloud](https://github.com/L-cloud)

[![](https://avatars.githubusercontent.com/u/88005863?v=4)](https://github.com/matiasjacob25)[@matiasjacob25](https://github.com/matiasjacob25)

[![](https://avatars.githubusercontent.com/u/130898843?u=0fedb2a0f0f82cd756dbd279e5fd34e2419d054c&v=4)](https://github.com/Haijian06)[@Haijian06](https://github.com/Haijian06)

[![](https://avatars.githubusercontent.com/u/1222232?v=4)](https://github.com/IlyaMichlin)[@IlyaMichlin](https://github.com/IlyaMichlin)

[![](https://avatars.githubusercontent.com/u/6346981?u=8ae43f7d588ffcc184df5948d2d034cc29dc1d7d&v=4)](https://github.com/dzmitry-kankalovich)[@dzmitry-kankalovich](https://github.com/dzmitry-kankalovich)

[![](https://avatars.githubusercontent.com/u/13366849?u=9f66646c23def822aac7d3dfecb49369bc8cdf7b&v=4)](https://github.com/EniasCailliau)[@EniasCailliau](https://github.com/EniasCailliau)

[![](https://avatars.githubusercontent.com/u/68635?u=0ebec81cc881b2428e2c45e549a1081e5fe3cddf&v=4)](https://github.com/kreneskyp)[@kreneskyp](https://github.com/kreneskyp)

[![](https://avatars.githubusercontent.com/u/4441850?u=532666e949309d38a33cda7b1e8b5f30fee0ef7c&v=4)](https://github.com/rsharath)[@rsharath](https://github.com/rsharath)

[![](https://avatars.githubusercontent.com/u/21039333?u=bba2c2d18d3a5ef41360778a7679662565f326d2&v=4)](https://github.com/izapolsk)[@izapolsk](https://github.com/izapolsk)

[![](https://avatars.githubusercontent.com/u/30639818?v=4)](https://github.com/rjadr)[@rjadr](https://github.com/rjadr)

[![](https://avatars.githubusercontent.com/u/17973367?u=135d566bd1e620e230b94bf5252acea571ba510f&v=4)](https://github.com/Lord-Haji)[@Lord-Haji](https://github.com/Lord-Haji)

[![](https://avatars.githubusercontent.com/u/85796?u=d66bb48107582804e6665cd33540cce5dea2fd8b&v=4)](https://github.com/woodworker)[@woodworker](https://github.com/woodworker)

[![](https://avatars.githubusercontent.com/u/32632186?u=3e1b1b0d8cc37c998508e3ab83dc20ef1e2f57e0&v=4)](https://github.com/philschmid)[@philschmid](https://github.com/philschmid)

[![](https://avatars.githubusercontent.com/u/13198452?v=4)](https://github.com/ChrKahl)[@ChrKahl](https://github.com/ChrKahl)

[![](https://avatars.githubusercontent.com/u/8433665?u=5adeb0f5b7d3af0f5f6149bc09076fd37f964e00&v=4)](https://github.com/bongsang)[@bongsang](https://github.com/bongsang)

[![](https://avatars.githubusercontent.com/u/49571870?v=4)](https://github.com/clwillhuang)[@clwillhuang](https://github.com/clwillhuang)

[![](https://avatars.githubusercontent.com/u/3122709?u=55c1160c7f870bcc582d2e0be42d5b1054262e04&v=4)](https://github.com/BidhanRoy)[@BidhanRoy](https://github.com/BidhanRoy)

[![](https://avatars.githubusercontent.com/u/108248080?v=4)](https://github.com/finger-bone)[@finger-bone](https://github.com/finger-bone)

[![](https://avatars.githubusercontent.com/u/26385522?v=4)](https://github.com/hiigao)[@hiigao](https://github.com/hiigao)

[![](https://avatars.githubusercontent.com/u/152659506?v=4)](https://github.com/samkhano1)[@samkhano1](https://github.com/samkhano1)

[![](https://avatars.githubusercontent.com/u/45119610?u=27b4bbe257e0cc055c70f05dc6f45e95d5b09d08&v=4)](https://github.com/ireneisdoomed)[@ireneisdoomed](https://github.com/ireneisdoomed)

[![](https://avatars.githubusercontent.com/u/12946725?u=42a21426742352cfbc210619eed7e76bc1bb5b22&v=4)](https://github.com/mahaddad)[@mahaddad](https://github.com/mahaddad)

[![](https://avatars.githubusercontent.com/u/44347519?v=4)](https://github.com/nicolasnk)[@nicolasnk](https://github.com/nicolasnk)

[![](https://avatars.githubusercontent.com/u/31326650?u=a4db4389f5fa6d2cd4b9f99e38bc08d61f1d8962&v=4)](https://github.com/bovlb)[@bovlb](https://github.com/bovlb)

[![](https://avatars.githubusercontent.com/u/18024571?u=c0e12c9590b7e0838b4ab96544bc875e08db0729&v=4)](https://github.com/tomhamer)[@tomhamer](https://github.com/tomhamer)

[![](https://avatars.githubusercontent.com/u/1282617?u=940c2e3a241c82af68edc6adf81bc5da0fef0bbe&v=4)](https://github.com/haoch)[@haoch](https://github.com/haoch)

[![](https://avatars.githubusercontent.com/u/32279503?u=b760deecdb05c098c0e4e19944b72bc22c6487dc&v=4)](https://github.com/SlapDrone)[@SlapDrone](https://github.com/SlapDrone)

[![](https://avatars.githubusercontent.com/u/4302268?u=69a5af6602ab4faa803dcf60b2c50ed33cf44d89&v=4)](https://github.com/taranjeet)[@taranjeet](https://github.com/taranjeet)

[![](https://avatars.githubusercontent.com/u/7312176?u=d986a46c4971c5d15feea254801efc5deb0bc358&v=4)](https://github.com/Pixeladed)[@Pixeladed](https://github.com/Pixeladed)

[![](https://avatars.githubusercontent.com/u/8475708?v=4)](https://github.com/mlot)[@mlot](https://github.com/mlot)

[![](https://avatars.githubusercontent.com/u/7282984?u=d8c2341fa91e31f3d1d90ec2b79ce25bffbe6cdf&v=4)](https://github.com/JGalego)[@JGalego](https://github.com/JGalego)

[![](https://avatars.githubusercontent.com/u/21073184?u=deed6fe562ed425be66c210398811b664b5039a2&v=4)](https://github.com/xieqihui)[@xieqihui](https://github.com/xieqihui)

[![](https://avatars.githubusercontent.com/u/9324867?v=4)](https://github.com/mhavey)[@mhavey](https://github.com/mhavey)

[![](https://avatars.githubusercontent.com/u/4526224?u=3a47513ee686870ddcbecaa70756e3e8224732af&v=4)](https://github.com/praveenv)[@praveenv](https://github.com/praveenv)

[![](https://avatars.githubusercontent.com/u/1734012?u=105d7344bcd5c0dee1a293d2740cefa05cc46b9b&v=4)](https://github.com/srics)[@srics](https://github.com/srics)

[![](https://avatars.githubusercontent.com/u/31218485?u=6ce575b365c0353b5b3d1ea03088f8da36764100&v=4)](https://github.com/16BitNarwhal)[@16BitNarwhal](https://github.com/16BitNarwhal)

[![](https://avatars.githubusercontent.com/u/33707069?u=8587c5bd028774c9af3674197fedca2380bbe2f9&v=4)](https://github.com/zanussbaum)[@zanussbaum](https://github.com/zanussbaum)

[![](https://avatars.githubusercontent.com/u/12967560?v=4)](https://github.com/zhangch9)[@zhangch9](https://github.com/zhangch9)

[![](https://avatars.githubusercontent.com/u/37284051?u=5c467b29af9d77213b8f3173d5c0319bc79e353d&v=4)](https://github.com/paulonasc)[@paulonasc](https://github.com/paulonasc)

[![](https://avatars.githubusercontent.com/u/2008740?u=4c8824a259e14e56c2d3501e32a3422b258704c5&v=4)](https://github.com/rubell)[@rubell](https://github.com/rubell)

[![](https://avatars.githubusercontent.com/u/37992436?u=21693d9e841c3b7f9f091a210fbeee7e415a0751&v=4)](https://github.com/izzymsft)[@izzymsft](https://github.com/izzymsft)

[![](https://avatars.githubusercontent.com/u/22676399?u=6b46c5acfe16b722badbfa6845516c1627171bbe&v=4)](https://github.com/richarda23)[@richarda23](https://github.com/richarda23)

[![](https://avatars.githubusercontent.com/u/7711036?v=4)](https://github.com/zifeiq)[@zifeiq](https://github.com/zifeiq)

[![](https://avatars.githubusercontent.com/u/56812134?v=4)](https://github.com/liuyonghengheng)[@liuyonghengheng](https://github.com/liuyonghengheng)

[![](https://avatars.githubusercontent.com/u/18428646?u=d26db3c0411bd1d62c1dca99e5c86dd1f7a3b53d&v=4)](https://github.com/tomaspiaggio)[@tomaspiaggio](https://github.com/tomaspiaggio)

[![](https://avatars.githubusercontent.com/u/71321890?u=71a53f3a743fb8a91733e2a4cfcc05e309e3ef87&v=4)](https://github.com/klaus-xiong)[@klaus-xiong](https://github.com/klaus-xiong)

[![](https://avatars.githubusercontent.com/u/16155041?u=bf86e1dd4aaeccde8ccf12bf8c16c494644b84e1&v=4)](https://github.com/alallema)[@alallema](https://github.com/alallema)

[![](https://avatars.githubusercontent.com/u/8777479?v=4)](https://github.com/fengjial)[@fengjial](https://github.com/fengjial)

[![](https://avatars.githubusercontent.com/u/18065113?u=6ea1812de26ecb108c18e50b719a109049d93ce2&v=4)](https://github.com/simon824)[@simon824](https://github.com/simon824)

[![](https://avatars.githubusercontent.com/u/28787976?u=07c76df6dce5d38c056fb0783128844e6c70f4c4&v=4)](https://github.com/AksAman)[@AksAman](https://github.com/AksAman)

[![](https://avatars.githubusercontent.com/u/14037726?u=e91cfcdb7606db58b059893368f3cf70a2340f5f&v=4)](https://github.com/mewim)[@mewim](https://github.com/mewim)

[![](https://avatars.githubusercontent.com/u/4874?v=4)](https://github.com/ruanwz)[@ruanwz](https://github.com/ruanwz)

[![](https://avatars.githubusercontent.com/u/1921353?v=4)](https://github.com/gdedrouas)[@gdedrouas](https://github.com/gdedrouas)

[![](https://avatars.githubusercontent.com/u/1917451?u=03092c3ad1acdeec40390db850b2df9d813b6f83&v=4)](https://github.com/mariokostelac)[@mariokostelac](https://github.com/mariokostelac)

[![](https://avatars.githubusercontent.com/u/22236370?u=289c19bfc89a43a7e0c6956f73305aab3a8bd978&v=4)](https://github.com/mosheber)[@mosheber](https://github.com/mosheber)

[![](https://avatars.githubusercontent.com/u/8844262?u=1f09d2fe41756368730c3684fc819fbad940b4ac&v=4)](https://github.com/laplaceon)[@laplaceon](https://github.com/laplaceon)

[![](https://avatars.githubusercontent.com/u/11781950?u=a34a78ac4d9dcc25fd084f423566c9443c2cc47d&v=4)](https://github.com/thepycoder)[@thepycoder](https://github.com/thepycoder)

[![](https://avatars.githubusercontent.com/u/42592581?v=4)](https://github.com/toddkim95)[@toddkim95](https://github.com/toddkim95)

[![](https://avatars.githubusercontent.com/u/82586689?u=f10792bb4d6db272be85086ae5a8159f56d8a64f&v=4)](https://github.com/RafaelXokito)[@RafaelXokito](https://github.com/RafaelXokito)

[![](https://avatars.githubusercontent.com/u/950938?u=5283ce0f42f555abe0cd3eb9e45d23206c2ba6b8&v=4)](https://github.com/agamble)[@agamble](https://github.com/agamble)

[![](https://avatars.githubusercontent.com/u/67593995?u=cfda26bc17e98e1f0b1d7829acb010e0783fd8dc&v=4)](https://github.com/ThanhNguye-n)[@ThanhNguye-n](https://github.com/ThanhNguye-n)

[![](https://avatars.githubusercontent.com/u/3660805?u=d06f0923c5f6478f935e317ad901a0a3aef06903&v=4)](https://github.com/igor-drozdov)[@igor-drozdov](https://github.com/igor-drozdov)

[![](https://avatars.githubusercontent.com/u/13607221?u=dda5bc6c396f17d83ada9dc57e77e411903e5057&v=4)](https://github.com/KastanDay)[@KastanDay](https://github.com/KastanDay)

[![](https://avatars.githubusercontent.com/u/931697?u=4ce45d183c52828da0b4f0ca298d67ad970d43f6&v=4)](https://github.com/seanaedmiston)[@seanaedmiston](https://github.com/seanaedmiston)

[![](https://avatars.githubusercontent.com/u/3028543?u=5096311a70425e82c9b1a143d29ccd502c155a7f&v=4)](https://github.com/Randl)[@Randl](https://github.com/Randl)

[![](https://avatars.githubusercontent.com/u/115017354?v=4)](https://github.com/NikolaosPapailiou)[@NikolaosPapailiou](https://github.com/NikolaosPapailiou)

[![](https://avatars.githubusercontent.com/u/460966?v=4)](https://github.com/ebrehault)[@ebrehault](https://github.com/ebrehault)

[![](https://avatars.githubusercontent.com/u/6872942?v=4)](https://github.com/wlleiiwang)[@wlleiiwang](https://github.com/wlleiiwang)

[![](https://avatars.githubusercontent.com/u/32112894?u=d317c16ef9614adbeb3cf18ac39239c585db2264&v=4)](https://github.com/santiagxf)[@santiagxf](https://github.com/santiagxf)

[![](https://avatars.githubusercontent.com/u/30162978?v=4)](https://github.com/thehappydinoa)[@thehappydinoa](https://github.com/thehappydinoa)

[![](https://avatars.githubusercontent.com/u/12056337?u=93476ac29d44271b67ffbce7f055af1f6f294345&v=4)](https://github.com/abhiaagarwal)[@abhiaagarwal](https://github.com/abhiaagarwal)

[![](https://avatars.githubusercontent.com/u/30344258?u=51c169c8996024b68e9b3ec0bfe93465940dc8b4&v=4)](https://github.com/LMC117)[@LMC117](https://github.com/LMC117)

[![](https://avatars.githubusercontent.com/u/131612909?u=5ea799680a40107fcd922392026312204abb9fe8&v=4)](https://github.com/WilliamEspegren)[@WilliamEspegren](https://github.com/WilliamEspegren)

[![](https://avatars.githubusercontent.com/u/7380988?u=ba9beadb7fd3bcd6d8439154bedbd32d5fdbd4d8&v=4)](https://github.com/sunbc0120)[@sunbc0120](https://github.com/sunbc0120)

[![](https://avatars.githubusercontent.com/u/18614423?u=1d3dba8e4e87d2a449cc90c204f422327af2d09d&v=4)](https://github.com/Simon-Stone)[@Simon-Stone](https://github.com/Simon-Stone)

[![](https://avatars.githubusercontent.com/u/15304273?u=7588e8d8f8a889950b0afd00c2457ec3126ce8f6&v=4)](https://github.com/Amyh102)[@Amyh102](https://github.com/Amyh102)

[![](https://avatars.githubusercontent.com/u/67831673?v=4)](https://github.com/shumway743)[@shumway743](https://github.com/shumway743)

[![](https://avatars.githubusercontent.com/u/12097018?u=ef0ff38c5959d7e7acf2c87e8e8051ca2d047c76&v=4)](https://github.com/gcheron)[@gcheron](https://github.com/gcheron)

[![](https://avatars.githubusercontent.com/u/7102288?u=52db4849a0136c1d78cbc5a5de99ee0073384300&v=4)](https://github.com/zachdj)[@zachdj](https://github.com/zachdj)

[![](https://avatars.githubusercontent.com/u/6980212?u=89202482380b379837fd7318dde75a00e83d2459&v=4)](https://github.com/ehsanmok)[@ehsanmok](https://github.com/ehsanmok)

[![](https://avatars.githubusercontent.com/u/24109?u=5801e139c8d3bbf45557ba0f79d536f56b7716a8&v=4)](https://github.com/bsbodden)[@bsbodden](https://github.com/bsbodden)

[![](https://avatars.githubusercontent.com/u/16619882?u=ed851c7ccfa20588d3cd5ca47e79d94c3e4b6427&v=4)](https://github.com/Trevato)[@Trevato](https://github.com/Trevato)

[![](https://avatars.githubusercontent.com/u/13738772?u=1685c6916759c2ec986434af557343f6b29bce32&v=4)](https://github.com/raoufchebri)[@raoufchebri](https://github.com/raoufchebri)

[![](https://avatars.githubusercontent.com/u/492616?u=c2ecf6dac54322df081577f6b8e1ca390535c4a6&v=4)](https://github.com/delgermurun)[@delgermurun](https://github.com/delgermurun)

[![](https://avatars.githubusercontent.com/u/9665243?u=e403da70029d61dbbb9a2f0e03daebc5418974ed&v=4)](https://github.com/jcjc712)[@jcjc712](https://github.com/jcjc712)

[![](https://avatars.githubusercontent.com/u/5901336?u=be9c290e3f2214d93c5e788d7d950c8f63390f06&v=4)](https://github.com/yonarw)[@yonarw](https://github.com/yonarw)

[![](https://avatars.githubusercontent.com/u/9089568?u=d2f8bc466003afc3558a96f3266a0e32d5c18c34&v=4)](https://github.com/EvilFreelancer)[@EvilFreelancer](https://github.com/EvilFreelancer)

[![](https://avatars.githubusercontent.com/u/32046231?u=db454b8e6da48120d78d3397006928cc86f01019&v=4)](https://github.com/zywilliamli)[@zywilliamli](https://github.com/zywilliamli)

[![](https://avatars.githubusercontent.com/u/48098520?u=aa4a7287f484eb32d408360ca340c2f5bc8444d0&v=4)](https://github.com/thaiminhpv)[@thaiminhpv](https://github.com/thaiminhpv)

[![](https://avatars.githubusercontent.com/u/8139170?u=a63f55e62ad26febcd94e193c22bfd867d022af2&v=4)](https://github.com/paperMoose)[@paperMoose](https://github.com/paperMoose)

[![](https://avatars.githubusercontent.com/u/71520361?v=4)](https://github.com/younis-bash)[@younis-bash](https://github.com/younis-bash)

[![](https://avatars.githubusercontent.com/u/16340036?v=4)](https://github.com/rajib76)[@rajib76](https://github.com/rajib76)

[![](https://avatars.githubusercontent.com/u/48952237?v=4)](https://github.com/TejaHara)[@TejaHara](https://github.com/TejaHara)

[![](https://avatars.githubusercontent.com/u/11153261?u=a5af26e0bd60a27ba4aba60d15b129fc410fe8cc&v=4)](https://github.com/ihpolash)[@ihpolash](https://github.com/ihpolash)

[![](https://avatars.githubusercontent.com/u/123224380?v=4)](https://github.com/scadEfUr)[@scadEfUr](https://github.com/scadEfUr)

[![](https://avatars.githubusercontent.com/u/51324450?u=25a4838c93e6237e3b6d6ea1fbd23442cfba5723&v=4)](https://github.com/SauhaardW)[@SauhaardW](https://github.com/SauhaardW)

[![](https://avatars.githubusercontent.com/u/119924780?v=4)](https://github.com/pranava-amzn)[@pranava-amzn](https://github.com/pranava-amzn)

[![](https://avatars.githubusercontent.com/u/16321871?u=458c077105f3c19b5011617fc133386b7ddefe9f&v=4)](https://github.com/fynnfluegge)[@fynnfluegge](https://github.com/fynnfluegge)

[![](https://avatars.githubusercontent.com/u/2469198?u=43a8a9e376a5a7db6972e720906fd6f66560d235&v=4)](https://github.com/adilansari)[@adilansari](https://github.com/adilansari)

[![](https://avatars.githubusercontent.com/u/13305222?u=6d00fe3cfd2414a9e309540fe49f532fc0e503dd&v=4)](https://github.com/bstadt)[@bstadt](https://github.com/bstadt)

[![](https://avatars.githubusercontent.com/in/29110?v=4)](https://github.com/apps/dependabot)[@dependabot](https://github.com/apps/dependabot)

[![](https://avatars.githubusercontent.com/u/42089598?v=4)](https://github.com/PenghuiCheng)[@PenghuiCheng](https://github.com/PenghuiCheng)

[![](https://avatars.githubusercontent.com/u/145396613?u=f0da33ee8d74a5353a43f8df3332c9cac2bd70f8&v=4)](https://github.com/giannis2two)[@giannis2two](https://github.com/giannis2two)

[![](https://avatars.githubusercontent.com/u/107621925?u=4a7b06f4c0cac2534521698383f58331c00c093f&v=4)](https://github.com/anilaltuner)[@anilaltuner](https://github.com/anilaltuner)

[![](https://avatars.githubusercontent.com/u/144132509?u=42f5528898e3f4e3790bf432b8ca662dc347c778&v=4)](https://github.com/bu2kx)[@bu2kx](https://github.com/bu2kx)

[![](https://avatars.githubusercontent.com/u/32715913?u=5de749a141259c3fdd8a16c6438aff2b7823fd69&v=4)](https://github.com/AmineDjeghri)[@AmineDjeghri](https://github.com/AmineDjeghri)

[![](https://avatars.githubusercontent.com/u/46400934?u=fe4dad00d1e0e6cb555e21ce72bcffa8ec9bf684&v=4)](https://github.com/francesco-kruk)[@francesco-kruk](https://github.com/francesco-kruk)

[![](https://avatars.githubusercontent.com/u/1918816?v=4)](https://github.com/bakebrain)[@bakebrain](https://github.com/bakebrain)

[![](https://avatars.githubusercontent.com/u/5349024?u=4875b6589899edb51cb083d209bd9fbfac58da18&v=4)](https://github.com/bburgin)[@bburgin](https://github.com/bburgin)

[![](https://avatars.githubusercontent.com/u/4271525?u=4e1678b7ca97f74ff6e371d4e234224fd033c524&v=4)](https://github.com/coolbeevip)[@coolbeevip](https://github.com/coolbeevip)

[![](https://avatars.githubusercontent.com/u/2806769?u=2969d39e1099584bc34b9e91a718f97107b38cbc&v=4)](https://github.com/sreiswig)[@sreiswig](https://github.com/sreiswig)

[![](https://avatars.githubusercontent.com/u/134934501?u=167199ff0bff447057fc5e291be0225ad5260111&v=4)](https://github.com/vrushankportkey)[@vrushankportkey](https://github.com/vrushankportkey)

[![](https://avatars.githubusercontent.com/u/4852235?u=f3927adbaafbeaccd6fd4d0bc805414ef08b8b54&v=4)](https://github.com/jxnl)[@jxnl](https://github.com/jxnl)

[![](https://avatars.githubusercontent.com/u/8412519?u=391d663c51163f604c14bc625f4d6c11042a0c36&v=4)](https://github.com/arron2003)[@arron2003](https://github.com/arron2003)

[![](https://avatars.githubusercontent.com/u/17466553?u=2510816fc74e11bb543f54f97afe1c78e9bda720&v=4)](https://github.com/HashemAlsaket)[@HashemAlsaket](https://github.com/HashemAlsaket)

[![](https://avatars.githubusercontent.com/u/20924562?u=3f61dc32f82124727d7157c0977240770ab82c02&v=4)](https://github.com/ea-open-source)[@ea-open-source](https://github.com/ea-open-source)

[![](https://avatars.githubusercontent.com/u/1473079?v=4)](https://github.com/constantinmusca)[@constantinmusca](https://github.com/constantinmusca)

[![](https://avatars.githubusercontent.com/u/100479?u=5d055e42f37638939325d39ad857dc3a7b466533&v=4)](https://github.com/andrewmbenton)[@andrewmbenton](https://github.com/andrewmbenton)

[![](https://avatars.githubusercontent.com/u/74497693?u=0d49e69abc1f1c5299d479d943285fcac7eee1ae&v=4)](https://github.com/Subsegment)[@Subsegment](https://github.com/Subsegment)

[![](https://avatars.githubusercontent.com/u/15026857?u=a5129b6393cb746e25fca20655458d248ec4f05d&v=4)](https://github.com/zrcni)[@zrcni](https://github.com/zrcni)

[![](https://avatars.githubusercontent.com/u/191493?u=3e803364d95e760cafa108ab29ee109ba0e0af83&v=4)](https://github.com/piizei)[@piizei](https://github.com/piizei)

[![](https://avatars.githubusercontent.com/u/58871401?u=81f900fd6c286d9e8c5c8673f68b88387ed491e5&v=4)](https://github.com/RohanDey02)[@RohanDey02](https://github.com/RohanDey02)

[![](https://avatars.githubusercontent.com/u/57868915?u=d81c79687784073afcbb10984b7c8f4e5f9d839e&v=4)](https://github.com/therontau0054)[@therontau0054](https://github.com/therontau0054)

[![](https://avatars.githubusercontent.com/u/14224983?u=2a696ae181971f12ace4f252b759e1ca75ccdb44&v=4)](https://github.com/demjened)[@demjened](https://github.com/demjened)

[![](https://avatars.githubusercontent.com/u/3285355?u=8f91986cb97c2efcd84d62e339d8be43562de13d&v=4)](https://github.com/killinsun)[@killinsun](https://github.com/killinsun)

[![](https://avatars.githubusercontent.com/u/291370?u=5802ab31e0feb7ae15465dedaa48ba646f0a4127&v=4)](https://github.com/sanzgiri)[@sanzgiri](https://github.com/sanzgiri)

[![](https://avatars.githubusercontent.com/u/51958314?u=ff1c617481aa52a540a1b444280c9308beb83db5&v=4)](https://github.com/sp35)[@sp35](https://github.com/sp35)

[![](https://avatars.githubusercontent.com/u/82636823?u=e7e57044f6fca0b0255dfeac66a8c3ec660263c6&v=4)](https://github.com/Yash-1511)[@Yash-1511](https://github.com/Yash-1511)

[![](https://avatars.githubusercontent.com/u/20760062?u=422c372863e9c42406db2241e41cc52c522431ef&v=4)](https://github.com/abdalrohman)[@abdalrohman](https://github.com/abdalrohman)

[![](https://avatars.githubusercontent.com/u/3118964?u=471d785af68097fa9edeaa7bcd130b56ddda6338&v=4)](https://github.com/coyotespike)[@coyotespike](https://github.com/coyotespike)

[![](https://avatars.githubusercontent.com/u/1039756?u=1e32f3165c823547362784b17f65f7690b56e0b0&v=4)](https://github.com/zchenyu)[@zchenyu](https://github.com/zchenyu)

[![](https://avatars.githubusercontent.com/u/83261447?v=4)](https://github.com/yuwenzho)[@yuwenzho](https://github.com/yuwenzho)

[![](https://avatars.githubusercontent.com/u/132831962?u=d91bc0c46bc4c4df36d752076418530eea55a5dc&v=4)](https://github.com/ricki-epsilla)[@ricki-epsilla](https://github.com/ricki-epsilla)

[![](https://avatars.githubusercontent.com/u/2914618?v=4)](https://github.com/HassanOuda)[@HassanOuda](https://github.com/HassanOuda)

[![](https://avatars.githubusercontent.com/u/2215597?u=d5558c7d5c1ab6d4a8e5381826abd1f00371a5be&v=4)](https://github.com/s-udhaya)[@s-udhaya](https://github.com/s-udhaya)

[![](https://avatars.githubusercontent.com/u/5522060?v=4)](https://github.com/tesfagabir)[@tesfagabir](https://github.com/tesfagabir)

[![](https://avatars.githubusercontent.com/u/56334152?v=4)](https://github.com/chocolate4)[@chocolate4](https://github.com/chocolate4)

[![](https://avatars.githubusercontent.com/u/13938372?u=0e3f80aa515c41b7d9084b73d761cad378ebdc7a&v=4)](https://github.com/jasondotparse)[@jasondotparse](https://github.com/jasondotparse)

[![](https://avatars.githubusercontent.com/u/12449236?u=f13eba9cfa9baf8fa9a0fce667eb2fe429ecd298&v=4)](https://github.com/bwmatson)[@bwmatson](https://github.com/bwmatson)

[![](https://avatars.githubusercontent.com/u/38718601?u=44687611a0b7bd160ee129d04d4220d98f32ebab&v=4)](https://github.com/Daggx)[@Daggx](https://github.com/Daggx)

[![](https://avatars.githubusercontent.com/u/848849?v=4)](https://github.com/seth-hg)[@seth-hg](https://github.com/seth-hg)

[![](https://avatars.githubusercontent.com/u/34580718?u=cf4ff62610ff72ad9580d328e38f32e306d6150f&v=4)](https://github.com/NolanTrem)[@NolanTrem](https://github.com/NolanTrem)

[![](https://avatars.githubusercontent.com/u/9007876?v=4)](https://github.com/mpb159753)[@mpb159753](https://github.com/mpb159753)

[![](https://avatars.githubusercontent.com/u/800430?v=4)](https://github.com/mikeknoop)[@mikeknoop](https://github.com/mikeknoop)

[![](https://avatars.githubusercontent.com/u/57349093?v=4)](https://github.com/datelier)[@datelier](https://github.com/datelier)

[![](https://avatars.githubusercontent.com/u/9028897?v=4)](https://github.com/jakerachleff)[@jakerachleff](https://github.com/jakerachleff)

[![](https://avatars.githubusercontent.com/u/13024750?u=6ae631199ec7c0bb34eb8d56200023cdd94720d3&v=4)](https://github.com/JamsheedMistri)[@JamsheedMistri](https://github.com/JamsheedMistri)

[![](https://avatars.githubusercontent.com/u/42374034?u=cfb14ff1a7c4f0a500cd9c282bc3fbcba170daef&v=4)](https://github.com/atherfawaz)[@atherfawaz](https://github.com/atherfawaz)

[![](https://avatars.githubusercontent.com/u/6012338?u=198f10817236beac03b10bb8f5cc6d7fcb133cc7&v=4)](https://github.com/Hugoberry)[@Hugoberry](https://github.com/Hugoberry)

[![](https://avatars.githubusercontent.com/u/54216004?u=6a387166a0e8599c4f3ff35f61c12458df539f96&v=4)](https://github.com/Haris-Ali007)[@Haris-Ali007](https://github.com/Haris-Ali007)

[![](https://avatars.githubusercontent.com/u/52078762?u=60001db024892c1a54ea9c91d2d6f1befbbe53df&v=4)](https://github.com/AlpinDale)[@AlpinDale](https://github.com/AlpinDale)

[![](https://avatars.githubusercontent.com/u/70274018?u=b6d5fd627cd26f590ed442d4dffa5bdddcb803cc&v=4)](https://github.com/jjovalle99)[@jjovalle99](https://github.com/jjovalle99)

[![](https://avatars.githubusercontent.com/u/7529846?u=bd1b12fa55583ac7f01c4440cad87163a0fe3c19&v=4)](https://github.com/DN6)[@DN6](https://github.com/DN6)

[![](https://avatars.githubusercontent.com/u/83648453?u=9cb34e2b4ef5be7311fc0cd9949280542f8947d8&v=4)](https://github.com/spike-spiegel-21)[@spike-spiegel-21](https://github.com/spike-spiegel-21)

[![](https://avatars.githubusercontent.com/u/91102080?u=c87d3f88e6b05445a121c204a0d39a0b9ec17e05&v=4)](https://github.com/mziru)[@mziru](https://github.com/mziru)

[![](https://avatars.githubusercontent.com/u/56706206?v=4)](https://github.com/Dylan20XX)[@Dylan20XX](https://github.com/Dylan20XX)

[![](https://avatars.githubusercontent.com/u/8936233?u=07eb2625319cd0fd18df747fcdeef42cd9fc981d&v=4)](https://github.com/xingfanxia)[@xingfanxia](https://github.com/xingfanxia)

[![](https://avatars.githubusercontent.com/u/16275535?v=4)](https://github.com/NoahBPeterson)[@NoahBPeterson](https://github.com/NoahBPeterson)

[![](https://avatars.githubusercontent.com/u/74933942?u=a952add7652d59815f24581d83f504216780521b&v=4)](https://github.com/0xJord4n)[@0xJord4n](https://github.com/0xJord4n)

[![](https://avatars.githubusercontent.com/u/9987313?u=4a0795de6a3fa3101b9469e9d2a626ee7a0451b5&v=4)](https://github.com/yuncliu)[@yuncliu](https://github.com/yuncliu)

[![](https://avatars.githubusercontent.com/u/29782447?u=a8804de5269d64ef1c2587945e1b40925349c4a0&v=4)](https://github.com/tabbyl21)[@tabbyl21](https://github.com/tabbyl21)

[![](https://avatars.githubusercontent.com/u/38180263?u=d514276e558f3f3aaba4844fdeb14eb84e9c8cc2&v=4)](https://github.com/naman-modi)[@naman-modi](https://github.com/naman-modi)

[![](https://avatars.githubusercontent.com/u/126395124?u=79cff420daf96b72b14caca0061b57b884139f4f&v=4)](https://github.com/sokolgood)[@sokolgood](https://github.com/sokolgood)

[![](https://avatars.githubusercontent.com/u/2310608?u=1e5009aa6681eed766a14cfb8849d820821dddce&v=4)](https://github.com/harelix)[@harelix](https://github.com/harelix)

[![](https://avatars.githubusercontent.com/u/107643?v=4)](https://github.com/standby24x7)[@standby24x7](https://github.com/standby24x7)

[![](https://avatars.githubusercontent.com/u/170204500?u=a36d4ca4f908cababd62f93eb4a3fbd287ad6b31&v=4)](https://github.com/changliu-0520)[@changliu-0520](https://github.com/changliu-0520)

[![](https://avatars.githubusercontent.com/u/37549748?v=4)](https://github.com/lts-rad)[@lts-rad](https://github.com/lts-rad)

[![](https://avatars.githubusercontent.com/u/9869689?u=b572050134e1e6a3c0096d2b032a5dec32725222&v=4)](https://github.com/nuric)[@nuric](https://github.com/nuric)

[![](https://avatars.githubusercontent.com/u/16749003?v=4)](https://github.com/akshaya-a)[@akshaya-a](https://github.com/akshaya-a)

[![](https://avatars.githubusercontent.com/u/16641288?u=f659a34367a54ea7ac49bc2a51ac27f4a72c770b&v=4)](https://github.com/edreisMD)[@edreisMD](https://github.com/edreisMD)

[![](https://avatars.githubusercontent.com/u/18373802?u=92b9ba56d4178115777a0a1a7d2bf88c162f3fce&v=4)](https://github.com/ar-mccabe)[@ar-mccabe](https://github.com/ar-mccabe)

[![](https://avatars.githubusercontent.com/u/23738320?u=5193798d57e7e887fb121101272718f4f120f56c&v=4)](https://github.com/BobMerkus)[@BobMerkus](https://github.com/BobMerkus)

[![](https://avatars.githubusercontent.com/u/98005188?u=21b5e30aa6464f46e85aa006cb44b2bd18c89347&v=4)](https://github.com/Navanit-git)[@Navanit-git](https://github.com/Navanit-git)

[![](https://avatars.githubusercontent.com/u/127131037?u=74ffbf6c2a443f51f7e72d00b0a4e9a30b9e1c4c&v=4)](https://github.com/david-huge)[@david-huge](https://github.com/david-huge)

[![](https://avatars.githubusercontent.com/u/91344214?u=5c34c21b464a6bbffd83a07aafac2cf9076856db&v=4)](https://github.com/rotemweiss57)[@rotemweiss57](https://github.com/rotemweiss57)

[![](https://avatars.githubusercontent.com/u/29104739?u=855b03bee0b9994fbf1e7eba2be25152ecfdefbc&v=4)](https://github.com/sharmisthasg)[@sharmisthasg](https://github.com/sharmisthasg)

[![](https://avatars.githubusercontent.com/u/9272497?u=bde02b58aebeb42b77cd6678456e8ead7f50ab66&v=4)](https://github.com/hmilkovi)[@hmilkovi](https://github.com/hmilkovi)

[![](https://avatars.githubusercontent.com/u/42059733?u=502e381ca0e17491298e90ac3c5db019dd484efc&v=4)](https://github.com/vreyespue)[@vreyespue](https://github.com/vreyespue)

[![](https://avatars.githubusercontent.com/u/2792?u=f5d3e57d22f60b27f9c87430dc45bceb49e88215&v=4)](https://github.com/deepblue)[@deepblue](https://github.com/deepblue)

[![](https://avatars.githubusercontent.com/u/79035539?u=fef791ebd0cfd10281254b81a96cd8bf6907f8c6&v=4)](https://github.com/mrugank-wadekar)[@mrugank-wadekar](https://github.com/mrugank-wadekar)

[![](https://avatars.githubusercontent.com/u/6087484?u=45381a549e19872d386ca7a7bf399dd571f2f3e8&v=4)](https://github.com/niklub)[@niklub](https://github.com/niklub)

[![](https://avatars.githubusercontent.com/u/1081215?v=4)](https://github.com/dirtysalt)[@dirtysalt](https://github.com/dirtysalt)

[![](https://avatars.githubusercontent.com/u/2138258?u=7de291a1ce0c95d6589496ba8e1d056c054ced00&v=4)](https://github.com/zeiler)[@zeiler](https://github.com/zeiler)

[![](https://avatars.githubusercontent.com/u/46605732?u=a77312f77cca12c30f506c0e68537ff80898bb25&v=4)](https://github.com/moyid)[@moyid](https://github.com/moyid)

[![](https://avatars.githubusercontent.com/u/16364994?u=d8603567cb87b4f76f0df2f7937252ae040cbebf&v=4)](https://github.com/sachinparyani)[@sachinparyani](https://github.com/sachinparyani)

[![](https://avatars.githubusercontent.com/u/20786?u=221de3ff264dd81ae3e168ba2c4e726e5d861881&v=4)](https://github.com/lsloan)[@lsloan](https://github.com/lsloan)

[![](https://avatars.githubusercontent.com/u/27913091?u=af5f1ab3c8383109dfed085fd2e2aa09599dece8&v=4)](https://github.com/ju-bezdek)[@ju-bezdek](https://github.com/ju-bezdek)

[![](https://avatars.githubusercontent.com/u/108557828?u=1f1cc6b7e04613034c6ee4add7846c5a7333da26&v=4)](https://github.com/ColabDog)[@ColabDog](https://github.com/ColabDog)

[![](https://avatars.githubusercontent.com/u/37485638?u=da12a62d4aab6404e41f77772ae61ecc0e816c88&v=4)](https://github.com/hanit-com)[@hanit-com](https://github.com/hanit-com)

[![](https://avatars.githubusercontent.com/u/68591522?u=af3217265b078969783ccbf5159101a5973e7aeb&v=4)](https://github.com/kevaldekivadiya2415)[@kevaldekivadiya2415](https://github.com/kevaldekivadiya2415)

[![](https://avatars.githubusercontent.com/u/2748495?v=4)](https://github.com/manmax31)[@manmax31](https://github.com/manmax31)

[![](https://avatars.githubusercontent.com/u/38863?v=4)](https://github.com/imrehg)[@imrehg](https://github.com/imrehg)

[![](https://avatars.githubusercontent.com/u/8631181?u=dc75ce249013c6fc7f30c742875cb236c9d2bec3&v=4)](https://github.com/ldorigo)[@ldorigo](https://github.com/ldorigo)

[![](https://avatars.githubusercontent.com/u/1454551?u=14928571307ed348c362e902edc913f6d81fea07&v=4)](https://github.com/janchorowski)[@janchorowski](https://github.com/janchorowski)

[![](https://avatars.githubusercontent.com/u/90774897?v=4)](https://github.com/AthulVincent)[@AthulVincent](https://github.com/AthulVincent)

[![](https://avatars.githubusercontent.com/u/23078323?u=7524c4ab19b061e21e62ddd6b48b6084fd6d54c1&v=4)](https://github.com/tamohannes)[@tamohannes](https://github.com/tamohannes)

[![](https://avatars.githubusercontent.com/u/49598618?u=2d8024560f2f936312e819348cc18db338961fb7&v=4)](https://github.com/boazwasserman)[@boazwasserman](https://github.com/boazwasserman)

[![](https://avatars.githubusercontent.com/u/3598211?u=4cae081d5f8478165567af32a1012b7819ad3de3&v=4)](https://github.com/gkermit)[@gkermit](https://github.com/gkermit)

[![](https://avatars.githubusercontent.com/u/30856?v=4)](https://github.com/dsummersl)[@dsummersl](https://github.com/dsummersl)

[![](https://avatars.githubusercontent.com/u/280981?u=6c969bb88d84ac2c2ea100389504f63ac9155425&v=4)](https://github.com/idvorkin)[@idvorkin](https://github.com/idvorkin)

[![](https://avatars.githubusercontent.com/u/24319338?v=4)](https://github.com/vempaliakhil96)[@vempaliakhil96](https://github.com/vempaliakhil96)

[![](https://avatars.githubusercontent.com/u/18140070?u=1992cdb13c62ee66f4ccc8f000d2c6efae3056c3&v=4)](https://github.com/C-K-Loan)[@C-K-Loan](https://github.com/C-K-Loan)

[![](https://avatars.githubusercontent.com/u/18020640?u=d47ad1cc8fb82340d1c77d1f191038372987f85a&v=4)](https://github.com/daniel-brenot)[@daniel-brenot](https://github.com/daniel-brenot)

[![](https://avatars.githubusercontent.com/u/20795854?u=e0a8116151662cf0126b274f74fd279f34febf93&v=4)](https://github.com/jwbeck97)[@jwbeck97](https://github.com/jwbeck97)

[![](https://avatars.githubusercontent.com/u/497264?u=b315947bb39241aa106b760051d903209096fc58&v=4)](https://github.com/giacbrd)[@giacbrd](https://github.com/giacbrd)

We're so thankful for your support!

And one more thank you to [@tiangolo](https://github.com/tiangolo) for inspiration via FastAPI's [excellent people page](https://fastapi.tiangolo.com/fastapi-people).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/people.mdx)

* * *

#### Was this page helpful?









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/routing.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/routing.ipynb)

# How to route between sub-chains

Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel/)
- [Chaining runnables](/docs/how_to/sequence/)
- [Configuring chain parameters at runtime](/docs/how_to/configure/)
- [Prompt templates](/docs/concepts/prompt_templates/)
- [Chat Messages](/docs/concepts/messages/)

Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing can help provide structure and consistency around interactions with models by allowing you to define states and use information related to those states as context to model calls.

There are two ways to perform routing:

1. Conditionally return runnables from a [`RunnableLambda`](/docs/how_to/functions/) (recommended)
2. Using a `RunnableBranch` (legacy)

We'll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain.

## Example Setup[â€‹](#example-setup "Direct link to Example Setup")

First, let's create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`:

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate

chain = (
    PromptTemplate.from_template(
        """Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.

Do not respond with more than one word.

<question>
{question}
</question>

Classification:"""
    )
    | ChatAnthropic(model_name="claude-3-haiku-20240307")
    | StrOutputParser()
)

chain.invoke({"question": "how do I call Anthropic?"})
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

```output
'Anthropic'
```

Now, let's create three sub chains:

```python
langchain_chain = PromptTemplate.from_template(
    """You are an expert in langchain. \
Always answer questions starting with "As Harrison Chase told me". \
Respond to the following question:

Question: {question}
Answer:"""
) | ChatAnthropic(model_name="claude-3-haiku-20240307")
anthropic_chain = PromptTemplate.from_template(
    """You are an expert in anthropic. \
Always answer questions starting with "As Dario Amodei told me". \
Respond to the following question:

Question: {question}
Answer:"""
) | ChatAnthropic(model_name="claude-3-haiku-20240307")
general_chain = PromptTemplate.from_template(
    """Respond to the following question:

Question: {question}
Answer:"""
) | ChatAnthropic(model_name="claude-3-haiku-20240307")
```

## Using a custom function (Recommended)[â€‹](#using-a-custom-function-recommended "Direct link to Using a custom function (Recommended)")

You can also use a custom function to route between different outputs. Here's an example:

```python
def route(info):
    if "anthropic" in info["topic"].lower():
        return anthropic_chain
    elif "langchain" in info["topic"].lower():
        return langchain_chain
    else:
        return general_chain
```

```python
from langchain_core.runnables import RunnableLambda

full_chain = {"topic": chain, "question": lambda x: x["question"]} | RunnableLambda(
    route
)
```

**API Reference:**[RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```python
full_chain.invoke({"question": "how do I use Anthropic?"})
```

```output
AIMessage(content="As Dario Amodei told me, to use Anthropic, you can start by exploring the company's website and learning about their mission, values, and the different services and products they offer. Anthropic is focused on developing safe and ethical AI systems, so they have a strong emphasis on transparency and responsible AI development. \n\nDepending on your specific needs, you can look into Anthropic's AI research and development services, which cover areas like natural language processing, computer vision, and reinforcement learning. They also offer consulting and advisory services to help organizations navigate the challenges and opportunities of AI integration.\n\nAdditionally, Anthropic has released some open-source AI models and tools that you can explore and experiment with. These can be a great way to get hands-on experience with Anthropic's approach to AI development.\n\nOverall, Anthropic aims to be a reliable and trustworthy partner in the AI space, so I'd encourage you to reach out to them directly to discuss how they can best support your specific requirements.", response_metadata={'id': 'msg_01CtLFgFSwvTaJomrihE87Ra', 'content': [ContentBlock(text="As Dario Amodei told me, to use Anthropic, you can start by exploring the company's website and learning about their mission, values, and the different services and products they offer. Anthropic is focused on developing safe and ethical AI systems, so they have a strong emphasis on transparency and responsible AI development. \n\nDepending on your specific needs, you can look into Anthropic's AI research and development services, which cover areas like natural language processing, computer vision, and reinforcement learning. They also offer consulting and advisory services to help organizations navigate the challenges and opportunities of AI integration.\n\nAdditionally, Anthropic has released some open-source AI models and tools that you can explore and experiment with. These can be a great way to get hands-on experience with Anthropic's approach to AI development.\n\nOverall, Anthropic aims to be a reliable and trustworthy partner in the AI space, so I'd encourage you to reach out to them directly to discuss how they can best support your specific requirements.", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=53, output_tokens=219)})
```

```python
full_chain.invoke({"question": "how do I use LangChain?"})
```

```output
AIMessage(content="As Harrison Chase told me, using LangChain involves a few key steps:\n\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\n\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\n\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\n\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\n\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your chosen language model to your LangChain application.\n\n6. **Implement your application logic**: Use LangChain's building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\n\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\n\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.", response_metadata={'id': 'msg_01H3UXAAHG4TwxJLpxwuuVU7', 'content': [ContentBlock(text="As Harrison Chase told me, using LangChain involves a few key steps:\n\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\n\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\n\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\n\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\n\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your chosen language model to your LangChain application.\n\n6. **Implement your application logic**: Use LangChain's building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\n\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\n\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=50, output_tokens=400)})
```

```python
full_chain.invoke({"question": "whats 2 + 2"})
```

```output
AIMessage(content='4', response_metadata={'id': 'msg_01UAKP81jTZu9fyiyFYhsbHc', 'content': [ContentBlock(text='4', type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=28, output_tokens=5)})
```

## Using a RunnableBranch[â€‹](#using-a-runnablebranch "Direct link to Using a RunnableBranch")

A `RunnableBranch` is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. It does **not** offer anything that you can't achieve in a custom function as described above, so we recommend using a custom function instead.

A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.

If no provided conditions match, it runs the default runnable.

Here's an example of what it looks like in action:

```python
from langchain_core.runnables import RunnableBranch

branch = RunnableBranch(
    (lambda x: "anthropic" in x["topic"].lower(), anthropic_chain),
    (lambda x: "langchain" in x["topic"].lower(), langchain_chain),
    general_chain,
)
full_chain = {"topic": chain, "question": lambda x: x["question"]} | branch
full_chain.invoke({"question": "how do I use Anthropic?"})
```

**API Reference:**[RunnableBranch](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.branch.RunnableBranch.html)

```output
AIMessage(content="As Dario Amodei told me, to use Anthropic, you should first familiarize yourself with our mission and principles. Anthropic is committed to developing safe and beneficial artificial intelligence that can help solve important problems facing humanity. \n\nTo get started, I recommend exploring the resources on our website, which cover our research, products, and approach to AI development. You can also reach out to our team to learn more about how Anthropic's technology and services can support your specific needs.\n\nThe key is to engage with us in a way that aligns with our values of transparency, ethical AI, and a commitment to the wellbeing of humanity. We're here to collaborate and help you harness the power of advanced AI responsibly.", response_metadata={'id': 'msg_0187BVnpniPDJnVvwf3M1LdY', 'content': [ContentBlock(text="As Dario Amodei told me, to use Anthropic, you should first familiarize yourself with our mission and principles. Anthropic is committed to developing safe and beneficial artificial intelligence that can help solve important problems facing humanity. \n\nTo get started, I recommend exploring the resources on our website, which cover our research, products, and approach to AI development. You can also reach out to our team to learn more about how Anthropic's technology and services can support your specific needs.\n\nThe key is to engage with us in a way that aligns with our values of transparency, ethical AI, and a commitment to the wellbeing of humanity. We're here to collaborate and help you harness the power of advanced AI responsibly.", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=53, output_tokens=160)})
```

```python
full_chain.invoke({"question": "how do I use LangChain?"})
```

```output
AIMessage(content="As Harrison Chase told me, using LangChain involves several key steps. First, you'll need to install the LangChain library and import the necessary modules. Then, you'll want to define your language model, any data sources you plan to use, and the specific tasks you want to accomplish, such as question answering, text generation, or agent-based reasoning. \n\nLangChain provides a flexible framework for building applications that leverage large language models. It includes abstractions for things like retrievers, prompts, and chains, which allow you to compose different components together to create powerful workflows. \n\nThe documentation on the LangChain website is excellent and covers many common use cases in detail. I'd recommend starting there to get a solid understanding of the core concepts and how to apply them to your specific needs. And of course, feel free to reach out if you have any other questions - I'm always happy to share more insights from my conversations with Harrison.", response_metadata={'id': 'msg_01T1naS99wGPkEAP4LME8iAv', 'content': [ContentBlock(text="As Harrison Chase told me, using LangChain involves several key steps. First, you'll need to install the LangChain library and import the necessary modules. Then, you'll want to define your language model, any data sources you plan to use, and the specific tasks you want to accomplish, such as question answering, text generation, or agent-based reasoning. \n\nLangChain provides a flexible framework for building applications that leverage large language models. It includes abstractions for things like retrievers, prompts, and chains, which allow you to compose different components together to create powerful workflows. \n\nThe documentation on the LangChain website is excellent and covers many common use cases in detail. I'd recommend starting there to get a solid understanding of the core concepts and how to apply them to your specific needs. And of course, feel free to reach out if you have any other questions - I'm always happy to share more insights from my conversations with Harrison.", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=50, output_tokens=205)})
```

```python
full_chain.invoke({"question": "whats 2 + 2"})
```

```output
AIMessage(content='4', response_metadata={'id': 'msg_01T6T3TS6hRCtU8JayN93QEi', 'content': [ContentBlock(text='4', type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=28, output_tokens=5)})
```

## Routing by semantic similarity[â€‹](#routing-by-semantic-similarity "Direct link to Routing by semantic similarity")

One especially useful technique is to use embeddings to route a query to the most relevant prompt. Here's an example.

```python
from langchain_community.utils.math import cosine_similarity
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import OpenAIEmbeddings

physics_template = """You are a very smart physics professor. \
You are great at answering questions about physics in a concise and easy to understand manner. \
When you don't know the answer to a question you admit that you don't know.

Here is a question:
{query}"""

math_template = """You are a very good mathematician. You are great at answering math questions. \
You are so good because you are able to break down hard problems into their component parts, \
answer the component parts, and then put them together to answer the broader question.

Here is a question:
{query}"""

embeddings = OpenAIEmbeddings()
prompt_templates = [physics_template, math_template]
prompt_embeddings = embeddings.embed_documents(prompt_templates)


def prompt_router(input):
    query_embedding = embeddings.embed_query(input["query"])
    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]
    most_similar = prompt_templates[similarity.argmax()]
    print("Using MATH" if most_similar == math_template else "Using PHYSICS")
    return PromptTemplate.from_template(most_similar)


chain = (
    {"query": RunnablePassthrough()}
    | RunnableLambda(prompt_router)
    | ChatAnthropic(model="claude-3-haiku-20240307")
    | StrOutputParser()
)
```

**API Reference:**[cosine\_similarity](https://python.langchain.com/api_reference/community/utils/langchain_community.utils.math.cosine_similarity.html) | [StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```python
print(chain.invoke("What's a black hole"))
```

```output
Using PHYSICS
As a physics professor, I would be happy to provide a concise and easy-to-understand explanation of what a black hole is.

A black hole is an incredibly dense region of space-time where the gravitational pull is so strong that nothing, not even light, can escape from it. This means that if you were to get too close to a black hole, you would be pulled in and crushed by the intense gravitational forces.

The formation of a black hole occurs when a massive star, much larger than our Sun, reaches the end of its life and collapses in on itself. This collapse causes the matter to become extremely dense, and the gravitational force becomes so strong that it creates a point of no return, known as the event horizon.

Beyond the event horizon, the laws of physics as we know them break down, and the intense gravitational forces create a singularity, which is a point of infinite density and curvature in space-time.

Black holes are fascinating and mysterious objects, and there is still much to be learned about their properties and behavior. If I were unsure about any specific details or aspects of black holes, I would readily admit that I do not have a complete understanding and would encourage further research and investigation.
```

```python
print(chain.invoke("What's a path integral"))
```

```output
Using MATH
A path integral is a powerful mathematical concept in physics, particularly in the field of quantum mechanics. It was developed by the renowned physicist Richard Feynman as an alternative formulation of quantum mechanics.

In a path integral, instead of considering a single, definite path that a particle might take from one point to another, as in classical mechanics, the particle is considered to take all possible paths simultaneously. Each path is assigned a complex-valued weight, and the total probability amplitude for the particle to go from one point to another is calculated by summing (integrating) over all possible paths.

The key ideas behind the path integral formulation are:

1. Superposition principle: In quantum mechanics, particles can exist in a superposition of multiple states or paths simultaneously.

2. Probability amplitude: The probability amplitude for a particle to go from one point to another is calculated by summing the complex-valued weights of all possible paths.

3. Weighting of paths: Each path is assigned a weight based on the action (the time integral of the Lagrangian) along that path. Paths with lower action have a greater weight.

4. Feynman's approach: Feynman developed the path integral formulation as an alternative to the traditional wave function approach in quantum mechanics, providing a more intuitive and conceptual understanding of quantum phenomena.

The path integral approach is particularly useful in quantum field theory, where it provides a powerful framework for calculating transition probabilities and understanding the behavior of quantum systems. It has also found applications in various areas of physics, such as condensed matter, statistical mechanics, and even in finance (the path integral approach to option pricing).

The mathematical construction of the path integral involves the use of advanced concepts from functional analysis and measure theory, making it a powerful and sophisticated tool in the physicist's arsenal.
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to add routing to your composed LCEL chains.

Next, check out the other how-to guides on runnables in this section.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/routing.ipynb)

* * *


- [Example Setup](#example-setup)
- [Using a custom function (Recommended)](#using-a-custom-function-recommended)
- [Using a RunnableBranch](#using-a-runnablebranch)
- [Routing by semantic similarity](#routing-by-semantic-similarity)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/tutorials/llm_chain.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/llm_chain.ipynb)

# Build a simple LLM application with chat models and prompt templates

In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!

After reading this tutorial, you'll have a high level overview of:

- Using [language models](/docs/concepts/chat_models/)
- Using [prompt templates](/docs/concepts/prompt_templates/)
- Debugging and tracing your application using [LangSmith](https://docs.smith.langchain.com/)

Let's dive in!

## Setup[â€‹](#setup "Direct link to Setup")

### Jupyter Notebook[â€‹](#jupyter-notebook "Direct link to Jupyter Notebook")

This and other tutorials are perhaps most conveniently run in a [Jupyter notebooks](https://jupyter.org/). Going through guides in an interactive environment is a great way to better understand them. See [here](https://jupyter.org/install) for instructions on how to install.

### Installation[â€‹](#installation "Direct link to Installation")

To install LangChain run:

- Pip
- Conda

```bash
pip install langchain
```

```bash
conda install langchain -c conda-forge
```

For more details, see our [Installation guide](/docs/how_to/installation/).

### LangSmith[â€‹](#langsmith "Direct link to LangSmith")

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
export LANGSMITH_PROJECT="default" # or any other project name
```

Or, if in a notebook, you can set them with:

```python
import getpass
import os

try:
    # load environment variables from .env file (requires `python-dotenv`)
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    pass

os.environ["LANGSMITH_TRACING"] = "true"
if "LANGSMITH_API_KEY" not in os.environ:
    os.environ["LANGSMITH_API_KEY"] = getpass.getpass(
        prompt="Enter your LangSmith API key (optional): "
    )
if "LANGSMITH_PROJECT" not in os.environ:
    os.environ["LANGSMITH_PROJECT"] = getpass.getpass(
        prompt='Enter your LangSmith Project Name (default = "default"): '
    )
    if not os.environ.get("LANGSMITH_PROJECT"):
        os.environ["LANGSMITH_PROJECT"] = "default"
if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass(
        prompt="Enter your OpenAI API key (required if using OpenAI): "
    )
```

## Using Language Models[â€‹](#using-language-models "Direct link to Using Language Models")

First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to [supported integrations](/docs/integrations/chat/).

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4o-mini", model_provider="openai")
```

Let's first use the model directly. [ChatModels](/docs/concepts/chat_models/) are instances of LangChain [Runnables](/docs/concepts/runnables/), which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of [messages](/docs/concepts/messages/) to the `.invoke` method.

```python
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage("Translate the following from English into Italian"),
    HumanMessage("hi!"),
]

model.invoke(messages)
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html)

```output
AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-32654a56-627c-40e1-a141-ad9350bbfd3e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
```

tip

If we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the [LangSmith trace](https://smith.langchain.com/public/88baa0b2-7c1a-4d09-ba30-a47985dde2ea/r). The LangSmith trace reports [token](/docs/concepts/tokens/) usage information, latency, [standard model parameters](/docs/concepts/chat_models/#standard-parameters) (such as temperature), and other information.

Note that ChatModels receive [message](/docs/concepts/messages/) objects as input and generate message objects as output. In addition to text content, message objects convey conversational [roles](/docs/concepts/messages/#role) and hold important data, such as [tool calls](/docs/concepts/tool_calling/) and token usage counts.

LangChain also supports chat model inputs via strings or [OpenAI format](/docs/concepts/messages/#openai-format). The following are equivalent:

```python
model.invoke("Hello")

model.invoke([{"role": "user", "content": "Hello"}])

model.invoke([HumanMessage("Hello")])
```

### Streaming[â€‹](#streaming "Direct link to Streaming")

Because chat models are [Runnables](/docs/concepts/runnables/), they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:

```python
for token in model.stream(messages):
    print(token.content, end="|")
```

```output
|C|iao|!||
```

You can find more details on streaming chat model outputs in [this guide](/docs/how_to/chat_streaming/).

## Prompt Templates[â€‹](#prompt-templates "Direct link to Prompt Templates")

Right now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.

[Prompt templates](/docs/concepts/prompt_templates/) are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.

Let's create a prompt template here. It will take in two user variables:

- `language`: The language to translate text into
- `text`: The text to translate

```python
from langchain_core.prompts import ChatPromptTemplate

system_template = "Translate the following from English into {language}"

prompt_template = ChatPromptTemplate.from_messages(
    [("system", system_template), ("user", "{text}")]
)
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

Note that `ChatPromptTemplate` supports multiple [message roles](/docs/concepts/messages/#role) in a single template. We format the `language` parameter into the system message, and the user `text` into a user message.

The input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself

```python
prompt = prompt_template.invoke({"language": "Italian", "text": "hi!"})

prompt
```

```output
ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])
```

We can see that it returns a `ChatPromptValue` that consists of two messages. If we want to access the messages directly we do:

```python
prompt.to_messages()
```

```output
[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]
```

Finally, we can invoke the chat model on the formatted prompt:

```python
response = model.invoke(prompt)
print(response.content)
```

```output
Ciao!
```

tip

Message `content` can contain both text and [content blocks](/docs/concepts/messages/#aimessage) with additional structure. See [this guide](/docs/how_to/output_parser_string/) for more information.

If we take a look at the [LangSmith trace](https://smith.langchain.com/public/3ccc2d5e-2869-467b-95d6-33a577df99a2/r), we can see exactly what prompt the chat model receives, along with [token](/docs/concepts/tokens/) usage information, latency, [standard model parameters](/docs/concepts/chat_models/#standard-parameters) (such as temperature), and other information.

## Conclusion[â€‹](#conclusion "Direct link to Conclusion")

That's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.

This just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources!

For further reading on the core concepts of LangChain, we've got detailed [Conceptual Guides](/docs/concepts/).

If you have more specific questions on these concepts, check out the following sections of the how-to guides:

- [Chat models](/docs/how_to/#chat-models)
- [Prompt templates](/docs/how_to/#prompt-templates)

And the LangSmith docs:

- [LangSmith](https://docs.smith.langchain.com)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/tutorials/llm_chain.ipynb)

* * *


- [Setup](#setup)
  
  - [Jupyter Notebook](#jupyter-notebook)
  - [Installation](#installation)
  - [LangSmith](#langsmith)
- [Using Language Models](#using-language-models)
  
  - [Streaming](#streaming)
- [Prompt Templates](#prompt-templates)
- [Conclusion](#conclusion)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/multi_vector.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/multi_vector.ipynb)

# How to retrieve using multiple vectors per document

It can often be useful to store multiple [vectors](/docs/concepts/vectorstores/) per document. There are multiple use cases where this is beneficial. For example, we can [embed](/docs/concepts/embedding_models/) multiple chunks of a document and associate those embeddings with the parent document, allowing [retriever](/docs/concepts/retrievers/) hits on the chunks to return the larger document.

LangChain implements a base [MultiVectorRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html), which simplifies this process. Much of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the `MultiVectorRetriever`.

The methods to create multiple vectors per document include:

- Smaller chunks: split a document into smaller chunks, and embed those (this is [ParentDocumentRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html)).
- Summary: create a summary for each document, embed that along with (or instead of) the document.
- Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.

Note that this also enables another method of adding embeddings - manually. This is useful because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control.

Below we walk through an example. First we instantiate some documents. We will index them in an (in-memory) [Chroma](/docs/integrations/providers/chroma/) vector store using [OpenAI](https://python.langchain.com/docs/integrations/text_embedding/openai/) embeddings, but any LangChain vector store or embeddings model will suffice.

```python
%pip install --upgrade --quiet  langchain-chroma langchain langchain-openai > /dev/null
```

```python
from langchain.storage import InMemoryByteStore
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

loaders = [
    TextLoader("paul_graham_essay.txt"),
    TextLoader("state_of_the_union.txt"),
]
docs = []
for loader in loaders:
    docs.extend(loader.load())
text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)
docs = text_splitter.split_documents(docs)

# The vectorstore to use to index the child chunks
vectorstore = Chroma(
    collection_name="full_documents", embedding_function=OpenAIEmbeddings()
)
```

**API Reference:**[InMemoryByteStore](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.InMemoryByteStore.html) | [TextLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

## Smaller chunks[â€‹](#smaller-chunks "Direct link to Smaller chunks")

Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks. This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the [ParentDocumentRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html) does. Here we show what is going on under the hood.

We will make a distinction between the vector store, which indexes embeddings of the (sub) documents, and the document store, which houses the "parent" documents and associates them with an identifier.

```python
import uuid

from langchain.retrievers.multi_vector import MultiVectorRetriever

# The storage layer for the parent documents
store = InMemoryByteStore()
id_key = "doc_id"

# The retriever (empty to start)
retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    byte_store=store,
    id_key=id_key,
)

doc_ids = [str(uuid.uuid4()) for _ in docs]
```

**API Reference:**[MultiVectorRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html)

We next generate the "sub" documents by splitting the original documents. Note that we store the document identifier in the `metadata` of the corresponding [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) object.

```python
# The splitter to use to create smaller chunks
child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

sub_docs = []
for i, doc in enumerate(docs):
    _id = doc_ids[i]
    _sub_docs = child_text_splitter.split_documents([doc])
    for _doc in _sub_docs:
        _doc.metadata[id_key] = _id
    sub_docs.extend(_sub_docs)
```

Finally, we index the documents in our vector store and document store:

```python
retriever.vectorstore.add_documents(sub_docs)
retriever.docstore.mset(list(zip(doc_ids, docs)))
```

The vector store alone will retrieve small chunks:

```python
retriever.vectorstore.similarity_search("justice breyer")[0]
```

```output
Document(page_content='Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.', metadata={'doc_id': '064eca46-a4c4-4789-8e3b-583f9597e54f', 'source': 'state_of_the_union.txt'})
```

Whereas the retriever will return the larger parent document:

```python
len(retriever.invoke("justice breyer")[0].page_content)
```

```output
9875
```

The default search type the retriever performs on the vector database is a similarity search. LangChain vector stores also support searching via [Max Marginal Relevance](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html#langchain_core.vectorstores.base.VectorStore.max_marginal_relevance_search). This can be controlled via the `search_type` parameter of the retriever:

```python
from langchain.retrievers.multi_vector import SearchType

retriever.search_type = SearchType.mmr

len(retriever.invoke("justice breyer")[0].page_content)
```

**API Reference:**[SearchType](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.multi_vector.SearchType.html)

```output
9875
```

## Associating summaries with a document for retrieval[â€‹](#associating-summaries-with-a-document-for-retrieval "Direct link to Associating summaries with a document for retrieval")

A summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those.

We construct a simple [chain](/docs/how_to/sequence/) that will receive an input [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) object and generate a summary using a LLM.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
import uuid

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

chain = (
    {"doc": lambda x: x.page_content}
    | ChatPromptTemplate.from_template("Summarize the following document:\n\n{doc}")
    | llm
    | StrOutputParser()
)
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

Note that we can [batch](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) the chain across documents:

```python
summaries = chain.batch(docs, {"max_concurrency": 5})
```

We can then initialize a `MultiVectorRetriever` as before, indexing the summaries in our vector store, and retaining the original documents in our document store:

```python
# The vectorstore to use to index the child chunks
vectorstore = Chroma(collection_name="summaries", embedding_function=OpenAIEmbeddings())
# The storage layer for the parent documents
store = InMemoryByteStore()
id_key = "doc_id"
# The retriever (empty to start)
retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    byte_store=store,
    id_key=id_key,
)
doc_ids = [str(uuid.uuid4()) for _ in docs]

summary_docs = [
    Document(page_content=s, metadata={id_key: doc_ids[i]})
    for i, s in enumerate(summaries)
]

retriever.vectorstore.add_documents(summary_docs)
retriever.docstore.mset(list(zip(doc_ids, docs)))
```

```python
# # We can also add the original chunks to the vectorstore if we so want
# for i, doc in enumerate(docs):
#     doc.metadata[id_key] = doc_ids[i]
# retriever.vectorstore.add_documents(docs)
```

Querying the vector store will return summaries:

```python
sub_docs = retriever.vectorstore.similarity_search("justice breyer")

sub_docs[0]
```

```output
Document(page_content="President Biden recently nominated Judge Ketanji Brown Jackson to serve on the United States Supreme Court, emphasizing her qualifications and broad support. The President also outlined a plan to secure the border, fix the immigration system, protect women's rights, support LGBTQ+ Americans, and advance mental health services. He highlighted the importance of bipartisan unity in passing legislation, such as the Violence Against Women Act. The President also addressed supporting veterans, particularly those impacted by exposure to burn pits, and announced plans to expand benefits for veterans with respiratory cancers. Additionally, he proposed a plan to end cancer as we know it through the Cancer Moonshot initiative. President Biden expressed optimism about the future of America and emphasized the strength of the American people in overcoming challenges.", metadata={'doc_id': '84015b1b-980e-400a-94d8-cf95d7e079bd'})
```

Whereas the retriever will return the larger source document:

```python
retrieved_docs = retriever.invoke("justice breyer")

len(retrieved_docs[0].page_content)
```

```output
9194
```

## Hypothetical Queries[â€‹](#hypothetical-queries "Direct link to Hypothetical Queries")

An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document, which might bear close semantic similarity to relevant queries in a [RAG](/docs/tutorials/rag/) application. These questions can then be embedded and associated with the documents to improve retrieval.

Below, we use the [with\_structured\_output](/docs/how_to/structured_output/) method to structure the LLM output into a list of strings.

```python
from typing import List

from pydantic import BaseModel, Field


class HypotheticalQuestions(BaseModel):
    """Generate hypothetical questions."""

    questions: List[str] = Field(..., description="List of questions")


chain = (
    {"doc": lambda x: x.page_content}
    # Only asking for 3 hypothetical questions, but this could be adjusted
    | ChatPromptTemplate.from_template(
        "Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\n\n{doc}"
    )
    | ChatOpenAI(max_retries=0, model="gpt-4o").with_structured_output(
        HypotheticalQuestions
    )
    | (lambda x: x.questions)
)
```

Invoking the chain on a single document demonstrates that it outputs a list of questions:

```python
chain.invoke(docs[0])
```

```output
["What impact did the IBM 1401 have on the author's early programming experiences?",
 "How did the transition from using the IBM 1401 to microcomputers influence the author's programming journey?",
 "What role did Lisp play in shaping the author's understanding and approach to AI?"]
```

We can batch then batch the chain over all documents and assemble our vector store and document store as before:

```python
# Batch chain over documents to generate hypothetical questions
hypothetical_questions = chain.batch(docs, {"max_concurrency": 5})


# The vectorstore to use to index the child chunks
vectorstore = Chroma(
    collection_name="hypo-questions", embedding_function=OpenAIEmbeddings()
)
# The storage layer for the parent documents
store = InMemoryByteStore()
id_key = "doc_id"
# The retriever (empty to start)
retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    byte_store=store,
    id_key=id_key,
)
doc_ids = [str(uuid.uuid4()) for _ in docs]


# Generate Document objects from hypothetical questions
question_docs = []
for i, question_list in enumerate(hypothetical_questions):
    question_docs.extend(
        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]
    )


retriever.vectorstore.add_documents(question_docs)
retriever.docstore.mset(list(zip(doc_ids, docs)))
```

Note that querying the underlying vector store will retrieve hypothetical questions that are semantically similar to the input query:

```python
sub_docs = retriever.vectorstore.similarity_search("justice breyer")

sub_docs
```

```output
[Document(page_content='What might be the potential benefits of nominating Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court?', metadata={'doc_id': '43292b74-d1b8-4200-8a8b-ea0cb57fbcdb'}),
 Document(page_content='How might the Bipartisan Infrastructure Law impact the economic competition between the U.S. and China?', metadata={'doc_id': '66174780-d00c-4166-9791-f0069846e734'}),
 Document(page_content='What factors led to the creation of Y Combinator?', metadata={'doc_id': '72003c4e-4cc9-4f09-a787-0b541a65b38c'}),
 Document(page_content='How did the ability to publish essays online change the landscape for writers and thinkers?', metadata={'doc_id': 'e8d2c648-f245-4bcc-b8d3-14e64a164b64'})]
```

And invoking the retriever will return the corresponding document:

```python
retrieved_docs = retriever.invoke("justice breyer")
len(retrieved_docs[0].page_content)
```

```output
9194
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/multi_vector.ipynb)

* * *


- [Smaller chunks](#smaller-chunks)
- [Associating summaries with a document for retrieval](#associating-summaries-with-a-document-for-retrieval)
- [Hypothetical Queries](#hypothetical-queries)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/output_parser_structured.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/output_parser_structured.ipynb)

# How to use output parsers to parse an LLM response into structured format

Language models output text. But there are times where you want to get more structured information than just text back. While some model providers support [built-in ways to return structured output](/docs/how_to/structured_output/), not all do.

[Output parsers](/docs/concepts/output_parsers/) are classes that help structure language model responses. There are two main methods an output parser must implement:

- "Get format instructions": A method which returns a string containing instructions for how the output of a language model should be formatted.
- "Parse": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.

And then one optional one:

- "Parse with prompt": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.

## Get started[â€‹](#get-started "Direct link to Get started")

Below we go over the main type of output parser, the `PydanticOutputParser`.

```python
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAI
from pydantic import BaseModel, Field, model_validator

model = OpenAI(model_name="gpt-3.5-turbo-instruct", temperature=0.0)


# Define your desired data structure.
class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")

    # You can add custom validation logic easily with Pydantic.
    @model_validator(mode="before")
    @classmethod
    def question_ends_with_question_mark(cls, values: dict) -> dict:
        setup = values.get("setup")
        if setup and setup[-1] != "?":
            raise ValueError("Badly formed question!")
        return values


# Set up a parser + inject instructions into the prompt template.
parser = PydanticOutputParser(pydantic_object=Joke)

prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

# And a query intended to prompt a language model to populate the data structure.
prompt_and_model = prompt | model
output = prompt_and_model.invoke({"query": "Tell me a joke."})
parser.invoke(output)
```

**API Reference:**[PydanticOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

```output
Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')
```

## LCEL[â€‹](#lcel "Direct link to LCEL")

Output parsers implement the [Runnable interface](/docs/concepts/runnables/), the basic building block of the [LangChain Expression Language (LCEL)](/docs/concepts/lcel/). This means they support `invoke`, `ainvoke`, `stream`, `astream`, `batch`, `abatch`, `astream_log` calls.

Output parsers accept a string or `BaseMessage` as input and can return an arbitrary type.

```python
parser.invoke(output)
```

```output
Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')
```

Instead of manually invoking the parser, we also could've just added it to our `Runnable` sequence:

```python
chain = prompt | model | parser
chain.invoke({"query": "Tell me a joke."})
```

```output
Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')
```

While all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.

The `SimpleJsonOutputParser` for example can stream through partial outputs:

```python
from langchain.output_parsers.json import SimpleJsonOutputParser

json_prompt = PromptTemplate.from_template(
    "Return a JSON object with an `answer` key that answers the following question: {question}"
)
json_parser = SimpleJsonOutputParser()
json_chain = json_prompt | model | json_parser
```

**API Reference:**[SimpleJsonOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.SimpleJsonOutputParser.html)

```python
list(json_chain.stream({"question": "Who invented the microscope?"}))
```

```output
[{},
 {'answer': ''},
 {'answer': 'Ant'},
 {'answer': 'Anton'},
 {'answer': 'Antonie'},
 {'answer': 'Antonie van'},
 {'answer': 'Antonie van Lee'},
 {'answer': 'Antonie van Leeu'},
 {'answer': 'Antonie van Leeuwen'},
 {'answer': 'Antonie van Leeuwenho'},
 {'answer': 'Antonie van Leeuwenhoek'}]
```

Similarly,for `PydanticOutputParser`:

```python
list(chain.stream({"query": "Tell me a joke."}))
```

```output
[Joke(setup='Why did the tomato turn red?', punchline=''),
 Joke(setup='Why did the tomato turn red?', punchline='Because'),
 Joke(setup='Why did the tomato turn red?', punchline='Because it'),
 Joke(setup='Why did the tomato turn red?', punchline='Because it saw'),
 Joke(setup='Why did the tomato turn red?', punchline='Because it saw the'),
 Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad'),
 Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing'),
 Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')]
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/output_parser_structured.ipynb)

* * *


- [Get started](#get-started)
- [LCEL](#lcel)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/release_policy.mdx)

# LangChain release policy

The LangChain ecosystem is composed of different component packages (e.g., `langchain-core`, `langchain`, `langchain-community`, `langgraph`, `langserve`, partner packages etc.)

## Versioning[â€‹](#versioning "Direct link to Versioning")

### `langchain`, `langchain-core`, and integration packages[â€‹](#langchain-langchain-core-and-integration-packages "Direct link to langchain-langchain-core-and-integration-packages")

`langchain`, `langchain-core`, `langchain-text-splitters`, and integration packages (`langchain-openai`, `langchain-anthropic`, etc.) follow [semantic versioning](https://semver.org/) in the format of 0.**Y**.**Z**. The packages are under rapid development, and so are currently versioning the packages with a major version of 0.

Minor version increases will occur for:

- Breaking changes for any public interfaces *not* marked as `beta`.

Patch version increases will occur for:

- Bug fixes,
- New features,
- Any changes to private interfaces,
- Any changes to `beta` features.

When upgrading between minor versions, users should review the list of breaking changes and deprecations.

From time to time, we will version packages as **release candidates**. These are versions that are intended to be released as stable versions, but we want to get feedback from the community before doing so. Release candidates will be versioned as 0.**Y**.**Z**rc**N**. For example, 0.2.0rc1. If no issues are found, the release candidate will be released as a stable version with the same version number. If issues are found, we will release a new release candidate with an incremented `N` value (e.g., 0.2.0rc2).

### `langchain-community`[â€‹](#langchain-community "Direct link to langchain-community")

`langchain-community` is currently on version `0.2.x`.

Minor version increases will occur for:

- Updates to the major/minor versions of required `langchain-x` dependencies. E.g., when updating the required version of `langchain-core` from `^0.2.x` to `0.3.0`.

Patch version increases will occur for:

- Bug fixes,
- New features,
- Any changes to private interfaces,
- Any changes to `beta` features,
- Breaking changes to integrations to reflect breaking changes in the third-party service.

Whenever possible we will avoid making breaking changes in patch versions. However, if an external API makes a breaking change then breaking changes to the corresponding `langchain-community` integration can occur in a patch version.

### `langchain-experimental`[â€‹](#langchain-experimental "Direct link to langchain-experimental")

`langchain-experimental` is currently on version `0.0.x`. All changes will be accompanied with patch version increases.

## Release cadence[â€‹](#release-cadence "Direct link to Release cadence")

We expect to space out **minor** releases (e.g., from 0.2.x to 0.3.0) of `langchain` and `langchain-core` by at least 2-3 months, as such releases may contain breaking changes.

Patch versions are released frequently, up to a few times per week, as they contain bug fixes and new features.

## API stability[â€‹](#api-stability "Direct link to API stability")

The development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs in `langchain` and `langchain-core` will continue to evolve to better serve the needs of our users.

Even though both `langchain` and `langchain-core` are currently in a pre-1.0 state, we are committed to maintaining API stability in these packages.

- Breaking changes to the public API will result in a minor version bump (the second digit)
- Any bug fixes or new features will result in a patch version bump (the third digit)

We will generally try to avoid making unnecessary changes, and will provide a deprecation policy for features that are being removed.

### Stability of other packages[â€‹](#stability-of-other-packages "Direct link to Stability of other packages")

The stability of other packages in the LangChain ecosystem may vary:

- `langchain-community` is a community maintained package that contains 3rd party integrations. While we do our best to review and test changes in `langchain-community`, `langchain-community` is expected to experience more breaking changes than `langchain` and `langchain-core` as it contains many community contributions.
- Partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information; however, in general these packages are expected to be stable.

### What is a "API stability"?[â€‹](#what-is-a-api-stability 'Direct link to What is a "API stability"?')

API stability means:

- All the public APIs (everything in this documentation) will not be moved or renamed without providing backwards-compatible aliases.
- If new features are added to these APIs â€“ which is quite possible â€“ they will not break or change the meaning of existing methods. In other words, "stable" does not (necessarily) mean "complete."
- If, for some reason, an API declared stable must be removed or replaced, it will be declared deprecated but will remain in the API for at least two minor releases. Warnings will be issued when the deprecated method is called.

### **APIs marked as internal**[â€‹](#apis-marked-as-internal "Direct link to apis-marked-as-internal")

Certain APIs are explicitly marked as â€œinternalâ€ in a couple of ways:

- Some documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change.
- Functions, methods, and other objects prefixed by a leading underscore ( **`_`** ). This is the standard Python convention of indicating that something is private; if any method starts with a single **`_`** , itâ€™s an internal API.
  
  - **Exception:** Certain methods are prefixed with `_` , but do not contain an implementation. These methods are *meant* to be overridden by sub-classes that provide the implementation. Such methods are generally part of the **Public API** of LangChain.

## Deprecation policy[â€‹](#deprecation-policy "Direct link to Deprecation policy")

We will generally avoid deprecating features until a better alternative is available.

When a feature is deprecated, it will continue to work in the current and next minor version of `langchain` and `langchain-core`. After that, the feature will be removed.

Since we're expecting to space out minor releases by at least 2-3 months, this means that a feature can be removed within 2-6 months of being deprecated.

In some situations, we may allow the feature to remain in the code base for longer periods of time, if it's not causing issues in the packages, to reduce the burden on users.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/release_policy.mdx)

* * *


- [Versioning](#versioning)
  
  - [`langchain`, `langchain-core`, and integration packages](#langchain-langchain-core-and-integration-packages)
  - [`langchain-community`](#langchain-community)
  - [`langchain-experimental`](#langchain-experimental)
- [Release cadence](#release-cadence)
- [API stability](#api-stability)
  
  - [Stability of other packages](#stability-of-other-packages)
  - [What is a "API stability"?](#what-is-a-api-stability)
  - [**APIs marked as internal**](#apis-marked-as-internal)
- [Deprecation policy](#deprecation-policy)








- [LangSmith](https://docs.smith.langchain.com)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [LangChain Hub](https://smith.langchain.com/hub)
- [LangChain JS/TS](https://js.langchain.com)

[v0.3](#)

- [v0.3](/docs/introduction/)
- [v0.2](https://python.langchain.com/v0.2/docs/introduction)
- [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)

[ðŸ’¬](https://chat.langchain.com)

Search

[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/troubleshooting/errors/index.mdx)

# Error reference

This page contains guides around resolving common errors you may find while building with LangChain. Errors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.

- [INVALID\_PROMPT\_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT/)
- [INVALID\_TOOL\_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS/)
- [MESSAGE\_COERCION\_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE/)
- [MODEL\_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION/)
- [MODEL\_NOT\_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND/)
- [MODEL\_RATE\_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT/)
- [OUTPUT\_PARSING\_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE/)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/troubleshooting/errors/index.mdx)

* * *

#### Was this page helpful?









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/contextual_compression.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/contextual_compression.ipynb)

# How to do retrieval with contextual compression

One challenge with [retrieval](/docs/concepts/retrieval/) is that usually you don't know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.

Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. â€œCompressingâ€ here refers to both compressing the contents of an individual document and filtering out documents wholesale.

To use the Contextual Compression Retriever, you'll need:

- a base [retriever](/docs/concepts/retrievers/)
- a Document Compressor

The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether.

## Get started[â€‹](#get-started "Direct link to Get started")

```python
# Helper function for printing docs


def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"Document {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )
```

## Using a vanilla vector store retriever[â€‹](#using-a-vanilla-vector-store-retriever "Direct link to Using a vanilla vector store retriever")

Let's start by initializing a simple vector store retriever and storing the 2023 State of the Union speech (in chunks). We can see that given an example question our retriever returns one or two relevant docs and a few irrelevant docs. And even the relevant docs have a lot of irrelevant information in them.

```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter

documents = TextLoader("state_of_the_union.txt").load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()

docs = retriever.invoke("What did the president say about Ketanji Brown Jackson")
pretty_print_docs(docs)
```

**API Reference:**[TextLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html) | [FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

```output
Document 1:

Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. 

Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.
----------------------------------------------------------------------------------------------------
Document 2:

A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since sheâ€™s been nominated, sheâ€™s received a broad range of supportâ€”from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. 

And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. 

We can do both. At our border, weâ€™ve installed new technology like cutting-edge scanners to better detect drug smuggling.  

Weâ€™ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  

Weâ€™re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. 

Weâ€™re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.
----------------------------------------------------------------------------------------------------
Document 3:

And for our LGBTQ+ Americans, letâ€™s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. 

As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. 

While it often appears that we never agree, that isnâ€™t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. 

And soon, weâ€™ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. 

So tonight Iâ€™m offering a Unity Agenda for the Nation. Four big things we can do together.  

First, beat the opioid epidemic.
----------------------------------------------------------------------------------------------------
Document 4:

Tonight, Iâ€™m announcing a crackdown on these companies overcharging American businesses and consumers. 

And as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  

That ends on my watch. 

Medicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. 

Weâ€™ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. 

Letâ€™s pass the Paycheck Fairness Act and paid leave.  

Raise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. 

Letâ€™s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jillâ€”our First Lady who teaches full-timeâ€”calls Americaâ€™s best-kept secret: community colleges.
```

## Adding contextual compression with an `LLMChainExtractor`[â€‹](#adding-contextual-compression-with-an-llmchainextractor "Direct link to adding-contextual-compression-with-an-llmchainextractor")

Now let's wrap our base retriever with a `ContextualCompressionRetriever`. We'll add an `LLMChainExtractor`, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query.

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_openai import OpenAI

llm = OpenAI(temperature=0)
compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```

**API Reference:**[ContextualCompressionRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.contextual_compression.ContextualCompressionRetriever.html) | [LLMChainExtractor](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.chain_extract.LLMChainExtractor.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

```output
Document 1:

I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson.
```

## More built-in compressors: filters[â€‹](#more-built-in-compressors-filters "Direct link to More built-in compressors: filters")

### `LLMChainFilter`[â€‹](#llmchainfilter "Direct link to llmchainfilter")

The `LLMChainFilter` is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents.

```python
from langchain.retrievers.document_compressors import LLMChainFilter

_filter = LLMChainFilter.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=_filter, base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```

**API Reference:**[LLMChainFilter](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.chain_filter.LLMChainFilter.html)

```output
Document 1:

Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. 

Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.
```

### `LLMListwiseRerank`[â€‹](#llmlistwisererank "Direct link to llmlistwisererank")

[LLMListwiseRerank](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.listwise_rerank.LLMListwiseRerank.html) uses [zero-shot listwise document reranking](https://arxiv.org/pdf/2305.02156) and functions similarly to `LLMChainFilter` as a robust but more expensive option. It is recommended to use a more powerful LLM.

Note that `LLMListwiseRerank` requires a model with the [with\_structured\_output](/docs/integrations/chat/) method implemented.

```python
from langchain.retrievers.document_compressors import LLMListwiseRerank
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

_filter = LLMListwiseRerank.from_llm(llm, top_n=1)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=_filter, base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```

**API Reference:**[LLMListwiseRerank](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.listwise_rerank.LLMListwiseRerank.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
Document 1:

Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. 

Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.
```

### `EmbeddingsFilter`[â€‹](#embeddingsfilter "Direct link to embeddingsfilter")

Making an extra LLM call over each retrieved document is expensive and slow. The `EmbeddingsFilter` provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query.

```python
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=embeddings_filter, base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```

**API Reference:**[EmbeddingsFilter](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.embeddings_filter.EmbeddingsFilter.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```output
Document 1:

Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections. 

Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.
----------------------------------------------------------------------------------------------------
Document 2:

A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since sheâ€™s been nominated, sheâ€™s received a broad range of supportâ€”from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. 

And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. 

We can do both. At our border, weâ€™ve installed new technology like cutting-edge scanners to better detect drug smuggling.  

Weâ€™ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  

Weâ€™re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. 

Weâ€™re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.
```

## Stringing compressors and document transformers together[â€‹](#stringing-compressors-and-document-transformers-together "Direct link to Stringing compressors and document transformers together")

Using the `DocumentCompressorPipeline` we can also easily combine multiple compressors in sequence. Along with compressors we can add `BaseDocumentTransformer`s to our pipeline, which don't perform any contextual compression but simply perform some transformation on a set of documents. For example `TextSplitter`s can be used as document transformers to split documents into smaller pieces, and the `EmbeddingsRedundantFilter` can be used to filter out redundant documents based on embedding similarity between documents.

Below we create a compressor pipeline by first splitting our docs into smaller chunks, then removing redundant documents, and then filtering based on relevance to the query.

```python
from langchain.retrievers.document_compressors import DocumentCompressorPipeline
from langchain_community.document_transformers import EmbeddingsRedundantFilter
from langchain_text_splitters import CharacterTextSplitter

splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=". ")
redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)
relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)
pipeline_compressor = DocumentCompressorPipeline(
    transformers=[splitter, redundant_filter, relevant_filter]
)
```

**API Reference:**[DocumentCompressorPipeline](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.base.DocumentCompressorPipeline.html) | [EmbeddingsRedundantFilter](https://python.langchain.com/api_reference/community/document_transformers/langchain_community.document_transformers.embeddings_redundant_filter.EmbeddingsRedundantFilter.html) | [CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

```python
compression_retriever = ContextualCompressionRetriever(
    base_compressor=pipeline_compressor, base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```

```output
Document 1:

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson
----------------------------------------------------------------------------------------------------
Document 2:

As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. 

While it often appears that we never agree, that isnâ€™t true. I signed 80 bipartisan bills into law last year
----------------------------------------------------------------------------------------------------
Document 3:

A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder
----------------------------------------------------------------------------------------------------
Document 4:

Since sheâ€™s been nominated, sheâ€™s received a broad range of supportâ€”from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. 

And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. 

We can do both
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/contextual_compression.ipynb)

* * *


- [Get started](#get-started)
- [Using a vanilla vector store retriever](#using-a-vanilla-vector-store-retriever)
- [Adding contextual compression with an `LLMChainExtractor`](#adding-contextual-compression-with-an-llmchainextractor)
- [More built-in compressors: filters](#more-built-in-compressors-filters)
  
  - [`LLMChainFilter`](#llmchainfilter)
  - [`LLMListwiseRerank`](#llmlistwisererank)
  - [`EmbeddingsFilter`](#embeddingsfilter)
- [Stringing compressors and document transformers together](#stringing-compressors-and-document-transformers-together)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_chat_model.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_chat_model.ipynb)

# How to create a custom chat model class

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models/)

In this guide, we'll learn how to create a custom [chat model](/docs/concepts/chat_models/) using LangChain abstractions.

Wrapping your LLM with the standard [`BaseChatModel`](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface allow you to use your LLM in existing LangChain programs with minimal code modifications!

As an bonus, your LLM will automatically become a LangChain [Runnable](/docs/concepts/runnables/) and will benefit from some optimizations out of the box (e.g., batch via a threadpool), async support, the `astream_events` API, etc.

## Inputs and outputs[â€‹](#inputs-and-outputs "Direct link to Inputs and outputs")

First, we need to talk about [**messages**](/docs/concepts/messages/), which are the inputs and outputs of chat models.

### Messages[â€‹](#messages "Direct link to Messages")

Chat models take messages as inputs and return a message as output.

LangChain has a few [built-in message types](/docs/concepts/messages/):

| Message Type                                 | Description                                                                                      |
|----------------------------------------------|--------------------------------------------------------------------------------------------------|
| `SystemMessage`                              | Used for priming AI behavior, usually passed in as the first of a sequence of input messages.    |
| `HumanMessage`                               | Represents a message from a person interacting with the chat model.                              |
| `AIMessage`                                  | Represents a message from the chat model. This can be either text or a request to invoke a tool. |
| `FunctionMessage` / `ToolMessage`            | Message for passing the results of tool invocation back to the model.                            |
| `AIMessageChunk` / `HumanMessageChunk` / ... | Chunk variant of each type of message.                                                           |

note

`ToolMessage` and `FunctionMessage` closely follow OpenAI's `function` and `tool` roles.

This is a rapidly developing field and as more models add function calling capabilities. Expect that there will be additions to this schema.

```python
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    FunctionMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [FunctionMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.function.FunctionMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html)

### Streaming Variant[â€‹](#streaming-variant "Direct link to Streaming Variant")

All the chat messages have a streaming variant that contains `Chunk` in the name.

```python
from langchain_core.messages import (
    AIMessageChunk,
    FunctionMessageChunk,
    HumanMessageChunk,
    SystemMessageChunk,
    ToolMessageChunk,
)
```

**API Reference:**[AIMessageChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html) | [FunctionMessageChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.function.FunctionMessageChunk.html) | [HumanMessageChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessageChunk.html) | [SystemMessageChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessageChunk.html) | [ToolMessageChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessageChunk.html)

These chunks are used when streaming output from chat models, and they all define an additive property!

```python
AIMessageChunk(content="Hello") + AIMessageChunk(content=" World!")
```

```output
AIMessageChunk(content='Hello World!')
```

## Base Chat Model[â€‹](#base-chat-model "Direct link to Base Chat Model")

Let's implement a chat model that echoes back the first `n` characters of the last message in the prompt!

To do so, we will inherit from `BaseChatModel` and we'll need to implement the following:

| Method/Property                  | Description                                                        | Required/Optional |
|----------------------------------|--------------------------------------------------------------------|-------------------|
| `_generate`                      | Use to generate a chat result from a prompt                        | Required          |
| `_llm_type` (property)           | Used to uniquely identify the type of the model. Used for logging. | Required          |
| `_identifying_params` (property) | Represent model parameterization for tracing purposes.             | Optional          |
| `_stream`                        | Use to implement streaming.                                        | Optional          |
| `_agenerate`                     | Use to implement a native async method.                            | Optional          |
| `_astream`                       | Use to implement async version of `_stream`.                       | Optional          |

tip

The `_astream` implementation uses `run_in_executor` to launch the sync `_stream` in a separate thread if `_stream` is implemented, otherwise it fallsback to use `_agenerate`.

You can use this trick if you want to reuse the `_stream` implementation, but if you're able to implement code that's natively async that's a better solution since that code will run with less overhead.

### Implementation[â€‹](#implementation "Direct link to Implementation")

```python
from typing import Any, Dict, Iterator, List, Optional

from langchain_core.callbacks import (
    CallbackManagerForLLMRun,
)
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import (
    AIMessage,
    AIMessageChunk,
    BaseMessage,
)
from langchain_core.messages.ai import UsageMetadata
from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
from pydantic import Field


class ChatParrotLink(BaseChatModel):
    """A custom chat model that echoes the first `parrot_buffer_length` characters
    of the input.

    When contributing an implementation to LangChain, carefully document
    the model including the initialization parameters, include
    an example of how to initialize the model and include any relevant
    links to the underlying models documentation or API.

    Example:

        .. code-block:: python

            model = ChatParrotLink(parrot_buffer_length=2, model="bird-brain-001")
            result = model.invoke([HumanMessage(content="hello")])
            result = model.batch([[HumanMessage(content="hello")],
                                 [HumanMessage(content="world")]])
    """

    model_name: str = Field(alias="model")
    """The name of the model"""
    parrot_buffer_length: int
    """The number of characters from the last message of the prompt to be echoed."""
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    timeout: Optional[int] = None
    stop: Optional[List[str]] = None
    max_retries: int = 2

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Override the _generate method to implement the chat model logic.

        This can be a call to an API, a call to a local model, or any other
        implementation that generates a response to the input prompt.

        Args:
            messages: the prompt composed of a list of messages.
            stop: a list of strings on which the model should stop generating.
                  If generation stops due to a stop token, the stop token itself
                  SHOULD BE INCLUDED as part of the output. This is not enforced
                  across models right now, but it's a good practice to follow since
                  it makes it much easier to parse the output of the model
                  downstream and understand why generation stopped.
            run_manager: A run manager with callbacks for the LLM.
        """
        # Replace this with actual logic to generate a response from a list
        # of messages.
        last_message = messages[-1]
        tokens = last_message.content[: self.parrot_buffer_length]
        ct_input_tokens = sum(len(message.content) for message in messages)
        ct_output_tokens = len(tokens)
        message = AIMessage(
            content=tokens,
            additional_kwargs={},  # Used to add additional payload to the message
            response_metadata={  # Use for response metadata
                "time_in_seconds": 3,
                "model_name": self.model_name,
            },
            usage_metadata={
                "input_tokens": ct_input_tokens,
                "output_tokens": ct_output_tokens,
                "total_tokens": ct_input_tokens + ct_output_tokens,
            },
        )
        ##

        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])

    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[ChatGenerationChunk]:
        """Stream the output of the model.

        This method should be implemented if the model can generate output
        in a streaming fashion. If the model does not support streaming,
        do not implement it. In that case streaming requests will be automatically
        handled by the _generate method.

        Args:
            messages: the prompt composed of a list of messages.
            stop: a list of strings on which the model should stop generating.
                  If generation stops due to a stop token, the stop token itself
                  SHOULD BE INCLUDED as part of the output. This is not enforced
                  across models right now, but it's a good practice to follow since
                  it makes it much easier to parse the output of the model
                  downstream and understand why generation stopped.
            run_manager: A run manager with callbacks for the LLM.
        """
        last_message = messages[-1]
        tokens = str(last_message.content[: self.parrot_buffer_length])
        ct_input_tokens = sum(len(message.content) for message in messages)

        for token in tokens:
            usage_metadata = UsageMetadata(
                {
                    "input_tokens": ct_input_tokens,
                    "output_tokens": 1,
                    "total_tokens": ct_input_tokens + 1,
                }
            )
            ct_input_tokens = 0
            chunk = ChatGenerationChunk(
                message=AIMessageChunk(content=token, usage_metadata=usage_metadata)
            )

            if run_manager:
                # This is optional in newer versions of LangChain
                # The on_llm_new_token will be called automatically
                run_manager.on_llm_new_token(token, chunk=chunk)

            yield chunk

        # Let's add some other information (e.g., response metadata)
        chunk = ChatGenerationChunk(
            message=AIMessageChunk(
                content="",
                response_metadata={"time_in_sec": 3, "model_name": self.model_name},
            )
        )
        if run_manager:
            # This is optional in newer versions of LangChain
            # The on_llm_new_token will be called automatically
            run_manager.on_llm_new_token(token, chunk=chunk)
        yield chunk

    @property
    def _llm_type(self) -> str:
        """Get the type of language model used by this chat model."""
        return "echoing-chat-model-advanced"

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Return a dictionary of identifying parameters.

        This information is used by the LangChain callback system, which
        is used for tracing purposes make it possible to monitor LLMs.
        """
        return {
            # The model name allows users to specify custom token counting
            # rules in LLM monitoring applications (e.g., in LangSmith users
            # can provide per token pricing for their model and monitor
            # costs for the given LLM.)
            "model_name": self.model_name,
        }
```

**API Reference:**[CallbackManagerForLLMRun](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.CallbackManagerForLLMRun.html) | [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) | [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [AIMessageChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [UsageMetadata](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.UsageMetadata.html) | [ChatGeneration](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.chat_generation.ChatGeneration.html) | [ChatGenerationChunk](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.chat_generation.ChatGenerationChunk.html) | [ChatResult](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.chat_result.ChatResult.html)

### Let's test it ðŸ§ª[â€‹](#lets-test-it- "Direct link to Let's test it ðŸ§ª")

The chat model will implement the standard `Runnable` interface of LangChain which many of the LangChain abstractions support!

```python
model = ChatParrotLink(parrot_buffer_length=3, model="my_custom_model")

model.invoke(
    [
        HumanMessage(content="hello!"),
        AIMessage(content="Hi there human!"),
        HumanMessage(content="Meow!"),
    ]
)
```

```output
AIMessage(content='Meo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-cf11aeb6-8ab6-43d7-8c68-c1ef89b6d78e-0', usage_metadata={'input_tokens': 26, 'output_tokens': 3, 'total_tokens': 29})
```

```python
model.invoke("hello")
```

```output
AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-618e5ed4-d611-4083-8cf1-c270726be8d9-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8})
```

```python
model.batch(["hello", "goodbye"])
```

```output
[AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-eea4ed7d-d750-48dc-90c0-7acca1ff388f-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8}),
 AIMessage(content='goo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-07cfc5c1-3c62-485f-b1e0-3d46e1547287-0', usage_metadata={'input_tokens': 7, 'output_tokens': 3, 'total_tokens': 10})]
```

```python
for chunk in model.stream("cat"):
    print(chunk.content, end="|")
```

```output
c|a|t||
```

Please see the implementation of `_astream` in the model! If you do not implement it, then no output will stream.!

```python
async for chunk in model.astream("cat"):
    print(chunk.content, end="|")
```

```output
c|a|t||
```

Let's try to use the astream events API which will also help double check that all the callbacks were implemented!

```python
async for event in model.astream_events("cat", version="v1"):
    print(event)
```

```output
{'event': 'on_chat_model_start', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'name': 'ChatParrotLink', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='c', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 1, 'total_tokens': 4})}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='a', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='t', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a')}, 'parent_ids': []}
{'event': 'on_chat_model_end', 'name': 'ChatParrotLink', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 3, 'total_tokens': 6})}, 'parent_ids': []}
```

## Contributing[â€‹](#contributing "Direct link to Contributing")

We appreciate all chat model integration contributions.

Here's a checklist to help make sure your contribution gets added to LangChain:

Documentation:

- The model contains doc-strings for all initialization arguments, as these will be surfaced in the [API Reference](https://python.langchain.com/api_reference/langchain/index.html).
- The class doc-string for the model contains a link to the model API if the model is powered by a service.

Tests:

- Add unit or integration tests to the overridden methods. Verify that `invoke`, `ainvoke`, `batch`, `stream` work if you've over-ridden the corresponding code.

Streaming (if you're implementing it):

- Implement the \_stream method to get streaming working

Stop Token Behavior:

- Stop token should be respected
- Stop token should be INCLUDED as part of the response

Secret API Keys:

- If your model connects to an API it will likely accept API keys as part of its initialization. Use Pydantic's `SecretStr` type for secrets, so they don't get accidentally printed out when folks print the model.

Identifying Params:

- Include a `model_name` in identifying params

Optimizations:

Consider providing native async support to reduce the overhead from the model!

- Provided a native async of `_agenerate` (used by `ainvoke`)
- Provided a native async of `_astream` (used by `astream`)

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to create your own custom chat models.

Next, check out the other how-to guides chat models in this section, like [how to get a model to return structured output](/docs/how_to/structured_output/) or [how to track chat model token usage](/docs/how_to/chat_token_usage_tracking/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/custom_chat_model.ipynb)

* * *


- [Inputs and outputs](#inputs-and-outputs)
  
  - [Messages](#messages)
  - [Streaming Variant](#streaming-variant)
- [Base Chat Model](#base-chat-model)
  
  - [Implementation](#implementation)
  - [Let's test it ðŸ§ª](#lets-test-it-)
- [Contributing](#contributing)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/multimodal_inputs.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/multimodal_inputs.ipynb)

# How to pass multimodal data to models

Here we demonstrate how to pass [multimodal](/docs/concepts/multimodality/) input directly to models.

LangChain supports multimodal data as input to chat models:

1. Following provider-specific formats
2. Adhering to a cross-provider standard

Below, we demonstrate the cross-provider standard. See [chat model integrations](/docs/integrations/chat/) for detail on native formats for specific providers.

note

Most chat models that support multimodal **image** inputs also accept those values in OpenAI's [Chat Completions format](https://platform.openai.com/docs/guides/images?api-mode=chat):

```python
{
    "type": "image_url",
    "image_url": {"url": image_url},
}
```

## Images[â€‹](#images "Direct link to Images")

Many providers will accept images passed in-line as base64 data. Some will additionally accept an image from a URL directly.

### Images from base64 data[â€‹](#images-from-base64-data "Direct link to Images from base64 data")

To pass images in-line, format them as content blocks of the following form:

```python
{
    "type": "image",
    "source_type": "base64",
    "mime_type": "image/jpeg",  # or image/png, etc.
    "data": "<base64 data string>",
}
```

Example:

```python
import base64

import httpx
from langchain.chat_models import init_chat_model

# Fetch image data
image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8")


# Pass to LLM
llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")

message = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "Describe the weather in this image:",
        },
        {
            "type": "image",
            "source_type": "base64",
            "data": image_data,
            "mime_type": "image/jpeg",
        },
    ],
}
response = llm.invoke([message])
print(response.text())
```

**API Reference:**[init\_chat\_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)

```output
The image shows a beautiful clear day with bright blue skies and wispy cirrus clouds stretching across the horizon. The clouds are thin and streaky, creating elegant patterns against the blue backdrop. The lighting suggests it's during the day, possibly late afternoon given the warm, golden quality of the light on the grass. The weather appears calm with no signs of wind (the grass looks relatively still) and no indication of rain. It's the kind of perfect, mild weather that's ideal for walking along the wooden boardwalk through the marsh grass.
```

See [LangSmith trace](https://smith.langchain.com/public/eab05a31-54e8-4fc9-911f-56805da67bef/r) for more detail.

### Images from a URL[â€‹](#images-from-a-url "Direct link to Images from a URL")

Some providers (including [OpenAI](/docs/integrations/chat/openai/), [Anthropic](/docs/integrations/chat/anthropic/), and [Google Gemini](/docs/integrations/chat/google_generative_ai/)) will also accept images from URLs directly.

To pass images as URLs, format them as content blocks of the following form:

```python
{
    "type": "image",
    "source_type": "url",
    "url": "https://...",
}
```

Example:

```python
message = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "Describe the weather in this image:",
        },
        {
            "type": "image",
            "source_type": "url",
            "url": image_url,
        },
    ],
}
response = llm.invoke([message])
print(response.text())
```

```output
The weather in this image appears to be pleasant and clear. The sky is mostly blue with a few scattered, light clouds, and there is bright sunlight illuminating the green grass and plants. There are no signs of rain or stormy conditions, suggesting it is a calm, likely warm dayâ€”typical of spring or summer.
```

We can also pass in multiple images:

```python
message = {
    "role": "user",
    "content": [
        {"type": "text", "text": "Are these two images the same?"},
        {"type": "image", "source_type": "url", "url": image_url},
        {"type": "image", "source_type": "url", "url": image_url},
    ],
}
response = llm.invoke([message])
print(response.text())
```

```output
Yes, these two images are the same. They depict a wooden boardwalk going through a grassy field under a blue sky with some clouds. The colors, composition, and elements in both images are identical.
```

## Documents (PDF)[â€‹](#documents-pdf "Direct link to Documents (PDF)")

Some providers (including [OpenAI](/docs/integrations/chat/openai/), [Anthropic](/docs/integrations/chat/anthropic/), and [Google Gemini](/docs/integrations/chat/google_generative_ai/)) will accept PDF documents.

### Documents from base64 data[â€‹](#documents-from-base64-data "Direct link to Documents from base64 data")

To pass documents in-line, format them as content blocks of the following form:

```python
{
    "type": "file",
    "source_type": "base64",
    "mime_type": "application/pdf",
    "data": "<base64 data string>",
}
```

Example:

```python
import base64

import httpx
from langchain.chat_models import init_chat_model

# Fetch PDF data
pdf_url = "https://pdfobject.com/pdf/sample.pdf"
pdf_data = base64.b64encode(httpx.get(pdf_url).content).decode("utf-8")


# Pass to LLM
llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")

message = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "Describe the document:",
        },
        {
            "type": "file",
            "source_type": "base64",
            "data": pdf_data,
            "mime_type": "application/pdf",
        },
    ],
}
response = llm.invoke([message])
print(response.text())
```

**API Reference:**[init\_chat\_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)

```output
This document appears to be a sample PDF file that contains Lorem ipsum placeholder text. It begins with a title "Sample PDF" followed by the subtitle "This is a simple PDF file. Fun fun fun."

The rest of the document consists of several paragraphs of Lorem ipsum text, which is a commonly used placeholder text in design and publishing. The text is formatted in a clean, readable layout with consistent paragraph spacing. The document appears to be a single page containing four main paragraphs of this placeholder text.

The Lorem ipsum text, while appearing to be Latin, is actually scrambled Latin-like text that is used primarily to demonstrate the visual form of a document or typeface without the distraction of meaningful content. It's commonly used in publishing and graphic design when the actual content is not yet available but the layout needs to be demonstrated.

The document has a professional, simple layout with generous margins and clear paragraph separation, making it an effective example of basic PDF formatting and structure.
```

### Documents from a URL[â€‹](#documents-from-a-url "Direct link to Documents from a URL")

Some providers (specifically [Anthropic](/docs/integrations/chat/anthropic/)) will also accept documents from URLs directly.

To pass documents as URLs, format them as content blocks of the following form:

```python
{
    "type": "file",
    "source_type": "url",
    "url": "https://...",
}
```

Example:

```python
message = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "Describe the document:",
        },
        {
            "type": "file",
            "source_type": "url",
            "url": pdf_url,
        },
    ],
}
response = llm.invoke([message])
print(response.text())
```

```output
This document appears to be a sample PDF file with both text and an image. It begins with a title "Sample PDF" followed by the text "This is a simple PDF file. Fun fun fun." The rest of the document contains Lorem ipsum placeholder text arranged in several paragraphs. The content is shown both as text and as an image of the formatted PDF, with the same content displayed in a clean, formatted layout with consistent spacing and typography. The document consists of a single page containing this sample text.
```

## Audio[â€‹](#audio "Direct link to Audio")

Some providers (including [OpenAI](/docs/integrations/chat/openai/) and [Google Gemini](/docs/integrations/chat/google_generative_ai/)) will accept audio inputs.

### Audio from base64 data[â€‹](#audio-from-base64-data "Direct link to Audio from base64 data")

To pass audio in-line, format them as content blocks of the following form:

```python
{
    "type": "audio",
    "source_type": "base64",
    "mime_type": "audio/wav",  # or appropriate mime-type
    "data": "<base64 data string>",
}
```

Example:

```python
import base64

import httpx
from langchain.chat_models import init_chat_model

# Fetch audio data
audio_url = "https://upload.wikimedia.org/wikipedia/commons/3/3d/Alcal%C3%A1_de_Henares_%28RPS_13-04-2024%29_canto_de_ruise%C3%B1or_%28Luscinia_megarhynchos%29_en_el_Soto_del_Henares.wav"
audio_data = base64.b64encode(httpx.get(audio_url).content).decode("utf-8")


# Pass to LLM
llm = init_chat_model("google_genai:gemini-2.0-flash-001")

message = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "Describe this audio:",
        },
        {
            "type": "audio",
            "source_type": "base64",
            "data": audio_data,
            "mime_type": "audio/wav",
        },
    ],
}
response = llm.invoke([message])
print(response.text())
```

**API Reference:**[init\_chat\_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)

```output
The audio appears to consist primarily of bird sounds, specifically bird vocalizations like chirping and possibly other bird songs.
```

## Provider-specific parameters[â€‹](#provider-specific-parameters "Direct link to Provider-specific parameters")

Some providers will support or require additional fields on content blocks containing multimodal data. For example, Anthropic lets you specify [caching](/docs/integrations/chat/anthropic/#prompt-caching) of specific content to reduce token consumption.

To use these fields, you can:

1. Store them on directly on the content block; or
2. Use the native format supported by each provider (see [chat model integrations](/docs/integrations/chat/) for detail).

We show three examples below.

### Example: Anthropic prompt caching[â€‹](#example-anthropic-prompt-caching "Direct link to Example: Anthropic prompt caching")

```python
llm = init_chat_model("anthropic:claude-3-5-sonnet-latest")

message = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "Describe the weather in this image:",
        },
        {
            "type": "image",
            "source_type": "url",
            "url": image_url,
            "cache_control": {"type": "ephemeral"},
        },
    ],
}
response = llm.invoke([message])
print(response.text())
response.usage_metadata
```

```output
The image shows a beautiful, clear day with partly cloudy skies. The sky is a vibrant blue with wispy, white cirrus clouds stretching across it. The lighting suggests it's during daylight hours, possibly late afternoon or early evening given the warm, golden quality of the light on the grass. The weather appears calm with no signs of wind (the grass looks relatively still) and no threatening weather conditions. It's the kind of perfect weather you'd want for a walk along this wooden boardwalk through the marshland or grassland area.
```

```output
{'input_tokens': 1586,
 'output_tokens': 117,
 'total_tokens': 1703,
 'input_token_details': {'cache_read': 0, 'cache_creation': 1582}}
```

```python
next_message = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "Summarize that in 5 words.",
        }
    ],
}
response = llm.invoke([message, response, next_message])
print(response.text())
response.usage_metadata
```

```output
Clear blue skies, wispy clouds.
```

```output
{'input_tokens': 1716,
 'output_tokens': 12,
 'total_tokens': 1728,
 'input_token_details': {'cache_read': 1582, 'cache_creation': 0}}
```

### Example: Anthropic citations[â€‹](#example-anthropic-citations "Direct link to Example: Anthropic citations")

```python
message = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "Generate a 5 word summary of this document.",
        },
        {
            "type": "file",
            "source_type": "base64",
            "data": pdf_data,
            "mime_type": "application/pdf",
            "citations": {"enabled": True},
        },
    ],
}
response = llm.invoke([message])
response.content
```

```output
[{'citations': [{'cited_text': 'Sample PDF\r\nThis is a simple PDF file. Fun fun fun.\r\n',
    'document_index': 0,
    'document_title': None,
    'end_page_number': 2,
    'start_page_number': 1,
    'type': 'page_location'}],
  'text': 'Simple PDF file: fun fun',
  'type': 'text'}]
```

### Example: OpenAI file names[â€‹](#example-openai-file-names "Direct link to Example: OpenAI file names")

OpenAI requires that PDF documents be associated with file names:

```python
llm = init_chat_model("openai:gpt-4.1")

message = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "Describe the document:",
        },
        {
            "type": "file",
            "source_type": "base64",
            "data": pdf_data,
            "mime_type": "application/pdf",
            "filename": "my-file",
        },
    ],
}
response = llm.invoke([message])
print(response.text())
```

```output
The document is a sample PDF file containing placeholder text. It consists of one page, titled "Sample PDF". The content is a mixture of English and the commonly used filler text "Lorem ipsum dolor sit amet..." and its extensions, which are often used in publishing and web design as generic text to demonstrate font, layout, and other visual elements.

**Key points about the document:**
- Length: 1 page
- Purpose: Demonstrative/sample content
- Content: No substantive or meaningful information, just demonstration text in paragraph form
- Language: English (with the Latin-like "Lorem Ipsum" text used for layout purposes)

There are no charts, tables, diagrams, or images on the pageâ€”only plain text. The document serves as an example of what a PDF file looks like rather than providing actual, useful content.
```

## Tool calls[â€‹](#tool-calls "Direct link to Tool calls")

Some multimodal models support [tool calling](/docs/concepts/tool_calling/) features as well. To call tools using such models, simply bind tools to them in the [usual way](/docs/how_to/tool_calling/), and invoke the model using content blocks of the desired type (e.g., containing image data).

```python
from typing import Literal

from langchain_core.tools import tool


@tool
def weather_tool(weather: Literal["sunny", "cloudy", "rainy"]) -> None:
    """Describe the weather"""
    pass


llm_with_tools = llm.bind_tools([weather_tool])

message = {
    "role": "user",
    "content": [
        {"type": "text", "text": "Describe the weather in this image:"},
        {"type": "image", "source_type": "url", "url": image_url},
    ],
}
response = llm_with_tools.invoke([message])
response.tool_calls
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

```output
[{'name': 'weather_tool',
  'args': {'weather': 'sunny'},
  'id': 'toolu_01G6JgdkhwggKcQKfhXZQPjf',
  'type': 'tool_call'}]
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/multimodal_inputs.ipynb)

* * *


- [Images](#images)
  
  - [Images from base64 data](#images-from-base64-data)
  - [Images from a URL](#images-from-a-url)
- [Documents (PDF)](#documents-pdf)
  
  - [Documents from base64 data](#documents-from-base64-data)
  - [Documents from a URL](#documents-from-a-url)
- [Audio](#audio)
  
  - [Audio from base64 data](#audio-from-base64-data)
- [Provider-specific parameters](#provider-specific-parameters)
  
  - [Example: Anthropic prompt caching](#example-anthropic-prompt-caching)
  - [Example: Anthropic citations](#example-anthropic-citations)
  - [Example: OpenAI file names](#example-openai-file-names)
- [Tool calls](#tool-calls)








- [LangSmith](https://docs.smith.langchain.com)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [LangChain Hub](https://smith.langchain.com/hub)
- [LangChain JS/TS](https://js.langchain.com)

[v0.3](#)

- [v0.3](/docs/introduction/)
- [v0.2](https://python.langchain.com/v0.2/docs/introduction)
- [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)

[ðŸ’¬](https://chat.langchain.com)

Search

- [Providers](/docs/integrations/providers/)
  
  - [Anthropic](/docs/integrations/providers/anthropic/)
  - [AWS](/docs/integrations/providers/aws/)
  - [Google](/docs/integrations/providers/google/)
  - [Hugging Face](/docs/integrations/providers/huggingface/)
  - [Microsoft](/docs/integrations/providers/microsoft/)
  - [OpenAI](/docs/integrations/providers/openai/)
  - [More](/docs/integrations/providers/all/)
    
    - [Providers](/docs/integrations/providers/)
    - [Abso](/docs/integrations/providers/abso/)
    - [Acreom](/docs/integrations/providers/acreom/)
    - [Activeloop Deep Lake](/docs/integrations/providers/activeloop_deeplake/)
    - [ADS4GPTs](/docs/integrations/providers/ads4gpts/)
    - [Aerospike](/docs/integrations/providers/aerospike/)
    - [AgentQL](/docs/integrations/providers/agentql/)
    - [AI21 Labs](/docs/integrations/providers/ai21/)
    - [Aim](/docs/integrations/providers/aim_tracking/)
    - [AINetwork](/docs/integrations/providers/ainetwork/)
    - [Airbyte](/docs/integrations/providers/airbyte/)
    - [Airtable](/docs/integrations/providers/airtable/)
    - [Alchemy](/docs/integrations/providers/alchemy/)
    - [Aleph Alpha](/docs/integrations/providers/aleph_alpha/)
    - [Alibaba Cloud](/docs/integrations/providers/alibaba_cloud/)
    - [AnalyticDB](/docs/integrations/providers/analyticdb/)
    - [Annoy](/docs/integrations/providers/annoy/)
    - [Anthropic](/docs/integrations/providers/anthropic/)
    - [Anyscale](/docs/integrations/providers/anyscale/)
    - [Apache Software Foundation](/docs/integrations/providers/apache/)
    - [Apache Doris](/docs/integrations/providers/apache_doris/)
    - [Apify](/docs/integrations/providers/apify/)
    - [Apple](/docs/integrations/providers/apple/)
    - [ArangoDB](/docs/integrations/providers/arangodb/)
    - [Arcee](/docs/integrations/providers/arcee/)
    - [ArcGIS](/docs/integrations/providers/arcgis/)
    - [Argilla](/docs/integrations/providers/argilla/)
    - [Arize](/docs/integrations/providers/arize/)
    - [Arthur](/docs/integrations/providers/arthur_tracking/)
    - [Arxiv](/docs/integrations/providers/arxiv/)
    - [Ascend](/docs/integrations/providers/ascend/)
    - [AskNews](/docs/integrations/providers/asknews/)
    - [AssemblyAI](/docs/integrations/providers/assemblyai/)
    - [Astra DB](/docs/integrations/providers/astradb/)
    - [Atlas](/docs/integrations/providers/atlas/)
    - [AwaDB](/docs/integrations/providers/awadb/)
    - [AWS](/docs/integrations/providers/aws/)
    - [AZLyrics](/docs/integrations/providers/azlyrics/)
    - [Azure AI](/docs/integrations/providers/azure_ai/)
    - [BAAI](/docs/integrations/providers/baai/)
    - [Bagel](/docs/integrations/providers/bagel/)
    - [BagelDB](/docs/integrations/providers/bageldb/)
    - [Baichuan](/docs/integrations/providers/baichuan/)
    - [Baidu](/docs/integrations/providers/baidu/)
    - [Banana](/docs/integrations/providers/bananadev/)
    - [Baseten](/docs/integrations/providers/baseten/)
    - [Beam](/docs/integrations/providers/beam/)
    - [Beautiful Soup](/docs/integrations/providers/beautiful_soup/)
    - [BibTeX](/docs/integrations/providers/bibtex/)
    - [BiliBili](/docs/integrations/providers/bilibili/)
    - [Bittensor](/docs/integrations/providers/bittensor/)
    - [Blackboard](/docs/integrations/providers/blackboard/)
    - [bookend.ai](/docs/integrations/providers/bookendai/)
    - [Box](/docs/integrations/providers/box/)
    - [Brave Search](/docs/integrations/providers/brave_search/)
    - [Breebs (Open Knowledge)](/docs/integrations/providers/breebs/)
    - [Browserbase](/docs/integrations/providers/browserbase/)
    - [Browserless](/docs/integrations/providers/browserless/)
    - [ByteDance](/docs/integrations/providers/byte_dance/)
    - [Cassandra](/docs/integrations/providers/cassandra/)
    - [Cerebras](/docs/integrations/providers/cerebras/)
    - [CerebriumAI](/docs/integrations/providers/cerebriumai/)
    - [Chaindesk](/docs/integrations/providers/chaindesk/)
    - [Chroma](/docs/integrations/providers/chroma/)
    - [Clarifai](/docs/integrations/providers/clarifai/)
    - [ClearML](/docs/integrations/providers/clearml_tracking/)
    - [ClickHouse](/docs/integrations/providers/clickhouse/)
    - [ClickUp](/docs/integrations/providers/clickup/)
    - [Cloudflare](/docs/integrations/providers/cloudflare/)
    - [Clova](/docs/integrations/providers/clova/)
    - [CnosDB](/docs/integrations/providers/cnosdb/)
    - [Cognee](/docs/integrations/providers/cognee/)
    - [CogniSwitch](/docs/integrations/providers/cogniswitch/)
    - [Cohere](/docs/integrations/providers/cohere/)
    - [College Confidential](/docs/integrations/providers/college_confidential/)
    - [Comet](/docs/integrations/providers/comet_tracking/)
    - [Confident AI](/docs/integrations/providers/confident/)
    - [Confluence](/docs/integrations/providers/confluence/)
    - [Connery](/docs/integrations/providers/connery/)
    - [Context](/docs/integrations/providers/context/)
    - [Contextual AI](/docs/integrations/providers/contextual/)
    - [Couchbase](/docs/integrations/providers/couchbase/)
    - [Coze](/docs/integrations/providers/coze/)
    - [CrateDB](/docs/integrations/providers/cratedb/)
    - [C Transformers](/docs/integrations/providers/ctransformers/)
    - [CTranslate2](/docs/integrations/providers/ctranslate2/)
    - [Cube](/docs/integrations/providers/cube/)
    - [Dappier](/docs/integrations/providers/dappier/)
    - [DashVector](/docs/integrations/providers/dashvector/)
    - [Databricks](/docs/integrations/providers/databricks/)
    - [Datadog Tracing](/docs/integrations/providers/datadog/)
    - [Datadog Logs](/docs/integrations/providers/datadog_logs/)
    - [DataForSEO](/docs/integrations/providers/dataforseo/)
    - [Dataherald](/docs/integrations/providers/dataherald/)
    - [Dedoc](/docs/integrations/providers/dedoc/)
    - [DeepInfra](/docs/integrations/providers/deepinfra/)
    - [Deeplake](/docs/integrations/providers/deeplake/)
    - [DeepSeek](/docs/integrations/providers/deepseek/)
    - [DeepSparse](/docs/integrations/providers/deepsparse/)
    - [Dell](/docs/integrations/providers/dell/)
    - [Diffbot](/docs/integrations/providers/diffbot/)
    - [DingoDB](/docs/integrations/providers/dingo/)
    - [Discord](/docs/integrations/providers/discord-shikenso/)
    - [Discord (community loader)](/docs/integrations/providers/discord/)
    - [DocArray](/docs/integrations/providers/docarray/)
    - [Docling](/docs/integrations/providers/docling/)
    - [Doctran](/docs/integrations/providers/doctran/)
    - [Docugami](/docs/integrations/providers/docugami/)
    - [Docusaurus](/docs/integrations/providers/docusaurus/)
    - [Dria](/docs/integrations/providers/dria/)
    - [Dropbox](/docs/integrations/providers/dropbox/)
    - [DSPy](/docs/integrations/providers/dspy/)
    - [DuckDB](/docs/integrations/providers/duckdb/)
    - [DuckDuckGo Search](/docs/integrations/providers/duckduckgo_search/)
    - [E2B](/docs/integrations/providers/e2b/)
    - [Eden AI](/docs/integrations/providers/edenai/)
    - [Elasticsearch](/docs/integrations/providers/elasticsearch/)
    - [ElevenLabs](/docs/integrations/providers/elevenlabs/)
    - [Embedchain](/docs/integrations/providers/embedchain/)
    - [Epsilla](/docs/integrations/providers/epsilla/)
    - [Etherscan](/docs/integrations/providers/etherscan/)
    - [Everly AI](/docs/integrations/providers/everlyai/)
    - [EverNote](/docs/integrations/providers/evernote/)
    - [Exa](/docs/integrations/providers/exa_search/)
    - [Facebook - Meta](/docs/integrations/providers/facebook/)
    - [FalkorDB](/docs/integrations/providers/falkordb/)
    - [Fauna](/docs/integrations/providers/fauna/)
    - [Fiddler](/docs/integrations/providers/fiddler/)
    - [Figma](/docs/integrations/providers/figma/)
    - [FireCrawl](/docs/integrations/providers/firecrawl/)
    - [Fireworks AI](/docs/integrations/providers/fireworks/)
    - [Flyte](/docs/integrations/providers/flyte/)
    - [FMP Data (Financial Data Prep)](/docs/integrations/providers/fmp-data/)
    - [Forefront AI](/docs/integrations/providers/forefrontai/)
    - [Friendli AI](/docs/integrations/providers/friendli/)
    - [Smabbler](/docs/integrations/providers/galaxia/)
    - [Geopandas](/docs/integrations/providers/geopandas/)
    - [Git](/docs/integrations/providers/git/)
    - [GitBook](/docs/integrations/providers/gitbook/)
    - [GitHub](/docs/integrations/providers/github/)
    - [GitLab](/docs/integrations/providers/gitlab/)
    - [GOAT](/docs/integrations/providers/goat/)
    - [Golden](/docs/integrations/providers/golden/)
    - [Goodfire](/docs/integrations/providers/goodfire/)
    - [Google](/docs/integrations/providers/google/)
    - [Serper - Google Search API](/docs/integrations/providers/google_serper/)
    - [GooseAI](/docs/integrations/providers/gooseai/)
    - [GPT4All](/docs/integrations/providers/gpt4all/)
    - [Gradient](/docs/integrations/providers/gradient/)
    - [Graph RAG](/docs/integrations/providers/graph_rag/)
    - [Graphsignal](/docs/integrations/providers/graphsignal/)
    - [Grobid](/docs/integrations/providers/grobid/)
    - [Groq](/docs/integrations/providers/groq/)
    - [Gutenberg](/docs/integrations/providers/gutenberg/)
    - [Hacker News](/docs/integrations/providers/hacker_news/)
    - [Hazy Research](/docs/integrations/providers/hazy_research/)
    - [Helicone](/docs/integrations/providers/helicone/)
    - [Hologres](/docs/integrations/providers/hologres/)
    - [HTML to text](/docs/integrations/providers/html2text/)
    - [Huawei](/docs/integrations/providers/huawei/)
    - [Hugging Face](/docs/integrations/providers/huggingface/)
    - [Hyperbrowser](/docs/integrations/providers/hyperbrowser/)
    - [IBM](/docs/integrations/providers/ibm/)
    - [IEIT Systems](/docs/integrations/providers/ieit_systems/)
    - [iFixit](/docs/integrations/providers/ifixit/)
    - [iFlytek](/docs/integrations/providers/iflytek/)
    - [IMSDb](/docs/integrations/providers/imsdb/)
    - [Infinispan VS](/docs/integrations/providers/infinispanvs/)
    - [Infinity](/docs/integrations/providers/infinity/)
    - [Infino](/docs/integrations/providers/infino/)
    - [Intel](/docs/integrations/providers/intel/)
    - [Iugu](/docs/integrations/providers/iugu/)
    - [Jaguar](/docs/integrations/providers/jaguar/)
    - [Javelin AI Gateway](/docs/integrations/providers/javelin_ai_gateway/)
    - [Jenkins](/docs/integrations/providers/jenkins/)
    - [Jina AI](/docs/integrations/providers/jina/)
    - [Johnsnowlabs](/docs/integrations/providers/johnsnowlabs/)
    - [Joplin](/docs/integrations/providers/joplin/)
    - [KDB.AI](/docs/integrations/providers/kdbai/)
    - [Kinetica](/docs/integrations/providers/kinetica/)
    - [KoboldAI](/docs/integrations/providers/koboldai/)
    - [Konko](/docs/integrations/providers/konko/)
    - [KoNLPY](/docs/integrations/providers/konlpy/)
    - [KÃ¹zu](/docs/integrations/providers/kuzu/)
    - [Label Studio](/docs/integrations/providers/labelstudio/)
    - [lakeFS](/docs/integrations/providers/lakefs/)
    - [LanceDB](/docs/integrations/providers/lancedb/)
    - [LangChain Decorators âœ¨](/docs/integrations/providers/langchain_decorators/)
    - [LangFair: Use-Case Level LLM Bias and Fairness Assessments](/docs/integrations/providers/langfair/)
    - [Langfuse ðŸª¢](/docs/integrations/providers/langfuse/)
    - [Lantern](/docs/integrations/providers/lantern/)
    - [Lindorm](/docs/integrations/providers/lindorm/)
    - [Linkup](/docs/integrations/providers/linkup/)
    - [LiteLLM](/docs/integrations/providers/litellm/)
    - [LlamaIndex](/docs/integrations/providers/llama_index/)
    - [Llama.cpp](/docs/integrations/providers/llamacpp/)
    - [LlamaEdge](/docs/integrations/providers/llamaedge/)
    - [llamafile](/docs/integrations/providers/llamafile/)
    - [LLMonitor](/docs/integrations/providers/llmonitor/)
    - [LocalAI](/docs/integrations/providers/localai/)
    - [Log10](/docs/integrations/providers/log10/)
    - [MariaDB](/docs/integrations/providers/mariadb/)
    - [MariTalk](/docs/integrations/providers/maritalk/)
    - [Marqo](/docs/integrations/providers/marqo/)
    - [MediaWikiDump](/docs/integrations/providers/mediawikidump/)
    - [Meilisearch](/docs/integrations/providers/meilisearch/)
    - [Memcached](/docs/integrations/providers/memcached/)
    - [Memgraph](/docs/integrations/providers/memgraph/)
    - [Metal](/docs/integrations/providers/metal/)
    - [Microsoft](/docs/integrations/providers/microsoft/)
    - [Milvus](/docs/integrations/providers/milvus/)
    - [MindsDB](/docs/integrations/providers/mindsdb/)
    - [Minimax](/docs/integrations/providers/minimax/)
    - [MistralAI](/docs/integrations/providers/mistralai/)
    - [MLflow AI Gateway for LLMs](/docs/integrations/providers/mlflow/)
    - [MLflow](/docs/integrations/providers/mlflow_tracking/)
    - [MLX](/docs/integrations/providers/mlx/)
    - [Modal](/docs/integrations/providers/modal/)
    - [ModelScope](/docs/integrations/providers/modelscope/)
    - [Modern Treasury](/docs/integrations/providers/modern_treasury/)
    - [Momento](/docs/integrations/providers/momento/)
    - [MongoDB](/docs/integrations/providers/mongodb/)
    - [MongoDB Atlas](/docs/integrations/providers/mongodb_atlas/)
    - [Motherduck](/docs/integrations/providers/motherduck/)
    - [MotÃ¶rhead](/docs/integrations/providers/motorhead/)
    - [MyScale](/docs/integrations/providers/myscale/)
    - [NAVER](/docs/integrations/providers/naver/)
    - [Neo4j](/docs/integrations/providers/neo4j/)
    - [Netmind](/docs/integrations/providers/netmind/)
    - [Nimble](/docs/integrations/providers/nimble/)
    - [NLPCloud](/docs/integrations/providers/nlpcloud/)
    - [Nomic](/docs/integrations/providers/nomic/)
    - [Notion DB](/docs/integrations/providers/notion/)
    - [Nuclia](/docs/integrations/providers/nuclia/)
    - [NVIDIA](/docs/integrations/providers/nvidia/)
    - [Obsidian](/docs/integrations/providers/obsidian/)
    - [OceanBase](/docs/integrations/providers/oceanbase/)
    - [Oracle Cloud Infrastructure (OCI)](/docs/integrations/providers/oci/)
    - [OctoAI](/docs/integrations/providers/octoai/)
    - [Ollama](/docs/integrations/providers/ollama/)
    - [Ontotext GraphDB](/docs/integrations/providers/ontotext_graphdb/)
    - [OpenAI](/docs/integrations/providers/openai/)
    - [OpenGradient](/docs/integrations/providers/opengradient/)
    - [OpenLLM](/docs/integrations/providers/openllm/)
    - [OpenSearch](/docs/integrations/providers/opensearch/)
    - [OpenWeatherMap](/docs/integrations/providers/openweathermap/)
    - [OracleAI Vector Search](/docs/integrations/providers/oracleai/)
    - [Outline](/docs/integrations/providers/outline/)
    - [Outlines](/docs/integrations/providers/outlines/)
    - [Oxylabs](/docs/integrations/providers/oxylabs/)
    - [Pandas](/docs/integrations/providers/pandas/)
    - [PaymanAI](/docs/integrations/providers/payman-tool/)
    - [Pebblo](/docs/integrations/providers/pebblo/)
    - [Permit](/docs/integrations/providers/permit/)
    - [Perplexity](/docs/integrations/providers/perplexity/)
    - [Petals](/docs/integrations/providers/petals/)
    - [Postgres Embedding](/docs/integrations/providers/pg_embedding/)
    - [PGVector](/docs/integrations/providers/pgvector/)
    - [Pinecone](/docs/integrations/providers/pinecone/)
    - [PipelineAI](/docs/integrations/providers/pipelineai/)
    - [Pipeshift](/docs/integrations/providers/pipeshift/)
    - [Portkey](/docs/integrations/providers/portkey/)
    - [Predibase](/docs/integrations/providers/predibase/)
    - [Prediction Guard](/docs/integrations/providers/predictionguard/)
    - [PremAI](/docs/integrations/providers/premai/)
    - [SWI-Prolog](/docs/integrations/providers/prolog/)
    - [PromptLayer](/docs/integrations/providers/promptlayer/)
    - [Psychic](/docs/integrations/providers/psychic/)
    - [PubMed](/docs/integrations/providers/pubmed/)
    - [PullMd Loader](/docs/integrations/providers/pull-md/)
    - [PygmalionAI](/docs/integrations/providers/pygmalionai/)
    - [PyMuPDF4LLM](/docs/integrations/providers/pymupdf4llm/)
    - [Qdrant](/docs/integrations/providers/qdrant/)
    - [RAGatouille](/docs/integrations/providers/ragatouille/)
    - [rank\_bm25](/docs/integrations/providers/rank_bm25/)
    - [Ray Serve](/docs/integrations/providers/ray_serve/)
    - [Rebuff](/docs/integrations/providers/rebuff/)
    - [Reddit](/docs/integrations/providers/reddit/)
    - [Redis](/docs/integrations/providers/redis/)
    - [Remembrall](/docs/integrations/providers/remembrall/)
    - [Replicate](/docs/integrations/providers/replicate/)
    - [Roam](/docs/integrations/providers/roam/)
    - [Sema4 (fka Robocorp)](/docs/integrations/providers/robocorp/)
    - [Rockset](/docs/integrations/providers/rockset/)
    - [Runhouse](/docs/integrations/providers/runhouse/)
    - [Runpod](/docs/integrations/providers/runpod/)
    - [RWKV-4](/docs/integrations/providers/rwkv/)
    - [Salesforce](/docs/integrations/providers/salesforce/)
    - [Salute Devices](/docs/integrations/providers/salute_devices/)
    - [SambaNova](/docs/integrations/providers/sambanova/)
    - [SAP](/docs/integrations/providers/sap/)
    - [ScrapeGraph AI](/docs/integrations/providers/scrapegraph/)
    - [SearchApi](/docs/integrations/providers/searchapi/)
    - [SearxNG Search API](/docs/integrations/providers/searx/)
    - [SemaDB](/docs/integrations/providers/semadb/)
    - [SerpAPI](/docs/integrations/providers/serpapi/)
    - [Shale Protocol](/docs/integrations/providers/shaleprotocol/)
    - [SingleStore Integration](/docs/integrations/providers/singlestore/)
    - [scikit-learn](/docs/integrations/providers/sklearn/)
    - [Slack](/docs/integrations/providers/slack/)
    - [Snowflake](/docs/integrations/providers/snowflake/)
    - [spaCy](/docs/integrations/providers/spacy/)
    - [Spark](/docs/integrations/providers/spark/)
    - [SparkLLM](/docs/integrations/providers/sparkllm/)
    - [Spreedly](/docs/integrations/providers/spreedly/)
    - [SQLite](/docs/integrations/providers/sqlite/)
    - [Stack Exchange](/docs/integrations/providers/stackexchange/)
    - [StarRocks](/docs/integrations/providers/starrocks/)
    - [StochasticAI](/docs/integrations/providers/stochasticai/)
    - [Streamlit](/docs/integrations/providers/streamlit/)
    - [Stripe](/docs/integrations/providers/stripe/)
    - [Supabase (Postgres)](/docs/integrations/providers/supabase/)
    - [Nebula](/docs/integrations/providers/symblai_nebula/)
    - [Tableau](/docs/integrations/providers/tableau/)
    - [Taiga](/docs/integrations/providers/taiga/)
    - [Tair](/docs/integrations/providers/tair/)
    - [Tavily](/docs/integrations/providers/tavily/)
    - [Telegram](/docs/integrations/providers/telegram/)
    - [Tencent](/docs/integrations/providers/tencent/)
    - [TensorFlow Datasets](/docs/integrations/providers/tensorflow_datasets/)
    - [TiDB](/docs/integrations/providers/tidb/)
    - [TigerGraph](/docs/integrations/providers/tigergraph/)
    - [Tigris](/docs/integrations/providers/tigris/)
    - [Tilores](/docs/integrations/providers/tilores/)
    - [Together AI](/docs/integrations/providers/together/)
    - [2Markdown](/docs/integrations/providers/tomarkdown/)
    - [Transwarp](/docs/integrations/providers/transwarp/)
    - [Trello](/docs/integrations/providers/trello/)
    - [Trubrics](/docs/integrations/providers/trubrics/)
    - [TruLens](/docs/integrations/providers/trulens/)
    - [Twitter](/docs/integrations/providers/twitter/)
    - [Typesense](/docs/integrations/providers/typesense/)
    - [Unstructured](/docs/integrations/providers/unstructured/)
    - [Upstage](/docs/integrations/providers/upstage/)
    - [upstash](/docs/integrations/providers/upstash/)
    - [UpTrain](/docs/integrations/providers/uptrain/)
    - [USearch](/docs/integrations/providers/usearch/)
    - [Valthera](/docs/integrations/providers/valthera/)
    - [VDMS](/docs/integrations/providers/vdms/)
    - [Vearch](/docs/integrations/providers/vearch/)
    - [Vectara](/docs/integrations/providers/vectara/)
    - [Vectorize](/docs/integrations/providers/vectorize/)
    - [Vespa](/docs/integrations/providers/vespa/)
    - [vlite](/docs/integrations/providers/vlite/)
    - [VoyageAI](/docs/integrations/providers/voyageai/)
    - [Weights &amp; Biases](/docs/integrations/providers/wandb/)
    - [Weights &amp; Biases tracing](/docs/integrations/providers/wandb_tracing/)
    - [Weights &amp; Biases tracking](/docs/integrations/providers/wandb_tracking/)
    - [Weather](/docs/integrations/providers/weather/)
    - [Weaviate](/docs/integrations/providers/weaviate/)
    - [WhatsApp](/docs/integrations/providers/whatsapp/)
    - [WhyLabs](/docs/integrations/providers/whylabs_profiling/)
    - [Wikipedia](/docs/integrations/providers/wikipedia/)
    - [Wolfram Alpha](/docs/integrations/providers/wolfram_alpha/)
    - [Writer, Inc.](/docs/integrations/providers/writer/)
    - [xAI](/docs/integrations/providers/xai/)
    - [Xata](/docs/integrations/providers/xata/)
    - [Xorbits Inference (Xinference)](/docs/integrations/providers/xinference/)
    - [Yahoo](/docs/integrations/providers/yahoo/)
    - [Yandex](/docs/integrations/providers/yandex/)
    - [YDB](/docs/integrations/providers/ydb/)
    - [Yeager.ai](/docs/integrations/providers/yeagerai/)
    - [Yellowbrick](/docs/integrations/providers/yellowbrick/)
    - [01.AI](/docs/integrations/providers/yi/)
    - [You](/docs/integrations/providers/you/)
    - [YouTube](/docs/integrations/providers/youtube/)
    - [Zep](/docs/integrations/providers/zep/)
    - [Zhipu AI](/docs/integrations/providers/zhipuai/)
    - [Zilliz](/docs/integrations/providers/zilliz/)
    - [Zotero](/docs/integrations/providers/zotero/)
- [Components](/docs/integrations/components/)
  
  - [Chat models](/docs/integrations/chat/)
    
    - [Chat models](/docs/integrations/chat/)
    - [Abso](/docs/integrations/chat/abso/)
    - [AI21 Labs](/docs/integrations/chat/ai21/)
    - [Alibaba Cloud PAI EAS](/docs/integrations/chat/alibaba_cloud_pai_eas/)
    - [Anthropic](/docs/integrations/chat/anthropic/)
    - [\[Deprecated\] Experimental Anthropic Tools Wrapper](/docs/integrations/chat/anthropic_functions/)
    - [Anyscale](/docs/integrations/chat/anyscale/)
    - [AzureAIChatCompletionsModel](/docs/integrations/chat/azure_ai/)
    - [Azure OpenAI](/docs/integrations/chat/azure_chat_openai/)
    - [Azure ML Endpoint](/docs/integrations/chat/azureml_chat_endpoint/)
    - [Baichuan Chat](/docs/integrations/chat/baichuan/)
    - [Baidu Qianfan](/docs/integrations/chat/baidu_qianfan_endpoint/)
    - [AWS Bedrock](/docs/integrations/chat/bedrock/)
    - [Cerebras](/docs/integrations/chat/cerebras/)
    - [CloudflareWorkersAI](/docs/integrations/chat/cloudflare_workersai/)
    - [Cohere](/docs/integrations/chat/cohere/)
    - [ContextualAI](/docs/integrations/chat/contextual/)
    - [Coze Chat](/docs/integrations/chat/coze/)
    - [Dappier AI](/docs/integrations/chat/dappier/)
    - [Databricks](/docs/integrations/chat/databricks/)
    - [DeepInfra](/docs/integrations/chat/deepinfra/)
    - [DeepSeek](/docs/integrations/chat/deepseek/)
    - [Eden AI](/docs/integrations/chat/edenai/)
    - [Ernie Bot Chat](/docs/integrations/chat/ernie/)
    - [EverlyAI](/docs/integrations/chat/everlyai/)
    - [Fireworks](/docs/integrations/chat/fireworks/)
    - [ChatFriendli](/docs/integrations/chat/friendli/)
    - [GigaChat](/docs/integrations/chat/gigachat/)
    - [Goodfire](/docs/integrations/chat/goodfire/)
    - [Google AI](/docs/integrations/chat/google_generative_ai/)
    - [Google Cloud Vertex AI](/docs/integrations/chat/google_vertex_ai_palm/)
    - [GPTRouter](/docs/integrations/chat/gpt_router/)
    - [Groq](/docs/integrations/chat/groq/)
    - [ChatHuggingFace](/docs/integrations/chat/huggingface/)
    - [IBM watsonx.ai](/docs/integrations/chat/ibm_watsonx/)
    - [JinaChat](/docs/integrations/chat/jinachat/)
    - [Kinetica](/docs/integrations/chat/kinetica/)
    - [Konko](/docs/integrations/chat/konko/)
    - [LiteLLM](/docs/integrations/chat/litellm/)
    - [LiteLLM Router](/docs/integrations/chat/litellm_router/)
    - [Llama 2 Chat](/docs/integrations/chat/llama2_chat/)
    - [Llama API](/docs/integrations/chat/llama_api/)
    - [LlamaEdge](/docs/integrations/chat/llama_edge/)
    - [Llama.cpp](/docs/integrations/chat/llamacpp/)
    - [maritalk](/docs/integrations/chat/maritalk/)
    - [MiniMax](/docs/integrations/chat/minimax/)
    - [MistralAI](/docs/integrations/chat/mistralai/)
    - [MLX](/docs/integrations/chat/mlx/)
    - [ModelScope](/docs/integrations/chat/modelscope_chat_endpoint/)
    - [Moonshot](/docs/integrations/chat/moonshot/)
    - [Naver](/docs/integrations/chat/naver/)
    - [Netmind](/docs/integrations/chat/netmind/)
    - [NVIDIA AI Endpoints](/docs/integrations/chat/nvidia_ai_endpoints/)
    - [ChatOCIModelDeployment](/docs/integrations/chat/oci_data_science/)
    - [OCIGenAI](/docs/integrations/chat/oci_generative_ai/)
    - [ChatOctoAI](/docs/integrations/chat/octoai/)
    - [Ollama](/docs/integrations/chat/ollama/)
    - [OpenAI](/docs/integrations/chat/openai/)
    - [Outlines](/docs/integrations/chat/outlines/)
    - [Perplexity](/docs/integrations/chat/perplexity/)
    - [Pipeshift](/docs/integrations/chat/pipeshift/)
    - [ChatPredictionGuard](/docs/integrations/chat/predictionguard/)
    - [PremAI](/docs/integrations/chat/premai/)
    - [PromptLayer ChatOpenAI](/docs/integrations/chat/promptlayer_chatopenai/)
    - [Qwen QwQ](/docs/integrations/chat/qwq/)
    - [Reka](/docs/integrations/chat/reka/)
    - [RunPod Chat Model](/docs/integrations/chat/runpod/)
    - [SambaNovaCloud](/docs/integrations/chat/sambanova/)
    - [SambaStudio](/docs/integrations/chat/sambastudio/)
    - [ChatSeekrFlow](/docs/integrations/chat/seekrflow/)
    - [Snowflake Cortex](/docs/integrations/chat/snowflake/)
    - [solar](/docs/integrations/chat/solar/)
    - [SparkLLM Chat](/docs/integrations/chat/sparkllm/)
    - [Nebula (Symbl.ai)](/docs/integrations/chat/symblai_nebula/)
    - [Tencent Hunyuan](/docs/integrations/chat/tencent_hunyuan/)
    - [Together](/docs/integrations/chat/together/)
    - [Tongyi Qwen](/docs/integrations/chat/tongyi/)
    - [Upstage](/docs/integrations/chat/upstage/)
    - [vectara](/docs/integrations/chat/vectara/)
    - [vLLM Chat](/docs/integrations/chat/vllm/)
    - [Volc Enging Maas](/docs/integrations/chat/volcengine_maas/)
    - [Chat Writer](/docs/integrations/chat/writer/)
    - [xAI](/docs/integrations/chat/xai/)
    - [Xinference](/docs/integrations/chat/xinference/)
    - [YandexGPT](/docs/integrations/chat/yandex/)
    - [ChatYI](/docs/integrations/chat/yi/)
    - [Yuan2.0](/docs/integrations/chat/yuan2/)
    - [ZHIPU AI](/docs/integrations/chat/zhipuai/)
  - [Retrievers](/docs/integrations/retrievers/)
    
    - [Retrievers](/docs/integrations/retrievers/)
    - [Activeloop Deep Memory](/docs/integrations/retrievers/activeloop/)
    - [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/)
    - [Arcee](/docs/integrations/retrievers/arcee/)
    - [Arxiv](/docs/integrations/retrievers/arxiv/)
    - [AskNews](/docs/integrations/retrievers/asknews/)
    - [Azure AI Search](/docs/integrations/retrievers/azure_ai_search/)
    - [Bedrock (Knowledge Bases)](/docs/integrations/retrievers/bedrock/)
    - [BM25](/docs/integrations/retrievers/bm25/)
    - [Box](/docs/integrations/retrievers/box/)
    - [BREEBS (Open Knowledge)](/docs/integrations/retrievers/breebs/)
    - [Chaindesk](/docs/integrations/retrievers/chaindesk/)
    - [ChatGPT plugin](/docs/integrations/retrievers/chatgpt-plugin/)
    - [Cognee](/docs/integrations/retrievers/cognee/)
    - [Cohere reranker](/docs/integrations/retrievers/cohere-reranker/)
    - [Cohere RAG](/docs/integrations/retrievers/cohere/)
    - [Contextual AI Reranker](/docs/integrations/retrievers/contextual/)
    - [Dappier](/docs/integrations/retrievers/dappier/)
    - [DocArray](/docs/integrations/retrievers/docarray_retriever/)
    - [Dria](/docs/integrations/retrievers/dria_index/)
    - [ElasticSearch BM25](/docs/integrations/retrievers/elastic_search_bm25/)
    - [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/)
    - [Embedchain](/docs/integrations/retrievers/embedchain/)
    - [FlashRank reranker](/docs/integrations/retrievers/flashrank-reranker/)
    - [Fleet AI Context](/docs/integrations/retrievers/fleet_context/)
    - [Galaxia](/docs/integrations/retrievers/galaxia-retriever/)
    - [Google Drive](/docs/integrations/retrievers/google_drive/)
    - [Google Vertex AI Search](/docs/integrations/retrievers/google_vertex_ai_search/)
    - [Graph RAG](/docs/integrations/retrievers/graph_rag/)
    - [IBM watsonx.ai](/docs/integrations/retrievers/ibm_watsonx_ranker/)
    - [JaguarDB Vector Database](/docs/integrations/retrievers/jaguar/)
    - [Kay.ai](/docs/integrations/retrievers/kay/)
    - [Kinetica Vectorstore based Retriever](/docs/integrations/retrievers/kinetica/)
    - [kNN](/docs/integrations/retrievers/knn/)
    - [LinkupSearchRetriever](/docs/integrations/retrievers/linkup_search/)
    - [LLMLingua Document Compressor](/docs/integrations/retrievers/llmlingua/)
    - [LOTR (Merger Retriever)](/docs/integrations/retrievers/merger_retriever/)
    - [Metal](/docs/integrations/retrievers/metal/)
    - [Milvus Hybrid Search](/docs/integrations/retrievers/milvus_hybrid_search/)
    - [NanoPQ (Product Quantization)](/docs/integrations/retrievers/nanopq/)
    - [needle](/docs/integrations/retrievers/needle/)
    - [Nimble](/docs/integrations/retrievers/nimble/)
    - [Outline](/docs/integrations/retrievers/outline/)
    - [Permit](/docs/integrations/retrievers/permit/)
    - [Pinecone Hybrid Search](/docs/integrations/retrievers/pinecone_hybrid_search/)
    - [PubMed](/docs/integrations/retrievers/pubmed/)
    - [Qdrant Sparse Vector](/docs/integrations/retrievers/qdrant-sparse/)
    - [RAGatouille](/docs/integrations/retrievers/ragatouille/)
    - [RePhraseQuery](/docs/integrations/retrievers/re_phrase/)
    - [Rememberizer](/docs/integrations/retrievers/rememberizer/)
    - [SEC filing](/docs/integrations/retrievers/sec_filings/)
    - [Self-querying retrievers](/docs/integrations/retrievers/self_query/)
    - [SVM](/docs/integrations/retrievers/svm/)
    - [TavilySearchAPI](/docs/integrations/retrievers/tavily/)
    - [TF-IDF](/docs/integrations/retrievers/tf_idf/)
    - [\*\*NeuralDB\*\*](/docs/integrations/retrievers/thirdai_neuraldb/)
    - [Vectorize](/docs/integrations/retrievers/vectorize/)
    - [Vespa](/docs/integrations/retrievers/vespa/)
    - [Wikipedia](/docs/integrations/retrievers/wikipedia/)
    - [You.com](/docs/integrations/retrievers/you-retriever/)
    - [Zep Cloud](/docs/integrations/retrievers/zep_cloud_memorystore/)
    - [Zep Open Source](/docs/integrations/retrievers/zep_memorystore/)
    - [Zilliz Cloud Pipeline](/docs/integrations/retrievers/zilliz_cloud_pipeline/)
    - [Zotero](/docs/integrations/retrievers/zotero/)
  - [Tools/Toolkits](/docs/integrations/tools/)
    
    - [Tools](/docs/integrations/tools/)
    - [ADS4GPTs](/docs/integrations/tools/ads4gpts/)
    - [AgentQL](/docs/integrations/tools/agentql/)
    - [AINetwork Toolkit](/docs/integrations/tools/ainetwork/)
    - [Alpha Vantage](/docs/integrations/tools/alpha_vantage/)
    - [Amadeus Toolkit](/docs/integrations/tools/amadeus/)
    - [Apify Actor](/docs/integrations/tools/apify_actors/)
    - [ArXiv](/docs/integrations/tools/arxiv/)
    - [AskNews](/docs/integrations/tools/asknews/)
    - [AWS Lambda](/docs/integrations/tools/awslambda/)
    - [Azure AI Services Toolkit](/docs/integrations/tools/azure_ai_services/)
    - [Azure Cognitive Services Toolkit](/docs/integrations/tools/azure_cognitive_services/)
    - [Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions/)
    - [Shell (bash)](/docs/integrations/tools/bash/)
    - [Bearly Code Interpreter](/docs/integrations/tools/bearly/)
    - [Bing Search](/docs/integrations/tools/bing_search/)
    - [Brave Search](/docs/integrations/tools/brave_search/)
    - [Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database/)
    - [CDP](/docs/integrations/tools/cdp_agentkit/)
    - [ChatGPT Plugins](/docs/integrations/tools/chatgpt_plugins/)
    - [ClickUp Toolkit](/docs/integrations/tools/clickup/)
    - [Cogniswitch Toolkit](/docs/integrations/tools/cogniswitch/)
    - [Connery Toolkit and Tools](/docs/integrations/tools/connery/)
    - [Dall-E Image Generator](/docs/integrations/tools/dalle_image_generator/)
    - [Dappier](/docs/integrations/tools/dappier/)
    - [Databricks Unity Catalog (UC)](/docs/integrations/tools/databricks/)
    - [DataForSEO](/docs/integrations/tools/dataforseo/)
    - [Dataherald](/docs/integrations/tools/dataherald/)
    - [DuckDuckGo Search](/docs/integrations/tools/ddg/)
    - [Discord](/docs/integrations/tools/discord/)
    - [E2B Data Analysis](/docs/integrations/tools/e2b_data_analysis/)
    - [Eden AI](/docs/integrations/tools/edenai_tools/)
    - [ElevenLabs Text2Speech](/docs/integrations/tools/eleven_labs_tts/)
    - [Exa Search](/docs/integrations/tools/exa_search/)
    - [File System](/docs/integrations/tools/filesystem/)
    - [FinancialDatasets Toolkit](/docs/integrations/tools/financial_datasets/)
    - [FMP Data](/docs/integrations/tools/fmp-data/)
    - [Github Toolkit](/docs/integrations/tools/github/)
    - [Gitlab Toolkit](/docs/integrations/tools/gitlab/)
    - [Gmail Toolkit](/docs/integrations/tools/gmail/)
    - [GOAT](/docs/integrations/tools/goat/)
    - [Golden Query](/docs/integrations/tools/golden_query/)
    - [Google Books](/docs/integrations/tools/google_books/)
    - [Google Calendar Toolkit](/docs/integrations/tools/google_calendar/)
    - [Google Cloud Text-to-Speech](/docs/integrations/tools/google_cloud_texttospeech/)
    - [Google Drive](/docs/integrations/tools/google_drive/)
    - [Google Finance](/docs/integrations/tools/google_finance/)
    - [Google Imagen](/docs/integrations/tools/google_imagen/)
    - [Google Jobs](/docs/integrations/tools/google_jobs/)
    - [Google Lens](/docs/integrations/tools/google_lens/)
    - [Google Places](/docs/integrations/tools/google_places/)
    - [Google Scholar](/docs/integrations/tools/google_scholar/)
    - [Google Search](/docs/integrations/tools/google_search/)
    - [Google Serper](/docs/integrations/tools/google_serper/)
    - [Google Trends](/docs/integrations/tools/google_trends/)
    - [Gradio](/docs/integrations/tools/gradio_tools/)
    - [GraphQL](/docs/integrations/tools/graphql/)
    - [HuggingFace Hub Tools](/docs/integrations/tools/huggingface_tools/)
    - [Human as a tool](/docs/integrations/tools/human_tools/)
    - [Hyperbrowser Browser Agent Tools](/docs/integrations/tools/hyperbrowser_browser_agent_tools/)
    - [Hyperbrowser Web Scraping Tools](/docs/integrations/tools/hyperbrowser_web_scraping_tools/)
    - [IBM watsonx.ai](/docs/integrations/tools/ibm_watsonx/)
    - [IFTTT WebHooks](/docs/integrations/tools/ifttt/)
    - [Infobip](/docs/integrations/tools/infobip/)
    - [Ionic Shopping Tool](/docs/integrations/tools/ionic_shopping/)
    - [Jenkins](/docs/integrations/tools/jenkins/)
    - [Jina Search](/docs/integrations/tools/jina_search/)
    - [Jira Toolkit](/docs/integrations/tools/jira/)
    - [JSON Toolkit](/docs/integrations/tools/json/)
    - [Lemon Agent](/docs/integrations/tools/lemonai/)
    - [LinkupSearchTool](/docs/integrations/tools/linkup_search/)
    - [Memgraph](/docs/integrations/tools/memgraph/)
    - [Memorize](/docs/integrations/tools/memorize/)
    - [Mojeek Search](/docs/integrations/tools/mojeek_search/)
    - [MultiOn Toolkit](/docs/integrations/tools/multion/)
    - [NASA Toolkit](/docs/integrations/tools/nasa/)
    - [Naver Search](/docs/integrations/tools/naver_search/)
    - [Nuclia Understanding](/docs/integrations/tools/nuclia/)
    - [NVIDIA Riva: ASR and TTS](/docs/integrations/tools/nvidia_riva/)
    - [Office365 Toolkit](/docs/integrations/tools/office365/)
    - [OpenAPI Toolkit](/docs/integrations/tools/openapi/)
    - [Natural Language API Toolkits](/docs/integrations/tools/openapi_nla/)
    - [OpenGradient](/docs/integrations/tools/opengradient_toolkit/)
    - [OpenWeatherMap](/docs/integrations/tools/openweathermap/)
    - [Oracle AI Vector Search: Generate Summary](/docs/integrations/tools/oracleai/)
    - [Oxylabs](/docs/integrations/tools/oxylabs/)
    - [Pandas Dataframe](/docs/integrations/tools/pandas/)
    - [Passio NutritionAI](/docs/integrations/tools/passio_nutrition_ai/)
    - [PaymanAI](/docs/integrations/tools/payman-tool/)
    - [Permit](/docs/integrations/tools/permit/)
    - [PlayWright Browser Toolkit](/docs/integrations/tools/playwright/)
    - [Polygon IO Toolkit and Tools](/docs/integrations/tools/polygon/)
    - [PowerBI Toolkit](/docs/integrations/tools/powerbi/)
    - [Prolog](/docs/integrations/tools/prolog_tool/)
    - [PubMed](/docs/integrations/tools/pubmed/)
    - [Python REPL](/docs/integrations/tools/python/)
    - [Reddit Search](/docs/integrations/tools/reddit_search/)
    - [Requests Toolkit](/docs/integrations/tools/requests/)
    - [Riza Code Interpreter](/docs/integrations/tools/riza/)
    - [Robocorp Toolkit](/docs/integrations/tools/robocorp/)
    - [Salesforce](/docs/integrations/tools/salesforce/)
    - [SceneXplain](/docs/integrations/tools/sceneXplain/)
    - [ScrapeGraph](/docs/integrations/tools/scrapegraph/)
    - [SearchApi](/docs/integrations/tools/searchapi/)
    - [SearxNG Search](/docs/integrations/tools/searx_search/)
    - [Semantic Scholar API Tool](/docs/integrations/tools/semanticscholar/)
    - [SerpAPI](/docs/integrations/tools/serpapi/)
    - [Slack Toolkit](/docs/integrations/tools/slack/)
    - [Spark SQL Toolkit](/docs/integrations/tools/spark_sql/)
    - [SQLDatabase Toolkit](/docs/integrations/tools/sql_database/)
    - [StackExchange](/docs/integrations/tools/stackexchange/)
    - [Steam Toolkit](/docs/integrations/tools/steam/)
    - [Stripe](/docs/integrations/tools/stripe/)
    - [Tableau](/docs/integrations/tools/tableau/)
    - [Taiga](/docs/integrations/tools/taiga/)
    - [Tavily Extract](/docs/integrations/tools/tavily_extract/)
    - [Tavily Search](/docs/integrations/tools/tavily_search/)
    - [Tilores](/docs/integrations/tools/tilores/)
    - [Twilio](/docs/integrations/tools/twilio/)
    - [Upstage](/docs/integrations/tools/upstage_groundedness_check/)
    - [Valthera](/docs/integrations/tools/valthera/)
    - [Wikidata](/docs/integrations/tools/wikidata/)
    - [Wikipedia](/docs/integrations/tools/wikipedia/)
    - [Wolfram Alpha](/docs/integrations/tools/wolfram_alpha/)
    - [Writer Tools](/docs/integrations/tools/writer/)
    - [Yahoo Finance News](/docs/integrations/tools/yahoo_finance_news/)
    - [You.com Search](/docs/integrations/tools/you/)
    - [YouTube](/docs/integrations/tools/youtube/)
    - [Zapier Natural Language Actions](/docs/integrations/tools/zapier/)
    - [ZenGuard AI](/docs/integrations/tools/zenguard/)
  - [Document loaders](/docs/integrations/document_loaders/)
    
    - [Document loaders](/docs/integrations/document_loaders/)
    - [acreom](/docs/integrations/document_loaders/acreom/)
    - [AgentQLLoader](/docs/integrations/document_loaders/agentql/)
    - [AirbyteLoader](/docs/integrations/document_loaders/airbyte/)
    - [Airbyte CDK (Deprecated)](/docs/integrations/document_loaders/airbyte_cdk/)
    - [Airbyte Gong (Deprecated)](/docs/integrations/document_loaders/airbyte_gong/)
    - [Airbyte Hubspot (Deprecated)](/docs/integrations/document_loaders/airbyte_hubspot/)
    - [Airbyte JSON (Deprecated)](/docs/integrations/document_loaders/airbyte_json/)
    - [Airbyte Salesforce (Deprecated)](/docs/integrations/document_loaders/airbyte_salesforce/)
    - [Airbyte Shopify (Deprecated)](/docs/integrations/document_loaders/airbyte_shopify/)
    - [Airbyte Stripe (Deprecated)](/docs/integrations/document_loaders/airbyte_stripe/)
    - [Airbyte Typeform (Deprecated)](/docs/integrations/document_loaders/airbyte_typeform/)
    - [Airbyte Zendesk Support (Deprecated)](/docs/integrations/document_loaders/airbyte_zendesk_support/)
    - [Airtable](/docs/integrations/document_loaders/airtable/)
    - [Alibaba Cloud MaxCompute](/docs/integrations/document_loaders/alibaba_cloud_maxcompute/)
    - [Amazon Textract](/docs/integrations/document_loaders/amazon_textract/)
    - [Apify Dataset](/docs/integrations/document_loaders/apify_dataset/)
    - [ArcGIS](/docs/integrations/document_loaders/arcgis/)
    - [ArxivLoader](/docs/integrations/document_loaders/arxiv/)
    - [AssemblyAI Audio Transcripts](/docs/integrations/document_loaders/assemblyai/)
    - [AstraDB](/docs/integrations/document_loaders/astradb/)
    - [Async Chromium](/docs/integrations/document_loaders/async_chromium/)
    - [AsyncHtml](/docs/integrations/document_loaders/async_html/)
    - [Athena](/docs/integrations/document_loaders/athena/)
    - [AWS S3 Directory](/docs/integrations/document_loaders/aws_s3_directory/)
    - [AWS S3 File](/docs/integrations/document_loaders/aws_s3_file/)
    - [AZLyrics](/docs/integrations/document_loaders/azlyrics/)
    - [Azure AI Data](/docs/integrations/document_loaders/azure_ai_data/)
    - [Azure Blob Storage Container](/docs/integrations/document_loaders/azure_blob_storage_container/)
    - [Azure Blob Storage File](/docs/integrations/document_loaders/azure_blob_storage_file/)
    - [Azure AI Document Intelligence](/docs/integrations/document_loaders/azure_document_intelligence/)
    - [BibTeX](/docs/integrations/document_loaders/bibtex/)
    - [BiliBili](/docs/integrations/document_loaders/bilibili/)
    - [Blackboard](/docs/integrations/document_loaders/blackboard/)
    - [Blockchain](/docs/integrations/document_loaders/blockchain/)
    - [Box](/docs/integrations/document_loaders/box/)
    - [Brave Search](/docs/integrations/document_loaders/brave_search/)
    - [Browserbase](/docs/integrations/document_loaders/browserbase/)
    - [Browserless](/docs/integrations/document_loaders/browserless/)
    - [BSHTMLLoader](/docs/integrations/document_loaders/bshtml/)
    - [Cassandra](/docs/integrations/document_loaders/cassandra/)
    - [ChatGPT Data](/docs/integrations/document_loaders/chatgpt_loader/)
    - [College Confidential](/docs/integrations/document_loaders/college_confidential/)
    - [Concurrent Loader](/docs/integrations/document_loaders/concurrent/)
    - [Confluence](/docs/integrations/document_loaders/confluence/)
    - [CoNLL-U](/docs/integrations/document_loaders/conll-u/)
    - [Copy Paste](/docs/integrations/document_loaders/copypaste/)
    - [Couchbase](/docs/integrations/document_loaders/couchbase/)
    - [CSV](/docs/integrations/document_loaders/csv/)
    - [Cube Semantic Layer](/docs/integrations/document_loaders/cube_semantic/)
    - [Datadog Logs](/docs/integrations/document_loaders/datadog_logs/)
    - [Dedoc](/docs/integrations/document_loaders/dedoc/)
    - [Diffbot](/docs/integrations/document_loaders/diffbot/)
    - [Discord](/docs/integrations/document_loaders/discord/)
    - [Docling](/docs/integrations/document_loaders/docling/)
    - [Docugami](/docs/integrations/document_loaders/docugami/)
    - [Docusaurus](/docs/integrations/document_loaders/docusaurus/)
    - [Dropbox](/docs/integrations/document_loaders/dropbox/)
    - [DuckDB](/docs/integrations/document_loaders/duckdb/)
    - [Email](/docs/integrations/document_loaders/email/)
    - [EPub](/docs/integrations/document_loaders/epub/)
    - [Etherscan](/docs/integrations/document_loaders/etherscan/)
    - [EverNote](/docs/integrations/document_loaders/evernote/)
    - [example\_data](/docs/integrations/document_loaders/example_data/example/)
    - [Facebook Chat](/docs/integrations/document_loaders/facebook_chat/)
    - [Fauna](/docs/integrations/document_loaders/fauna/)
    - [Figma](/docs/integrations/document_loaders/figma/)
    - [FireCrawl](/docs/integrations/document_loaders/firecrawl/)
    - [Geopandas](/docs/integrations/document_loaders/geopandas/)
    - [Git](/docs/integrations/document_loaders/git/)
    - [GitBook](/docs/integrations/document_loaders/gitbook/)
    - [GitHub](/docs/integrations/document_loaders/github/)
    - [Glue Catalog](/docs/integrations/document_loaders/glue_catalog/)
    - [Google AlloyDB for PostgreSQL](/docs/integrations/document_loaders/google_alloydb/)
    - [Google BigQuery](/docs/integrations/document_loaders/google_bigquery/)
    - [Google Bigtable](/docs/integrations/document_loaders/google_bigtable/)
    - [Google Cloud SQL for SQL server](/docs/integrations/document_loaders/google_cloud_sql_mssql/)
    - [Google Cloud SQL for MySQL](/docs/integrations/document_loaders/google_cloud_sql_mysql/)
    - [Google Cloud SQL for PostgreSQL](/docs/integrations/document_loaders/google_cloud_sql_pg/)
    - [Google Cloud Storage Directory](/docs/integrations/document_loaders/google_cloud_storage_directory/)
    - [Google Cloud Storage File](/docs/integrations/document_loaders/google_cloud_storage_file/)
    - [Google Firestore in Datastore Mode](/docs/integrations/document_loaders/google_datastore/)
    - [Google Drive](/docs/integrations/document_loaders/google_drive/)
    - [Google El Carro for Oracle Workloads](/docs/integrations/document_loaders/google_el_carro/)
    - [Google Firestore (Native Mode)](/docs/integrations/document_loaders/google_firestore/)
    - [Google Memorystore for Redis](/docs/integrations/document_loaders/google_memorystore_redis/)
    - [Google Spanner](/docs/integrations/document_loaders/google_spanner/)
    - [Google Speech-to-Text Audio Transcripts](/docs/integrations/document_loaders/google_speech_to_text/)
    - [Grobid](/docs/integrations/document_loaders/grobid/)
    - [Gutenberg](/docs/integrations/document_loaders/gutenberg/)
    - [Hacker News](/docs/integrations/document_loaders/hacker_news/)
    - [Huawei OBS Directory](/docs/integrations/document_loaders/huawei_obs_directory/)
    - [Huawei OBS File](/docs/integrations/document_loaders/huawei_obs_file/)
    - [HuggingFace dataset](/docs/integrations/document_loaders/hugging_face_dataset/)
    - [HyperbrowserLoader](/docs/integrations/document_loaders/hyperbrowser/)
    - [iFixit](/docs/integrations/document_loaders/ifixit/)
    - [Images](/docs/integrations/document_loaders/image/)
    - [Image captions](/docs/integrations/document_loaders/image_captions/)
    - [IMSDb](/docs/integrations/document_loaders/imsdb/)
    - [Iugu](/docs/integrations/document_loaders/iugu/)
    - [Joplin](/docs/integrations/document_loaders/joplin/)
    - [JSONLoader](/docs/integrations/document_loaders/json/)
    - [Jupyter Notebook](/docs/integrations/document_loaders/jupyter_notebook/)
    - [Kinetica](/docs/integrations/document_loaders/kinetica/)
    - [lakeFS](/docs/integrations/document_loaders/lakefs/)
    - [LangSmith](/docs/integrations/document_loaders/langsmith/)
    - [LarkSuite (FeiShu)](/docs/integrations/document_loaders/larksuite/)
    - [LLM Sherpa](/docs/integrations/document_loaders/llmsherpa/)
    - [Mastodon](/docs/integrations/document_loaders/mastodon/)
    - [MathPixPDFLoader](/docs/integrations/document_loaders/mathpix/)
    - [MediaWiki Dump](/docs/integrations/document_loaders/mediawikidump/)
    - [Merge Documents Loader](/docs/integrations/document_loaders/merge_doc/)
    - [mhtml](/docs/integrations/document_loaders/mhtml/)
    - [Microsoft Excel](/docs/integrations/document_loaders/microsoft_excel/)
    - [Microsoft OneDrive](/docs/integrations/document_loaders/microsoft_onedrive/)
    - [Microsoft OneNote](/docs/integrations/document_loaders/microsoft_onenote/)
    - [Microsoft PowerPoint](/docs/integrations/document_loaders/microsoft_powerpoint/)
    - [Microsoft SharePoint](/docs/integrations/document_loaders/microsoft_sharepoint/)
    - [Microsoft Word](/docs/integrations/document_loaders/microsoft_word/)
    - [Near Blockchain](/docs/integrations/document_loaders/mintbase/)
    - [Modern Treasury](/docs/integrations/document_loaders/modern_treasury/)
    - [MongoDB](/docs/integrations/document_loaders/mongodb/)
    - [Needle Document Loader](/docs/integrations/document_loaders/needle/)
    - [News URL](/docs/integrations/document_loaders/news/)
    - [Notion DB 2/2](/docs/integrations/document_loaders/notion/)
    - [Nuclia](/docs/integrations/document_loaders/nuclia/)
    - [Obsidian](/docs/integrations/document_loaders/obsidian/)
    - [Open Document Format (ODT)](/docs/integrations/document_loaders/odt/)
    - [Open City Data](/docs/integrations/document_loaders/open_city_data/)
    - [Oracle Autonomous Database](/docs/integrations/document_loaders/oracleadb_loader/)
    - [Oracle AI Vector Search: Document Processing](/docs/integrations/document_loaders/oracleai/)
    - [Org-mode](/docs/integrations/document_loaders/org_mode/)
    - [Pandas DataFrame](/docs/integrations/document_loaders/pandas_dataframe/)
    - [parsers](/docs/integrations/document_loaders/parsers/azure_openai_whisper_parser/)
    - [PDFMinerLoader](/docs/integrations/document_loaders/pdfminer/)
    - [PDFPlumber](/docs/integrations/document_loaders/pdfplumber/)
    - [Pebblo Safe DocumentLoader](/docs/integrations/document_loaders/pebblo/)
    - [Polars DataFrame](/docs/integrations/document_loaders/polars_dataframe/)
    - [Dell PowerScale Document Loader](/docs/integrations/document_loaders/powerscale/)
    - [Psychic](/docs/integrations/document_loaders/psychic/)
    - [PubMed](/docs/integrations/document_loaders/pubmed/)
    - [PullMdLoader](/docs/integrations/document_loaders/pull_md/)
    - [PyMuPDFLoader](/docs/integrations/document_loaders/pymupdf/)
    - [PyMuPDF4LLM](/docs/integrations/document_loaders/pymupdf4llm/)
    - [PyPDFDirectoryLoader](/docs/integrations/document_loaders/pypdfdirectory/)
    - [PyPDFium2Loader](/docs/integrations/document_loaders/pypdfium2/)
    - [PyPDFLoader](/docs/integrations/document_loaders/pypdfloader/)
    - [PySpark](/docs/integrations/document_loaders/pyspark_dataframe/)
    - [Quip](/docs/integrations/document_loaders/quip/)
    - [ReadTheDocs Documentation](/docs/integrations/document_loaders/readthedocs_documentation/)
    - [Recursive URL](/docs/integrations/document_loaders/recursive_url/)
    - [Reddit](/docs/integrations/document_loaders/reddit/)
    - [Roam](/docs/integrations/document_loaders/roam/)
    - [Rockset](/docs/integrations/document_loaders/rockset/)
    - [rspace](/docs/integrations/document_loaders/rspace/)
    - [RSS Feeds](/docs/integrations/document_loaders/rss/)
    - [RST](/docs/integrations/document_loaders/rst/)
    - [scrapfly](/docs/integrations/document_loaders/scrapfly/)
    - [ScrapingAnt](/docs/integrations/document_loaders/scrapingant/)
    - [SingleStore](/docs/integrations/document_loaders/singlestore/)
    - [Sitemap](/docs/integrations/document_loaders/sitemap/)
    - [Slack](/docs/integrations/document_loaders/slack/)
    - [Snowflake](/docs/integrations/document_loaders/snowflake/)
    - [Source Code](/docs/integrations/document_loaders/source_code/)
    - [Spider](/docs/integrations/document_loaders/spider/)
    - [Spreedly](/docs/integrations/document_loaders/spreedly/)
    - [Stripe](/docs/integrations/document_loaders/stripe/)
    - [Subtitle](/docs/integrations/document_loaders/subtitle/)
    - [SurrealDB](/docs/integrations/document_loaders/surrealdb/)
    - [Telegram](/docs/integrations/document_loaders/telegram/)
    - [Tencent COS Directory](/docs/integrations/document_loaders/tencent_cos_directory/)
    - [Tencent COS File](/docs/integrations/document_loaders/tencent_cos_file/)
    - [TensorFlow Datasets](/docs/integrations/document_loaders/tensorflow_datasets/)
    - [TiDB](/docs/integrations/document_loaders/tidb/)
    - [2Markdown](/docs/integrations/document_loaders/tomarkdown/)
    - [TOML](/docs/integrations/document_loaders/toml/)
    - [Trello](/docs/integrations/document_loaders/trello/)
    - [TSV](/docs/integrations/document_loaders/tsv/)
    - [Twitter](/docs/integrations/document_loaders/twitter/)
    - [Unstructured](/docs/integrations/document_loaders/unstructured_file/)
    - [UnstructuredMarkdownLoader](/docs/integrations/document_loaders/unstructured_markdown/)
    - [UnstructuredPDFLoader](/docs/integrations/document_loaders/unstructured_pdfloader/)
    - [Upstage](/docs/integrations/document_loaders/upstage/)
    - [URL](/docs/integrations/document_loaders/url/)
    - [Vsdx](/docs/integrations/document_loaders/vsdx/)
    - [Weather](/docs/integrations/document_loaders/weather/)
    - [WebBaseLoader](/docs/integrations/document_loaders/web_base/)
    - [WhatsApp Chat](/docs/integrations/document_loaders/whatsapp_chat/)
    - [Wikipedia](/docs/integrations/document_loaders/wikipedia/)
    - [UnstructuredXMLLoader](/docs/integrations/document_loaders/xml/)
    - [Xorbits Pandas DataFrame](/docs/integrations/document_loaders/xorbits/)
    - [YouTube audio](/docs/integrations/document_loaders/youtube_audio/)
    - [YouTube transcripts](/docs/integrations/document_loaders/youtube_transcript/)
    - [YoutubeLoaderDL](/docs/integrations/document_loaders/yt_dlp/)
    - [Yuque](/docs/integrations/document_loaders/yuque/)
    - [ZeroxPDFLoader](/docs/integrations/document_loaders/zeroxpdfloader/)
  - [Vector stores](/docs/integrations/vectorstores/)
    
    - [Vector stores](/docs/integrations/vectorstores/)
    - [Activeloop Deep Lake](/docs/integrations/vectorstores/activeloop_deeplake/)
    - [Aerospike](/docs/integrations/vectorstores/aerospike/)
    - [Alibaba Cloud OpenSearch](/docs/integrations/vectorstores/alibabacloud_opensearch/)
    - [AnalyticDB](/docs/integrations/vectorstores/analyticdb/)
    - [Annoy](/docs/integrations/vectorstores/annoy/)
    - [Apache Doris](/docs/integrations/vectorstores/apache_doris/)
    - [ApertureDB](/docs/integrations/vectorstores/aperturedb/)
    - [Astra DB Vector Store](/docs/integrations/vectorstores/astradb/)
    - [Atlas](/docs/integrations/vectorstores/atlas/)
    - [AwaDB](/docs/integrations/vectorstores/awadb/)
    - [Azure Cosmos DB Mongo vCore](/docs/integrations/vectorstores/azure_cosmos_db/)
    - [Azure Cosmos DB No SQL](/docs/integrations/vectorstores/azure_cosmos_db_no_sql/)
    - [Azure AI Search](/docs/integrations/vectorstores/azuresearch/)
    - [Bagel](/docs/integrations/vectorstores/bagel/)
    - [BagelDB](/docs/integrations/vectorstores/bageldb/)
    - [Baidu Cloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/baiducloud_vector_search/)
    - [Baidu VectorDB](/docs/integrations/vectorstores/baiduvectordb/)
    - [Apache Cassandra](/docs/integrations/vectorstores/cassandra/)
    - [Chroma](/docs/integrations/vectorstores/chroma/)
    - [Clarifai](/docs/integrations/vectorstores/clarifai/)
    - [ClickHouse](/docs/integrations/vectorstores/clickhouse/)
    - [CloudflareVectorize](/docs/integrations/vectorstores/cloudflare_vectorize/)
    - [Couchbase](/docs/integrations/vectorstores/couchbase/)
    - [DashVector](/docs/integrations/vectorstores/dashvector/)
    - [Databricks](/docs/integrations/vectorstores/databricks_vector_search/)
    - [DingoDB](/docs/integrations/vectorstores/dingo/)
    - [DocArray HnswSearch](/docs/integrations/vectorstores/docarray_hnsw/)
    - [DocArray InMemorySearch](/docs/integrations/vectorstores/docarray_in_memory/)
    - [Amazon Document DB](/docs/integrations/vectorstores/documentdb/)
    - [DuckDB](/docs/integrations/vectorstores/duckdb/)
    - [China Mobile ECloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/ecloud_vector_search/)
    - [Elasticsearch](/docs/integrations/vectorstores/elasticsearch/)
    - [Epsilla](/docs/integrations/vectorstores/epsilla/)
    - [Faiss](/docs/integrations/vectorstores/faiss/)
    - [Faiss (Async)](/docs/integrations/vectorstores/faiss_async/)
    - [FalkorDBVectorStore](/docs/integrations/vectorstores/falkordbvector/)
    - [Google AlloyDB for PostgreSQL](/docs/integrations/vectorstores/google_alloydb/)
    - [Google BigQuery Vector Search](/docs/integrations/vectorstores/google_bigquery_vector_search/)
    - [Google Cloud SQL for MySQL](/docs/integrations/vectorstores/google_cloud_sql_mysql/)
    - [Google Cloud SQL for PostgreSQL](/docs/integrations/vectorstores/google_cloud_sql_pg/)
    - [Firestore](/docs/integrations/vectorstores/google_firestore/)
    - [Google Memorystore for Redis](/docs/integrations/vectorstores/google_memorystore_redis/)
    - [Google Spanner](/docs/integrations/vectorstores/google_spanner/)
    - [Google Vertex AI Feature Store](/docs/integrations/vectorstores/google_vertex_ai_feature_store/)
    - [Google Vertex AI Vector Search](/docs/integrations/vectorstores/google_vertex_ai_vector_search/)
    - [Hippo](/docs/integrations/vectorstores/hippo/)
    - [Hologres](/docs/integrations/vectorstores/hologres/)
    - [Infinispan](/docs/integrations/vectorstores/infinispanvs/)
    - [Jaguar Vector Database](/docs/integrations/vectorstores/jaguar/)
    - [KDB.AI](/docs/integrations/vectorstores/kdbai/)
    - [Kinetica](/docs/integrations/vectorstores/kinetica/)
    - [LanceDB](/docs/integrations/vectorstores/lancedb/)
    - [Lantern](/docs/integrations/vectorstores/lantern/)
    - [Lindorm](/docs/integrations/vectorstores/lindorm/)
    - [LLMRails](/docs/integrations/vectorstores/llm_rails/)
    - [ManticoreSearch VectorStore](/docs/integrations/vectorstores/manticore_search/)
    - [MariaDB](/docs/integrations/vectorstores/mariadb/)
    - [Marqo](/docs/integrations/vectorstores/marqo/)
    - [Meilisearch](/docs/integrations/vectorstores/meilisearch/)
    - [Amazon MemoryDB](/docs/integrations/vectorstores/memorydb/)
    - [Milvus](/docs/integrations/vectorstores/milvus/)
    - [Momento Vector Index (MVI)](/docs/integrations/vectorstores/momento_vector_index/)
    - [MongoDB Atlas](/docs/integrations/vectorstores/mongodb_atlas/)
    - [MyScale](/docs/integrations/vectorstores/myscale/)
    - [Neo4j Vector Index](/docs/integrations/vectorstores/neo4jvector/)
    - [NucliaDB](/docs/integrations/vectorstores/nucliadb/)
    - [Oceanbase](/docs/integrations/vectorstores/oceanbase/)
    - [openGauss](/docs/integrations/vectorstores/opengauss/)
    - [OpenSearch](/docs/integrations/vectorstores/opensearch/)
    - [Oracle AI Vector Search: Vector Store](/docs/integrations/vectorstores/oracle/)
    - [Pathway](/docs/integrations/vectorstores/pathway/)
    - [Postgres Embedding](/docs/integrations/vectorstores/pgembedding/)
    - [PGVecto.rs](/docs/integrations/vectorstores/pgvecto_rs/)
    - [PGVector](/docs/integrations/vectorstores/pgvector/)
    - [Pinecone](/docs/integrations/vectorstores/pinecone/)
    - [Qdrant](/docs/integrations/vectorstores/qdrant/)
    - [Redis](/docs/integrations/vectorstores/redis/)
    - [Relyt](/docs/integrations/vectorstores/relyt/)
    - [Rockset](/docs/integrations/vectorstores/rockset/)
    - [SAP HANA Cloud Vector Engine](/docs/integrations/vectorstores/sap_hanavector/)
    - [ScaNN](/docs/integrations/vectorstores/scann/)
    - [SemaDB](/docs/integrations/vectorstores/semadb/)
    - [SingleStore](/docs/integrations/vectorstores/singlestore/)
    - [scikit-learn](/docs/integrations/vectorstores/sklearn/)
    - [SQLiteVec](/docs/integrations/vectorstores/sqlitevec/)
    - [SQLite-VSS](/docs/integrations/vectorstores/sqlitevss/)
    - [SQLServer](/docs/integrations/vectorstores/sqlserver/)
    - [StarRocks](/docs/integrations/vectorstores/starrocks/)
    - [Supabase (Postgres)](/docs/integrations/vectorstores/supabase/)
    - [SurrealDB](/docs/integrations/vectorstores/surrealdb/)
    - [Tablestore](/docs/integrations/vectorstores/tablestore/)
    - [Tair](/docs/integrations/vectorstores/tair/)
    - [Tencent Cloud VectorDB](/docs/integrations/vectorstores/tencentvectordb/)
    - [ThirdAI NeuralDB](/docs/integrations/vectorstores/thirdai_neuraldb/)
    - [TiDB Vector](/docs/integrations/vectorstores/tidb_vector/)
    - [Tigris](/docs/integrations/vectorstores/tigris/)
    - [TileDB](/docs/integrations/vectorstores/tiledb/)
    - [Timescale Vector (Postgres)](/docs/integrations/vectorstores/timescalevector/)
    - [Typesense](/docs/integrations/vectorstores/typesense/)
    - [Upstash Vector](/docs/integrations/vectorstores/upstash/)
    - [USearch](/docs/integrations/vectorstores/usearch/)
    - [Vald](/docs/integrations/vectorstores/vald/)
    - [VDMS](/docs/integrations/vectorstores/vdms/)
    - [Vearch](/docs/integrations/vectorstores/vearch/)
    - [Vectara](/docs/integrations/vectorstores/vectara/)
    - [Vespa](/docs/integrations/vectorstores/vespa/)
    - [viking DB](/docs/integrations/vectorstores/vikingdb/)
    - [vlite](/docs/integrations/vectorstores/vlite/)
    - [Weaviate](/docs/integrations/vectorstores/weaviate/)
    - [Xata](/docs/integrations/vectorstores/xata/)
    - [YDB](/docs/integrations/vectorstores/ydb/)
    - [Yellowbrick](/docs/integrations/vectorstores/yellowbrick/)
    - [Zep](/docs/integrations/vectorstores/zep/)
    - [Zep Cloud](/docs/integrations/vectorstores/zep_cloud/)
    - [Zilliz](/docs/integrations/vectorstores/zilliz/)
  - [Embedding models](/docs/integrations/text_embedding/)
    
    - [Embedding models](/docs/integrations/text_embedding/)
    - [AI21](/docs/integrations/text_embedding/ai21/)
    - [Aleph Alpha](/docs/integrations/text_embedding/aleph_alpha/)
    - [Anyscale](/docs/integrations/text_embedding/anyscale/)
    - [ascend](/docs/integrations/text_embedding/ascend/)
    - [AwaDB](/docs/integrations/text_embedding/awadb/)
    - [AzureOpenAI](/docs/integrations/text_embedding/azureopenai/)
    - [Baichuan Text Embeddings](/docs/integrations/text_embedding/baichuan/)
    - [Baidu Qianfan](/docs/integrations/text_embedding/baidu_qianfan_endpoint/)
    - [Bedrock](/docs/integrations/text_embedding/bedrock/)
    - [BGE on Hugging Face](/docs/integrations/text_embedding/bge_huggingface/)
    - [Bookend AI](/docs/integrations/text_embedding/bookend/)
    - [Clarifai](/docs/integrations/text_embedding/clarifai/)
    - [Cloudflare Workers AI](/docs/integrations/text_embedding/cloudflare_workersai/)
    - [Clova Embeddings](/docs/integrations/text_embedding/clova/)
    - [Cohere](/docs/integrations/text_embedding/cohere/)
    - [DashScope](/docs/integrations/text_embedding/dashscope/)
    - [Databricks](/docs/integrations/text_embedding/databricks/)
    - [DeepInfra](/docs/integrations/text_embedding/deepinfra/)
    - [EDEN AI](/docs/integrations/text_embedding/edenai/)
    - [Elasticsearch](/docs/integrations/text_embedding/elasticsearch/)
    - [Embaas](/docs/integrations/text_embedding/embaas/)
    - [ERNIE](/docs/integrations/text_embedding/ernie/)
    - [Fake Embeddings](/docs/integrations/text_embedding/fake/)
    - [FastEmbed by Qdrant](/docs/integrations/text_embedding/fastembed/)
    - [Fireworks](/docs/integrations/text_embedding/fireworks/)
    - [GigaChat](/docs/integrations/text_embedding/gigachat/)
    - [Google Generative AI Embeddings](/docs/integrations/text_embedding/google_generative_ai/)
    - [Google Vertex AI](/docs/integrations/text_embedding/google_vertex_ai_palm/)
    - [GPT4All](/docs/integrations/text_embedding/gpt4all/)
    - [Gradient](/docs/integrations/text_embedding/gradient/)
    - [Hugging Face](/docs/integrations/text_embedding/huggingfacehub/)
    - [IBM watsonx.ai](/docs/integrations/text_embedding/ibm_watsonx/)
    - [Infinity](/docs/integrations/text_embedding/infinity/)
    - [Instruct Embeddings on Hugging Face](/docs/integrations/text_embedding/instruct_embeddings/)
    - [IPEX-LLM: Local BGE Embeddings on Intel CPU](/docs/integrations/text_embedding/ipex_llm/)
    - [IPEX-LLM: Local BGE Embeddings on Intel GPU](/docs/integrations/text_embedding/ipex_llm_gpu/)
    - [IntelÂ® Extension for Transformers Quantized Text Embeddings](/docs/integrations/text_embedding/itrex/)
    - [Jina](/docs/integrations/text_embedding/jina/)
    - [John Snow Labs](/docs/integrations/text_embedding/johnsnowlabs_embedding/)
    - [LASER Language-Agnostic SEntence Representations Embeddings by Meta AI](/docs/integrations/text_embedding/laser/)
    - [Lindorm](/docs/integrations/text_embedding/lindorm/)
    - [Llama.cpp](/docs/integrations/text_embedding/llamacpp/)
    - [llamafile](/docs/integrations/text_embedding/llamafile/)
    - [LLMRails](/docs/integrations/text_embedding/llm_rails/)
    - [LocalAI](/docs/integrations/text_embedding/localai/)
    - [MiniMax](/docs/integrations/text_embedding/minimax/)
    - [MistralAI](/docs/integrations/text_embedding/mistralai/)
    - [model2vec](/docs/integrations/text_embedding/model2vec/)
    - [ModelScope](/docs/integrations/text_embedding/modelscope_embedding/)
    - [MosaicML](/docs/integrations/text_embedding/mosaicml/)
    - [Naver](/docs/integrations/text_embedding/naver/)
    - [Netmind](/docs/integrations/text_embedding/netmind/)
    - [NLP Cloud](/docs/integrations/text_embedding/nlp_cloud/)
    - [Nomic](/docs/integrations/text_embedding/nomic/)
    - [NVIDIA NIMs](/docs/integrations/text_embedding/nvidia_ai_endpoints/)
    - [Oracle Cloud Infrastructure Generative AI](/docs/integrations/text_embedding/oci_generative_ai/)
    - [Ollama](/docs/integrations/text_embedding/ollama/)
    - [OpenClip](/docs/integrations/text_embedding/open_clip/)
    - [OpenAI](/docs/integrations/text_embedding/openai/)
    - [OpenVINO](/docs/integrations/text_embedding/openvino/)
    - [Embedding Documents using Optimized and Quantized Embedders](/docs/integrations/text_embedding/optimum_intel/)
    - [Oracle AI Vector Search: Generate Embeddings](/docs/integrations/text_embedding/oracleai/)
    - [OVHcloud](/docs/integrations/text_embedding/ovhcloud/)
    - [Pinecone Embeddings](/docs/integrations/text_embedding/pinecone/)
    - [PredictionGuardEmbeddings](/docs/integrations/text_embedding/predictionguard/)
    - [PremAI](/docs/integrations/text_embedding/premai/)
    - [SageMaker](/docs/integrations/text_embedding/sagemaker-endpoint/)
    - [SambaNovaCloud](/docs/integrations/text_embedding/sambanova/)
    - [SambaStudio](/docs/integrations/text_embedding/sambastudio/)
    - [Self Hosted](/docs/integrations/text_embedding/self-hosted/)
    - [Sentence Transformers on Hugging Face](/docs/integrations/text_embedding/sentence_transformers/)
    - [Solar](/docs/integrations/text_embedding/solar/)
    - [SpaCy](/docs/integrations/text_embedding/spacy_embedding/)
    - [SparkLLM Text Embeddings](/docs/integrations/text_embedding/sparkllm/)
    - [TensorFlow Hub](/docs/integrations/text_embedding/tensorflowhub/)
    - [Text Embeddings Inference](/docs/integrations/text_embedding/text_embeddings_inference/)
    - [TextEmbed - Embedding Inference Server](/docs/integrations/text_embedding/textembed/)
    - [Titan Takeoff](/docs/integrations/text_embedding/titan_takeoff/)
    - [Together AI](/docs/integrations/text_embedding/together/)
    - [Upstage](/docs/integrations/text_embedding/upstage/)
    - [Volc Engine](/docs/integrations/text_embedding/volcengine/)
    - [Voyage AI](/docs/integrations/text_embedding/voyageai/)
    - [Xorbits inference (Xinference)](/docs/integrations/text_embedding/xinference/)
    - [YandexGPT](/docs/integrations/text_embedding/yandex/)
    - [ZhipuAI](/docs/integrations/text_embedding/zhipuai/)
  - [Other](/docs/integrations/llms/)

<!--THE END-->

- [Components](/docs/integrations/components/)
- Chat models


[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/chat/index.mdx)

# Chat models

[Chat models](/docs/concepts/chat_models/) are language models that use a sequence of [messages](/docs/concepts/messages/) as inputs and return messages as outputs (as opposed to using plain text). These are generally newer models.

info

If you'd like to write your own chat model, see [this how-to](/docs/how_to/custom_chat_model/). If you'd like to contribute an integration, see [Contributing integrations](/docs/contributing/how_to/integrations/).

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
model.invoke("Hello, world!")
```

## Featured Providers[â€‹](#featured-providers "Direct link to Featured Providers")

info

While all these LangChain classes support the indicated advanced feature, you may have to open the provider-specific documentation to learn which hosted models or backends support the feature.

| Provider                                        | [Tool calling](/docs/how_to/tool_calling) | [Structured output](/docs/how_to/structured_output/) | JSON mode | Local | [Multimodal](/docs/how_to/multimodal_inputs/) | Package                                                                                                                                                               |
|-------------------------------------------------|-------------------------------------------|------------------------------------------------------|-----------|-------|-----------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [ChatAnthropic](anthropic/)                     | âœ…                                         | âœ…                                                    | âŒ         | âŒ     | âœ…                                             | [langchain-anthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)                            |
| [ChatMistralAI](mistralai/)                     | âœ…                                         | âœ…                                                    | âŒ         | âŒ     | âŒ                                             | [langchain-mistralai](https://python.langchain.com/api_reference/mistralai/chat_models/langchain_mistralai.chat_models.ChatMistralAI.html)                            |
| [ChatFireworks](fireworks/)                     | âœ…                                         | âœ…                                                    | âœ…         | âŒ     | âŒ                                             | [langchain-fireworks](https://python.langchain.com/api_reference/fireworks/chat_models/langchain_fireworks.chat_models.ChatFireworks.html)                            |
| [AzureChatOpenAI](azure_chat_openai/)           | âœ…                                         | âœ…                                                    | âœ…         | âŒ     | âœ…                                             | [langchain-openai](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html)                             |
| [ChatOpenAI](openai/)                           | âœ…                                         | âœ…                                                    | âœ…         | âŒ     | âœ…                                             | [langchain-openai](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)                                   |
| [ChatTogether](together/)                       | âœ…                                         | âœ…                                                    | âœ…         | âŒ     | âŒ                                             | [langchain-together](https://python.langchain.com/api_reference/together/chat_models/langchain_together.chat_models.ChatTogether.html)                                |
| [ChatVertexAI](google_vertex_ai_palm/)          | âœ…                                         | âœ…                                                    | âŒ         | âŒ     | âœ…                                             | [langchain-google-vertexai](https://python.langchain.com/api_reference/google_vertexai/chat_models/langchain_google_vertexai.chat_models.ChatVertexAI.html)           |
| [ChatGoogleGenerativeAI](google_generative_ai/) | âœ…                                         | âœ…                                                    | âŒ         | âŒ     | âœ…                                             | [langchain-google-genai](https://python.langchain.com/api_reference/google_genai/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html)          |
| [ChatGroq](groq/)                               | âœ…                                         | âœ…                                                    | âœ…         | âŒ     | âŒ                                             | [langchain-groq](https://python.langchain.com/api_reference/groq/chat_models/langchain_groq.chat_models.ChatGroq.html)                                                |
| [ChatCohere](cohere/)                           | âœ…                                         | âœ…                                                    | âŒ         | âŒ     | âŒ                                             | [langchain-cohere](https://python.langchain.com/api_reference/cohere/chat_models/langchain_cohere.chat_models.ChatCohere.html)                                        |
| [ChatBedrock](bedrock/)                         | âœ…                                         | âœ…                                                    | âŒ         | âŒ     | âŒ                                             | [langchain-aws](https://python.langchain.com/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock.ChatBedrock.html)                                        |
| [ChatHuggingFace](huggingface/)                 | âœ…                                         | âœ…                                                    | âŒ         | âœ…     | âŒ                                             | [langchain-huggingface](https://python.langchain.com/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.ChatHuggingFace.html)        |
| [ChatNVIDIA](nvidia_ai_endpoints/)              | âœ…                                         | âœ…                                                    | âœ…         | âœ…     | âœ…                                             | [langchain-nvidia-ai-endpoints](https://python.langchain.com/api_reference/nvidia_ai_endpoints/chat_models/langchain_nvidia_ai_endpoints.chat_models.ChatNVIDIA.html) |
| [ChatOllama](ollama/)                           | âœ…                                         | âœ…                                                    | âœ…         | âœ…     | âŒ                                             | [langchain-ollama](https://python.langchain.com/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html)                                        |
| [ChatLlamaCpp](llamacpp)                        | âœ…                                         | âœ…                                                    | âŒ         | âœ…     | âŒ                                             | [langchain-community](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.llamacpp.ChatLlamaCpp.html)                    |
| [ChatAI21](ai21)                                | âœ…                                         | âœ…                                                    | âŒ         | âŒ     | âŒ                                             | [langchain-ai21](https://python.langchain.com/api_reference/ai21/chat_models/langchain_ai21.chat_models.ChatAI21.html)                                                |
| [ChatUpstage](upstage)                          | âœ…                                         | âœ…                                                    | âŒ         | âŒ     | âŒ                                             | [langchain-upstage](https://python.langchain.com/api_reference/upstage/chat_models/langchain_upstage.chat_models.ChatUpstage.html)                                    |
| [ChatDatabricks](databricks)                    | âœ…                                         | âœ…                                                    | âŒ         | âŒ     | âŒ                                             | [databricks-langchain](https://api-docs.databricks.com/python/databricks-ai-bridge/latest/databricks_langchain.html#databricks_langchain.ChatDatabricks)              |
| [ChatWatsonx](ibm_watsonx)                      | âœ…                                         | âœ…                                                    | âœ…         | âŒ     | âŒ                                             | [langchain-ibm](https://python.langchain.com/api_reference/ibm/chat_models/langchain_ibm.chat_models.ChatWatsonx.html)                                                |
| [ChatXAI](xai)                                  | âœ…                                         | âœ…                                                    | âŒ         | âŒ     | âŒ                                             | [langchain-xai](https://python.langchain.com/api_reference/xai/chat_models/langchain_xai.chat_models.ChatXAI.html)                                                    |
| [ChatPerplexity](perplexity)                    | âŒ                                         | âœ…                                                    | âœ…         | âŒ     | âœ…                                             | [langchain-perplexity](https://python.langchain.com/api_reference/perplexity/chat_models/langchain_perplexity.chat_models.ChatPerplexity.html)                        |

## All chat models[â€‹](#all-chat-models "Direct link to All chat models")

| Name                                                                     | Description                                                               |
|--------------------------------------------------------------------------|---------------------------------------------------------------------------|
| [Abso](/docs/integrations/chat/abso)                                     | This will help you getting started with ChatAbso chat models. For det...  |
| [AI21 Labs](/docs/integrations/chat/ai21)                                | Overview                                                                  |
| [Alibaba Cloud PAI EAS](/docs/integrations/chat/alibaba_cloud_pai_eas)   | Alibaba Cloud PAI (Platform for AI) is a lightweight and cost-efficie...  |
| [Anthropic](/docs/integrations/chat/anthropic)                           | This notebook provides a quick overview for getting started with Anth...  |
| [Anyscale](/docs/integrations/chat/anyscale)                             | This notebook demonstrates the use of langchain.chat\_models.ChatAnysc... |
| [AzureAIChatCompletionsModel](/docs/integrations/chat/azure_ai)          | This will help you getting started with AzureAIChatCompletionsModel c...  |
| [Azure OpenAI](/docs/integrations/chat/azure_chat_openai)                | This guide will help you get started with AzureOpenAI chat models. Fo...  |
| [Azure ML Endpoint](/docs/integrations/chat/azureml_chat_endpoint)       | Azure Machine Learning is a platform used to build, train, and deploy...  |
| [Baichuan Chat](/docs/integrations/chat/baichuan)                        | Baichuan chat models API by Baichuan Intelligent Technology. For more...  |
| [Baidu Qianfan](/docs/integrations/chat/baidu_qianfan_endpoint)          | Baidu AI Cloud Qianfan Platform is a one-stop large model development...  |
| [AWS Bedrock](/docs/integrations/chat/bedrock)                           | This doc will help you get started with AWS Bedrock chat models. Amaz...  |
| [Cerebras](/docs/integrations/chat/cerebras)                             | This notebook provides a quick overview for getting started with Cere...  |
| [CloudflareWorkersAI](/docs/integrations/chat/cloudflare_workersai)      | This will help you getting started with CloudflareWorkersAI chat mode...  |
| [Cohere](/docs/integrations/chat/cohere)                                 | This notebook covers how to get started with Cohere chat models.          |
| [ContextualAI](/docs/integrations/chat/contextual)                       | This will help you getting started with Contextual AI's Grounded Lang...  |
| [Coze Chat](/docs/integrations/chat/coze)                                | ChatCoze chat models API by coze.com. For more information, see https...  |
| [Dappier AI](/docs/integrations/chat/dappier)                            | Dappier: Powering AI with Dynamic, Real-Time Data Models                  |
| [Databricks](/docs/integrations/chat/databricks)                         | Databricks Lakehouse Platform unifies data, analytics, and AI on one ...  |
| [DeepInfra](/docs/integrations/chat/deepinfra)                           | DeepInfra is a serverless inference as a service that provides access...  |
| [DeepSeek](/docs/integrations/chat/deepseek)                             | This will help you getting started with DeepSeek's hosted chat models...  |
| [Eden AI](/docs/integrations/chat/edenai)                                | Eden AI is revolutionizing the AI landscape by uniting the best AI pr...  |
| [EverlyAI](/docs/integrations/chat/everlyai)                             | EverlyAI allows you to run your ML models at scale in the cloud. It a...  |
| [Fireworks](/docs/integrations/chat/fireworks)                           | This doc help you get started with Fireworks AI chat models. For deta...  |
| [ChatFriendli](/docs/integrations/chat/friendli)                         | Friendli enhances AI application performance and optimizes cost savin...  |
| [GigaChat](/docs/integrations/chat/gigachat)                             | This notebook shows how to use LangChain with GigaChat.                   |
| [Goodfire](/docs/integrations/chat/goodfire)                             | This will help you getting started with Goodfire chat models. For det...  |
| [Google AI](/docs/integrations/chat/google_generative_ai)                | This docs will help you get started with Google AI chat models. For d...  |
| [Google Cloud Vertex AI](/docs/integrations/chat/google_vertex_ai_palm)  | This page provides a quick overview for getting started with VertexAI...  |
| [GPTRouter](/docs/integrations/chat/gpt_router)                          | GPTRouter is an open source LLM API Gateway that offers a universal A...  |
| [Groq](/docs/integrations/chat/groq)                                     | This will help you getting started with Groq chat models. For detaile...  |
| [ChatHuggingFace](/docs/integrations/chat/huggingface)                   | This will help you getting started with langchainhuggingface chat mod...  |
| [IBM watsonx.ai](/docs/integrations/chat/ibm_watsonx)                    | ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.            |
| [JinaChat](/docs/integrations/chat/jinachat)                             | This notebook covers how to get started with JinaChat chat models.        |
| [Kinetica](/docs/integrations/chat/kinetica)                             | This notebook demonstrates how to use Kinetica to transform natural l...  |
| [Konko](/docs/integrations/chat/konko)                                   | Konko API is a fully managed Web API designed to help application dev...  |
| [LiteLLM](/docs/integrations/chat/litellm)                               | LiteLLM is a library that simplifies calling Anthropic, Azure, Huggin...  |
| [LiteLLM Router](/docs/integrations/chat/litellm_router)                 | LiteLLM is a library that simplifies calling Anthropic, Azure, Huggin...  |
| [Llama 2 Chat](/docs/integrations/chat/llama2_chat)                      | This notebook shows how to augment Llama-2 LLMs with the Llama2Chat w...  |
| [Llama API](/docs/integrations/chat/llama_api)                           | This notebook shows how to use LangChain with LlamaAPI - a hosted ver...  |
| [LlamaEdge](/docs/integrations/chat/llama_edge)                          | LlamaEdge allows you to chat with LLMs of GGUF format both locally an...  |
| [Llama.cpp](/docs/integrations/chat/llamacpp)                            | llama.cpp python library is a simple Python bindings for @ggerganov       |
| [maritalk](/docs/integrations/chat/maritalk)                             | Introduction                                                              |
| [MiniMax](/docs/integrations/chat/minimax)                               | Minimax is a Chinese startup that provides LLM service for companies ...  |
| [MistralAI](/docs/integrations/chat/mistralai)                           | This will help you getting started with Mistral chat models. For deta...  |
| [MLX](/docs/integrations/chat/mlx)                                       | This notebook shows how to get started using MLX LLM's as chat models.    |
| [ModelScope](/docs/integrations/chat/modelscope_chat_endpoint)           | ModelScope (Home \| GitHub) is built upon the notion of â€œModel-as-a-Se... |
| [Moonshot](/docs/integrations/chat/moonshot)                             | Moonshot is a Chinese startup that provides LLM service for companies...  |
| [Naver](/docs/integrations/chat/naver)                                   | This notebook provides a quick overview for getting started with Nave...  |
| [Netmind](/docs/integrations/chat/netmind)                               | This will help you getting started with Netmind chat models. For deta...  |
| [NVIDIA AI Endpoints](/docs/integrations/chat/nvidia_ai_endpoints)       | This will help you getting started with NVIDIA chat models. For detai...  |
| [ChatOCIModelDeployment](/docs/integrations/chat/oci_data_science)       | This will help you getting started with OCIModelDeployment chat model...  |
| [OCIGenAI](/docs/integrations/chat/oci_generative_ai)                    | This notebook provides a quick overview for getting started with OCIG...  |
| [ChatOctoAI](/docs/integrations/chat/octoai)                             | OctoAI offers easy access to efficient compute and enables users to i...  |
| [Ollama](/docs/integrations/chat/ollama)                                 | Ollama allows you to run open-source large language models, such as L...  |
| [OpenAI](/docs/integrations/chat/openai)                                 | This notebook provides a quick overview for getting started with Open...  |
| [Outlines](/docs/integrations/chat/outlines)                             | This will help you getting started with Outlines chat models. For det...  |
| [Perplexity](/docs/integrations/chat/perplexity)                         | This page will help you get started with Perplexity chat models. For ...  |
| [Pipeshift](/docs/integrations/chat/pipeshift)                           | This will help you getting started with Pipeshift chat models. For de...  |
| [ChatPredictionGuard](/docs/integrations/chat/predictionguard)           | Prediction Guard is a secure, scalable GenAI platform that safeguards...  |
| [PremAI](/docs/integrations/chat/premai)                                 | PremAI is an all-in-one platform that simplifies the creation of robu...  |
| [PromptLayer ChatOpenAI](/docs/integrations/chat/promptlayer_chatopenai) | This example showcases how to connect to PromptLayer to start recordi...  |
| [Qwen QwQ](/docs/integrations/chat/qwq)                                  | This will help you getting started with QwQ chat models. For detailed...  |
| [Reka](/docs/integrations/chat/reka)                                     | This notebook provides a quick overview for getting started with Reka...  |
| [RunPod Chat Model](/docs/integrations/chat/runpod)                      | Get started with RunPod chat models.                                      |
| [SambaNovaCloud](/docs/integrations/chat/sambanova)                      | This will help you getting started with SambaNovaCloud chat models. F...  |
| [SambaStudio](/docs/integrations/chat/sambastudio)                       | This will help you getting started with SambaStudio chat models. For ...  |
| [ChatSeekrFlow](/docs/integrations/chat/seekrflow)                       | Seekr provides AI-powered solutions for structured, explainable, and ...  |
| [Snowflake Cortex](/docs/integrations/chat/snowflake)                    | Snowflake Cortex gives you instant access to industry-leading large l...  |
| [solar](/docs/integrations/chat/solar)                                   | Related                                                                   |
| [SparkLLM Chat](/docs/integrations/chat/sparkllm)                        | SparkLLM chat models API by iFlyTek. For more information, see iFlyTe...  |
| [Nebula (Symbl.ai)](/docs/integrations/chat/symblai_nebula)              | Overview                                                                  |
| [Tencent Hunyuan](/docs/integrations/chat/tencent_hunyuan)               | Tencent's hybrid model API (Hunyuan API)                                  |
| [Together](/docs/integrations/chat/together)                             | This page will help you get started with Together AI chat models. For...  |
| [Tongyi Qwen](/docs/integrations/chat/tongyi)                            | Tongyi Qwen is a large language model developed by Alibaba's Damo Aca...  |
| [Upstage](/docs/integrations/chat/upstage)                               | This notebook covers how to get started with Upstage chat models.         |
| [vectara](/docs/integrations/chat/vectara)                               | Overview                                                                  |
| [vLLM Chat](/docs/integrations/chat/vllm)                                | vLLM can be deployed as a server that mimics the OpenAI API protocol....  |
| [Volc Enging Maas](/docs/integrations/chat/volcengine_maas)              | This notebook provides you with a guide on how to get started with vo...  |
| [Chat Writer](/docs/integrations/chat/writer)                            | This notebook provides a quick overview for getting started with Writ...  |
| [xAI](/docs/integrations/chat/xai)                                       | This page will help you get started with xAI chat models. For detaile...  |
| [Xinference](/docs/integrations/chat/xinference)                         | Xinference is a powerful and versatile library designed to serve LLMs,    |
| [YandexGPT](/docs/integrations/chat/yandex)                              | This notebook goes over how to use Langchain with YandexGPT chat mode...  |
| [ChatYI](/docs/integrations/chat/yi)                                     | This will help you getting started with Yi chat models. For detailed ...  |
| [Yuan2.0](/docs/integrations/chat/yuan2)                                 | This notebook shows how to use YUAN2 API in LangChain with the langch...  |
| [ZHIPU AI](/docs/integrations/chat/zhipuai)                              | This notebook shows how to use ZHIPU AI API in LangChain with the lan...  |

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/integrations/chat/index.mdx)

* * *


- [Featured Providers](#featured-providers)
- [All chat models](#all-chat-models)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/example_selectors.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/example_selectors.ipynb)

# How to use example selectors

If you have a large number of examples, you may need to select which ones to include in the prompt. The [Example Selector](/docs/concepts/example_selectors/) is the class responsible for doing so.

The base interface is defined as below:

```python
class BaseExampleSelector(ABC):
    """Interface for selecting examples to include in prompts."""

    @abstractmethod
    def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:
        """Select which examples to use based on the inputs."""
        
    @abstractmethod
    def add_example(self, example: Dict[str, str]) -> Any:
        """Add new example to store."""
```

The only method it needs to define is a `select_examples` method. This takes in the input variables and then returns a list of examples. It is up to each specific implementation as to how those examples are selected.

LangChain has a few different types of example selectors. For an overview of all these types, see the [below table](#example-selector-types).

In this guide, we will walk through creating a custom example selector.

## Examples[â€‹](#examples "Direct link to Examples")

In order to use an example selector, we need to create a list of examples. These should generally be example inputs and outputs. For this demo purpose, let's imagine we are selecting examples of how to translate English to Italian.

```python
examples = [
    {"input": "hi", "output": "ciao"},
    {"input": "bye", "output": "arrivederci"},
    {"input": "soccer", "output": "calcio"},
]
```

## Custom Example Selector[â€‹](#custom-example-selector "Direct link to Custom Example Selector")

Let's write an example selector that chooses what example to pick based on the length of the word.

```python
from langchain_core.example_selectors.base import BaseExampleSelector


class CustomExampleSelector(BaseExampleSelector):
    def __init__(self, examples):
        self.examples = examples

    def add_example(self, example):
        self.examples.append(example)

    def select_examples(self, input_variables):
        # This assumes knowledge that part of the input will be a 'text' key
        new_word = input_variables["input"]
        new_word_length = len(new_word)

        # Initialize variables to store the best match and its length difference
        best_match = None
        smallest_diff = float("inf")

        # Iterate through each example
        for example in self.examples:
            # Calculate the length difference with the first word of the example
            current_diff = abs(len(example["input"]) - new_word_length)

            # Update the best match if the current one is closer in length
            if current_diff < smallest_diff:
                smallest_diff = current_diff
                best_match = example

        return [best_match]
```

**API Reference:**[BaseExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.base.BaseExampleSelector.html)

```python
example_selector = CustomExampleSelector(examples)
```

```python
example_selector.select_examples({"input": "okay"})
```

```output
[{'input': 'bye', 'output': 'arrivederci'}]
```

```python
example_selector.add_example({"input": "hand", "output": "mano"})
```

```python
example_selector.select_examples({"input": "okay"})
```

```output
[{'input': 'hand', 'output': 'mano'}]
```

## Use in a Prompt[â€‹](#use-in-a-prompt "Direct link to Use in a Prompt")

We can now use this example selector in a prompt

```python
from langchain_core.prompts.few_shot import FewShotPromptTemplate
from langchain_core.prompts.prompt import PromptTemplate

example_prompt = PromptTemplate.from_template("Input: {input} -> Output: {output}")
```

**API Reference:**[FewShotPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

```python
prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    suffix="Input: {input} -> Output:",
    prefix="Translate the following words from English to Italian:",
    input_variables=["input"],
)

print(prompt.format(input="word"))
```

```output
Translate the following words from English to Italian:

Input: hand -> Output: mano

Input: word -> Output:
```

## Example Selector Types[â€‹](#example-selector-types "Direct link to Example Selector Types")

| Name       | Description                                                                                 |
|------------|---------------------------------------------------------------------------------------------|
| Similarity | Uses semantic similarity between inputs and examples to decide which examples to choose.    |
| MMR        | Uses Max Marginal Relevance between inputs and examples to decide which examples to choose. |
| Length     | Selects examples based on how many can fit within a certain length                          |
| Ngram      | Uses ngram overlap between inputs and examples to decide which examples to choose.          |

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/example_selectors.ipynb)

* * *


- [Examples](#examples)
- [Custom Example Selector](#custom-example-selector)
- [Use in a Prompt](#use-in-a-prompt)
- [Example Selector Types](#example-selector-types)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/messages.mdx)

# Messages

Prerequisites

- [Chat Models](/docs/concepts/chat_models/)

## Overview[â€‹](#overview "Direct link to Overview")

Messages are the unit of communication in [chat models](/docs/concepts/chat_models/). They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.

Each message has a **role** (e.g., "user", "assistant") and **content** (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.

LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.

## What is inside a message?[â€‹](#what-is-inside-a-message "Direct link to What is inside a message?")

A message typically consists of the following pieces of information:

- **Role**: The role of the message (e.g., "user", "assistant").
- **Content**: The content of the message (e.g., text, multimodal data).
- Additional metadata: id, name, [token usage](/docs/concepts/tokens/) and other model-specific metadata.

### Role[â€‹](#role "Direct link to Role")

Roles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.

| **Role**              | **Description**                                                                                                                                                                                                  |
|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **system**            | Used to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.                                                                                             |
| **user**              | Represents input from a user interacting with the model, usually in the form of text or other interactive input.                                                                                                 |
| **assistant**         | Represents a response from the model, which can include text or a request to invoke tools.                                                                                                                       |
| **tool**              | A message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support [tool calling](/docs/concepts/tool_calling/). |
| **function** (legacy) | This is a legacy role, corresponding to OpenAI's legacy function-calling API. **tool** role should be used instead.                                                                                              |

### Content[â€‹](#content "Direct link to Content")

The content of a message text or a list of dictionaries representing [multimodal data](/docs/concepts/multimodality/) (e.g., images, audio, video). The exact format of the content can vary between different chat model providers.

Currently, most chat models support text as the primary content type, with some models also supporting multimodal data. However, support for multimodal data is still limited across most chat model providers.

For more information see:

- [SystemMessage](#systemmessage) -- for content which should be passed to direct the conversation
- [HumanMessage](#humanmessage) -- for content in the input from the user.
- [AIMessage](#aimessage) -- for content in the response from the model.
- [Multimodality](/docs/concepts/multimodality/) -- for more information on multimodal content.

### Other Message Data[â€‹](#other-message-data "Direct link to Other Message Data")

Depending on the chat model provider, messages can include other data such as:

- **ID**: An optional unique identifier for the message.
- **Name**: An optional `name` property which allows differentiate between different entities/speakers with the same role. Not all models support this!
- **Metadata**: Additional information about the message, such as timestamps, token usage, etc.
- **Tool Calls**: A request made by the model to call one or more tools&gt; See [tool calling](/docs/concepts/tool_calling/) for more information.

## Conversation Structure[â€‹](#conversation-structure "Direct link to Conversation Structure")

The sequence of messages into a chat model should follow a specific structure to ensure that the chat model can generate a valid response.

For example, a typical conversation structure might look like this:

1. **User Message**: "Hello, how are you?"
2. **Assistant Message**: "I'm doing well, thank you for asking."
3. **User Message**: "Can you tell me a joke?"
4. **Assistant Message**: "Sure! Why did the scarecrow win an award? Because he was outstanding in his field!"

Please read the [chat history](/docs/concepts/chat_history/) guide for more information on managing chat history and ensuring that the conversation structure is correct.

## LangChain Messages[â€‹](#langchain-messages "Direct link to LangChain Messages")

LangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.

LangChain messages are Python objects that subclass from a [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html).

The five main message types are:

- [SystemMessage](#systemmessage): corresponds to **system** role
- [HumanMessage](#humanmessage): corresponds to **user** role
- [AIMessage](#aimessage): corresponds to **assistant** role
- [AIMessageChunk](#aimessagechunk): corresponds to **assistant** role, used for [streaming](/docs/concepts/streaming/) responses
- [ToolMessage](#toolmessage): corresponds to **tool** role

Other important messages include:

- [RemoveMessage](#removemessage) -- does not correspond to any role. This is an abstraction, mostly used in [LangGraph](/docs/concepts/architecture/#langgraph) to manage chat history.
- **Legacy** [FunctionMessage](#legacy-functionmessage): corresponds to the **function** role in OpenAI's **legacy** function-calling API.

You can find more information about **messages** in the [API Reference](https://python.langchain.com/api_reference/core/messages.html).

### SystemMessage[â€‹](#systemmessage "Direct link to SystemMessage")

A `SystemMessage` is used to prime the behavior of the AI model and provide additional context, such as instructing the model to adopt a specific persona or setting the tone of the conversation (e.g., "This is a conversation about cooking").

Different chat providers may support system message in one of the following ways:

- **Through a "system" message role**: In this case, a system message is included as part of the message sequence with the role explicitly set as "system."
- **Through a separate API parameter for system instructions**: Instead of being included as a message, system instructions are passed via a dedicated API parameter.
- **No support for system messages**: Some models do not support system messages at all.

Most major chat model providers support system instructions via either a chat message or a separate API parameter. LangChain will automatically adapt based on the providerâ€™s capabilities. If the provider supports a separate API parameter for system instructions, LangChain will extract the content of a system message and pass it through that parameter.

If no system message is supported by the provider, in most cases LangChain will attempt to incorporate the system message's content into a HumanMessage or raise an exception if that is not possible. However, this behavior is not yet consistently enforced across all implementations, and if using a less popular implementation of a chat model (e.g., an implementation from the `langchain-community` package) it is recommended to check the specific documentation for that model.

### HumanMessage[â€‹](#humanmessage "Direct link to HumanMessage")

The `HumanMessage` corresponds to the **"user"** role. A human message represents input from a user interacting with the model.

#### Text Content[â€‹](#text-content "Direct link to Text Content")

Most chat models expect the user input to be in the form of text.

```python
from langchain_core.messages import HumanMessage

model.invoke([HumanMessage(content="Hello, how are you?")])
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

tip

When invoking a chat model with a string as input, LangChain will automatically convert the string into a `HumanMessage` object. This is mostly useful for quick testing.

```python
model.invoke("Hello, how are you?")
```

#### Multi-modal Content[â€‹](#multi-modal-content "Direct link to Multi-modal Content")

Some chat models accept multimodal inputs, such as images, audio, video, or files like PDFs.

Please see the [multimodality](/docs/concepts/multimodality/) guide for more information.

### AIMessage[â€‹](#aimessage "Direct link to AIMessage")

`AIMessage` is used to represent a message with the role **"assistant"**. This is the response from the model, which can include text or a request to invoke tools. It could also include other media types like images, audio, or video -- though this is still uncommon at the moment.

```python
from langchain_core.messages import HumanMessage
ai_message = model.invoke([HumanMessage("Tell me a joke")])
ai_message # <-- AIMessage
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

An `AIMessage` has the following attributes. The attributes which are **standardized** are the ones that LangChain attempts to standardize across different chat model providers. **raw** fields are specific to the model provider and may vary.

| Attribute            | Standardized/Raw | Description                                                                                                                                                                                                               |
|----------------------|------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `content`            | Raw              | Usually a string, but can be a list of content blocks. See [content](#content) for details.                                                                                                                               |
| `tool_calls`         | Standardized     | Tool calls associated with the message. See [tool calling](/docs/concepts/tool_calling/) for details.                                                                                                                     |
| `invalid_tool_calls` | Standardized     | Tool calls with parsing errors associated with the message. See [tool calling](/docs/concepts/tool_calling/) for details.                                                                                                 |
| `usage_metadata`     | Standardized     | Usage metadata for a message, such as [token counts](/docs/concepts/tokens/). See [Usage Metadata API Reference](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.UsageMetadata.html). |
| `id`                 | Standardized     | An optional unique identifier for the message, ideally provided by the provider/model that created the message.                                                                                                           |
| `response_metadata`  | Raw              | Response metadata, e.g., response headers, logprobs, token counts.                                                                                                                                                        |

#### content[â€‹](#content-1 "Direct link to content")

The **content** property of an `AIMessage` represents the response generated by the chat model.

The content is either:

- **text** -- the norm for virtually all chat models.
- A **list of dictionaries** -- Each dictionary represents a content block and is associated with a `type`.
  
  - Used by Anthropic for surfacing agent thought process when doing [tool calling](/docs/concepts/tool_calling/).
  - Used by OpenAI for audio outputs. Please see [multi-modal content](/docs/concepts/multimodality/) for more information.

important

The **content** property is **not** standardized across different chat model providers, mostly because there are still few examples to generalize from.

### AIMessageChunk[â€‹](#aimessagechunk "Direct link to AIMessageChunk")

It is common to [stream](/docs/concepts/streaming/) responses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it.

It is returned from the `stream`, `astream` and `astream_events` methods of the chat model.

For example,

```python
for chunk in model.stream([HumanMessage("what color is the sky?")]):
    print(chunk)
```

`AIMessageChunk` follows nearly the same structure as `AIMessage`, but uses a different [ToolCallChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolCallChunk.html#langchain_core.messages.tool.ToolCallChunk) to be able to stream tool calling in a standardized manner.

#### Aggregating[â€‹](#aggregating "Direct link to Aggregating")

`AIMessageChunks` support the `+` operator to merge them into a single `AIMessage`. This is useful when you want to display the final response to the user.

```python
ai_message = chunk1 + chunk2 + chunk3 + ...
```

### ToolMessage[â€‹](#toolmessage "Direct link to ToolMessage")

This represents a message with role "tool", which contains the result of [calling a tool](/docs/concepts/tool_calling/). In addition to `role` and `content`, this message has:

- a `tool_call_id` field which conveys the id of the call to the tool that was called to produce this result.
- an `artifact` field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.

Please see [tool calling](/docs/concepts/tool_calling/) for more information.

### RemoveMessage[â€‹](#removemessage "Direct link to RemoveMessage")

This is a special message type that does not correspond to any roles. It is used for managing chat history in [LangGraph](/docs/concepts/architecture/#langgraph).

Please see the following for more information on how to use the `RemoveMessage`:

- [Memory conceptual guide](https://langchain-ai.github.io/langgraph/concepts/memory/)
- [How to delete messages](https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages/)

### (Legacy) FunctionMessage[â€‹](#legacy-functionmessage "Direct link to (Legacy) FunctionMessage")

This is a legacy message type, corresponding to OpenAI's legacy function-calling API. `ToolMessage` should be used instead to correspond to the updated tool-calling API.

## OpenAI Format[â€‹](#openai-format "Direct link to OpenAI Format")

### Inputs[â€‹](#inputs "Direct link to Inputs")

Chat models also accept OpenAI's format as **inputs** to chat models:

```python
chat_model.invoke([
    {
        "role": "user",
        "content": "Hello, how are you?",
    },
    {
        "role": "assistant",
        "content": "I'm doing well, thank you for asking.",
    },
    {
        "role": "user",
        "content": "Can you tell me a joke?",
    }
])
```

### Outputs[â€‹](#outputs "Direct link to Outputs")

At the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you need OpenAI format for the output as well.

The [convert\_to\_openai\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.convert_to_openai_messages.html) utility function can be used to convert from LangChain messages to OpenAI format.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/messages.mdx)

* * *


- [Overview](#overview)
- [What is inside a message?](#what-is-inside-a-message)
  
  - [Role](#role)
  - [Content](#content)
  - [Other Message Data](#other-message-data)
- [Conversation Structure](#conversation-structure)
- [LangChain Messages](#langchain-messages)
  
  - [SystemMessage](#systemmessage)
  - [HumanMessage](#humanmessage)
  - [AIMessage](#aimessage)
  - [AIMessageChunk](#aimessagechunk)
  - [ToolMessage](#toolmessage)
  - [RemoveMessage](#removemessage)
  - [(Legacy) FunctionMessage](#legacy-functionmessage)
- [OpenAI Format](#openai-format)
  
  - [Inputs](#inputs)
  - [Outputs](#outputs)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/text_splitters.mdx)

# Text splitters

Prerequisites

- [Documents](/docs/concepts/retrievers/#interface)
- Tokenization(/docs/concepts/tokens)

## Overview[â€‹](#overview "Direct link to Overview")

Document splitting is often a crucial preprocessing step for many applications. It involves breaking down large texts into smaller, manageable chunks. This process offers several benefits, such as ensuring consistent processing of varying document lengths, overcoming input size limitations of models, and improving the quality of text representations used in retrieval systems. There are several strategies for splitting documents, each with its own advantages.

## Key concepts[â€‹](#key-concepts "Direct link to Key concepts")

![Conceptual Overview](/assets/images/text_splitters-7961ccc13e05e2fd7f7f58048e082f47.png)

Text splitters split documents into smaller chunks for use in downstream applications.

## Why split documents?[â€‹](#why-split-documents "Direct link to Why split documents?")

There are several reasons to split documents:

- **Handling non-uniform document lengths**: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.
- **Overcoming model limitations**: Many embedding models and language models have maximum input size constraints. Splitting allows us to process documents that would otherwise exceed these limits.
- **Improving representation quality**: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.
- **Enhancing retrieval precision**: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.
- **Optimizing computational resources**: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.

Now, the next question is *how* to split the documents into chunks! There are several strategies, each with its own advantages.

Further reading

- See Greg Kamradt's [chunkviz](https://chunkviz.up.railway.app/) to visualize different splitting strategies discussed below.

## Approaches[â€‹](#approaches "Direct link to Approaches")

### Length-based[â€‹](#length-based "Direct link to Length-based")

The most intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit. Key benefits of length-based splitting:

- Straightforward implementation
- Consistent chunk sizes
- Easily adaptable to different model requirements

Types of length-based splitting:

- **Token-based**: Splits text based on the number of tokens, which is useful when working with language models.
- **Character-based**: Splits text based on the number of characters, which can be more consistent across different types of text.

Example implementation using LangChain's `CharacterTextSplitter` with token-based splitting:

```python
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    encoding_name="cl100k_base", chunk_size=100, chunk_overlap=0
)
texts = text_splitter.split_text(document)
```

**API Reference:**[CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

Further reading

- See the how-to guide for [token-based](/docs/how_to/split_by_token/) splitting.
- See the how-to guide for [character-based](/docs/how_to/character_text_splitter/) splitting.

### Text-structured based[â€‹](#text-structured-based "Direct link to Text-structured based")

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChain's [`RecursiveCharacterTextSplitter`](/docs/how_to/recursive_text_splitter/) implements this concept:

- The `RecursiveCharacterTextSplitter` attempts to keep larger units (e.g., paragraphs) intact.
- If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).
- This process continues down to the word level if necessary.

Here is example usage:

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)
texts = text_splitter.split_text(document)
```

**API Reference:**[RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

Further reading

- See the how-to guide for [recursive text splitting](/docs/how_to/recursive_text_splitter/).

### Document-structured based[â€‹](#document-structured-based "Direct link to Document-structured based")

Some documents have an inherent structure, such as HTML, Markdown, or JSON files. In these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text. Key benefits of structure-based splitting:

- Preserves the logical organization of the document
- Maintains context within each chunk
- Can be more effective for downstream tasks like retrieval or summarization

Examples of structure-based splitting:

- **Markdown**: Split based on headers (e.g., #, ##, ###)
- **HTML**: Split using tags
- **JSON**: Split by object or array elements
- **Code**: Split by functions, classes, or logical blocks

Further reading

- See the how-to guide for [Markdown splitting](/docs/how_to/markdown_header_metadata_splitter/).
- See the how-to guide for [Recursive JSON splitting](/docs/how_to/recursive_json_splitter/).
- See the how-to guide for [Code splitting](/docs/how_to/code_splitter/).
- See the how-to guide for [HTML splitting](/docs/how_to/split_html/).

### Semantic meaning based[â€‹](#semantic-meaning-based "Direct link to Semantic meaning based")

Unlike the previous methods, semantic-based splitting actually considers the *content* of the text. While other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text's semantics. There are several ways to implement this, but conceptually the approach is split text when there are significant changes in text *meaning*. As an example, we can use a sliding window approach to generate embeddings, and compare the embeddings to find significant differences:

- Start with the first few sentences and generate an embedding.
- Move to the next group of sentences and generate another embedding (e.g., using a sliding window approach).
- Compare the embeddings to find significant differences, which indicate potential "break points" between semantic sections.

This technique helps create chunks that are more semantically coherent, potentially improving the quality of downstream tasks like retrieval or summarization.

Further reading

- See the how-to guide for [splitting text based on semantic meaning](/docs/how_to/semantic-chunker/).
- See Greg Kamradt's [notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) showcasing semantic splitting.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/text_splitters.mdx)

* * *


- [Overview](#overview)
- [Key concepts](#key-concepts)
- [Why split documents?](#why-split-documents)
- [Approaches](#approaches)
  
  - [Length-based](#length-based)
  - [Text-structured based](#text-structured-based)
  - [Document-structured based](#document-structured-based)
  - [Semantic meaning based](#semantic-meaning-based)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/conversation_chain.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/conversation_chain.ipynb)

# Migrating from ConversationalChain

[`ConversationChain`](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.conversation.base.ConversationChain.html) incorporated a memory of previous messages to sustain a stateful conversation.

Some advantages of switching to the Langgraph implementation are:

- Innate support for threads/separate sessions. To make this work with `ConversationChain`, you'd need to instantiate a separate memory class outside the chain.
- More explicit parameters. `ConversationChain` contains a hidden default prompt, which can cause confusion.
- Streaming support. `ConversationChain` only supports streaming via callbacks.

Langgraph's [checkpointing](https://langchain-ai.github.io/langgraph/how-tos/persistence/) system supports multiple threads or sessions, which can be specified via the `"thread_id"` key in its configuration parameters.

```python
%pip install --upgrade --quiet langchain langchain-openai
```

```python
import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

## Legacy[â€‹](#legacy "Direct link to Legacy")

Details

```python
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

template = """
You are a pirate. Answer the following questions as best you can.
Chat history: {history}
Question: {input}
"""

prompt = ChatPromptTemplate.from_template(template)

memory = ConversationBufferMemory()

chain = ConversationChain(
    llm=ChatOpenAI(),
    memory=memory,
    prompt=prompt,
)

chain({"input": "I'm Bob, how are you?"})
```

**API Reference:**[ConversationChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.conversation.base.ConversationChain.html) | [ConversationBufferMemory](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer.ConversationBufferMemory.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
{'input': "I'm Bob, how are you?",
 'history': '',
 'response': "Arrr matey, I be a pirate sailin' the high seas. What be yer business with me?"}
```

```python
chain({"input": "What is my name?"})
```

```output
{'input': 'What is my name?',
 'history': "Human: I'm Bob, how are you?\nAI: Arrr matey, I be a pirate sailin' the high seas. What be yer business with me?",
 'response': 'Your name be Bob, matey.'}
```

## Langgraph[â€‹](#langgraph "Direct link to Langgraph")

Details

```python
import uuid

from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph

model = ChatOpenAI(model="gpt-4o-mini")

# Define a new graph
workflow = StateGraph(state_schema=MessagesState)


# Define the function that calls the model
def call_model(state: MessagesState):
    response = model.invoke(state["messages"])
    return {"messages": response}


# Define the two nodes we will cycle between
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# Add memory
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)


# The thread id is a unique key that identifies
# this particular conversation.
# We'll just generate a random uuid here.
thread_id = uuid.uuid4()
config = {"configurable": {"thread_id": thread_id}}
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```python
query = "I'm Bob, how are you?"

input_messages = [
    {
        "role": "system",
        "content": "You are a pirate. Answer the following questions as best you can.",
    },
    {"role": "user", "content": query},
]
for event in app.stream({"messages": input_messages}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

I'm Bob, how are you?
==================================[1m Ai Message [0m==================================

Ahoy, Bob! I be feelin' as lively as a ship in full sail! How be ye on this fine day?
```

```python
query = "What is my name?"

input_messages = [{"role": "user", "content": query}]
for event in app.stream({"messages": input_messages}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

What is my name?
==================================[1m Ai Message [0m==================================

Ye be callin' yerself Bob, I reckon! A fine name for a swashbuckler like yerself!
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

See [this tutorial](/docs/tutorials/chatbot/) for a more end-to-end guide on building with [`RunnableWithMessageHistory`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html).

Check out the [LCEL conceptual docs](/docs/concepts/lcel/) for more background information.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_chains/conversation_chain.ipynb)

* * *


- [Legacy](#legacy)
- [Langgraph](#langgraph)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/tutorials/graph.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/graph.ipynb)

# Build a Question Answering application over a Graph Database

In this guide we'll go over the basic ways to create a Q&amp;A chain over a graph database. These systems will allow us to ask a question about the data in a graph database and get back a natural language answer. First, we will show a simple out-of-the-box option and then implement a more sophisticated version with LangGraph.

## âš ï¸ Security note âš ï¸[â€‹](#%EF%B8%8F-security-note-%EF%B8%8F "Direct link to âš ï¸ Security note âš ï¸")

Building Q&amp;A systems of graph databases requires executing model-generated graph queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, [see here](/docs/security/).

## Architecture[â€‹](#architecture "Direct link to Architecture")

At a high-level, the steps of most graph chains are:

1. **Convert question to a graph database query**: Model converts user input to a graph database query (e.g. Cypher).
2. **Execute graph database query**: Execute the graph database query.
3. **Answer the question**: Model responds to user input using the query results.

![sql_usecase.png](/assets/images/graph_usecase-34d891523e6284bb6230b38c5f8392e5.png)

## Setup[â€‹](#setup "Direct link to Setup")

First, get required packages and set environment variables. In this example, we will be using Neo4j graph database.

```python
%pip install --upgrade --quiet langchain langchain-neo4j langchain-openai langgraph
```

We default to OpenAI models in this guide.

```python
import getpass
import os

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")

# Uncomment the below to use LangSmith. Not required.
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
# os.environ["LANGSMITH_TRACING"] = "true"
```

```output
Enter your OpenAI API key:  Â·Â·Â·Â·Â·Â·Â·Â·
```

Next, we need to define Neo4j credentials. Follow [these installation steps](https://neo4j.com/docs/operations-manual/current/installation/) to set up a Neo4j database.

```python
os.environ["NEO4J_URI"] = "bolt://localhost:7687"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "password"
```

The below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.

```python
from langchain_neo4j import Neo4jGraph

graph = Neo4jGraph()

# Import movie information

movies_query = """
LOAD CSV WITH HEADERS FROM 
'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'
AS row
MERGE (m:Movie {id:row.movieId})
SET m.released = date(row.released),
    m.title = row.title,
    m.imdbRating = toFloat(row.imdbRating)
FOREACH (director in split(row.director, '|') | 
    MERGE (p:Person {name:trim(director)})
    MERGE (p)-[:DIRECTED]->(m))
FOREACH (actor in split(row.actors, '|') | 
    MERGE (p:Person {name:trim(actor)})
    MERGE (p)-[:ACTED_IN]->(m))
FOREACH (genre in split(row.genres, '|') | 
    MERGE (g:Genre {name:trim(genre)})
    MERGE (m)-[:IN_GENRE]->(g))
"""

graph.query(movies_query)
```

**API Reference:**[Neo4jGraph](https://python.langchain.com/api_reference/neo4j/graphs/langchain_neo4j.graphs.neo4j_graph.Neo4jGraph.html)

```output
[]
```

## Graph schema[â€‹](#graph-schema "Direct link to Graph schema")

In order for an LLM to be able to generate a Cypher statement, it needs information about the graph schema. When you instantiate a graph object, it retrieves the information about the graph schema. If you later make any changes to the graph, you can run the `refresh_schema` method to refresh the schema information.

```python
graph.refresh_schema()
print(graph.schema)
```

```output
Node properties:
Person {name: STRING}
Movie {id: STRING, released: DATE, title: STRING, imdbRating: FLOAT}
Genre {name: STRING}
Chunk {id: STRING, embedding: LIST, text: STRING, question: STRING, query: STRING}
Relationship properties:

The relationships:
(:Person)-[:DIRECTED]->(:Movie)
(:Person)-[:ACTED_IN]->(:Movie)
(:Movie)-[:IN_GENRE]->(:Genre)
```

For more involved schema information, you can use `enhanced_schema` option.

```python
enhanced_graph = Neo4jGraph(enhanced_schema=True)
print(enhanced_graph.schema)
```

```````output
Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: "CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships"
``````output
Node properties:
- **Person**
  - `name`: STRING Example: "John Lasseter"
- **Movie**
  - `id`: STRING Example: "1"
  - `released`: DATE Min: 1964-12-16, Max: 1996-09-15
  - `title`: STRING Example: "Toy Story"
  - `imdbRating`: FLOAT Min: 2.4, Max: 9.3
- **Genre**
  - `name`: STRING Example: "Adventure"
- **Chunk**
  - `id`: STRING Available options: ['d66006059fd78d63f3df90cc1059639a', '0e3dcb4502853979d12357690a95ec17', 'c438c6bcdcf8e4fab227f29f8e7ff204', '97fe701ec38057594464beaa2df0710e', 'b54f9286e684373498c4504b4edd9910', '5b50a72c3a4954b0ff7a0421be4f99b9', 'fb28d41771e717255f0d8f6c799ede32', '58e6f14dd2e6c6702cf333f2335c499c']
  - `text`: STRING Available options: ['How many artists are there?', 'Which actors played in the movie Casino?', 'How many movies has Tom Hanks acted in?', "List all the genres of the movie Schindler's List", 'Which actors have worked in movies from both the c', 'Which directors have made movies with at least thr', 'Identify movies where directors also played a role', 'Find the actor with the highest number of movies i']
  - `question`: STRING Available options: ['How many artists are there?', 'Which actors played in the movie Casino?', 'How many movies has Tom Hanks acted in?', "List all the genres of the movie Schindler's List", 'Which actors have worked in movies from both the c', 'Which directors have made movies with at least thr', 'Identify movies where directors also played a role', 'Find the actor with the highest number of movies i']
  - `query`: STRING Available options: ['MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN coun', "MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a)", "MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->", "MATCH (m:Movie {title: 'Schindler's List'})-[:IN_G", 'MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]', 'MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_I', 'MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACT', 'MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.na']
Relationship properties:

The relationships:
(:Person)-[:DIRECTED]->(:Movie)
(:Person)-[:ACTED_IN]->(:Movie)
(:Movie)-[:IN_GENRE]->(:Genre)
```````

The `enhanced_schema` option enriches property information by including details such as minimum and maximum values for floats and dates, as well as example values for string properties. This additional context helps guide the LLM toward generating more accurate and effective queries.

Great! We've got a graph database that we can query. Now let's try hooking it up to an LLM.

## GraphQACypherChain[â€‹](#graphqacypherchain "Direct link to GraphQACypherChain")

Let's use a simple out-of-the-box chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question.

![graph_chain.webp](/assets/images/graph_chain-6379941793e0fa985e51e4bda0329403.webp)

LangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: [GraphCypherQAChain](/docs/integrations/graphs/neo4j_cypher/)

```python
from langchain_neo4j import GraphCypherQAChain
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o", temperature=0)
chain = GraphCypherQAChain.from_llm(
    graph=enhanced_graph, llm=llm, verbose=True, allow_dangerous_requests=True
)
response = chain.invoke({"query": "What was the cast of the Casino?"})
response
```

**API Reference:**[GraphCypherQAChain](https://python.langchain.com/api_reference/neo4j/chains/langchain_neo4j.chains.graph_qa.cypher.GraphCypherQAChain.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output


[1m> Entering new GraphCypherQAChain chain...[0m
Generated Cypher:
[32;1m[1;3mcypher
MATCH (p:Person)-[:ACTED_IN]->(m:Movie {title: "Casino"})
RETURN p.name
[0m
Full Context:
[32;1m[1;3m[{'p.name': 'Robert De Niro'}, {'p.name': 'Joe Pesci'}, {'p.name': 'Sharon Stone'}, {'p.name': 'James Woods'}][0m

[1m> Finished chain.[0m
```

```output
{'query': 'What was the cast of the Casino?',
 'result': 'Robert De Niro, Joe Pesci, Sharon Stone, and James Woods were the cast of Casino.'}
```

## Advanced implementation with LangGraph[â€‹](#advanced-implementation-with-langgraph "Direct link to Advanced implementation with LangGraph")

While the GraphCypherQAChain is effective for quick demonstrations, it may face challenges in production environments. Transitioning to LangGraph can enhance the workflow, but implementing natural language to query flows in production remains a complex task. Nevertheless, there are several strategies to significantly improve accuracy and reliability, which we will explore next.

Here is the visualized LangGraph flow we will implement:

![langgraph_text2cypher](/assets/images/langgraph_text2cypher-1414c073e151b391ef08fed77915e3c0.webp)

We will begin by defining the Input, Output, and Overall state of the LangGraph application.

```python
from operator import add
from typing import Annotated, List

from typing_extensions import TypedDict


class InputState(TypedDict):
    question: str


class OverallState(TypedDict):
    question: str
    next_action: str
    cypher_statement: str
    cypher_errors: List[str]
    database_records: List[dict]
    steps: Annotated[List[str], add]


class OutputState(TypedDict):
    answer: str
    steps: List[str]
    cypher_statement: str
```

The first step is a simple `guardrails` step, where we validate whether the question pertains to movies or their cast. If it doesn't, we notify the user that we cannot answer any other questions. Otherwise, we move on to the Cypher generation step.

```python
from typing import Literal

from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

guardrails_system = """
As an intelligent assistant, your primary objective is to decide whether a given question is related to movies or not. 
If the question is related to movies, output "movie". Otherwise, output "end".
To make this decision, assess the content of the question and determine if it refers to any movie, actor, director, film industry, 
or related topics. Provide only the specified output: "movie" or "end".
"""
guardrails_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            guardrails_system,
        ),
        (
            "human",
            ("{question}"),
        ),
    ]
)


class GuardrailsOutput(BaseModel):
    decision: Literal["movie", "end"] = Field(
        description="Decision on whether the question is related to movies"
    )


guardrails_chain = guardrails_prompt | llm.with_structured_output(GuardrailsOutput)


def guardrails(state: InputState) -> OverallState:
    """
    Decides if the question is related to movies or not.
    """
    guardrails_output = guardrails_chain.invoke({"question": state.get("question")})
    database_records = None
    if guardrails_output.decision == "end":
        database_records = "This questions is not about movies or their cast. Therefore I cannot answer this question."
    return {
        "next_action": guardrails_output.decision,
        "database_records": database_records,
        "steps": ["guardrail"],
    }
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

### Few-shot prompting[â€‹](#few-shot-prompting "Direct link to Few-shot prompting")

Converting natural language into accurate queries is challenging. One way to enhance this process is by providing relevant few-shot examples to guide the LLM in query generation. To achieve this, we will use the `SemanticSimilarityExampleSelector` to dynamically select the most relevant examples.

```python
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_neo4j import Neo4jVector
from langchain_openai import OpenAIEmbeddings

examples = [
    {
        "question": "How many artists are there?",
        "query": "MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)",
    },
    {
        "question": "Which actors played in the movie Casino?",
        "query": "MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a) RETURN a.name",
    },
    {
        "question": "How many movies has Tom Hanks acted in?",
        "query": "MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->(m:Movie) RETURN count(m)",
    },
    {
        "question": "List all the genres of the movie Schindler's List",
        "query": "MATCH (m:Movie {title: 'Schindler's List'})-[:IN_GENRE]->(g:Genre) RETURN g.name",
    },
    {
        "question": "Which actors have worked in movies from both the comedy and action genres?",
        "query": "MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g2:Genre) WHERE g1.name = 'Comedy' AND g2.name = 'Action' RETURN DISTINCT a.name",
    },
    {
        "question": "Which directors have made movies with at least three different actors named 'John'?",
        "query": "MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person) WHERE a.name STARTS WITH 'John' WITH d, COUNT(DISTINCT a) AS JohnsCount WHERE JohnsCount >= 3 RETURN d.name",
    },
    {
        "question": "Identify movies where directors also played a role in the film.",
        "query": "MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACTED_IN]->(m) RETURN m.title, p.name",
    },
    {
        "question": "Find the actor with the highest number of movies in the database.",
        "query": "MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1",
    },
]

example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples, OpenAIEmbeddings(), Neo4jVector, k=5, input_keys=["question"]
)
```

**API Reference:**[SemanticSimilarityExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector.html) | [Neo4jVector](https://python.langchain.com/api_reference/neo4j/vectorstores/langchain_neo4j.vectorstores.neo4j_vector.Neo4jVector.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

Next, we implement the Cypher generation chain, also known as **text2cypher**. The prompt includes an enhanced graph schema, dynamically selected few-shot examples, and the userâ€™s question. This combination enables the generation of a Cypher query to retrieve relevant information from the database.

```python
from langchain_core.output_parsers import StrOutputParser

text2cypher_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            (
                "Given an input question, convert it to a Cypher query. No pre-amble."
                "Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!"
            ),
        ),
        (
            "human",
            (
                """You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.
Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!
Here is the schema information
{schema}

Below are a number of examples of questions and their corresponding Cypher queries.

{fewshot_examples}

User input: {question}
Cypher query:"""
            ),
        ),
    ]
)

text2cypher_chain = text2cypher_prompt | llm | StrOutputParser()


def generate_cypher(state: OverallState) -> OverallState:
    """
    Generates a cypher statement based on the provided schema and user input
    """
    NL = "\n"
    fewshot_examples = (NL * 2).join(
        [
            f"Question: {el['question']}{NL}Cypher:{el['query']}"
            for el in example_selector.select_examples(
                {"question": state.get("question")}
            )
        ]
    )
    generated_cypher = text2cypher_chain.invoke(
        {
            "question": state.get("question"),
            "fewshot_examples": fewshot_examples,
            "schema": enhanced_graph.schema,
        }
    )
    return {"cypher_statement": generated_cypher, "steps": ["generate_cypher"]}
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html)

### Query validation[â€‹](#query-validation "Direct link to Query validation")

The next step is to validate the generated Cypher statement and ensuring that all property values are accurate. While numbers and dates typically donâ€™t require validation, strings such as movie titles or peopleâ€™s names do. In this example, weâ€™ll use a basic `CONTAINS` clause for validation, though more advanced mapping and validation techniques can be implemented if needed.

First, we will create a chain that detects any errors in the Cypher statement and extracts the property values it references.

```python
from typing import List, Optional

validate_cypher_system = """
You are a Cypher expert reviewing a statement written by a junior developer.
"""

validate_cypher_user = """You must check the following:
* Are there any syntax errors in the Cypher statement?
* Are there any missing or undefined variables in the Cypher statement?
* Are any node labels missing from the schema?
* Are any relationship types missing from the schema?
* Are any of the properties not included in the schema?
* Does the Cypher statement include enough information to answer the question?

Examples of good errors:
* Label (:Foo) does not exist, did you mean (:Bar)?
* Property bar does not exist for label Foo, did you mean baz?
* Relationship FOO does not exist, did you mean FOO_BAR?

Schema:
{schema}

The question is:
{question}

The Cypher statement is:
{cypher}

Make sure you don't make any mistakes!"""

validate_cypher_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            validate_cypher_system,
        ),
        (
            "human",
            (validate_cypher_user),
        ),
    ]
)


class Property(BaseModel):
    """
    Represents a filter condition based on a specific node property in a graph in a Cypher statement.
    """

    node_label: str = Field(
        description="The label of the node to which this property belongs."
    )
    property_key: str = Field(description="The key of the property being filtered.")
    property_value: str = Field(
        description="The value that the property is being matched against."
    )


class ValidateCypherOutput(BaseModel):
    """
    Represents the validation result of a Cypher query's output,
    including any errors and applied filters.
    """

    errors: Optional[List[str]] = Field(
        description="A list of syntax or semantical errors in the Cypher statement. Always explain the discrepancy between schema and Cypher statement"
    )
    filters: Optional[List[Property]] = Field(
        description="A list of property-based filters applied in the Cypher statement."
    )


validate_cypher_chain = validate_cypher_prompt | llm.with_structured_output(
    ValidateCypherOutput
)
```

LLMs often struggle with correctly determining relationship directions in generated Cypher statements. Since we have access to the schema, we can deterministically correct these directions using the **CypherQueryCorrector**.

*Note: The `CypherQueryCorrector` is an experimental feature and doesn't support all the newest Cypher syntax.*

```python
from langchain_neo4j.chains.graph_qa.cypher_utils import CypherQueryCorrector, Schema

# Cypher query corrector is experimental
corrector_schema = [
    Schema(el["start"], el["type"], el["end"])
    for el in enhanced_graph.structured_schema.get("relationships")
]
cypher_query_corrector = CypherQueryCorrector(corrector_schema)
```

**API Reference:**[CypherQueryCorrector](https://python.langchain.com/api_reference/neo4j/chains/langchain_neo4j.chains.graph_qa.cypher_utils.CypherQueryCorrector.html) | [Schema](https://python.langchain.com/api_reference/neo4j/chains/langchain_neo4j.chains.graph_qa.cypher_utils.Schema.html)

Now we can implement the Cypher validation step. First, we use the `EXPLAIN` method to detect any syntax errors. Next, we leverage the LLM to identify potential issues and extract the properties used for filtering. For string properties, we validate them against the database using a simple `CONTAINS` clause.

Based on the validation results, the process can take the following paths:

- If value mapping fails, we end the conversation and inform the user that we couldn't identify a specific property value (e.g., a person or movie title).
- If errors are found, we route the query for correction.
- If no issues are detected, we proceed to the Cypher execution step.

```python
from neo4j.exceptions import CypherSyntaxError


def validate_cypher(state: OverallState) -> OverallState:
    """
    Validates the Cypher statements and maps any property values to the database.
    """
    errors = []
    mapping_errors = []
    # Check for syntax errors
    try:
        enhanced_graph.query(f"EXPLAIN {state.get('cypher_statement')}")
    except CypherSyntaxError as e:
        errors.append(e.message)
    # Experimental feature for correcting relationship directions
    corrected_cypher = cypher_query_corrector(state.get("cypher_statement"))
    if not corrected_cypher:
        errors.append("The generated Cypher statement doesn't fit the graph schema")
    if not corrected_cypher == state.get("cypher_statement"):
        print("Relationship direction was corrected")
    # Use LLM to find additional potential errors and get the mapping for values
    llm_output = validate_cypher_chain.invoke(
        {
            "question": state.get("question"),
            "schema": enhanced_graph.schema,
            "cypher": state.get("cypher_statement"),
        }
    )
    if llm_output.errors:
        errors.extend(llm_output.errors)
    if llm_output.filters:
        for filter in llm_output.filters:
            # Do mapping only for string values
            if (
                not [
                    prop
                    for prop in enhanced_graph.structured_schema["node_props"][
                        filter.node_label
                    ]
                    if prop["property"] == filter.property_key
                ][0]["type"]
                == "STRING"
            ):
                continue
            mapping = enhanced_graph.query(
                f"MATCH (n:{filter.node_label}) WHERE toLower(n.`{filter.property_key}`) = toLower($value) RETURN 'yes' LIMIT 1",
                {"value": filter.property_value},
            )
            if not mapping:
                print(
                    f"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}"
                )
                mapping_errors.append(
                    f"Missing value mapping for {filter.node_label} on property {filter.property_key} with value {filter.property_value}"
                )
    if mapping_errors:
        next_action = "end"
    elif errors:
        next_action = "correct_cypher"
    else:
        next_action = "execute_cypher"

    return {
        "next_action": next_action,
        "cypher_statement": corrected_cypher,
        "cypher_errors": errors,
        "steps": ["validate_cypher"],
    }
```

The Cypher correction step takes the existing Cypher statement, any identified errors, and the original question to generate a corrected version of the query.

```python
correct_cypher_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            (
                "You are a Cypher expert reviewing a statement written by a junior developer. "
                "You need to correct the Cypher statement based on the provided errors. No pre-amble."
                "Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!"
            ),
        ),
        (
            "human",
            (
                """Check for invalid syntax or semantics and return a corrected Cypher statement.

Schema:
{schema}

Note: Do not include any explanations or apologies in your responses.
Do not wrap the response in any backticks or anything else.
Respond with a Cypher statement only!

Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.

The question is:
{question}

The Cypher statement is:
{cypher}

The errors are:
{errors}

Corrected Cypher statement: """
            ),
        ),
    ]
)

correct_cypher_chain = correct_cypher_prompt | llm | StrOutputParser()


def correct_cypher(state: OverallState) -> OverallState:
    """
    Correct the Cypher statement based on the provided errors.
    """
    corrected_cypher = correct_cypher_chain.invoke(
        {
            "question": state.get("question"),
            "errors": state.get("cypher_errors"),
            "cypher": state.get("cypher_statement"),
            "schema": enhanced_graph.schema,
        }
    )

    return {
        "next_action": "validate_cypher",
        "cypher_statement": corrected_cypher,
        "steps": ["correct_cypher"],
    }
```

We need to add a step that executes the given Cypher statement. If no results are returned, we should explicitly handle this scenario, as leaving the context empty can sometimes lead to LLM hallucinations.

```python
no_results = "I couldn't find any relevant information in the database"


def execute_cypher(state: OverallState) -> OverallState:
    """
    Executes the given Cypher statement.
    """

    records = enhanced_graph.query(state.get("cypher_statement"))
    return {
        "database_records": records if records else no_results,
        "next_action": "end",
        "steps": ["execute_cypher"],
    }
```

The final step is to generate the answer. This involves combining the initial question with the database output to produce a relevant response.

```python
generate_final_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant",
        ),
        (
            "human",
            (
                """Use the following results retrieved from a database to provide
a succinct, definitive answer to the user's question.

Respond as if you are answering the question directly.

Results: {results}
Question: {question}"""
            ),
        ),
    ]
)

generate_final_chain = generate_final_prompt | llm | StrOutputParser()


def generate_final_answer(state: OverallState) -> OutputState:
    """
    Decides if the question is related to movies.
    """
    final_answer = generate_final_chain.invoke(
        {"question": state.get("question"), "results": state.get("database_records")}
    )
    return {"answer": final_answer, "steps": ["generate_final_answer"]}
```

Next, we will implement the LangGraph workflow, starting with defining the conditional edge functions.

```python
def guardrails_condition(
    state: OverallState,
) -> Literal["generate_cypher", "generate_final_answer"]:
    if state.get("next_action") == "end":
        return "generate_final_answer"
    elif state.get("next_action") == "movie":
        return "generate_cypher"


def validate_cypher_condition(
    state: OverallState,
) -> Literal["generate_final_answer", "correct_cypher", "execute_cypher"]:
    if state.get("next_action") == "end":
        return "generate_final_answer"
    elif state.get("next_action") == "correct_cypher":
        return "correct_cypher"
    elif state.get("next_action") == "execute_cypher":
        return "execute_cypher"
```

Let's put it all together now.

```python
from IPython.display import Image, display
from langgraph.graph import END, START, StateGraph

langgraph = StateGraph(OverallState, input=InputState, output=OutputState)
langgraph.add_node(guardrails)
langgraph.add_node(generate_cypher)
langgraph.add_node(validate_cypher)
langgraph.add_node(correct_cypher)
langgraph.add_node(execute_cypher)
langgraph.add_node(generate_final_answer)

langgraph.add_edge(START, "guardrails")
langgraph.add_conditional_edges(
    "guardrails",
    guardrails_condition,
)
langgraph.add_edge("generate_cypher", "validate_cypher")
langgraph.add_conditional_edges(
    "validate_cypher",
    validate_cypher_condition,
)
langgraph.add_edge("execute_cypher", "generate_final_answer")
langgraph.add_edge("correct_cypher", "validate_cypher")
langgraph.add_edge("generate_final_answer", END)

langgraph = langgraph.compile()

# View
display(Image(langgraph.get_graph().draw_mermaid_png()))
```

**API Reference:**[StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

![](data:image/png;base64,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)

We can now test the application by asking an irrelevant question.

```python
langgraph.invoke({"question": "What's the weather in Spain?"})
```

```output
{'answer': "I'm sorry, but I cannot provide current weather information. Please check a reliable weather website or app for the latest updates on the weather in Spain.",
 'steps': ['guardrail', 'generate_final_answer']}
```

Let's now ask something relevant about the movies.

```python
langgraph.invoke({"question": "What was the cast of the Casino?"})
```

```output
{'answer': 'The cast of "Casino" includes Robert De Niro, Joe Pesci, Sharon Stone, and James Woods.',
 'steps': ['guardrail',
  'generate_cypher',
  'validate_cypher',
  'execute_cypher',
  'generate_final_answer'],
 'cypher_statement': "MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a:Person) RETURN a.name"}
```

### Next steps[â€‹](#next-steps "Direct link to Next steps")

For other graph techniques like this and more check out:

- [Semantic layer](/docs/how_to/graph_semantic/): Techniques for implementing semantic layers.
- [Constructing graphs](/docs/how_to/graph_constructing/): Techniques for constructing knowledge graphs.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/tutorials/graph.ipynb)

* * *


- [âš ï¸ Security note âš ï¸](#%EF%B8%8F-security-note-%EF%B8%8F)
- [Architecture](#architecture)
- [Setup](#setup)
- [Graph schema](#graph-schema)
- [GraphQACypherChain](#graphqacypherchain)
- [Advanced implementation with LangGraph](#advanced-implementation-with-langgraph)
  
  - [Few-shot prompting](#few-shot-prompting)
  - [Query validation](#query-validation)
  - [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/summarize_stuff.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/summarize_stuff.ipynb)

# How to summarize text in a single LLM call

LLMs can summarize and otherwise distill desired information from text, including large volumes of text. In many cases, especially for models with larger context windows, this can be adequately achieved via a single LLM call.

LangChain implements a simple [pre-built chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html) that "stuffs" a prompt with the desired context for summarization and other purposes. In this guide we demonstrate how to use the chain.

## Load chat model[â€‹](#load-chat-model "Direct link to Load chat model")

Let's first load a [chat model](/docs/concepts/chat_models/):

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

## Load documents[â€‹](#load-documents "Direct link to Load documents")

Next, we need some documents to summarize. Below, we generate some toy documents for illustrative purposes. See the document loader [how-to guides](/docs/how_to/#document-loaders) and [integration pages](/docs/integrations/document_loaders/) for additional sources of data. The [summarization tutorial](/docs/tutorials/summarization/) also includes an example summarizing a blog post.

```python
from langchain_core.documents import Document

documents = [
    Document(page_content="Apples are red", metadata={"title": "apple_book"}),
    Document(page_content="Blueberries are blue", metadata={"title": "blueberry_book"}),
    Document(page_content="Bananas are yelow", metadata={"title": "banana_book"}),
]
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

## Load chain[â€‹](#load-chain "Direct link to Load chain")

Below, we define a simple prompt and instantiate the chain with our chat model and documents:

```python
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("Summarize this content: {context}")
chain = create_stuff_documents_chain(llm, prompt)
```

**API Reference:**[create\_stuff\_documents\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

## Invoke chain[â€‹](#invoke-chain "Direct link to Invoke chain")

Because the chain is a [Runnable](/docs/concepts/runnables/), it implements the usual methods for invocation:

```python
result = chain.invoke({"context": documents})
result
```

```output
'The content describes the colors of three fruits: apples are red, blueberries are blue, and bananas are yellow.'
```

### Streaming[â€‹](#streaming "Direct link to Streaming")

Note that the chain also supports streaming of individual output tokens:

```python
for chunk in chain.stream({"context": documents}):
    print(chunk, end="|")
```

```output
|The| content| describes| the| colors| of| three| fruits|:| apples| are| red|,| blueberries| are| blue|,| and| bananas| are| yellow|.||
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

See the summarization [how-to guides](/docs/how_to/#summarization) for additional summarization strategies, including those designed for larger volumes of text.

See also [this tutorial](/docs/tutorials/summarization/) for more detail on summarization.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/summarize_stuff.ipynb)

* * *


- [Load chat model](#load-chat-model)
- [Load documents](#load-documents)
- [Load chain](#load-chain)
- [Invoke chain](#invoke-chain)
  
  - [Streaming](#streaming)
- [Next steps](#next-steps)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/prompt_templates.mdx)

# Prompt Templates

Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.

Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.

Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages. The reason this PromptValue exists is to make it easy to switch between strings and messages.

There are a few different types of prompt templates:

## String PromptTemplates[â€‹](#string-prompttemplates "Direct link to String PromptTemplates")

These prompt templates are used to format a single string, and generally are used for simpler inputs. For example, a common way to construct and use a PromptTemplate is as follows:

```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template("Tell me a joke about {topic}")

prompt_template.invoke({"topic": "cats"})
```

**API Reference:**[PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

## ChatPromptTemplates[â€‹](#chatprompttemplates "Direct link to ChatPromptTemplates")

These prompt templates are used to format a list of messages. These "templates" consist of a list of templates themselves. For example, a common way to construct and use a ChatPromptTemplate is as follows:

```python
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate([
    ("system", "You are a helpful assistant"),
    ("user", "Tell me a joke about {topic}")
])

prompt_template.invoke({"topic": "cats"})
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

In the above example, this ChatPromptTemplate will construct two messages when called. The first is a system message, that has no variables to format. The second is a HumanMessage, and will be formatted by the `topic` variable the user passes in.

## MessagesPlaceholder[â€‹](#messagesplaceholder "Direct link to MessagesPlaceholder")

This prompt template is responsible for adding a list of messages in a particular place. In the above ChatPromptTemplate, we saw how we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would slot into a particular spot? This is how you use MessagesPlaceholder.

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage

prompt_template = ChatPromptTemplate([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder("msgs")
])

prompt_template.invoke({"msgs": [HumanMessage(content="hi!")]})
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

This will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in). This is useful for letting a list of messages be slotted into a particular spot.

An alternative way to accomplish the same thing without using the `MessagesPlaceholder` class explicitly is:

```python
prompt_template = ChatPromptTemplate([
    ("system", "You are a helpful assistant"),
    ("placeholder", "{msgs}") # <-- This is the changed part
])
```

For specifics on how to use prompt templates, see the [relevant how-to guides here](/docs/how_to/#prompt-templates).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/prompt_templates.mdx)

* * *


- [String PromptTemplates](#string-prompttemplates)
- [ChatPromptTemplates](#chatprompttemplates)
- [MessagesPlaceholder](#messagesplaceholder)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_directory.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_directory.ipynb)

# How to load documents from a directory

LangChain's [DirectoryLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html) implements functionality for reading files from disk into LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects. Here we demonstrate:

- How to load from a filesystem, including use of wildcard patterns;
- How to use multithreading for file I/O;
- How to use custom loader classes to parse specific file types (e.g., code);
- How to handle errors, such as those due to decoding.

```python
from langchain_community.document_loaders import DirectoryLoader
```

**API Reference:**[DirectoryLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html)

`DirectoryLoader` accepts a `loader_cls` kwarg, which defaults to [UnstructuredLoader](/docs/integrations/document_loaders/unstructured_file/). [Unstructured](https://unstructured-io.github.io/unstructured/) supports parsing for a number of formats, such as PDF and HTML. Here we use it to read in a markdown (.md) file.

We can use the `glob` parameter to control which files to load. Note that here it doesn't load the `.rst` file or the `.html` files.

```python
loader = DirectoryLoader("../", glob="**/*.md")
docs = loader.load()
len(docs)
```

```output
20
```

```python
print(docs[0].page_content[:100])
```

```output
Security

LangChain has a large ecosystem of integrations with various external resources like local
```

## Show a progress bar[â€‹](#show-a-progress-bar "Direct link to Show a progress bar")

By default a progress bar will not be shown. To show a progress bar, install the `tqdm` library (e.g. `pip install tqdm`), and set the `show_progress` parameter to `True`.

```python
loader = DirectoryLoader("../", glob="**/*.md", show_progress=True)
docs = loader.load()
```

```output
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 54.56it/s]
```

## Use multithreading[â€‹](#use-multithreading "Direct link to Use multithreading")

By default the loading happens in one thread. In order to utilize several threads set the `use_multithreading` flag to true.

```python
loader = DirectoryLoader("../", glob="**/*.md", use_multithreading=True)
docs = loader.load()
```

## Change loader class[â€‹](#change-loader-class "Direct link to Change loader class")

By default this uses the `UnstructuredLoader` class. To customize the loader, specify the loader class in the `loader_cls` kwarg. Below we show an example using [TextLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html):

```python
from langchain_community.document_loaders import TextLoader

loader = DirectoryLoader("../", glob="**/*.md", loader_cls=TextLoader)
docs = loader.load()
```

**API Reference:**[TextLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html)

```python
print(docs[0].page_content[:100])
```

```output
# Security

LangChain has a large ecosystem of integrations with various external resources like loc
```

Notice that while the `UnstructuredLoader` parses Markdown headers, `TextLoader` does not.

If you need to load Python source code files, use the `PythonLoader`:

```python
from langchain_community.document_loaders import PythonLoader

loader = DirectoryLoader("../../../../../", glob="**/*.py", loader_cls=PythonLoader)
```

**API Reference:**[PythonLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.python.PythonLoader.html)

## Auto-detect file encodings with TextLoader[â€‹](#auto-detect-file-encodings-with-textloader "Direct link to Auto-detect file encodings with TextLoader")

`DirectoryLoader` can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.

```python
path = "../../../libs/langchain/tests/unit_tests/examples/"

loader = DirectoryLoader(path, glob="**/*.txt", loader_cls=TextLoader)
```

### A. Default Behavior[â€‹](#a-default-behavior "Direct link to A. Default Behavior")

By default we raise an error:

```python
loader.load()
```

```output
Error loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt
```

```````output
---------------------------------------------------------------------------
``````output
UnicodeDecodeError                        Traceback (most recent call last)
``````output
File ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)
     42     with open(self.file_path, encoding=self.encoding) as f:
---> 43         text = f.read()
     44 except UnicodeDecodeError as e:
``````output
File ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)
    321 data = self.buffer + input
--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)
    323 # keep undecoded input until the next call
``````output
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte
``````output

The above exception was the direct cause of the following exception:
``````output
RuntimeError                              Traceback (most recent call last)
``````output
Cell In[10], line 1
----> 1 loader.load()
``````output
File ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)
    115 def load(self) -> List[Document]:
    116     """Load documents."""
--> 117     return list(self.lazy_load())
``````output
File ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)
    180 else:
    181     for i in items:
--> 182         yield from self._lazy_load_file(i, p, pbar)
    184 if pbar:
    185     pbar.close()
``````output
File ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)
    218     else:
    219         logger.error(f"Error loading file {str(item)}")
--> 220         raise e
    221 finally:
    222     if pbar:
``````output
File ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)
    208 loader = self.loader_cls(str(item), **self.loader_kwargs)
    209 try:
--> 210     for subdoc in loader.lazy_load():
    211         yield subdoc
    212 except NotImplementedError:
``````output
File ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)
     54                 continue
     55     else:
---> 56         raise RuntimeError(f"Error loading {self.file_path}") from e
     57 except Exception as e:
     58     raise RuntimeError(f"Error loading {self.file_path}") from e
``````output
RuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt
```````

The file `example-non-utf8.txt` uses a different encoding, so the `load()` function fails with a helpful message indicating which file failed decoding.

With the default behavior of `TextLoader` any failure to load any of the documents will fail the whole loading process and no documents are loaded.

### B. Silent fail[â€‹](#b-silent-fail "Direct link to B. Silent fail")

We can pass the parameter `silent_errors` to the `DirectoryLoader` to skip the files which could not be loaded and continue the load process.

```python
loader = DirectoryLoader(
    path, glob="**/*.txt", loader_cls=TextLoader, silent_errors=True
)
docs = loader.load()
```

```output
Error loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt
```

```python
doc_sources = [doc.metadata["source"] for doc in docs]
doc_sources
```

```output
['../../../../libs/langchain/tests/unit_tests/examples/example-utf8.txt']
```

### C. Auto detect encodings[â€‹](#c-auto-detect-encodings "Direct link to C. Auto detect encodings")

We can also ask `TextLoader` to auto detect the file encoding before failing, by passing the `autodetect_encoding` to the loader class.

```python
text_loader_kwargs = {"autodetect_encoding": True}
loader = DirectoryLoader(
    path, glob="**/*.txt", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs
)
docs = loader.load()
```

```python
doc_sources = [doc.metadata["source"] for doc in docs]
doc_sources
```

```output
['../../../../libs/langchain/tests/unit_tests/examples/example-utf8.txt',
 '../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt']
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/document_loader_directory.ipynb)

* * *


- [Show a progress bar](#show-a-progress-bar)
- [Use multithreading](#use-multithreading)
- [Change loader class](#change-loader-class)
- [Auto-detect file encodings with TextLoader](#auto-detect-file-encodings-with-textloader)
  
  - [A. Default Behavior](#a-default-behavior)
  - [B. Silent fail](#b-silent-fail)
  - [C. Auto detect encodings](#c-auto-detect-encodings)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/fallbacks.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/fallbacks.ipynb)

# How to add fallbacks to a runnable

When working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safeguard against these. That's why we've introduced the concept of fallbacks.

A **fallback** is an alternative plan that may be used in an emergency.

Crucially, fallbacks can be applied not only on the LLM level but on the whole runnable level. This is important because often times different models require different prompts. So if your call to OpenAI fails, you don't just want to send the same prompt to Anthropic - you probably want to use a different prompt template and send a different version there.

## Fallback for LLM API Errors[â€‹](#fallback-for-llm-api-errors "Direct link to Fallback for LLM API Errors")

This is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things.

IMPORTANT: By default, a lot of the LLM wrappers catch errors and retry. You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying and not failing.

```python
%pip install --upgrade --quiet  langchain langchain-openai
```

```python
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

First, let's mock out what happens if we hit a RateLimitError from OpenAI

```python
from unittest.mock import patch

import httpx
from openai import RateLimitError

request = httpx.Request("GET", "/")
response = httpx.Response(200, request=request)
error = RateLimitError("rate limit", response=response, body="")
```

```python
# Note that we set max_retries = 0 to avoid retrying on RateLimits, etc
openai_llm = ChatOpenAI(model="gpt-4o-mini", max_retries=0)
anthropic_llm = ChatAnthropic(model="claude-3-haiku-20240307")
llm = openai_llm.with_fallbacks([anthropic_llm])
```

```python
# Let's use just the OpenAI LLm first, to show that we run into an error
with patch("openai.resources.chat.completions.Completions.create", side_effect=error):
    try:
        print(openai_llm.invoke("Why did the chicken cross the road?"))
    except RateLimitError:
        print("Hit error")
```

```output
Hit error
```

```python
# Now let's try with fallbacks to Anthropic
with patch("openai.resources.chat.completions.Completions.create", side_effect=error):
    try:
        print(llm.invoke("Why did the chicken cross the road?"))
    except RateLimitError:
        print("Hit error")
```

```output
content=' I don\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\n\n- To get to the other side!\n\n- It was too chicken to just stand there. \n\n- It wanted a change of scenery.\n\n- It wanted to show the possum it could be done.\n\n- It was on its way to a poultry farmers\' convention.\n\nThe joke plays on the double meaning of "the other side" - literally crossing the road to the other side, or the "other side" meaning the afterlife. So it\'s an anti-joke, with a silly or unexpected pun as the answer.' additional_kwargs={} example=False
```

We can use our "LLM with Fallbacks" as we would a normal LLM.

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You're a nice assistant who always includes a compliment in your response",
        ),
        ("human", "Why did the {animal} cross the road"),
    ]
)
chain = prompt | llm
with patch("openai.resources.chat.completions.Completions.create", side_effect=error):
    try:
        print(chain.invoke({"animal": "kangaroo"}))
    except RateLimitError:
        print("Hit error")
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
content=" I don't actually know why the kangaroo crossed the road, but I can take a guess! Here are some possible reasons:\n\n- To get to the other side (the classic joke answer!)\n\n- It was trying to find some food or water \n\n- It was trying to find a mate during mating season\n\n- It was fleeing from a predator or perceived threat\n\n- It was disoriented and crossed accidentally \n\n- It was following a herd of other kangaroos who were crossing\n\n- It wanted a change of scenery or environment \n\n- It was trying to reach a new habitat or territory\n\nThe real reason is unknown without more context, but hopefully one of those potential explanations does the joke justice! Let me know if you have any other animal jokes I can try to decipher." additional_kwargs={} example=False
```

## Fallback for Sequences[â€‹](#fallback-for-sequences "Direct link to Fallback for Sequences")

We can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt.

```python
# First let's create a chain with a ChatModel
# We add in a string output parser here so the outputs between the two are the same type
from langchain_core.output_parsers import StrOutputParser

chat_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You're a nice assistant who always includes a compliment in your response",
        ),
        ("human", "Why did the {animal} cross the road"),
    ]
)
# Here we're going to use a bad model name to easily create a chain that will error
chat_model = ChatOpenAI(model="gpt-fake")
bad_chain = chat_prompt | chat_model | StrOutputParser()
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html)

```python
# Now lets create a chain with the normal OpenAI model
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAI

prompt_template = """Instructions: You should always include a compliment in your response.

Question: Why did the {animal} cross the road?"""
prompt = PromptTemplate.from_template(prompt_template)
llm = OpenAI()
good_chain = prompt | llm
```

**API Reference:**[PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

```python
# We can now create a final chain which combines the two
chain = bad_chain.with_fallbacks([good_chain])
chain.invoke({"animal": "turtle"})
```

```output
'\n\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.'
```

## Fallback for Long Inputs[â€‹](#fallback-for-long-inputs "Direct link to Fallback for Long Inputs")

One of the big limiting factors of LLMs is their context window. Usually, you can count and track the length of prompts before sending them to an LLM, but in situations where that is hard/complicated, you can fallback to a model with a longer context length.

```python
short_llm = ChatOpenAI()
long_llm = ChatOpenAI(model="gpt-3.5-turbo-16k")
llm = short_llm.with_fallbacks([long_llm])
```

```python
inputs = "What is the next number: " + ", ".join(["one", "two"] * 3000)
```

```python
try:
    print(short_llm.invoke(inputs))
except Exception as e:
    print(e)
```

```output
This model's maximum context length is 4097 tokens. However, your messages resulted in 12012 tokens. Please reduce the length of the messages.
```

```python
try:
    print(llm.invoke(inputs))
except Exception as e:
    print(e)
```

```output
content='The next number in the sequence is two.' additional_kwargs={} example=False
```

## Fallback to Better Model[â€‹](#fallback-to-better-model "Direct link to Fallback to Better Model")

Often times we ask models to output format in a specific format (like JSON). Models like GPT-3.5 can do this okay, but sometimes struggle. This naturally points to fallbacks - we can try with GPT-3.5 (faster, cheaper), but then if parsing fails we can use GPT-4.

```python
from langchain.output_parsers import DatetimeOutputParser
```

**API Reference:**[DatetimeOutputParser](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html)

```python
prompt = ChatPromptTemplate.from_template(
    "what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)"
)
```

```python
# In this case we are going to do the fallbacks on the LLM + output parser level
# Because the error will get raised in the OutputParser
openai_35 = ChatOpenAI() | DatetimeOutputParser()
openai_4 = ChatOpenAI(model="gpt-4") | DatetimeOutputParser()
```

```python
only_35 = prompt | openai_35
fallback_4 = prompt | openai_35.with_fallbacks([openai_4])
```

```python
try:
    print(only_35.invoke({"event": "the superbowl in 1994"}))
except Exception as e:
    print(f"Error: {e}")
```

```output
Error: Could not parse datetime string: The Super Bowl in 1994 took place on January 30th at 3:30 PM local time. Converting this to the specified format (%Y-%m-%dT%H:%M:%S.%fZ) results in: 1994-01-30T15:30:00.000Z
```

```python
try:
    print(fallback_4.invoke({"event": "the superbowl in 1994"}))
except Exception as e:
    print(f"Error: {e}")
```

```output
1994-01-30 15:30:00
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/fallbacks.ipynb)

* * *


- [Fallback for LLM API Errors](#fallback-for-llm-api-errors)
- [Fallback for Sequences](#fallback-for-sequences)
- [Fallback for Long Inputs](#fallback-for-long-inputs)
- [Fallback to Better Model](#fallback-to-better-model)









# How to stream tool calls

When [tools](/docs/concepts/tools/) are called in a streaming context, [message chunks](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html#langchain_core.messages.ai.AIMessageChunk) will be populated with [tool call chunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolCallChunk.html#langchain_core.messages.tool.ToolCallChunk) objects in a list via the `.tool_call_chunks` attribute. A `ToolCallChunk` includes optional string fields for the tool `name`, `args`, and `id`, and includes an optional integer field `index` that can be used to join chunks together. Fields are optional because portions of a tool call may be streamed across different chunks (e.g., a chunk that includes a substring of the arguments may have null values for the tool name and id).

Because message chunks inherit from their parent message class, an [AIMessageChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html#langchain_core.messages.ai.AIMessageChunk) with tool call chunks will also include `.tool_calls` and `.invalid_tool_calls` fields. These fields are parsed best-effort from the message's tool call chunks.

Note that not all providers currently support streaming for tool calls. Before we start let's define our tools and our model.

```python
from langchain_core.tools import tool


@tool
def add(a: int, b: int) -> int:
    """Adds a and b."""
    return a + b


@tool
def multiply(a: int, b: int) -> int:
    """Multiplies a and b."""
    return a * b


tools = [add, multiply]
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

```python
import os
from getpass import getpass

from langchain_openai import ChatOpenAI

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
llm_with_tools = llm.bind_tools(tools)
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

Now let's define our query and stream our output:

```python
query = "What is 3 * 12? Also, what is 11 + 49?"

async for chunk in llm_with_tools.astream(query):
    print(chunk.tool_call_chunks)
```

```output
[]
[{'name': 'Multiply', 'args': '', 'id': 'call_3aQwTP9CYlFxwOvQZPHDu6wL', 'index': 0}]
[{'name': None, 'args': '{"a"', 'id': None, 'index': 0}]
[{'name': None, 'args': ': 3, ', 'id': None, 'index': 0}]
[{'name': None, 'args': '"b": 1', 'id': None, 'index': 0}]
[{'name': None, 'args': '2}', 'id': None, 'index': 0}]
[{'name': 'Add', 'args': '', 'id': 'call_SQUoSsJz2p9Kx2x73GOgN1ja', 'index': 1}]
[{'name': None, 'args': '{"a"', 'id': None, 'index': 1}]
[{'name': None, 'args': ': 11,', 'id': None, 'index': 1}]
[{'name': None, 'args': ' "b": ', 'id': None, 'index': 1}]
[{'name': None, 'args': '49}', 'id': None, 'index': 1}]
[]
```

Note that adding message chunks will merge their corresponding tool call chunks. This is the principle by which LangChain's various [tool output parsers](/docs/how_to/output_parser_structured/) support streaming.

For example, below we accumulate tool call chunks:

```python
first = True
async for chunk in llm_with_tools.astream(query):
    if first:
        gathered = chunk
        first = False
    else:
        gathered = gathered + chunk

    print(gathered.tool_call_chunks)
```

```output
[]
[{'name': 'Multiply', 'args': '', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a"', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, ', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 1', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{"a"', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{"a": 11,', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{"a": 11, "b": ', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{"a": 11, "b": 49}', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{"a": 11, "b": 49}', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]
```

```python
print(type(gathered.tool_call_chunks[0]["args"]))
```

```output
<class 'str'>
```

And below we accumulate tool calls to demonstrate partial parsing:

```python
first = True
async for chunk in llm_with_tools.astream(query):
    if first:
        gathered = chunk
        first = False
    else:
        gathered = gathered + chunk

    print(gathered.tool_calls)
```

```output
[]
[]
[{'name': 'Multiply', 'args': {}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]
[{'name': 'Multiply', 'args': {'a': 3}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 1}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]
```

```python
print(type(gathered.tool_calls[0]["args"]))
```

```output
<class 'dict'>
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tool_streaming.ipynb)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/map_rerank_docs_chain.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/map_rerank_docs_chain.ipynb)

# Migrating from MapRerankDocumentsChain

[MapRerankDocumentsChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain.html) implements a strategy for analyzing long texts. The strategy is as follows:

- Split a text into smaller documents;
- Map a process to the set of documents, where the process includes generating a score;
- Rank the results by score and return the maximum.

A common process in this scenario is question-answering using pieces of context from a document. Forcing the model to generate a score along with its answer helps to select for answers generated only by relevant context.

An [LangGraph](https://langchain-ai.github.io/langgraph/) implementation allows for the incorporation of [tool calling](/docs/concepts/tool_calling/) and other features for this problem. Below we will go through both `MapRerankDocumentsChain` and a corresponding LangGraph implementation on a simple example for illustrative purposes.

## Example[â€‹](#example "Direct link to Example")

Let's go through an example where we analyze a set of documents. Let's use the following 3 documents:

```python
from langchain_core.documents import Document

documents = [
    Document(page_content="Alice has blue eyes", metadata={"title": "book_chapter_2"}),
    Document(page_content="Bob has brown eyes", metadata={"title": "book_chapter_1"}),
    Document(
        page_content="Charlie has green eyes", metadata={"title": "book_chapter_3"}
    ),
]
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

### Legacy[â€‹](#legacy "Direct link to Legacy")

Details

Below we show an implementation with `MapRerankDocumentsChain`. We define the prompt template for a question-answering task and instantiate a [LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) object for this purpose. We define how documents are formatted into the prompt and ensure consistency among the keys in the various prompts.

```python
from langchain.chains import LLMChain, MapRerankDocumentsChain
from langchain.output_parsers.regex import RegexParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAI

document_variable_name = "context"
llm = OpenAI()
# The prompt here should take as an input variable the
# `document_variable_name`
# The actual prompt will need to be a lot more complex, this is just
# an example.
prompt_template = (
    "What color are Bob's eyes? "
    "Output both your answer and a score (1-10) of how confident "
    "you are in the format: <Answer>\nScore: <Score>.\n\n"
    "Provide no other commentary.\n\n"
    "Context: {context}"
)
output_parser = RegexParser(
    regex=r"(.*?)\nScore: (.*)",
    output_keys=["answer", "score"],
)
prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["context"],
    output_parser=output_parser,
)
llm_chain = LLMChain(llm=llm, prompt=prompt)
chain = MapRerankDocumentsChain(
    llm_chain=llm_chain,
    document_variable_name=document_variable_name,
    rank_key="score",
    answer_key="answer",
)
```

**API Reference:**[LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [MapRerankDocumentsChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain.html) | [RegexParser](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.regex.RegexParser.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

```python
response = chain.invoke(documents)
response["output_text"]
```

```output
/langchain/libs/langchain/langchain/chains/llm.py:369: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
  warnings.warn(
```

```output
'Brown'
```

Inspecting the [LangSmith trace](https://smith.langchain.com/public/7a071bd1-0283-4b90-898c-6e4a2b5a0593/r) for the above run, we can see three LLM calls-- one for each document-- and that the scoring mechanism mitigated against hallucinations.

### LangGraph[â€‹](#langgraph "Direct link to LangGraph")

Details

Below we show a LangGraph implementation of this process. Note that our template is simplified, as we delegate the formatting instructions to the chat model's tool-calling features via the [.with\_structured\_output](/docs/how_to/structured_output/) method.

Here we follow a basic [map-reduce](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) workflow to execute the LLM calls in parallel.

We will need to install `langgraph`:

```python
pip install -qU langgraph
```

```python
import operator
from typing import Annotated, List, TypedDict

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.constants import Send
from langgraph.graph import END, START, StateGraph


class AnswerWithScore(TypedDict):
    answer: str
    score: Annotated[int, ..., "Score from 1-10."]


llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

prompt_template = "What color are Bob's eyes?\n\n" "Context: {context}"
prompt = ChatPromptTemplate.from_template(prompt_template)

# The below chain formats context from a document into a prompt, then
# generates a response structured according to the AnswerWithScore schema.
map_chain = prompt | llm.with_structured_output(AnswerWithScore)

# Below we define the components that will make up the graph


# This will be the overall state of the graph.
# It will contain the input document contents, corresponding
# answers with scores, and a final answer.
class State(TypedDict):
    contents: List[str]
    answers_with_scores: Annotated[list, operator.add]
    answer: str


# This will be the state of the node that we will "map" all
# documents to in order to generate answers with scores
class MapState(TypedDict):
    content: str


# Here we define the logic to map out over the documents
# We will use this an edge in the graph
def map_analyses(state: State):
    # We will return a list of `Send` objects
    # Each `Send` object consists of the name of a node in the graph
    # as well as the state to send to that node
    return [
        Send("generate_analysis", {"content": content}) for content in state["contents"]
    ]


# Here we generate an answer with score, given a document
async def generate_analysis(state: MapState):
    response = await map_chain.ainvoke(state["content"])
    return {"answers_with_scores": [response]}


# Here we will select the top answer
def pick_top_ranked(state: State):
    ranked_answers = sorted(
        state["answers_with_scores"], key=lambda x: -int(x["score"])
    )
    return {"answer": ranked_answers[0]}


# Construct the graph: here we put everything together to construct our graph
graph = StateGraph(State)
graph.add_node("generate_analysis", generate_analysis)
graph.add_node("pick_top_ranked", pick_top_ranked)
graph.add_conditional_edges(START, map_analyses, ["generate_analysis"])
graph.add_edge("generate_analysis", "pick_top_ranked")
graph.add_edge("pick_top_ranked", END)
app = graph.compile()
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [Send](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```python
from IPython.display import Image

Image(app.get_graph().draw_mermaid_png())
```

![](data:image/jpg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCAEvAJ8DASIAAhEBAxEB/8QAHQABAAIDAQEBAQAAAAAAAAAAAAYHBAUIAwIJAf/EAFUQAAEDAwICAgoMCwQIBwEAAAECAwQABQYHERIhEzEIFBUWIkFVVpTTFyNRVGFxdZKTldLUMjY3OEJTgaGys9EJUnJ0MzWCkZaisbQYJUNXY3PV8P/EABoBAQEAAwEBAAAAAAAAAAAAAAABAgMEBgX/xAA0EQACAAMDCgMIAwEAAAAAAAAAAQIDERJRkQQTFCExQVJhodEFgbEVIzNCU3HB4SIy8PH/2gAMAwEAAhEDEQA/AP1TpSlAKUpQCv4SEgknYDrJrWX299yGWkssKmz5CujjREK4S4rxkq58KEjmpWx2HUCSEnWpwlm6kPZE6b2+SFdrujaI0fcQz1EfCviV8PiG6GBUtRui6lpebRzJbQyspcusJCh1pVIQD/1r576rL5Ygeko/rXw3iNiZQEN2W3IQOpKYjYA/dX33q2XyPA9GR/Ssvc8+g1DvqsvliB6Sj+tO+qy+WIHpKP6071bL5HgejI/pTvVsvkeB6Mj+lPc8+hdQ76rL5Ygeko/rTvqsvliB6Sj+tO9Wy+R4HoyP6U71bL5HgejI/pT3PPoNQ76rL5Ygeko/rXoxkFrlOBDNyhvLPUlt9Cif2A1596tl8jwPRkf0rzew6wSUcD1jtrqf7q4jZH7xT3PPoNRuKVFziruPJ6bG3VR0oHO1POExHRv1DcEtHxAo8H3UqrdWa7s3uCmSylxo7lDjDw4XGVjkpCxudlA+4SD1gkEE4RQJK1C6r/bSUM6lKVqIKUpQClKUApSlARix7XbL77cHNldoKTbI3XugcCHXSP8AEpaAf/qTUnqMYkntO95TCVuFiemWncbbtuso2I93wkOD/ZqT10T/AO1OS9EVise4XCNaYEmdNfbiw4zSnnn3VBKG0JBKlKJ6gACSfgrIrUZfGiTcTvcefbnbxBdgvtyLcwniclNltQU0kbjcqG6QNxzPWK5yFVZd2V+IW3SbKs1xxyRkSbHGbe7WVBlRg6XN+hPEtnfo1cKvbACjYHnUqm684basOgZNPmXCFbJr5isB6zTUyHHQCSlMcs9KeSVHfg22BO+1c+R8czbL9G9WMHsltyiRh6cfbZxtrMIPalxRI4V8cNBUEqdbSlLYStY61cPEoDepxqFn2QZnZsIlwLNn1gxFya8xkTNttMiPeUlLCVMJShKemSypwkKcaH6OwUASaAsqZ2QWnsDELLlD2SsCw3mUYMGYhl1YdkBLii0UhBUlftTg4VAHiHD+EQDGZHZSY61qhj+Kog3dUO72p2eicqyzw4hwSG2UNlntfiSk8SypxWyU8KeLbiBNNYHgl9bjYhFfxXIozMTVqTeejvEdx95qE5EkONSHXd1hXhOICllR2cJBPFVvamyLhhXZA4lmasevV8sSsfn2V5djgrmOx31vxnWy42gFQQoNLHFtsCOe1AXhSlKAVGE7WjUHo29ks3mGt9aRvzfYLaOL3N1NrSPiaFSeoxcE9u6h2dtO57RgyH3TtyHSKbQgb/DwufNrok7Yk9lH6autCok9KUrnIKUpQClKUApSlAaG/WySzPYvdta6edHQWXo3EE9tMEglAJIAWkjiQVctypJKQsqT8TYmN6m49Jt1whw75anVJTJt89gLCVpIWEutLG6VJISeFQBBAqQ1prviVsvUgSn2VszUgATIjy474A6gVoIJA9wkjmeXOt6ihiShmbt5fuRBPY26UIO6dN8WSdiNxaWOo8j+jWXZ9A9NceukW5WvAsct9wiuB1iVGtjLbjSx1KSoJ3BHuitr3jupGyMnvyE+IdsNq/epsn99O8mR51X76Zn1VXNy+PoxRXkopUX7yZHnVfvpmfVVH8/wnLe827d5+U3Dvn6E9od03m+1ul3G3ScLW+22/VTNy+PoxRXlkUqGWnCbr3Khd0cqvPdDoUds9A810fS8I4+Hdrq332+CsvvJkedV++mZ9VTNy+PoxRXmikdjnpXLkOvv6dYw886orW4u0sFSlE7kk8PMk15/+GvSf/23xY/HaGPs1Ie8mR51X76Zn1VO8da9g7kl+eSP0e2kt7/tQhJ/fTNy+PoxRXmbKuVsw+BCt0aOlJQ0liDaYKEhxSUgJShtG4ASBsNzslI5kgDevrHLM9b0yps8tru09YdkqZJLaNhslpBOxKUDlvsNyVK2BUQPWy4zbcf6Qwo3A64AHJDq1OvObdXE4slavH1k9dbSsYooUnDBv3j7ClKVpIKUpQClKUApSlAKUpQClKUAqBa8QLDdNH8qiZReX8ex92GUzbpG36SM3uPCTslXPfbxHrqe1CNbLhFtWlOTS5uMKzSIzEKnLAhvjM0bj2sJ4Vb+71HqoCRYq1FYxeztQZCpkJENlLEhf4TrYQOFZ5DmRsf21ta1mMuofxu1OtQjbW1xGlJhEbGOCgbN7bDbh6urxVs6AUpSgFKUoBSlKAUpSgFKUoBSlKAUpSgFKUoBUY1Mj5RKwK9tYVJiw8qXHItz80Asod3GxUClQ2238RqT1AteIFhumj+VRMovL+PY+7DKZt0jb9JGb3HhJ2Srnvt4j10BMLIic3ZoCbmtDlyEdsSlt/gqd4Rxkchy4t/FWbWqxVqKxi9nagyFTISIbKWJC/wnWwgcKzyHMjY/tra0ApSlAKUpQClKUApSlAKV/FKCElSiEpA3JJ5AVCjmF7uwEiy2yCbavmzIuElbbjyfEsNpbPCk9Y3O5HWBW6XKim1s9i0qTalQju7mHvCx+lverp3dzD3hY/S3vV1u0WO9YoUJvSoR3dzD3hY/S3vV07u5h7wsfpb3q6aLHesUKE3pUI7u5h7wsfpb3q6d3cw94WP0t71dNFjvWKFCb1+WP9p9o5LxjVqLqCwhbtqyZltl9w8wzLZbS3wfAFNIQR7pS57lfov3dzD3hY/S3vV1X2vOmd51901uOIXmJZorchSHo81qQ6pyK8g7pcSC3z8aSPGlShuN6aLHesUKHMv9lroWCq8aqXSP1cdrs/SJ8fLp3k7/ALGwR/8AKK/RKqn08sV90yweyYrZrXY2rbaYqIrO8p0KXsOa1bNbFSlbqJ8ZUTUh7u5h7wsfpb3q6aLHesUKE3pUI7u5h7wsfpb3q6d3cw94WP0t71dNFjvWKFCb0qEd3cw94WP0t71dO7uYe8LH6W96umix3rFChN6VCO7uYe8LH6W96und3MPeFj9Le9XTRY71ihQm9KiELLrpBlMov1viR4r7iWkTIMhbqULUQEhxKkJKQSQAoE8yN9hzqX1omSopbpEKUNXlBKcZu5B2IhvEEf4DUexkAY3agAABEa2A/wAAqQ5V+LF4/wAm9/Aaj2Nfi5av8o1/AK7JPwX9/wADcbKlcmYlrBmeVX7T+49+zbj9+yWTb7lg0SHGDttjM9PvxKKS8ODoUdIV9fSDh4eRP8xDV7V/UGBb80x+0XyXbp07iYs3aNsTa1Qg+W1AyFSRKDobClcfCBxjbo9qxtoh1pSuWsj1K1Ct+KanZsxloRDw7JZEKPZO5sctSorbjRUh1wp49+FwhJQUkbAkq3rKyzU/UnLs/wA0gYaxfY8DGpSbcwm0W62yGpMjoUOKMlUuQhwJ3cAAaA8Eb8RJ2FtIHTdK5+teRak5vqzEx2VfFYOy3h9uvFygQ4kaS6zPcfeQ62hxxKxweBsd+LkhPDsSSY5kutGVWjUaJcLJfrrkOJKyqPYZjS7JFZtbIdfDC225PGJDjralfhgKQVJIO1LQOokuIUtSApJUnbiSDzG/VvX1XN2nDVzxPU3XfJZOS3GdbrTcjKftPa8YIkgW5lxO6g1xgoTshPCoAhAKuIkk6/TPUPWPKJOG5Eq23qfaL47Hfnw5MG2M2yLDfSFFyO63JMk9GFJUOkSorAO6Uk7BaB1DSlctzNaMqg6l2adab9dclwy45UmwOl6xxY9rbDjqmuFiQFiQ4ttYAK9lNqKFcxyFVugOpKVy1kepWoVvxTU7NmMtCIeHZLIhR7J3NjlqVFbcaKkOuFPHvwuEJKCkjYElW9enZAax5Xh15yq54ff7rcmcXaZeuFpjWSK5bY54UrU3JlOKS6VKQeLZncoCk7ipaQOlY95t8y5TLcxOjP3CEltUqI28lTrAWCUFaAd0hQSrbcDfY7dVZlc8BN/n6ma+KxKZ3PyNVlsjtueLSHB0wYkqQkpWCkhRHCeXIKJHPavM6z5dqoe2dNlMkw8MN4egvNoUl25yd0xYy1KG6S30L5ICkgkpCjtS0DoC7XiBYLe9Puc2NboLIBdlS3UtNNgkAcSlEAcyBz92suuTMszq9XXsdMsm9/Ey636zy4ZuVsvuOQ2JEbjWhKosiOtooKFFfGlYTz4Bss7E1O3M2yq2a/PWrJ8jk4zYJM1pnH4QtLTlvu7ZZBU2ZZBW3I6Tj8AqTySOEK3paBa+oB2xOaR1hTRHwHpUVYlV1qB+KU742v5qasWmUfCg+79ITLcavKvxYvH+Te/gNR7GvxctX+Ua/gFS6bERPhvxnd+iebU2rbr2I2P/AFqv4lyk4vCjWy52u5OvRW0siVBguSWnwkABY6JKinfbmlQBB3HMbE3J/wCUtwLbULWjnzFNL9Rsc1VbuFktl4ssaReS/dJ12utsmw5UEulTiU8DAlqWpO3Dxq8E7AqIFWxYNALVimQdvWTIsltVp7eVce9uLcAm2h5SuNeyODjCFKJUWwsIJJ8Gph35xvJl++pJfqqd+cbyZfvqSX6qtqyeNfKxZdxF7loRYLphebYw7MuSYGW3B65TnEOth1tx3g4g0SjYJ9rTsFBR5nma8cn0EtV+yu5ZDb8hyPE591bbbuYx+emOifwJ4UKcCkKIWE+CFoKVbeOpd35xvJl++pJfqqd+cbyZfvqSX6qrmI+Fiy7jwhYBb4Ofy8vQ/LXc5NrYtC23FpLXRNOOOJUBw8XGS6rclRGwHIcyYFcexfx64KkNJyDJYlsVdO7Ua1RpyExYU3punLzSS2Sd3OJXA4VoBUSEg7EWJ35xvJl++pJfqqd+cbyZfvqSX6qmYjfysWXcaJOjtsY1EuOXRbpdoi7oEd07O0+g2+epLJZSt1tSCdwjYeCpIPCncHatVh2hMTTeUw9YMiyV2228Oqt+My7p/wCWslSVAN8my4UDiOwWpYTyIG4FTLvzjeTL99SS/VVhXrU6y43apNzuzd1tdtjI6R+ZMtMpplpPuqWpsAD4SaZiPhYsu41AyLVLcb4NjIHjIyt7/wDPrQL7F/Hi5HbbyDJWLbBuqb1bbU1OQItvlB/p+NpHR7qBWV+C4VgBatgDzE/jZ5AmxmpEeDe32HUBxt1uyy1JWkjcEEN7EEc969O/ON5Mv31JL9VTMTN8LJZZF7loRYLphebYw7MuSYGW3B65TnEOth1tx3g4g0SjYJ9rTsFBR5nma1uY9jZj2aTsmXIvGQQLbkgCrraLfOSzElOhtLYeI4CsK4UI3AUEq4BxJVz3nXfnG8mX76kl+qp35xvJl++pJfqqZiPhLZdxHUaZx8RyCbmdpcu90v4syID9vEtptu7qYSehU6FJSgPc1JCwUJHGdxtUa0Z0ScxTA8tjXBDmOXfLrnMuktFnme225Lyj0bLT4A5tp25gbBSlbcudWP35xvJl++pJfqqd+cbyZfvqSX6qmYj4WLLuIIOxtsD+M5Xabher/d5WTqjd0rxOlNrmOJjkFlCSGwhKU7EfgfpK35862l50Qt2RZtFyG6ZDkM6PFuDV1j2J6ak29qU0kBtxKODjHCRxBPHw8XPbnUn7843ky/fUkv1VO/ON5Mv31JL9VTMR8LFl3HxqB+KU742v5qasWq6eL+boRbo1unxYanW1yZc+KuMlLaVpUUpS4ApSlbcI2Gw3JJ5AKsWufKdUEMD2pvrTsHsoKUpXAYilKUApSlAKUpQCqZ7Mr817Uf5LV/EmrmqmezK/Ne1H+S1fxJoCxNOPyeYv8lxf5KakVR3Tj8nmL/JcX+SmpFQClKUApSlAKUpQClKUApSlAKUpQClKUApSlAKpnsyvzXtR/ktX8SauaqZ7Mr817Uf5LV/EmgLE04/J5i/yXF/kpqRVHdOPyeYv8lxf5KakVAKUpQClKUApSlAKUpQClKUApXmZLSSQXUAjkQVCv520z+ub+cKtGD1pXl20z+ub+cKdtM/rm/nClGD1pXl20z+ub+cKdtM/rm/nClGD1rgbs7+zDkY1JznRp7CFqbmQWW2b8u5FAWh1ttwrSz0PMJUVI5L5lB5jqHenbTP65v5wriz+0z0RazjTmFn9qbS7ecb9qlpa5qegrVzPLmejWeL3AlbhPVSjBuew67Mi6a83yHhsTT3uZbrLa0GbelXgupbCEBCAG+107qWoDZPGNgFHnw7Hr+uauwM0UZ0X0OhSJ6G2cjyPhuc/i2C20FPtDJ6iOBB3IPUpxYrpDtpn9c384UowetK8u2mf1zfzhTtpn9c384UowetK8u2mf1zfzhTtpn9c384UowetK8u2mf1zfzhX9S+0tQSlxCifEFA0owelKUqAVDszeVcL9abEtakQpEeRMkIQopLwbU0lLZI/QJd3UNxvwgHdJUDMahOR/lHsfyTO/nRK68lXvfJ+jMkYfsf4udt8btB2AA3gtdQ5D9GnsfYt5tWf0Br7NeOd6jY9ppb4M3I56oEedLRAjFEd19Tr6kqUlsJbSo7kIVty6xt1kAwfMOyLxmLpjkOS2K+RW37YsRlquttnFMOQobpEqO2107aCOe5SB8NdjnzF87xJV3k+9j7FvNqz+gNfZp7H2LebVn9Aa+zWmzPWnD9Op8W3ZDeBHuj7HbAhxIr8p0Nb7F1SGULUhvfccSgByPPkajsPXeDDynUEX2XEhYtjzNqeiTW2XVPPCW0pWxSOIrUVcIQlCAo77bE0z8zjeIq7yd+x9i3m1Z/QGvs09j7FvNqz+gNfZreMupfaQ4kKCVpCgFpKVbH3QdiD8B51WN716tdg1tiafSoU7jftqZYmsQZL46ZbyG0N+1tFIRsokulXAk8iQaufmL5niKu8mXsfYt5tWf0Br7NPY+xbzas/oDX2aj83XvArdlpxqRkLSLqmUiCsBh0x25CtuFlcgI6JLh3A4FLCtyBtvWPfuyJ0/wAYvd0tVzvjkaXankMXBXc+UtmIpaErT0ryWi2hJStJ4lKCesb7g7TPzON4irvJR7H2LebVn9Aa+zT2PsW82rP6A19mt4y83IaQ60tLjS0hSVoO4UDzBB8YqGZfrPiGCZA3Yrxc3Wry5FE1uBGgyJTzjJUpHElLTairYoVuBuQBuQBzq5+YvneIq7zb+x9i3m1Z/QGvs09j7FvNqz+gNfZrQSNd8Gh5ojFJF87Xva5KYaWnYj6GS+obpa6co6LjO42TxbncDavC/dkLp/jF4uVruV/7XmWx9EaekQ5C0Q1LShSC8tLZS2ghxOy1EJJ3G+6VATPzON4irvJN7H2LebVn9Aa+zT2PsW82rP6A19moY9rTHsGfahwsikQrbjGMW22TkzQhZdJkl8KCtiePm0gIShPESrbwiRW4y7W/C8FfiMXq7ORJUmMJiYqIMh55tg/+o6222pTSesbuBI3BHWDTPzON4irvN37H2LebVn9Aa+zQaf4wAeHHbUgnxohNpPXuOYT7oB/ZWnybWzCcRttln3C/sqj3pHSW3tJpyYuWjhCittDKVqUgAglQGw3G551iaDakydWtNouTSUxQZM2cy0YaFJbU01KdabUApRO5QhJPPrJ5Dqpn5laWniKu8nuBzXltXe2vPLkptc3tZp11ZW4Wyy26kKUeaikO8O53JCQSSdzUoqHYD/rjMvlRv/so1TGuHKUlMdOXVJh7RUJyP8o9j+SZ386JU2qF5Kgp1BsTh5JNsmtjl1q6WKdv9wP/APA1lkvxPJ+jKio+yhuwsLeltwVElzkx81iOGPAZLz6wIsrcIQOaj8A5nxVWWoeP5FqRi+u2V2/FL3bo17sUK02y2S4K259wcYLilvGNsVj/AEoQncbkJPIV1FkOI2nKnbQ5dInbS7TORcoR6RaOikIStCV+CRxbJcWNlbjn1chW4ra4amJQjlxuOkeuOd3y44pf7/a8oj29yBPsFvVOLJjslpcZ1KPCb8Lw0kjhPGdyCKgeY6Z5Vkerua6h2yBdW37H3FvVqsE2MExro63HV0zRJBCnkIK20lJPA4vfr2Ndb0pZqDDs1zTerRCuCGJMVEplDwYmMqZebCkg8K0KAKVDfYpPMGqizZ+fhvZE2XKXLDeLvZJmNvWYv2aEuWY8jtpt1PSpRuUIKd/DPLcc6mN40N07yG6SbldMHx+4XCSsuPypNtacccUesqUU7k/HUosGPWvFLRHtVmt8a1WyPxBmHDaS003uoqPClIAG5JPxk1aNg5RveOZIzpHmGj7eH3uTkt3vspyNe0wlKtzjT87thE1yV+AkoQRuknj4mwAD11nzMzftOSdkBjsPEr/lNyvFwTGiottvU9GU45a47YS87+A0NyCor2HCeW/VXV9aey4jaceul7uNvidrzL1JTLnudItXTOpbS0FbKJCfAQkbJAHLfr3NY2QYOmGNysN01xOwTnhIm2q0xIL7qTuFuNspQog+MbpNQ9FgmHsqHb0q3Pm3jDEQ0XAsq6EO9vLWpoObbcXDwqKd99tjW+vGhuneQ3STcrpg+P3C4SVlx+VJtrTjjij1lSincn46lFgx614paI9qs1vjWq2R+IMw4bSWmm91FR4UpAA3JJ+MmsqA5G1VgZhkjuSC7WjO7rfoGUMS4EW3MPdx2rUxLacbcQlBDb7haSSR4bvGeSQBynWQYleJOKdk8ymyznXbwl421sRVlU09yWkJ6Ebe2e2ApHDv4QI666PpUsg49v8Ao7l2Q6gXvJoDFwamY9ZceuVutkyOUwrvNjpeUplwqT4S0J40DY7oW+lR2IFbG6211vVbIswvmOakLsmW2+3Srd3ruz4z8RbTHRuRJbDDiFIWFeEFLHD4avCHOusaVLAOardjyNDtT8dvdvwvIpmHSMTbs8VmBGcuMu0viSuQtt5AUpYSsOAFYKhxNgE7AGp52LttuFr0gjNXS1zbNLcul0kGFcGCy82lye+tHEk9W6VJIPUQQQSDvVs0rJQ0YMHAf9cZl8qN/wDZRqmNRDAkHunlzg5oXdEbHY+KJHSf3g1L60ZT8TyXojJ7RWuvdii3+KhmSFpU0sOsvsrKHGXACAtCh1HYkHxEKUkggkHY0rnhicLrC9ZiQ5WAzyfBzG9JGwAHQwj/ANY9fzvAuHnne/oIX3eplSujSZnLBdi1Ib3gXDzzvf0EL7vTvAuHnne/oIX3eplSmkzOWC7CpDe8C4eed7+ghfd6d4Fw88739BC+71MqU0mZywXYVIb3gXDzzvf0EL7vUA19VkGlWjeWZbassuUm42mEZLDUyNDU0pQIGyglhJI5+IirxqmezK/Ne1H+S1fxJppMzlguwqb/ABPFrrfsWs1zfzG8IfmwmZLiW2IQSFLQFEDeOTtufdra94Fw88739BC+71sNOPyeYv8AJcX+SmpFTSZnLBdhUhveBcPPO9/QQvu9O8C4eed7+ghfd6mVKaTM5YLsKkN7wLh553v6CF93p3gXDzzvf0EL7vUypTSZnLBdhUhveBcPPO9/QQvu9fSMBmg7OZdenUHrT0cNPj91LAIqYUppMzlguwqYlqtUWyQGoUJroY7e+wKiokkkqUpRJKlKJJKiSSSSSSTWXSlczbidXtIKUpUApSlAKUpQClKUAqmezK/Ne1H+S1fxJq5qpnsyvzXtR/ktX8SaAsTTj8nmL/JcX+SmpFUd04/J5i/yXF/kpqRUApSlAKUpQClKUApSlAKUpQClKUApSlAKUpQCqZ7Mr817Uf5LV/Emrmr8qf7TvSOZi2s7OdI43rXlLDaVL25MyWGkNFHVyBbS2obnmeP+7QH6aacfk8xf5Li/yU1Iq/OL+y40J7buV11Uusc9HE4rbZuNPW4pOz7w+JKg2COR43B1iv0doBSlKAUpSgFKUoBSlKAUpSgFKUoBWoyTK7ViUISrpLTHQo8LaACtx0+4hA3Kj8Qr3v8Ae42OWWbdJiimNFaU6vhG6jsOQA8ZJ2AHjJFc43O7Tcjubl1uagqa6OEIB3SwjfcNI+Ae74zzNfY8O8PeWROKJ0hX+ohzLCm69ul09z8Zdda57KnTEsKPw8KEucvjO/wVi+zzePNWH9bq+71X9K9WvCsiSo5dfN9xa5FgezzePNWH9bq+71WfZEvO9kPpfPxC44/DtzjjjciJcRcVPKiPoPJYQWBvukqSRuOSzzFZlKvsvIvp9Yu5LXI2+l2bOaT6fWHEbPicMQLTGSwlZuykl1XWtxQEf8JaypR+FRqU+zzePNWH9bq+71X9Y0e5Q5cuVFYlMPSohSmQw24FLZKk8SQtIO6dwdxv1jnT2ZkX0+sXcteRZPs83jzVh/W6vu9Z9u16QXQLrj8mI0Tt00J9MkJHuqSQhW3+EKPwVWFKxi8KyNqigp5v8ti1yOl7LfIGRW9udbZTcyKvqcbPUfGCOsEeMHmKz65qxvKpGDXXuoxxKiqKe3oyRv0zQ61Af30jcg+Pbh6jy6RYfbksNvNLS404kLQtJ3CgRuCDXkcvyF5FGqOsL2P8F5npSlK+WQUpSgFKUoBSlKArjXd9TeGxWB/o5FxYQ4PEQklwf8yE1UFX5qfjb2U4ZOiRU8c5ookxk/3nG1BYT/tAFP8AtVQDD6ZLKHUb8KxuNxsR8BHiPwV7nwWOF5M4VtT1+dP95B7D7pUXut0zFi4PN27HLRMhJPtT8i9OMOLG3jQIywnnv+kaxTes98WJ2P8A4gd+519pzYU6a8H2MCJ6i5xkrmoScVx1u5tIjW5FwkyLTGiPvqK3FIQnaS4lAQOAkkBRJIHg7c8GPkuoFxuuDWO5S1YxPuSLoJrgix3HXUMFosuhO7iG1lKuY3UkFSuR8HaXXLTx3NJUK+XNcrEsmjNri9tY9cekKo5VxdGpbjIC07+FsUcj1Gtuxp/CZumN3Bc24SpdiYkR2HJL4cU8HggLU6ojdSvAGxBHWevxcblTYonFV0bW+mqq3bU6VKVnD1Gyq4xbdizNyZbyGVkU+yqvi4qDwsRkqdU6GvwOkKAlIG3Dvudq3mkUCdbdRdTI1yuarxLRJgcU1xlDKnB2okjdKAEggbDkBvtvsK3M/ReyT4Epjtu5RpDt4dvjM+M+luRFkufhFpQTsE7bjhUFbg896/lswifp+9dJ9g6bKLnd3mlzXL/cwyR0bfAkpUhhXiA8HYD3NuqsYZc2GOGKPXTm3qo1s3uu8E/pUM7tZ95p2L/iF37nWysNxyiVOKLxYrZbonASHol1XJXxcthwGO2NuvnxeLqruUyFumvB9iEhIBGx5ir10fkLk6aWErO/RsFhP+FC1IT/AMqRVDu9MoJajNF+W8oNMMp61uKOyR/vP7Bua6TxOxJxjGbXaUr6TtOOhlTm23GoDwlftO5/bXn/AByOFSYIN7dfKn7M1sNtSlK8YBSlKAUpSgFKUoBVX5/pM7cZj92x/oW5jxK5EF5XA0+rxrSoA8Cz4+XCo7E8J4lG0KV05PlMzJY7cp6/UHME213a2Oqbm2K6x1pJBIhOOo+e2FI/fWNu/wCT7l9Xv/YrqilegXjsdNctYjUcr7v+T7l9Xv8A2Kbv+T7l9Xv/AGK6opV9vRfT6/oURyvu/wCT7l9Xv/Ypu/5PuX1e/wDYrqilPb0X0+v6FEcr7v8Ak+5fV7/2KzrfYb5eHQ3AsNydUd/DfjKjNj4St0JBHxb/ABGumqVjF47HT+MtV+//AAaiAaeaYJxh4XO6ONTLyU8KA1uWYoI2UGyQCpR6isgHbkAkFXFP6Urz0+fMyiNzJjqwKUpWgClKUB//2Q==)

```python
result = await app.ainvoke({"contents": [doc.page_content for doc in documents]})
result["answer"]
```

```output
{'answer': 'Bob has brown eyes.', 'score': 10}
```

Inspecting the [LangSmith trace](https://smith.langchain.com/public/b64bf9aa-7558-4c1b-be5c-ba8924069039/r) for the above run, we can see three LLM calls as before. Using the model's tool-calling features have also enabled us to remove the parsing step.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

See these [how-to guides](/docs/how_to/#qa-with-rag) for more on question-answering tasks with RAG.

Check out the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) for detail on building with LangGraph, including [this guide](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) on the details of map-reduce in LangGraph.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_chains/map_rerank_docs_chain.ipynb)

* * *


- [Example](#example)
  
  - [Legacy](#legacy)
  - [LangGraph](#langgraph)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/tool_artifacts.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/tool_artifacts.ipynb)

# How to return artifacts from a tool

Prerequisites

This guide assumes familiarity with the following concepts:

- [ToolMessage](/docs/concepts/messages/#toolmessage)
- [Tools](/docs/concepts/tools/)
- [Function/tool calling](/docs/concepts/tool_calling/)

[Tools](/docs/concepts/tools/) are utilities that can be [called by a model](/docs/concepts/tool_calling/), and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.

The Tool and [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html) interfaces make it possible to distinguish between the parts of the tool output meant for the model (this is the ToolMessage.content) and those parts which are meant for use outside the model (ToolMessage.artifact).

Requires `langchain-core >= 0.2.19`

This functionality was added in `langchain-core == 0.2.19`. Please make sure your package is up to date.

## Defining the tool[â€‹](#defining-the-tool "Direct link to Defining the tool")

If we want our tool to distinguish between message content and other artifacts, we need to specify `response_format="content_and_artifact"` when defining our tool and make sure that we return a tuple of (content, artifact):

```python
%pip install -qU "langchain-core>=0.2.19"
```

```python
import random
from typing import List, Tuple

from langchain_core.tools import tool


@tool(response_format="content_and_artifact")
def generate_random_ints(min: int, max: int, size: int) -> Tuple[str, List[int]]:
    """Generate size random ints in the range [min, max]."""
    array = [random.randint(min, max) for _ in range(size)]
    content = f"Successfully generated array of {size} random ints in [{min}, {max}]."
    return content, array
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

## Invoking the tool with ToolCall[â€‹](#invoking-the-tool-with-toolcall "Direct link to Invoking the tool with ToolCall")

If we directly invoke our tool with just the tool arguments, you'll notice that we only get back the content part of the Tool output:

```python
generate_random_ints.invoke({"min": 0, "max": 9, "size": 10})
```

```output
'Successfully generated array of 10 random ints in [0, 9].'
```

In order to get back both the content and the artifact, we need to invoke our model with a ToolCall (which is just a dictionary with "name", "args", "id" and "type" keys), which has additional info needed to generate a ToolMessage like the tool call ID:

```python
generate_random_ints.invoke(
    {
        "name": "generate_random_ints",
        "args": {"min": 0, "max": 9, "size": 10},
        "id": "123",  # required
        "type": "tool_call",  # required
    }
)
```

```output
ToolMessage(content='Successfully generated array of 10 random ints in [0, 9].', name='generate_random_ints', tool_call_id='123', artifact=[2, 8, 0, 6, 0, 0, 1, 5, 0, 0])
```

## Using with a model[â€‹](#using-with-a-model "Direct link to Using with a model")

With a [tool-calling model](/docs/how_to/tool_calling/), we can easily use a model to call our Tool and generate ToolMessages:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
llm_with_tools = llm.bind_tools([generate_random_ints])

ai_msg = llm_with_tools.invoke("generate 6 positive ints less than 25")
ai_msg.tool_calls
```

```output
[{'name': 'generate_random_ints',
  'args': {'min': 1, 'max': 24, 'size': 6},
  'id': 'toolu_01EtALY3Wz1DVYhv1TLvZGvE',
  'type': 'tool_call'}]
```

```python
generate_random_ints.invoke(ai_msg.tool_calls[0])
```

```output
ToolMessage(content='Successfully generated array of 6 random ints in [1, 24].', name='generate_random_ints', tool_call_id='toolu_01EtALY3Wz1DVYhv1TLvZGvE', artifact=[2, 20, 23, 8, 1, 15])
```

If we just pass in the tool call args, we'll only get back the content:

```python
generate_random_ints.invoke(ai_msg.tool_calls[0]["args"])
```

```output
'Successfully generated array of 6 random ints in [1, 24].'
```

If we wanted to declaratively create a chain, we could do this:

```python
from operator import attrgetter

chain = llm_with_tools | attrgetter("tool_calls") | generate_random_ints.map()

chain.invoke("give me a random number between 1 and 5")
```

```output
[ToolMessage(content='Successfully generated array of 1 random ints in [1, 5].', name='generate_random_ints', tool_call_id='toolu_01FwYhnkwDPJPbKdGq4ng6uD', artifact=[5])]
```

## Creating from BaseTool class[â€‹](#creating-from-basetool-class "Direct link to Creating from BaseTool class")

If you want to create a BaseTool object directly, instead of decorating a function with `@tool`, you can do so like this:

```python
from langchain_core.tools import BaseTool


class GenerateRandomFloats(BaseTool):
    name: str = "generate_random_floats"
    description: str = "Generate size random floats in the range [min, max]."
    response_format: str = "content_and_artifact"

    ndigits: int = 2

    def _run(self, min: float, max: float, size: int) -> Tuple[str, List[float]]:
        range_ = max - min
        array = [
            round(min + (range_ * random.random()), ndigits=self.ndigits)
            for _ in range(size)
        ]
        content = f"Generated {size} floats in [{min}, {max}], rounded to {self.ndigits} decimals."
        return content, array

    # Optionally define an equivalent async method

    # async def _arun(self, min: float, max: float, size: int) -> Tuple[str, List[float]]:
    #     ...
```

**API Reference:**[BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html)

```python
rand_gen = GenerateRandomFloats(ndigits=4)
rand_gen.invoke({"min": 0.1, "max": 3.3333, "size": 3})
```

```output
'Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.'
```

```python
rand_gen.invoke(
    {
        "name": "generate_random_floats",
        "args": {"min": 0.1, "max": 3.3333, "size": 3},
        "id": "123",
        "type": "tool_call",
    }
)
```

```output
ToolMessage(content='Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.', name='generate_random_floats', tool_call_id='123', artifact=[1.5789, 2.464, 2.2719])
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tool_artifacts.ipynb)

* * *


- [Defining the tool](#defining-the-tool)
- [Invoking the tool with ToolCall](#invoking-the-tool-with-toolcall)
- [Using with a model](#using-with-a-model)
- [Creating from BaseTool class](#creating-from-basetool-class)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/callbacks_constructor.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/callbacks_constructor.ipynb)

# How to propagate callbacks constructor

Prerequisites

This guide assumes familiarity with the following concepts:

- [Callbacks](/docs/concepts/callbacks/)
- [Custom callback handlers](/docs/how_to/custom_callbacks/)

Most LangChain modules allow you to pass `callbacks` directly into the constructor (i.e., initializer). In this case, the callbacks will only be called for that instance (and any nested runs).

warning

Constructor callbacks are scoped only to the object they are defined on. They are **not** inherited by children of the object. This can lead to confusing behavior, and it's generally better to pass callbacks as a run time argument.

Here's an example:

```python
from typing import Any, Dict, List

from langchain_anthropic import ChatAnthropic
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage
from langchain_core.outputs import LLMResult
from langchain_core.prompts import ChatPromptTemplate


class LoggingHandler(BaseCallbackHandler):
    def on_chat_model_start(
        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs
    ) -> None:
        print("Chat model started")

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        print(f"Chat model ended, response: {response}")

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs
    ) -> None:
        print(f"Chain {serialized.get('name')} started")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:
        print(f"Chain ended, outputs: {outputs}")


callbacks = [LoggingHandler()]
llm = ChatAnthropic(model="claude-3-sonnet-20240229", callbacks=callbacks)
prompt = ChatPromptTemplate.from_template("What is 1 + {number}?")

chain = prompt | llm

chain.invoke({"number": "2"})
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [BaseCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [LLMResult](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.llm_result.LLMResult.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
Chat model started
Chat model ended, response: generations=[[ChatGeneration(text='1 + 2 = 3', message=AIMessage(content='1 + 2 = 3', response_metadata={'id': 'msg_01CdKsRmeS9WRb8BWnHDEHm7', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}}, id='run-2d7fdf2a-7405-4e17-97c0-67e6b2a65305-0'))]] llm_output={'id': 'msg_01CdKsRmeS9WRb8BWnHDEHm7', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}} run=None
```

```output
AIMessage(content='1 + 2 = 3', response_metadata={'id': 'msg_01CdKsRmeS9WRb8BWnHDEHm7', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}}, id='run-2d7fdf2a-7405-4e17-97c0-67e6b2a65305-0')
```

You can see that we only see events from the chat model run - no chain events from the prompt or broader chain.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to pass callbacks into a constructor.

Next, check out the other how-to guides in this section, such as how to [pass callbacks at runtime](/docs/how_to/callbacks_runtime/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/callbacks_constructor.ipynb)

* * *


- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/filter_messages.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/filter_messages.ipynb)

# How to filter messages

In more complex chains and agents we might track state with a list of [messages](/docs/concepts/messages/). This list can start to accumulate messages from multiple different models, speakers, sub-chains, etc., and we may only want to pass subsets of this full list of messages to each model call in the chain/agent.

The `filter_messages` utility makes it easy to filter messages by type, id, or name.

## Basic usage[â€‹](#basic-usage "Direct link to Basic usage")

```python
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    filter_messages,
)

messages = [
    SystemMessage("you are a good assistant", id="1"),
    HumanMessage("example input", id="2", name="example_user"),
    AIMessage("example output", id="3", name="example_assistant"),
    HumanMessage("real input", id="4", name="bob"),
    AIMessage("real output", id="5", name="alice"),
]

filter_messages(messages, include_types="human")
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [filter\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.filter_messages.html)

```output
[HumanMessage(content='example input', name='example_user', id='2'),
 HumanMessage(content='real input', name='bob', id='4')]
```

```python
filter_messages(messages, exclude_names=["example_user", "example_assistant"])
```

```output
[SystemMessage(content='you are a good assistant', id='1'),
 HumanMessage(content='real input', name='bob', id='4'),
 AIMessage(content='real output', name='alice', id='5')]
```

```python
filter_messages(messages, include_types=[HumanMessage, AIMessage], exclude_ids=["3"])
```

```output
[HumanMessage(content='example input', name='example_user', id='2'),
 HumanMessage(content='real input', name='bob', id='4'),
 AIMessage(content='real output', name='alice', id='5')]
```

## Chaining[â€‹](#chaining "Direct link to Chaining")

`filter_messages` can be used in an imperatively (like above) or declaratively, making it easy to compose with other components in a chain:

```python
%pip install -qU langchain-anthropic
```

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-sonnet-20240229", temperature=0)
# Notice we don't pass in messages. This creates
# a RunnableLambda that takes messages as input
filter_ = filter_messages(exclude_names=["example_user", "example_assistant"])
chain = filter_ | llm
chain.invoke(messages)
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)

```output
AIMessage(content=[], response_metadata={'id': 'msg_01Wz7gBHahAwkZ1KCBNtXmwA', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 3}}, id='run-b5d8a3fe-004f-4502-a071-a6c025031827-0', usage_metadata={'input_tokens': 16, 'output_tokens': 3, 'total_tokens': 19})
```

Looking at the LangSmith trace we can see that before the messages are passed to the model they are filtered: [https://smith.langchain.com/public/f808a724-e072-438e-9991-657cc9e7e253/r](https://smith.langchain.com/public/f808a724-e072-438e-9991-657cc9e7e253/r)

Looking at just the filter\_, we can see that it's a Runnable object that can be invoked like all Runnables:

```python
filter_.invoke(messages)
```

```output
[HumanMessage(content='real input', name='bob', id='4'),
 AIMessage(content='real output', name='alice', id='5')]
```

## API reference[â€‹](#api-reference "Direct link to API reference")

For a complete description of all arguments head to the API reference: [https://python.langchain.com/api\_reference/core/messages/langchain\_core.messages.utils.filter\_messages.html](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.filter_messages.html)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/filter_messages.ipynb)

* * *


- [Basic usage](#basic-usage)
- [Chaining](#chaining)
- [API reference](#api-reference)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/recursive_text_splitter.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/recursive_text_splitter.ipynb)

# How to recursively split text by characters

This [text splitter](/docs/concepts/text_splitters/) is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `["\n\n", "\n", " ", ""]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.

1. How the text is split: by list of characters.
2. How the chunk size is measured: by number of characters.

Below we show example usage.

To obtain the string content directly, use `.split_text`.

To create LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects (e.g., for use in downstream tasks), use `.create_documents`.

```python
%pip install -qU langchain-text-splitters
```

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Load example document
with open("state_of_the_union.txt") as f:
    state_of_the_union = f.read()

text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size=100,
    chunk_overlap=20,
    length_function=len,
    is_separator_regex=False,
)
texts = text_splitter.create_documents([state_of_the_union])
print(texts[0])
print(texts[1])
```

**API Reference:**[RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

```output
page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and'
page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.'
```

```python
text_splitter.split_text(state_of_the_union)[:2]
```

```output
['Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and',
 'of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.']
```

Let's go through the parameters set above for `RecursiveCharacterTextSplitter`:

- `chunk_size`: The maximum size of a chunk, where size is determined by the `length_function`.
- `chunk_overlap`: Target overlap between chunks. Overlapping chunks helps to mitigate loss of information when context is divided between chunks.
- `length_function`: Function determining the chunk size.
- `is_separator_regex`: Whether the separator list (defaulting to `["\n\n", "\n", " ", ""]`) should be interpreted as regex.

## Splitting text from languages without word boundaries[â€‹](#splitting-text-from-languages-without-word-boundaries "Direct link to Splitting text from languages without word boundaries")

Some writing systems do not have [word boundaries](https://en.wikipedia.org/wiki/Category:Writing_systems_without_word_boundaries), for example Chinese, Japanese, and Thai. Splitting text with the default separator list of `["\n\n", "\n", " ", ""]` can cause words to be split between chunks. To keep words together, you can override the list of separators to include additional punctuation:

- Add ASCII full-stop "`.`", [Unicode fullwidth](https://en.wikipedia.org/wiki/Halfwidth_and_Fullwidth_Forms_%28Unicode_block%29) full stop "`ï¼Ž`" (used in Chinese text), and [ideographic full stop](https://en.wikipedia.org/wiki/CJK_Symbols_and_Punctuation) "`ã€‚`" (used in Japanese and Chinese)
- Add [Zero-width space](https://en.wikipedia.org/wiki/Zero-width_space) used in Thai, Myanmar, Kmer, and Japanese.
- Add ASCII comma "`,`", Unicode fullwidth comma "`ï¼Œ`", and Unicode ideographic comma "`ã€`"

```python
text_splitter = RecursiveCharacterTextSplitter(
    separators=[
        "\n\n",
        "\n",
        " ",
        ".",
        ",",
        "\u200b",  # Zero-width space
        "\uff0c",  # Fullwidth comma
        "\u3001",  # Ideographic comma
        "\uff0e",  # Fullwidth full stop
        "\u3002",  # Ideographic full stop
        "",
    ],
    # Existing args
)
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/recursive_text_splitter.ipynb)

* * *


- [Splitting text from languages without word boundaries](#splitting-text-from-languages-without-word-boundaries)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/qa_per_user.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/qa_per_user.ipynb)

# How to do per-user retrieval

This guide demonstrates how to configure runtime properties of a retrieval chain. An example application is to limit the documents available to a [retriever](/docs/concepts/retrievers/) based on the user.

When building a [retrieval app](/docs/concepts/rag/), you often have to build it with multiple users in mind. This means that you may be storing data not just for one user, but for many different users, and they should not be able to see eachother's data. This means that you need to be able to configure your retrieval chain to only retrieve certain information. This generally involves two steps.

**Step 1: Make sure the retriever you are using supports multiple users**

At the moment, there is no unified flag or filter for this in LangChain. Rather, each vectorstore and retriever may have their own, and may be called different things (namespaces, multi-tenancy, etc). For vectorstores, this is generally exposed as a keyword argument that is passed in during `similarity_search`. By reading the documentation or source code, figure out whether the retriever you are using supports multiple users, and, if so, how to use it.

Note: adding documentation and/or support for multiple users for retrievers that do not support it (or document it) is a GREAT way to contribute to LangChain

**Step 2: Add that parameter as a configurable field for the chain**

This will let you easily call the chain and configure any relevant flags at runtime. See [this documentation](/docs/how_to/configure/) for more information on configuration.

Now, at runtime you can call this chain with configurable field.

## Code Example[â€‹](#code-example "Direct link to Code Example")

Let's see a concrete example of what this looks like in code. We will use Pinecone for this example.

To configure Pinecone, set the following environment variable:

- `PINECONE_API_KEY`: Your Pinecone API key

```python
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

embeddings = OpenAIEmbeddings()
vectorstore = PineconeVectorStore(index_name="test-example", embedding=embeddings)

vectorstore.add_texts(["I worked at Kensho"], namespace="harrison")
vectorstore.add_texts(["I worked at Facebook"], namespace="ankush")
```

**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [PineconeVectorStore](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html)

```output
['f907aab7-77c7-4347-acc2-6859f8142f92']
```

The pinecone kwarg for `namespace` can be used to separate documents

```python
# This will only get documents for Ankush
vectorstore.as_retriever(search_kwargs={"namespace": "ankush"}).invoke(
    "where did i work?"
)
```

```output
[Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')]
```

```python
# This will only get documents for Harrison
vectorstore.as_retriever(search_kwargs={"namespace": "harrison"}).invoke(
    "where did i work?"
)
```

```output
[Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')]
```

We can now create the chain that we will use to do question-answering over.

Let's first select a LLM.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

This will follow the basic implementation from the [RAG tutorial](/docs/tutorials/rag/), but we will allow the retrieval step to be configurable.

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import ConfigurableField

template = """Answer the question based only on the following context:
{context}
Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

retriever = vectorstore.as_retriever()
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ConfigurableField](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html)

Here we mark the retriever as having a configurable field. All vectorstore retrievers have `search_kwargs` as a field. This is just a dictionary, with vectorstore specific fields.

This will let us pass in a value for `search_kwargs` when invoking the chain.

```python
configurable_retriever = retriever.configurable_fields(
    search_kwargs=ConfigurableField(
        id="search_kwargs",
        name="Search Kwargs",
        description="The search kwargs to use",
    )
)
```

We can now create the chain using our configurable retriever.

```python
from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig
from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict


class State(TypedDict):
    question: str
    context: List[Document]
    answer: str


def retrieve(state: State, config: RunnableConfig):
    retrieved_docs = configurable_retriever.invoke(state["question"], config)
    return {"context": retrieved_docs}


def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}


graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```python
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

We can now invoke the chain with configurable options. `search_kwargs` is the id of the configurable field. The value is the search kwargs to use for Pinecone.

```python
result = graph.invoke(
    {"question": "Where did the user work?"},
    config={"configurable": {"search_kwargs": {"namespace": "harrison"}}},
)

result
```

```output
{'question': 'Where did the user work?',
 'context': [Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')],
 'answer': 'The user worked at Kensho.'}
```

```python
result = graph.invoke(
    {"question": "Where did the user work?"},
    config={"configurable": {"search_kwargs": {"namespace": "ankush"}}},
)

result
```

```output
{'question': 'Where did the user work?',
 'context': [Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')],
 'answer': 'The user worked at Facebook.'}
```

For details operating your specific vector store, see the [integration pages](/docs/integrations/vectorstores/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/qa_per_user.ipynb)

* * *


- [Code Example](#code-example)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/prompts_partial.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/prompts_partial.ipynb)

# How to partially format prompt templates

Prerequisites

This guide assumes familiarity with the following concepts:

- [Prompt templates](/docs/concepts/prompt_templates/)

Like partially binding arguments to a function, it can make sense to "partial" a [prompt template](/docs/concepts/prompt_templates/) - e.g. pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.

LangChain supports this in two ways:

1. Partial formatting with string values.
2. Partial formatting with functions that return string values.

In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain.

## Partial with strings[â€‹](#partial-with-strings "Direct link to Partial with strings")

One common use case for wanting to partial a prompt template is if you get access to some of the variables in a prompt before others. For example, suppose you have a prompt template that requires two variables, `foo` and `baz`. If you get the `foo` value early on in your chain, but the `baz` value later, it can be inconvenient to pass both variables all the way through the chain. Instead, you can partial the prompt template with the `foo` value, and then pass the partialed prompt template along and just use that. Below is an example of doing this:

```python
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("{foo}{bar}")
partial_prompt = prompt.partial(foo="foo")
print(partial_prompt.format(bar="baz"))
```

**API Reference:**[PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

```output
foobaz
```

You can also just initialize the prompt with the partialed variables.

```python
prompt = PromptTemplate(
    template="{foo}{bar}", input_variables=["bar"], partial_variables={"foo": "foo"}
)
print(prompt.format(bar="baz"))
```

```output
foobaz
```

## Partial with functions[â€‹](#partial-with-functions "Direct link to Partial with functions")

The other common use is to partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables is inconvenient. In this case, it's handy to be able to partial the prompt with a function that always returns the current date.

```python
from datetime import datetime


def _get_datetime():
    now = datetime.now()
    return now.strftime("%m/%d/%Y, %H:%M:%S")


prompt = PromptTemplate(
    template="Tell me a {adjective} joke about the day {date}",
    input_variables=["adjective", "date"],
)
partial_prompt = prompt.partial(date=_get_datetime)
print(partial_prompt.format(adjective="funny"))
```

```output
Tell me a funny joke about the day 04/21/2024, 19:43:57
```

You can also just initialize the prompt with the partialed variables, which often makes more sense in this workflow.

```python
prompt = PromptTemplate(
    template="Tell me a {adjective} joke about the day {date}",
    input_variables=["adjective"],
    partial_variables={"date": _get_datetime},
)
print(prompt.format(adjective="funny"))
```

```output
Tell me a funny joke about the day 04/21/2024, 19:43:57
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to partially apply variables to your prompt templates.

Next, check out the other how-to guides on prompt templates in this section, like [adding few-shot examples to your prompt templates](/docs/how_to/few_shot_examples_chat/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/prompts_partial.ipynb)

* * *


- [Partial with strings](#partial-with-strings)
- [Partial with functions](#partial-with-functions)
- [Next steps](#next-steps)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/tools.mdx)

# Tools

Prerequisites

- [Chat models](/docs/concepts/chat_models/)

## Overview[â€‹](#overview "Direct link to Overview")

The **tool** abstraction in LangChain associates a Python **function** with a **schema** that defines the function's **name**, **description** and **expected arguments**.

**Tools** can be passed to [chat models](/docs/concepts/chat_models/) that support [tool calling](/docs/concepts/tool_calling/) allowing the model to request the execution of a specific function with specific inputs.

## Key concepts[â€‹](#key-concepts "Direct link to Key concepts")

- Tools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.
- Create tools using the [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) decorator, which simplifies the process of tool creation, supporting the following:
  
  - Automatically infer the tool's **name**, **description** and **expected arguments**, while also supporting customization.
  - Defining tools that return **artifacts** (e.g. images, dataframes, etc.)
  - Hiding input arguments from the schema (and hence from the model) using **injected tool arguments**.

## Tool interface[â€‹](#tool-interface "Direct link to Tool interface")

The tool interface is defined in the [BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool) class which is a subclass of the [Runnable Interface](/docs/concepts/runnables/).

The key attributes that correspond to the tool's **schema**:

- **name**: The name of the tool.
- **description**: A description of what the tool does.
- **args**: Property that returns the JSON schema for the tool's arguments.

The key methods to execute the function associated with the **tool**:

- **invoke**: Invokes the tool with the given arguments.
- **ainvoke**: Invokes the tool with the given arguments, asynchronously. Used for [async programming with Langchain](/docs/concepts/async/).

## Create tools using the `@tool` decorator[â€‹](#create-tools-using-the-tool-decorator "Direct link to create-tools-using-the-tool-decorator")

The recommended way to create tools is using the [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) decorator. This decorator is designed to simplify the process of tool creation and should be used in most cases. After defining a function, you can decorate it with [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) to create a tool that implements the [Tool Interface](#tool-interface).

```python
from langchain_core.tools import tool

@tool
def multiply(a: int, b: int) -> int:
   """Multiply two numbers."""
   return a * b
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

For more details on how to create tools, see the [how to create custom tools](/docs/how_to/custom_tools/) guide.

note

LangChain has a few other ways to create tools; e.g., by sub-classing the [BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool) class or by using `StructuredTool`. These methods are shown in the [how to create custom tools guide](/docs/how_to/custom_tools/), but we generally recommend using the `@tool` decorator for most cases.

## Use the tool directly[â€‹](#use-the-tool-directly "Direct link to Use the tool directly")

Once you have defined a tool, you can use it directly by calling the function. For example, to use the `multiply` tool defined above:

```python
multiply.invoke({"a": 2, "b": 3})
```

### Inspect[â€‹](#inspect "Direct link to Inspect")

You can also inspect the tool's schema and other properties:

```python
print(multiply.name) # multiply
print(multiply.description) # Multiply two numbers.
print(multiply.args) 
# {
# 'type': 'object', 
# 'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 
# 'required': ['a', 'b']
# }
```

note

If you're using pre-built LangChain or LangGraph components like [create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent),you might not need to interact with tools directly. However, understanding how to use them can be valuable for debugging and testing. Additionally, when building custom LangGraph workflows, you may find it necessary to work with tools directly.

## Configuring the schema[â€‹](#configuring-the-schema "Direct link to Configuring the schema")

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description or parse the function's doc-string to infer the schema).

Please see the [API reference for @tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) for more details and review the [how to create custom tools](/docs/how_to/custom_tools/) guide for examples.

## Tool artifacts[â€‹](#tool-artifacts "Direct link to Tool artifacts")

**Tools** are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.

```python
@tool(response_format="content_and_artifact")
def some_tool(...) -> Tuple[str, Any]:
    """Tool that does something."""
    ...
    return 'Message for chat model', some_artifact 
```

See [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations[â€‹](#special-type-annotations "Direct link to Special type annotations")

There are a number of special type annotations that can be used in the tool's function signature to configure the run time behavior of the tool.

The following type annotations will end up **removing** the argument from the tool's schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.

- **InjectedToolArg**: Value should be injected manually at runtime using `.invoke` or `.ainvoke`.
- **RunnableConfig**: Pass in the RunnableConfig object to the tool.
- **InjectedState**: Pass in the overall state of the LangGraph graph to the tool.
- **InjectedStore**: Pass in the LangGraph store object to the tool.

You can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated\[..., "string literal"]** -- Adds a description to the argument that will be exposed in the tool's schema.

### InjectedToolArg[â€‹](#injectedtoolarg "Direct link to InjectedToolArg")

There are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the `InjectedToolArg` annotation, which allows certain parameters to be hidden from the tool's schema.

For example, if a tool requires a `user_id` to be injected dynamically at runtime, it can be structured in this way:

```python
from langchain_core.tools import tool, InjectedToolArg

@tool
def user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:
    """Tool that processes input data."""
    return f"User {user_id} processed {input_data}"
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [InjectedToolArg](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.InjectedToolArg.html)

Annotating the `user_id` argument with `InjectedToolArg` tells LangChain that this argument should not be exposed as part of the tool's schema.

See [how to pass run time values to tools](/docs/how_to/tool_runtime/) for more details on how to use `InjectedToolArg`.

### RunnableConfig[â€‹](#runnableconfig "Direct link to RunnableConfig")

You can use the `RunnableConfig` object to pass custom run time values to tools.

If you need to access the [RunnableConfig](/docs/concepts/runnables/#runnableconfig) object from within a tool. This can be done by using the `RunnableConfig` annotation in the tool's function signature.

```python
from langchain_core.runnables import RunnableConfig

@tool
async def some_func(..., config: RunnableConfig) -> ...:
    """Tool that does something."""
    # do something with config
    ...

await some_func.ainvoke(..., config={"configurable": {"value": "some_value"}})
```

**API Reference:**[RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html)

The `config` will not be part of the tool's schema and will be injected at runtime with appropriate values.

note

You may need to access the `config` object to manually propagate it to subclass. This happens if you're working with python 3.9 / 3.10 in an [async](/docs/concepts/async/) environment and need to manually propagate the `config` object to sub-calls.

Please read [Propagation RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig) for more details to learn how to propagate the `RunnableConfig` down the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).

### InjectedState[â€‹](#injectedstate "Direct link to InjectedState")

Please see the [InjectedState](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.InjectedState) documentation for more details.

### InjectedStore[â€‹](#injectedstore "Direct link to InjectedStore")

Please see the [InjectedStore](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.InjectedStore) documentation for more details.

## Best practices[â€‹](#best-practices "Direct link to Best practices")

When designing tools to be used by models, keep the following in mind:

- Tools that are well-named, correctly-documented and properly type-hinted are easier for models to use.
- Design simple and narrowly scoped tools, as they are easier for models to use correctly.
- Use chat models that support [tool-calling](/docs/concepts/tool_calling/) APIs to take advantage of tools.

## Toolkits[â€‹](#toolkits "Direct link to Toolkits")

LangChain has a concept of **toolkits**. This a very thin abstraction that groups tools together that are designed to be used together for specific tasks.

### Interface[â€‹](#interface "Direct link to Interface")

All Toolkits expose a `get_tools` method which returns a list of tools. You can therefore do:

```python
# Initialize a toolkit
toolkit = ExampleTookit(...)

# Get list of tools
tools = toolkit.get_tools()
```

## Related resources[â€‹](#related-resources "Direct link to Related resources")

See the following resources for more information:

- [API Reference for @tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)
- [How to create custom tools](/docs/how_to/custom_tools/)
- [How to pass run time values to tools](/docs/how_to/tool_runtime/)
- [All LangChain tool how-to guides](https://docs.langchain.com/docs/how_to/#tools)
- [Additional how-to guides that show usage with LangGraph](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/)
- Tool integrations, see the [tool integration docs](https://docs.langchain.com/docs/integrations/tools/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/tools.mdx)

* * *


- [Overview](#overview)
- [Key concepts](#key-concepts)
- [Tool interface](#tool-interface)
- [Create tools using the `@tool` decorator](#create-tools-using-the-tool-decorator)
- [Use the tool directly](#use-the-tool-directly)
  
  - [Inspect](#inspect)
- [Configuring the schema](#configuring-the-schema)
- [Tool artifacts](#tool-artifacts)
- [Special type annotations](#special-type-annotations)
  
  - [InjectedToolArg](#injectedtoolarg)
  - [RunnableConfig](#runnableconfig)
  - [InjectedState](#injectedstate)
  - [InjectedStore](#injectedstore)
- [Best practices](#best-practices)
- [Toolkits](#toolkits)
  
  - [Interface](#interface)
- [Related resources](#related-resources)









# How to reorder retrieved results to mitigate the "lost in the middle" effect

Substantial performance degradations in [RAG](/docs/tutorials/rag/) applications have been [documented](https://arxiv.org/abs/2307.03172) as the number of retrieved documents grows (e.g., beyond ten). In brief: models are liable to miss relevant information in the middle of long contexts.

By contrast, queries against vector stores will typically return documents in descending order of relevance (e.g., as measured by cosine similarity of [embeddings](/docs/concepts/embedding_models/)).

To mitigate the ["lost in the middle"](https://arxiv.org/abs/2307.03172) effect, you can re-order documents after retrieval such that the most relevant documents are positioned at extrema (e.g., the first and last pieces of context), and the least relevant documents are positioned in the middle. In some cases this can help surface the most relevant information to LLMs.

The [LongContextReorder](https://python.langchain.com/api_reference/community/document_transformers/langchain_community.document_transformers.long_context_reorder.LongContextReorder.html) document transformer implements this re-ordering procedure. Below we demonstrate an example.

```python
%pip install -qU langchain langchain-community langchain-openai
```

First we embed some artificial documents and index them in a basic in-memory vector store. We will use [OpenAI](/docs/integrations/providers/openai/) embeddings, but any LangChain vector store or embeddings model will suffice.

```python
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings

# Get embeddings.
embeddings = OpenAIEmbeddings()

texts = [
    "Basquetball is a great sport.",
    "Fly me to the moon is one of my favourite songs.",
    "The Celtics are my favourite team.",
    "This is a document about the Boston Celtics",
    "I simply love going to the movies",
    "The Boston Celtics won the game by 20 points",
    "This is just a random text.",
    "Elden Ring is one of the best games in the last 15 years.",
    "L. Kornet is one of the best Celtics players.",
    "Larry Bird was an iconic NBA player.",
]

# Create a retriever
retriever = InMemoryVectorStore.from_texts(texts, embedding=embeddings).as_retriever(
    search_kwargs={"k": 10}
)
query = "What can you tell me about the Celtics?"

# Get relevant documents ordered by relevance score
docs = retriever.invoke(query)
for doc in docs:
    print(f"- {doc.page_content}")
```

**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```output
- The Celtics are my favourite team.
- This is a document about the Boston Celtics
- The Boston Celtics won the game by 20 points
- L. Kornet is one of the best Celtics players.
- Basquetball is a great sport.
- Larry Bird was an iconic NBA player.
- This is just a random text.
- I simply love going to the movies
- Fly me to the moon is one of my favourite songs.
- Elden Ring is one of the best games in the last 15 years.
```

Note that documents are returned in descending order of relevance to the query. The `LongContextReorder` document transformer will implement the re-ordering described above:

```python
from langchain_community.document_transformers import LongContextReorder

# Reorder the documents:
# Less relevant document will be at the middle of the list and more
# relevant elements at beginning / end.
reordering = LongContextReorder()
reordered_docs = reordering.transform_documents(docs)

# Confirm that the 4 relevant documents are at beginning and end.
for doc in reordered_docs:
    print(f"- {doc.page_content}")
```

**API Reference:**[LongContextReorder](https://python.langchain.com/api_reference/community/document_transformers/langchain_community.document_transformers.long_context_reorder.LongContextReorder.html)

```output
- This is a document about the Boston Celtics
- L. Kornet is one of the best Celtics players.
- Larry Bird was an iconic NBA player.
- I simply love going to the movies
- Elden Ring is one of the best games in the last 15 years.
- Fly me to the moon is one of my favourite songs.
- This is just a random text.
- Basquetball is a great sport.
- The Boston Celtics won the game by 20 points
- The Celtics are my favourite team.
```

Below, we show how to incorporate the re-ordered documents into a simple question-answering chain:

```python
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

prompt_template = """
Given these texts:
-----
{context}
-----
Please answer the following question:
{query}
"""

prompt = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "query"],
)

# Create and invoke the chain:
chain = create_stuff_documents_chain(llm, prompt)
response = chain.invoke({"context": reordered_docs, "query": query})
print(response)
```

**API Reference:**[create\_stuff\_documents\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
The Boston Celtics are a professional basketball team known for their rich history and success in the NBA. L. Kornet is recognized as one of the best players on the team, and the Celtics recently won a game by 20 points. The Celtics are favored by some fans, as indicated by the statement, "The Celtics are my favourite team." Overall, they have a strong following and are considered a significant part of basketball culture.
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/long_context_reorder.ipynb)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/configure.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/configure.ipynb)

# How to configure runtime chain internals

Prerequisites

This guide assumes familiarity with the following concepts:

- [The Runnable interface](/docs/concepts/runnables/)
- [Chaining runnables](/docs/how_to/sequence/)
- [Binding runtime arguments](/docs/how_to/binding/)

Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things within your chains. This can include tweaking parameters such as temperature or even swapping out one model for another. In order to make this experience as easy as possible, we have defined two methods.

- A `configurable_fields` method. This lets you configure particular fields of a runnable.
  
  - This is related to the [`.bind`](/docs/how_to/binding/) method on runnables, but allows you to specify parameters for a given step in a chain at runtime rather than specifying them beforehand.
- A `configurable_alternatives` method. With this method, you can list out alternatives for any particular runnable that can be set during runtime, and swap them for those specified alternatives.

## Configurable Fields[â€‹](#configurable-fields "Direct link to Configurable Fields")

Let's walk through an example that configures chat model fields like temperature at runtime:

```python
%pip install --upgrade --quiet langchain langchain-openai

import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

### Configuring fields on a chat model[â€‹](#configuring-fields-on-a-chat-model "Direct link to Configuring fields on a chat model")

If using [init\_chat\_model](/docs/how_to/chat_models_universal_init/) to create a chat model, you can specify configurable fields in the constructor:

```python
from langchain.chat_models import init_chat_model

llm = init_chat_model(
    "openai:gpt-4o-mini",
    configurable_fields=("temperature",),
)
```

**API Reference:**[init\_chat\_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)

You can then set the parameter at runtime using `.with_config`:

```python
response = llm.with_config({"temperature": 0}).invoke("Hello")
print(response.content)
```

```output
Hello! How can I assist you today?
```

tip

In addition to invocation parameters like temperature, configuring fields this way extends to clients and other attributes.

#### Use with tools[â€‹](#use-with-tools "Direct link to Use with tools")

This method is applicable when [binding tools](/docs/concepts/tool_calling/) as well:

```python
from langchain_core.tools import tool


@tool
def get_weather(location: str):
    """Get the weather."""
    return "It's sunny."


llm_with_tools = llm.bind_tools([get_weather])
response = llm_with_tools.with_config({"temperature": 0}).invoke(
    "What's the weather in SF?"
)
response.tool_calls
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

```output
[{'name': 'get_weather',
  'args': {'location': 'San Francisco'},
  'id': 'call_B93EttzlGyYUhzbIIiMcl3bE',
  'type': 'tool_call'}]
```

In addition to `.with_config`, we can now include the parameter when passing a configuration directly. See example below, where we allow the underlying model temperature to be configurable inside of a [langgraph agent](/docs/tutorials/agents/):

```python
! pip install --upgrade langgraph
```

```python
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(llm, [get_weather])

response = agent.invoke(
    {"messages": "What's the weather in Boston?"},
    {"configurable": {"temperature": 0}},
)
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

### Configuring fields on arbitrary Runnables[â€‹](#configuring-fields-on-arbitrary-runnables "Direct link to Configuring fields on arbitrary Runnables")

You can also use the `.configurable_fields` method on arbitrary [Runnables](/docs/concepts/runnables/), as shown below:

```python
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI

model = ChatOpenAI(temperature=0).configurable_fields(
    temperature=ConfigurableField(
        id="llm_temperature",
        name="LLM Temperature",
        description="The temperature of the LLM",
    )
)

model.invoke("pick a random number")
```

**API Reference:**[PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [ConfigurableField](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
AIMessage(content='17', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba26a0da-0a69-4533-ab7f-21178a73d303-0')
```

Above, we defined `temperature` as a [`ConfigurableField`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html#langchain_core.runnables.utils.ConfigurableField) that we can set at runtime. To do so, we use the [`with_config`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.with_config) method like this:

```python
model.with_config(configurable={"llm_temperature": 0.9}).invoke("pick a random number")
```

```output
AIMessage(content='12', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba8422ad-be77-4cb1-ac45-ad0aae74e3d9-0')
```

Note that the passed `llm_temperature` entry in the dict has the same key as the `id` of the `ConfigurableField`.

We can also do this to affect just one step that's part of a chain:

```python
prompt = PromptTemplate.from_template("Pick a random number above {x}")
chain = prompt | model

chain.invoke({"x": 0})
```

```output
AIMessage(content='27', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ecd4cadd-1b72-4f92-b9a0-15e08091f537-0')
```

```python
chain.with_config(configurable={"llm_temperature": 0.9}).invoke({"x": 0})
```

```output
AIMessage(content='35', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-a916602b-3460-46d3-a4a8-7c926ec747c0-0')
```

### With HubRunnables[â€‹](#with-hubrunnables "Direct link to With HubRunnables")

This is useful to allow for switching of prompts

```python
from langchain.runnables.hub import HubRunnable

prompt = HubRunnable("rlm/rag-prompt").configurable_fields(
    owner_repo_commit=ConfigurableField(
        id="hub_commit",
        name="Hub Commit",
        description="The Hub commit to pull from",
    )
)

prompt.invoke({"question": "foo", "context": "bar"})
```

**API Reference:**[HubRunnable](https://python.langchain.com/api_reference/langchain/runnables/langchain.runnables.hub.HubRunnable.html)

```output
ChatPromptValue(messages=[HumanMessage(content="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: foo \nContext: bar \nAnswer:")])
```

```python
prompt.with_config(configurable={"hub_commit": "rlm/rag-prompt-llama"}).invoke(
    {"question": "foo", "context": "bar"}
)
```

```output
ChatPromptValue(messages=[HumanMessage(content="[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \nQuestion: foo \nContext: bar \nAnswer: [/INST]")])
```

## Configurable Alternatives[â€‹](#configurable-alternatives "Direct link to Configurable Alternatives")

The `configurable_alternatives()` method allows us to swap out steps in a chain with an alternative. Below, we swap out one chat model for another:

```python
%pip install --upgrade --quiet langchain-anthropic

import os
from getpass import getpass

if "ANTHROPIC_API_KEY" not in os.environ:
    os.environ["ANTHROPIC_API_KEY"] = getpass()
```

```output
[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.
You should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m
[0mNote: you may need to restart the kernel to use updated packages.
```

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI

llm = ChatAnthropic(
    model="claude-3-haiku-20240307", temperature=0
).configurable_alternatives(
    # This gives this field an id
    # When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField(id="llm"),
    # This sets a default_key.
    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used
    default_key="anthropic",
    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`
    openai=ChatOpenAI(),
    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model="gpt-4")`
    gpt4=ChatOpenAI(model="gpt-4"),
    # You can add more configuration options here
)
prompt = PromptTemplate.from_template("Tell me a joke about {topic}")
chain = prompt | llm

# By default it will call Anthropic
chain.invoke({"topic": "bears"})
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [ConfigurableField](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
AIMessage(content="Here's a bear joke for you:\n\nWhy don't bears wear socks? \nBecause they have bear feet!\n\nHow's that? I tried to come up with a simple, silly pun-based joke about bears. Puns and wordplay are a common way to create humorous bear jokes. Let me know if you'd like to hear another one!", response_metadata={'id': 'msg_018edUHh5fUbWdiimhrC3dZD', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 80}}, id='run-775bc58c-28d7-4e6b-a268-48fa6661f02f-0')
```

```python
# We can use `.with_config(configurable={"llm": "openai"})` to specify an llm to use
chain.with_config(configurable={"llm": "openai"}).invoke({"topic": "bears"})
```

```output
AIMessage(content="Why don't bears like fast food?\n\nBecause they can't catch it!", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-7bdaa992-19c9-4f0d-9a0c-1f326bc992d4-0')
```

```python
# If we use the `default_key` then it uses the default
chain.with_config(configurable={"llm": "anthropic"}).invoke({"topic": "bears"})
```

```output
AIMessage(content="Here's a bear joke for you:\n\nWhy don't bears wear socks? \nBecause they have bear feet!\n\nHow's that? I tried to come up with a simple, silly pun-based joke about bears. Puns and wordplay are a common way to create humorous bear jokes. Let me know if you'd like to hear another one!", response_metadata={'id': 'msg_01BZvbmnEPGBtcxRWETCHkct', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 80}}, id='run-59b6ee44-a1cd-41b8-a026-28ee67cdd718-0')
```

### With Prompts[â€‹](#with-prompts "Direct link to With Prompts")

We can do a similar thing, but alternate between prompts

```python
llm = ChatAnthropic(model="claude-3-haiku-20240307", temperature=0)
prompt = PromptTemplate.from_template(
    "Tell me a joke about {topic}"
).configurable_alternatives(
    # This gives this field an id
    # When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField(id="prompt"),
    # This sets a default_key.
    # If we specify this key, the default prompt (asking for a joke, as initialized above) will be used
    default_key="joke",
    # This adds a new option, with name `poem`
    poem=PromptTemplate.from_template("Write a short poem about {topic}"),
    # You can add more configuration options here
)
chain = prompt | llm

# By default it will write a joke
chain.invoke({"topic": "bears"})
```

```output
AIMessage(content="Here's a bear joke for you:\n\nWhy don't bears wear socks? \nBecause they have bear feet!", response_metadata={'id': 'msg_01DtM1cssjNFZYgeS3gMZ49H', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 28}}, id='run-8199af7d-ea31-443d-b064-483693f2e0a1-0')
```

```python
# We can configure it write a poem
chain.with_config(configurable={"prompt": "poem"}).invoke({"topic": "bears"})
```

```output
AIMessage(content="Here is a short poem about bears:\n\nMajestic bears, strong and true,\nRoaming the forests, wild and free.\nPowerful paws, fur soft and brown,\nCommanding respect, nature's crown.\n\nForaging for berries, fishing streams,\nProtecting their young, fierce and keen.\nMighty bears, a sight to behold,\nGuardians of the wilderness, untold.\n\nIn the wild they reign supreme,\nEmbodying nature's grand theme.\nBears, a symbol of strength and grace,\nCaptivating all who see their face.", response_metadata={'id': 'msg_01Wck3qPxrjURtutvtodaJFn', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 134}}, id='run-69414a1e-51d7-4bec-a307-b34b7d61025e-0')
```

### With Prompts and LLMs[â€‹](#with-prompts-and-llms "Direct link to With Prompts and LLMs")

We can also have multiple things configurable! Here's an example doing that with both prompts and LLMs.

```python
llm = ChatAnthropic(
    model="claude-3-haiku-20240307", temperature=0
).configurable_alternatives(
    # This gives this field an id
    # When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField(id="llm"),
    # This sets a default_key.
    # If we specify this key, the default LLM (ChatAnthropic initialized above) will be used
    default_key="anthropic",
    # This adds a new option, with name `openai` that is equal to `ChatOpenAI()`
    openai=ChatOpenAI(),
    # This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model="gpt-4")`
    gpt4=ChatOpenAI(model="gpt-4"),
    # You can add more configuration options here
)
prompt = PromptTemplate.from_template(
    "Tell me a joke about {topic}"
).configurable_alternatives(
    # This gives this field an id
    # When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField(id="prompt"),
    # This sets a default_key.
    # If we specify this key, the default prompt (asking for a joke, as initialized above) will be used
    default_key="joke",
    # This adds a new option, with name `poem`
    poem=PromptTemplate.from_template("Write a short poem about {topic}"),
    # You can add more configuration options here
)
chain = prompt | llm

# We can configure it write a poem with OpenAI
chain.with_config(configurable={"prompt": "poem", "llm": "openai"}).invoke(
    {"topic": "bears"}
)
```

```output
AIMessage(content="In the forest deep and wide,\nBears roam with grace and pride.\nWith fur as dark as night,\nThey rule the land with all their might.\n\nIn winter's chill, they hibernate,\nIn spring they emerge, hungry and great.\nWith claws sharp and eyes so keen,\nThey hunt for food, fierce and lean.\n\nBut beneath their tough exterior,\nLies a gentle heart, warm and superior.\nThey love their cubs with all their might,\nProtecting them through day and night.\n\nSo let us admire these majestic creatures,\nIn awe of their strength and features.\nFor in the wild, they reign supreme,\nThe mighty bears, a timeless dream.", response_metadata={'token_usage': {'completion_tokens': 133, 'prompt_tokens': 13, 'total_tokens': 146}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-5eec0b96-d580-49fd-ac4e-e32a0803b49b-0')
```

```python
# We can always just configure only one if we want
chain.with_config(configurable={"llm": "openai"}).invoke({"topic": "bears"})
```

```output
AIMessage(content="Why don't bears wear shoes?\n\nBecause they have bear feet!", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-c1b14c9c-4988-49b8-9363-15bfd479973a-0')
```

### Saving configurations[â€‹](#saving-configurations "Direct link to Saving configurations")

We can also easily save configured chains as their own objects

```python
openai_joke = chain.with_config(configurable={"llm": "openai"})

openai_joke.invoke({"topic": "bears"})
```

```output
AIMessage(content="Why did the bear break up with his girlfriend? \nBecause he couldn't bear the relationship anymore!", response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-391ebd55-9137-458b-9a11-97acaff6a892-0')
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You now know how to configure a chain's internal steps at runtime.

To learn more, see the other how-to guides on runnables in this section, including:

- Using [.bind()](/docs/how_to/binding/) as a simpler way to set a runnable's runtime parameters

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/configure.ipynb)

* * *


- [Configurable Fields](#configurable-fields)
  
  - [Configuring fields on a chat model](#configuring-fields-on-a-chat-model)
  - [Configuring fields on arbitrary Runnables](#configuring-fields-on-arbitrary-runnables)
  - [With HubRunnables](#with-hubrunnables)
- [Configurable Alternatives](#configurable-alternatives)
  
  - [With Prompts](#with-prompts)
  - [With Prompts and LLMs](#with-prompts-and-llms)
  - [Saving configurations](#saving-configurations)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/code_splitter.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/code_splitter.ipynb)

# How to split code

[RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html) includes pre-built lists of separators that are useful for [splitting text](/docs/concepts/text_splitters/) in a specific programming language.

Supported languages are stored in the `langchain_text_splitters.Language` enum. They include:

```text
"cpp",
"go",
"java",
"kotlin",
"js",
"ts",
"php",
"proto",
"python",
"rst",
"ruby",
"rust",
"scala",
"swift",
"markdown",
"latex",
"html",
"sol",
"csharp",
"cobol",
"c",
"lua",
"perl",
"haskell"
```

To view the list of separators for a given language, pass a value from this enum into

```python
RecursiveCharacterTextSplitter.get_separators_for_language
```

To instantiate a splitter that is tailored for a specific language, pass a value from the enum into

```python
RecursiveCharacterTextSplitter.from_language
```

Below we demonstrate examples for the various languages.

```python
%pip install -qU langchain-text-splitters
```

```python
from langchain_text_splitters import (
    Language,
    RecursiveCharacterTextSplitter,
)
```

**API Reference:**[Language](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.Language.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

To view the full list of supported languages:

```python
[e.value for e in Language]
```

```output
['cpp',
 'go',
 'java',
 'kotlin',
 'js',
 'ts',
 'php',
 'proto',
 'python',
 'rst',
 'ruby',
 'rust',
 'scala',
 'swift',
 'markdown',
 'latex',
 'html',
 'sol',
 'csharp',
 'cobol',
 'c',
 'lua',
 'perl',
 'haskell']
```

You can also see the separators used for a given language:

```python
RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)
```

```output
['\nclass ', '\ndef ', '\n\tdef ', '\n\n', '\n', ' ', '']
```

## Python[â€‹](#python "Direct link to Python")

Here's an example using the PythonTextSplitter:

```python
PYTHON_CODE = """
def hello_world():
    print("Hello, World!")

# Call the function
hello_world()
"""
python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON, chunk_size=50, chunk_overlap=0
)
python_docs = python_splitter.create_documents([PYTHON_CODE])
python_docs
```

```output
[Document(page_content='def hello_world():\n    print("Hello, World!")'),
 Document(page_content='# Call the function\nhello_world()')]
```

## JS[â€‹](#js "Direct link to JS")

Here's an example using the JS text splitter:

```python
JS_CODE = """
function helloWorld() {
  console.log("Hello, World!");
}

// Call the function
helloWorld();
"""

js_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.JS, chunk_size=60, chunk_overlap=0
)
js_docs = js_splitter.create_documents([JS_CODE])
js_docs
```

```output
[Document(page_content='function helloWorld() {\n  console.log("Hello, World!");\n}'),
 Document(page_content='// Call the function\nhelloWorld();')]
```

## TS[â€‹](#ts "Direct link to TS")

Here's an example using the TS text splitter:

```python
TS_CODE = """
function helloWorld(): void {
  console.log("Hello, World!");
}

// Call the function
helloWorld();
"""

ts_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.TS, chunk_size=60, chunk_overlap=0
)
ts_docs = ts_splitter.create_documents([TS_CODE])
ts_docs
```

```output
[Document(page_content='function helloWorld(): void {'),
 Document(page_content='console.log("Hello, World!");\n}'),
 Document(page_content='// Call the function\nhelloWorld();')]
```

## Markdown[â€‹](#markdown "Direct link to Markdown")

Here's an example using the Markdown text splitter:

```python
markdown_text = """
# ðŸ¦œï¸ðŸ”— LangChain

âš¡ Building applications with LLMs through composability âš¡

## What is LangChain?

# Hopefully this code block isn't split
LangChain is a framework for...

As an open-source project in a rapidly developing field, we are extremely open to contributions.
"""
```

```python
md_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0
)
md_docs = md_splitter.create_documents([markdown_text])
md_docs
```

```output
[Document(metadata={}, page_content='# ðŸ¦œï¸ðŸ”— LangChain'),
 Document(metadata={}, page_content='âš¡ Building applications with LLMs through composability âš¡'),
 Document(metadata={}, page_content='## What is LangChain?'),
 Document(metadata={}, page_content="# Hopefully this code block isn't split"),
 Document(metadata={}, page_content='LangChain is a framework for...'),
 Document(metadata={}, page_content='As an open-source project in a rapidly developing field, we'),
 Document(metadata={}, page_content='are extremely open to contributions.')]
```

## Latex[â€‹](#latex "Direct link to Latex")

Here's an example on Latex text:

```python
latex_text = """
\documentclass{article}

\begin{document}

\maketitle

\section{Introduction}
Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in a variety of natural language processing tasks, including language translation, text generation, and sentiment analysis.

\subsection{History of LLMs}
The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.

\subsection{Applications of LLMs}
LLMs have many applications in industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.

\end{document}
"""
```

```python
latex_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.MARKDOWN, chunk_size=60, chunk_overlap=0
)
latex_docs = latex_splitter.create_documents([latex_text])
latex_docs
```

```output
[Document(page_content='\\documentclass{article}\n\n\x08egin{document}\n\n\\maketitle'),
 Document(page_content='\\section{Introduction}'),
 Document(page_content='Large language models (LLMs) are a type of machine learning'),
 Document(page_content='model that can be trained on vast amounts of text data to'),
 Document(page_content='generate human-like language. In recent years, LLMs have'),
 Document(page_content='made significant advances in a variety of natural language'),
 Document(page_content='processing tasks, including language translation, text'),
 Document(page_content='generation, and sentiment analysis.'),
 Document(page_content='\\subsection{History of LLMs}'),
 Document(page_content='The earliest LLMs were developed in the 1980s and 1990s,'),
 Document(page_content='but they were limited by the amount of data that could be'),
 Document(page_content='processed and the computational power available at the'),
 Document(page_content='time. In the past decade, however, advances in hardware and'),
 Document(page_content='software have made it possible to train LLMs on massive'),
 Document(page_content='datasets, leading to significant improvements in'),
 Document(page_content='performance.'),
 Document(page_content='\\subsection{Applications of LLMs}'),
 Document(page_content='LLMs have many applications in industry, including'),
 Document(page_content='chatbots, content creation, and virtual assistants. They'),
 Document(page_content='can also be used in academia for research in linguistics,'),
 Document(page_content='psychology, and computational linguistics.'),
 Document(page_content='\\end{document}')]
```

## HTML[â€‹](#html "Direct link to HTML")

Here's an example using an HTML text splitter:

```python
html_text = """
<!DOCTYPE html>
<html>
    <head>
        <title>ðŸ¦œï¸ðŸ”— LangChain</title>
        <style>
            body {
                font-family: Arial, sans-serif;
            }
            h1 {
                color: darkblue;
            }
        </style>
    </head>
    <body>
        <div>
            <h1>ðŸ¦œï¸ðŸ”— LangChain</h1>
            <p>âš¡ Building applications with LLMs through composability âš¡</p>
        </div>
        <div>
            As an open-source project in a rapidly developing field, we are extremely open to contributions.
        </div>
    </body>
</html>
"""
```

```python
html_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.HTML, chunk_size=60, chunk_overlap=0
)
html_docs = html_splitter.create_documents([html_text])
html_docs
```

```output
[Document(page_content='<!DOCTYPE html>\n<html>'),
 Document(page_content='<head>\n        <title>ðŸ¦œï¸ðŸ”— LangChain</title>'),
 Document(page_content='<style>\n            body {\n                font-family: Aria'),
 Document(page_content='l, sans-serif;\n            }\n            h1 {'),
 Document(page_content='color: darkblue;\n            }\n        </style>\n    </head'),
 Document(page_content='>'),
 Document(page_content='<body>'),
 Document(page_content='<div>\n            <h1>ðŸ¦œï¸ðŸ”— LangChain</h1>'),
 Document(page_content='<p>âš¡ Building applications with LLMs through composability âš¡'),
 Document(page_content='</p>\n        </div>'),
 Document(page_content='<div>\n            As an open-source project in a rapidly dev'),
 Document(page_content='eloping field, we are extremely open to contributions.'),
 Document(page_content='</div>\n    </body>\n</html>')]
```

## Solidity[â€‹](#solidity "Direct link to Solidity")

Here's an example using the Solidity text splitter:

```python
SOL_CODE = """
pragma solidity ^0.8.20;
contract HelloWorld {
   function add(uint a, uint b) pure public returns(uint) {
       return a + b;
   }
}
"""

sol_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.SOL, chunk_size=128, chunk_overlap=0
)
sol_docs = sol_splitter.create_documents([SOL_CODE])
sol_docs
```

```output
[Document(page_content='pragma solidity ^0.8.20;'),
 Document(page_content='contract HelloWorld {\n   function add(uint a, uint b) pure public returns(uint) {\n       return a + b;\n   }\n}')]
```

## C#[â€‹](#c "Direct link to C#")

Here's an example using the C# text splitter:

```python
C_CODE = """
using System;
class Program
{
    static void Main()
    {
        int age = 30; // Change the age value as needed

        // Categorize the age without any console output
        if (age < 18)
        {
            // Age is under 18
        }
        else if (age >= 18 && age < 65)
        {
            // Age is an adult
        }
        else
        {
            // Age is a senior citizen
        }
    }
}
"""
c_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.CSHARP, chunk_size=128, chunk_overlap=0
)
c_docs = c_splitter.create_documents([C_CODE])
c_docs
```

```output
[Document(page_content='using System;'),
 Document(page_content='class Program\n{\n    static void Main()\n    {\n        int age = 30; // Change the age value as needed'),
 Document(page_content='// Categorize the age without any console output\n        if (age < 18)\n        {\n            // Age is under 18'),
 Document(page_content='}\n        else if (age >= 18 && age < 65)\n        {\n            // Age is an adult\n        }\n        else\n        {'),
 Document(page_content='// Age is a senior citizen\n        }\n    }\n}')]
```

## Haskell[â€‹](#haskell "Direct link to Haskell")

Here's an example using the Haskell text splitter:

```python
HASKELL_CODE = """
main :: IO ()
main = do
    putStrLn "Hello, World!"
-- Some sample functions
add :: Int -> Int -> Int
add x y = x + y
"""
haskell_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.HASKELL, chunk_size=50, chunk_overlap=0
)
haskell_docs = haskell_splitter.create_documents([HASKELL_CODE])
haskell_docs
```

```output
[Document(page_content='main :: IO ()'),
 Document(page_content='main = do\n    putStrLn "Hello, World!"\n-- Some'),
 Document(page_content='sample functions\nadd :: Int -> Int -> Int\nadd x y'),
 Document(page_content='= x + y')]
```

## PHP[â€‹](#php "Direct link to PHP")

Here's an example using the PHP text splitter:

```python
PHP_CODE = """<?php
namespace foo;
class Hello {
    public function __construct() { }
}
function hello() {
    echo "Hello World!";
}
interface Human {
    public function breath();
}
trait Foo { }
enum Color
{
    case Red;
    case Blue;
}"""
php_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PHP, chunk_size=50, chunk_overlap=0
)
php_docs = php_splitter.create_documents([PHP_CODE])
php_docs
```

```output
[Document(page_content='<?php\nnamespace foo;'),
 Document(page_content='class Hello {'),
 Document(page_content='public function __construct() { }\n}'),
 Document(page_content='function hello() {\n    echo "Hello World!";\n}'),
 Document(page_content='interface Human {\n    public function breath();\n}'),
 Document(page_content='trait Foo { }\nenum Color\n{\n    case Red;'),
 Document(page_content='case Blue;\n}')]
```

## PowerShell[â€‹](#powershell "Direct link to PowerShell")

Here's an example using the PowerShell text splitter:

```python
POWERSHELL_CODE = """
$directoryPath = Get-Location

$items = Get-ChildItem -Path $directoryPath

$files = $items | Where-Object { -not $_.PSIsContainer }

$sortedFiles = $files | Sort-Object LastWriteTime

foreach ($file in $sortedFiles) {
    Write-Output ("Name: " + $file.Name + " | Last Write Time: " + $file.LastWriteTime)
}
"""
powershell_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.POWERSHELL, chunk_size=100, chunk_overlap=0
)
powershell_docs = powershell_splitter.create_documents([POWERSHELL_CODE])
powershell_docs
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/code_splitter.ipynb)

* * *


- [Python](#python)
- [JS](#js)
- [TS](#ts)
- [Markdown](#markdown)
- [Latex](#latex)
- [HTML](#html)
- [Solidity](#solidity)
- [C#](#c)
- [Haskell](#haskell)
- [PHP](#php)
- [PowerShell](#powershell)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/output_parser_json.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/output_parser_json.ipynb)

# How to parse JSON output

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models/)
- [Output parsers](/docs/concepts/output_parsers/)
- [Prompt templates](/docs/concepts/prompt_templates/)
- [Structured output](/docs/how_to/structured_output/)
- [Chaining runnables together](/docs/how_to/sequence/)

While some model providers support [built-in ways to return structured output](/docs/how_to/structured_output/), not all do. We can use an output parser to help users to specify an arbitrary JSON schema via the prompt, query a model for outputs that conform to that schema, and finally parse that schema as JSON.

note

Keep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed JSON.

The [`JsonOutputParser`](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html) is one built-in option for prompting for and then parsing JSON output. While it is similar in functionality to the [`PydanticOutputParser`](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html), it also supports streaming back partial JSON objects.

Here's an example of how it can be used alongside [Pydantic](https://docs.pydantic.dev/) to conveniently declare the expected schema:

```python
%pip install -qU langchain langchain-openai

import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

```python
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

model = ChatOpenAI(temperature=0)


# Define your desired data structure.
class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")


# And a query intented to prompt a language model to populate the data structure.
joke_query = "Tell me a joke."

# Set up a parser + inject instructions into the prompt template.
parser = JsonOutputParser(pydantic_object=Joke)

prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

chain = prompt | model | parser

chain.invoke({"query": joke_query})
```

**API Reference:**[JsonOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
{'setup': "Why couldn't the bicycle stand up by itself?",
 'punchline': 'Because it was two tired!'}
```

Note that we are passing `format_instructions` from the parser directly into the prompt. You can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions:

```python
parser.get_format_instructions()
```

```output
'The output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}\nthe object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.\n\nHere is the output schema:\n\`\`\`\n{"properties": {"setup": {"title": "Setup", "description": "question to set up a joke", "type": "string"}, "punchline": {"title": "Punchline", "description": "answer to resolve the joke", "type": "string"}}, "required": ["setup", "punchline"]}\n\`\`\`'
```

## Streaming[â€‹](#streaming "Direct link to Streaming")

As mentioned above, a key difference between the `JsonOutputParser` and the `PydanticOutputParser` is that the `JsonOutputParser` output parser supports streaming partial chunks. Here's what that looks like:

```python
for s in chain.stream({"query": joke_query}):
    print(s)
```

```output
{}
{'setup': ''}
{'setup': 'Why'}
{'setup': 'Why couldn'}
{'setup': "Why couldn't"}
{'setup': "Why couldn't the"}
{'setup': "Why couldn't the bicycle"}
{'setup': "Why couldn't the bicycle stand"}
{'setup': "Why couldn't the bicycle stand up"}
{'setup': "Why couldn't the bicycle stand up by"}
{'setup': "Why couldn't the bicycle stand up by itself"}
{'setup': "Why couldn't the bicycle stand up by itself?"}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': ''}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'Because'}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'Because it'}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'Because it was'}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'Because it was two'}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'Because it was two tired'}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'Because it was two tired!'}
```

## Without Pydantic[â€‹](#without-pydantic "Direct link to Without Pydantic")

You can also use the `JsonOutputParser` without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.

```python
joke_query = "Tell me a joke."

parser = JsonOutputParser()

prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

chain = prompt | model | parser

chain.invoke({"query": joke_query})
```

```output
{'response': "Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!"}
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned one way to prompt a model to return structured JSON. Next, check out the [broader guide on obtaining structured output](/docs/how_to/structured_output/) for other techniques.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/output_parser_json.ipynb)

* * *


- [Streaming](#streaming)
- [Without Pydantic](#without-pydantic)
- [Next steps](#next-steps)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/lcel.mdx)

# LangChain Expression Language (LCEL)

Prerequisites

- [Runnable Interface](/docs/concepts/runnables/)

The **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables/) from existing Runnables.

This means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.

We often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables/).

note

- The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.
- Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.
- A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using LCEL.

## Benefits of LCEL[â€‹](#benefits-of-lcel "Direct link to Benefits of LCEL")

LangChain optimizes the run-time execution of chains built with LCEL in a number of ways:

- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.
- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.
- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models/) or [llm](/docs/concepts/text_llms/) comes out).

Other benefits include:

- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com) As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.
- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.
- [**Deployable with LangServe**](/docs/concepts/architecture/#langserve): Chains built with LCEL can be deployed using for production use.

## Should I use LCEL?[â€‹](#should-i-use-lcel "Direct link to Should I use LCEL?")

LCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_%28computing%29) -- it allows LangChain to handle run-time execution of chains in an optimized way.

While we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture/#langgraph).

In LangGraph, users define graphs that specify the application's flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.

Here are some guidelines:

- If you are making a single LLM call, you don't need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models/) directly.
- If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you're taking advantage of the LCEL benefits.
- If you're building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use [LangGraph](/docs/concepts/architecture/#langgraph) instead. Remember that you can always use LCEL within individual nodes in LangGraph.

## Composition Primitives[â€‹](#composition-primitives "Direct link to Composition Primitives")

`LCEL` chains are built by composing existing `Runnables` together. The two main composition primitives are [RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html#langchain_core.runnables.base.RunnableSequence) and [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html#langchain_core.runnables.base.RunnableParallel).

Many other composition primitives (e.g., [RunnableAssign](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnableAssign.html#langchain_core.runnables.passthrough.RunnableAssign)) can be thought of as variations of these two primitives.

note

You can find a list of all composition primitives in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html).

### RunnableSequence[â€‹](#runnablesequence "Direct link to RunnableSequence")

`RunnableSequence` is a composition primitive that allows you "chain" multiple runnables sequentially, with the output of one runnable serving as the input to the next.

```python
from langchain_core.runnables import RunnableSequence
chain = RunnableSequence([runnable1, runnable2])
```

**API Reference:**[RunnableSequence](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html)

Invoking the `chain` with some input:

```python
final_output = chain.invoke(some_input)
```

corresponds to the following:

```python
output1 = runnable1.invoke(some_input)
final_output = runnable2.invoke(output1)
```

note

`runnable1` and `runnable2` are placeholders for any `Runnable` that you want to chain together.

### RunnableParallel[â€‹](#runnableparallel "Direct link to RunnableParallel")

`RunnableParallel` is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.

```python
from langchain_core.runnables import RunnableParallel
chain = RunnableParallel({
    "key1": runnable1,
    "key2": runnable2,
})
```

**API Reference:**[RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html)

Invoking the `chain` with some input:

```python
final_output = chain.invoke(some_input)
```

Will yield a `final_output` dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.

```python
{
    "key1": runnable1.invoke(some_input),
    "key2": runnable2.invoke(some_input),
}
```

Recall, that the runnables are executed in parallel, so while the result is the same as dictionary comprehension shown above, the execution time is much faster.

note

`RunnableParallel`supports both synchronous and asynchronous execution (as all `Runnables` do).

- For synchronous execution, `RunnableParallel` uses a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor) to run the runnables concurrently.
- For asynchronous execution, `RunnableParallel` uses [asyncio.gather](https://docs.python.org/3/library/asyncio.html#asyncio.gather) to run the runnables concurrently.

## Composition Syntax[â€‹](#composition-syntax "Direct link to Composition Syntax")

The usage of `RunnableSequence` and `RunnableParallel` is so common that we created a shorthand syntax for using them. This helps to make the code more readable and concise.

### The `|` operator[â€‹](#the--operator "Direct link to the--operator")

We have [overloaded](https://docs.python.org/3/reference/datamodel.html#special-method-names) the `|` operator to create a `RunnableSequence` from two `Runnables`.

```python
chain = runnable1 | runnable2
```

is Equivalent to:

```python
chain = RunnableSequence([runnable1, runnable2])
```

### The `.pipe` method[â€‹](#the-pipe-method "Direct link to the-pipe-method")

If you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.

```python
chain = runnable1.pipe(runnable2)
```

### Coercion[â€‹](#coercion "Direct link to Coercion")

LCEL applies automatic type coercion to make it easier to compose chains.

If you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.

This will make the code more verbose, but it will also make it more explicit.

#### Dictionary to RunnableParallel[â€‹](#dictionary-to-runnableparallel "Direct link to Dictionary to RunnableParallel")

Inside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.

For example, the following code:

```python
mapping = {
    "key1": runnable1,
    "key2": runnable2,
}

chain = mapping | runnable3
```

It gets automatically converted to the following:

```python
chain = RunnableSequence([RunnableParallel(mapping), runnable3])
```

caution

You have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:

```python
mapping.invoke(some_input)
```

#### Function to RunnableLambda[â€‹](#function-to-runnablelambda "Direct link to Function to RunnableLambda")

Inside an LCEL expression, a function is automatically converted to a `RunnableLambda`.

```text
def some_func(x):
    return x

chain = some_func | runnable1
```

It gets automatically converted to the following:

```python
chain = RunnableSequence([RunnableLambda(some_func), runnable1])
```

caution

You have to be careful because the lambda function is not a `RunnableLambda` object, it is just a function. This means that the following code will raise an `AttributeError`:

```python
lambda x: x + 1.invoke(some_input)
```

## Legacy chains[â€‹](#legacy-chains "Direct link to Legacy chains")

LCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as `LLMChain` and `ConversationalRetrievalChain`. Many of these legacy chains hide important details like prompts, and as a wider variety of viable models emerge, customization has become more and more important.

If you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains/).

For guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/lcel.mdx)

* * *


- [Benefits of LCEL](#benefits-of-lcel)
- [Should I use LCEL?](#should-i-use-lcel)
- [Composition Primitives](#composition-primitives)
  
  - [RunnableSequence](#runnablesequence)
  - [RunnableParallel](#runnableparallel)
- [Composition Syntax](#composition-syntax)
  
  - [The `|` operator](#the--operator)
  - [The `.pipe` method](#the-pipe-method)
  - [Coercion](#coercion)
- [Legacy chains](#legacy-chains)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/split_by_token.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/split_by_token.ipynb)

# How to split text by tokens

Language models have a [token](/docs/concepts/tokens/) limit. You should not exceed the token limit. When you [split your text](/docs/concepts/text_splitters/) into chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model.

## tiktoken[â€‹](#tiktoken "Direct link to tiktoken")

note

[tiktoken](https://github.com/openai/tiktoken) is a fast `BPE` tokenizer created by `OpenAI`.

We can use `tiktoken` to estimate tokens used. It will probably be more accurate for the OpenAI models.

1. How the text is split: by character passed in.
2. How the chunk size is measured: by `tiktoken` tokenizer.

[CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html), [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html), and [TokenTextSplitter](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TokenTextSplitter.html) can be used with `tiktoken` directly.

```python
%pip install --upgrade --quiet langchain-text-splitters tiktoken
```

```python
from langchain_text_splitters import CharacterTextSplitter

# This is a long document we can split up.
with open("state_of_the_union.txt") as f:
    state_of_the_union = f.read()
```

**API Reference:**[CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

To split with a [CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html) and then merge chunks with `tiktoken`, use its `.from_tiktoken_encoder()` method. Note that splits from this method can be larger than the chunk size measured by the `tiktoken` tokenizer.

The `.from_tiktoken_encoder()` method takes either `encoding_name` as an argument (e.g. `cl100k_base`), or the `model_name` (e.g. `gpt-4`). All additional arguments like `chunk_size`, `chunk_overlap`, and `separators` are used to instantiate `CharacterTextSplitter`:

```python
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    encoding_name="cl100k_base", chunk_size=100, chunk_overlap=0
)
texts = text_splitter.split_text(state_of_the_union)
```

```python
print(texts[0])
```

```output
Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  

Last year COVID-19 kept us apart. This year we are finally together again. 

Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. 

With a duty to one another to the American people to the Constitution.
```

To implement a hard constraint on the chunk size, we can use `RecursiveCharacterTextSplitter.from_tiktoken_encoder`, where each split will be recursively split if it has a larger size:

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    model_name="gpt-4",
    chunk_size=100,
    chunk_overlap=0,
)
```

**API Reference:**[RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

We can also load a `TokenTextSplitter` splitter, which works with `tiktoken` directly and will ensure each split is smaller than chunk size.

```python
from langchain_text_splitters import TokenTextSplitter

text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)

texts = text_splitter.split_text(state_of_the_union)
print(texts[0])
```

**API Reference:**[TokenTextSplitter](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TokenTextSplitter.html)

```output
Madam Speaker, Madam Vice President, our
```

Some written languages (e.g. Chinese and Japanese) have characters which encode to 2 or more tokens. Using the `TokenTextSplitter` directly can split the tokens for a character between two chunks causing malformed Unicode characters. Use `RecursiveCharacterTextSplitter.from_tiktoken_encoder` or `CharacterTextSplitter.from_tiktoken_encoder` to ensure chunks contain valid Unicode strings.

## spaCy[â€‹](#spacy "Direct link to spaCy")

note

[spaCy](https://spacy.io/) is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.

LangChain implements splitters based on the [spaCy tokenizer](https://spacy.io/api/tokenizer).

1. How the text is split: by `spaCy` tokenizer.
2. How the chunk size is measured: by number of characters.

```python
%pip install --upgrade --quiet  spacy
```

```python
# This is a long document we can split up.
with open("state_of_the_union.txt") as f:
    state_of_the_union = f.read()
```

```python
from langchain_text_splitters import SpacyTextSplitter

text_splitter = SpacyTextSplitter(chunk_size=1000)

texts = text_splitter.split_text(state_of_the_union)
print(texts[0])
```

**API Reference:**[SpacyTextSplitter](https://python.langchain.com/api_reference/text_splitters/spacy/langchain_text_splitters.spacy.SpacyTextSplitter.html)

```output
Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.

Members of Congress and the Cabinet.

Justices of the Supreme Court.

My fellow Americans.  



Last year COVID-19 kept us apart.

This year we are finally together again. 



Tonight, we meet as Democrats Republicans and Independents.

But most importantly as Americans. 



With a duty to one another to the American people to the Constitution. 



And with an unwavering resolve that freedom will always triumph over tyranny. 



Six days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.

But he badly miscalculated. 



He thought he could roll into Ukraine and the world would roll over.

Instead he met a wall of strength he never imagined. 



He met the Ukrainian people. 



From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.
```

## SentenceTransformers[â€‹](#sentencetransformers "Direct link to SentenceTransformers")

The [SentenceTransformersTokenTextSplitter](https://python.langchain.com/api_reference/text_splitters/sentence_transformers/langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter.html) is a specialized text splitter for use with the sentence-transformer models. The default behaviour is to split the text into chunks that fit the token window of the sentence transformer model that you would like to use.

To split text and constrain token counts according to the sentence-transformers tokenizer, instantiate a `SentenceTransformersTokenTextSplitter`. You can optionally specify:

- `chunk_overlap`: integer count of token overlap;
- `model_name`: sentence-transformer model name, defaulting to `"sentence-transformers/all-mpnet-base-v2"`;
- `tokens_per_chunk`: desired token count per chunk.

```python
from langchain_text_splitters import SentenceTransformersTokenTextSplitter

splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0)
text = "Lorem "

count_start_and_stop_tokens = 2
text_token_count = splitter.count_tokens(text=text) - count_start_and_stop_tokens
print(text_token_count)
```

**API Reference:**[SentenceTransformersTokenTextSplitter](https://python.langchain.com/api_reference/text_splitters/sentence_transformers/langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter.html)

```output
2
```

```python
token_multiplier = splitter.maximum_tokens_per_chunk // text_token_count + 1

# `text_to_split` does not fit in a single chunk
text_to_split = text * token_multiplier

print(f"tokens in text to split: {splitter.count_tokens(text=text_to_split)}")
```

```output
tokens in text to split: 514
```

```python
text_chunks = splitter.split_text(text=text_to_split)

print(text_chunks[1])
```

```output
lorem
```

## NLTK[â€‹](#nltk "Direct link to NLTK")

note

[The Natural Language Toolkit](https://en.wikipedia.org/wiki/Natural_Language_Toolkit), or more commonly [NLTK](https://www.nltk.org/), is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language.

Rather than just splitting on "\\n\\n", we can use `NLTK` to split based on [NLTK tokenizers](https://www.nltk.org/api/nltk.tokenize.html).

1. How the text is split: by `NLTK` tokenizer.
2. How the chunk size is measured: by number of characters.

```python
# pip install nltk
```

```python
# This is a long document we can split up.
with open("state_of_the_union.txt") as f:
    state_of_the_union = f.read()
```

```python
from langchain_text_splitters import NLTKTextSplitter

text_splitter = NLTKTextSplitter(chunk_size=1000)
```

**API Reference:**[NLTKTextSplitter](https://python.langchain.com/api_reference/text_splitters/nltk/langchain_text_splitters.nltk.NLTKTextSplitter.html)

```python
texts = text_splitter.split_text(state_of_the_union)
print(texts[0])
```

```output
Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.

Members of Congress and the Cabinet.

Justices of the Supreme Court.

My fellow Americans.

Last year COVID-19 kept us apart.

This year we are finally together again.

Tonight, we meet as Democrats Republicans and Independents.

But most importantly as Americans.

With a duty to one another to the American people to the Constitution.

And with an unwavering resolve that freedom will always triumph over tyranny.

Six days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.

But he badly miscalculated.

He thought he could roll into Ukraine and the world would roll over.

Instead he met a wall of strength he never imagined.

He met the Ukrainian people.

From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.

Groups of citizens blocking tanks with their bodies.
```

## KoNLPY[â€‹](#konlpy "Direct link to KoNLPY")

note

[KoNLPy: Korean NLP in Python](https://konlpy.org/en/latest/) is is a Python package for natural language processing (NLP) of the Korean language.

Token splitting involves the segmentation of text into smaller, more manageable units called tokens. These tokens are often words, phrases, symbols, or other meaningful elements crucial for further processing and analysis. In languages like English, token splitting typically involves separating words by spaces and punctuation marks. The effectiveness of token splitting largely depends on the tokenizer's understanding of the language structure, ensuring the generation of meaningful tokens. Since tokenizers designed for the English language are not equipped to understand the unique semantic structures of other languages, such as Korean, they cannot be effectively used for Korean language processing.

### Token splitting for Korean with KoNLPy's Kkma Analyzer[â€‹](#token-splitting-for-korean-with-konlpys-kkma-analyzer "Direct link to Token splitting for Korean with KoNLPy's Kkma Analyzer")

In case of Korean text, KoNLPY includes at morphological analyzer called `Kkma` (Korean Knowledge Morpheme Analyzer). `Kkma` provides detailed morphological analysis of Korean text. It breaks down sentences into words and words into their respective morphemes, identifying parts of speech for each token. It can segment a block of text into individual sentences, which is particularly useful for processing long texts.

### Usage Considerations[â€‹](#usage-considerations "Direct link to Usage Considerations")

While `Kkma` is renowned for its detailed analysis, it is important to note that this precision may impact processing speed. Thus, `Kkma` is best suited for applications where analytical depth is prioritized over rapid text processing.

```python
# pip install konlpy
```

```python
# This is a long Korean document that we want to split up into its component sentences.
with open("./your_korean_doc.txt") as f:
    korean_document = f.read()
```

```python
from langchain_text_splitters import KonlpyTextSplitter

text_splitter = KonlpyTextSplitter()
```

**API Reference:**[KonlpyTextSplitter](https://python.langchain.com/api_reference/text_splitters/konlpy/langchain_text_splitters.konlpy.KonlpyTextSplitter.html)

```python
texts = text_splitter.split_text(korean_document)
# The sentences are split with "\n\n" characters.
print(texts[0])
```

```output
ì¶˜í–¥ì „ ì˜›ë‚ ì— ë‚¨ì›ì— ì´ ë„ë ¹ì´ë¼ëŠ” ë²¼ìŠ¬ì•„ì¹˜ ì•„ë“¤ì´ ìžˆì—ˆë‹¤.

ê·¸ì˜ ì™¸ëª¨ëŠ” ë¹›ë‚˜ëŠ” ë‹¬ì²˜ëŸ¼ ìž˜ìƒê²¼ê³ , ê·¸ì˜ í•™ì‹ê³¼ ê¸°ì˜ˆëŠ” ë‚¨ë³´ë‹¤ ë›°ì–´ë‚¬ë‹¤.

í•œíŽ¸, ì´ ë§ˆì„ì—ëŠ” ì¶˜í–¥ì´ë¼ëŠ” ì ˆì„¸ ê°€ì¸ì´ ì‚´ê³  ìžˆì—ˆë‹¤.

ì¶˜ í–¥ì˜ ì•„ë¦„ë‹¤ì›€ì€ ê½ƒê³¼ ê°™ì•„ ë§ˆì„ ì‚¬ëžŒë“¤ ë¡œë¶€í„° ë§Žì€ ì‚¬ëž‘ì„ ë°›ì•˜ë‹¤.

ì–´ëŠ ë´„ë‚ , ë„ë ¹ì€ ì¹œêµ¬ë“¤ê³¼ ë†€ëŸ¬ ë‚˜ê°”ë‹¤ê°€ ì¶˜ í–¥ì„ ë§Œ ë‚˜ ì²« ëˆˆì— ë°˜í•˜ê³  ë§ì•˜ë‹¤.

ë‘ ì‚¬ëžŒì€ ì„œë¡œ ì‚¬ëž‘í•˜ê²Œ ë˜ì—ˆê³ , ì´ë‚´ ë¹„ë°€ìŠ¤ëŸ¬ìš´ ì‚¬ëž‘ì˜ ë§¹ì„¸ë¥¼ ë‚˜ëˆ„ì—ˆë‹¤.

í•˜ì§€ë§Œ ì¢‹ì€ ë‚ ë“¤ì€ ì˜¤ëž˜ê°€ì§€ ì•Šì•˜ë‹¤.

ë„ë ¹ì˜ ì•„ë²„ì§€ê°€ ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ì „ê·¼ì„ ê°€ê²Œ ë˜ì–´ ë„ë ¹ë„ ë– ë‚˜ ì•¼ë§Œ í–ˆë‹¤.

ì´ë³„ì˜ ì•„í”” ì†ì—ì„œë„, ë‘ ì‚¬ëžŒì€ ìž¬íšŒë¥¼ ê¸°ì•½í•˜ë©° ì„œë¡œë¥¼ ë¯¿ê³  ê¸°ë‹¤ë¦¬ê¸°ë¡œ í–ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ìƒˆë¡œ ë¶€ìž„í•œ ê´€ì•„ì˜ ì‚¬ë˜ê°€ ì¶˜ í–¥ì˜ ì•„ë¦„ë‹¤ì›€ì— ìš•ì‹¬ì„ ë‚´ ì–´ ê·¸ë…€ì—ê²Œ ê°•ìš”ë¥¼ ì‹œìž‘í–ˆë‹¤.

ì¶˜ í–¥ ì€ ë„ë ¹ì— ëŒ€í•œ ìžì‹ ì˜ ì‚¬ëž‘ì„ ì§€í‚¤ê¸° ìœ„í•´, ì‚¬ë˜ì˜ ìš”êµ¬ë¥¼ ë‹¨í˜¸ížˆ ê±°ì ˆí–ˆë‹¤.

ì´ì— ë¶„ë…¸í•œ ì‚¬ë˜ëŠ” ì¶˜ í–¥ì„ ê°ì˜¥ì— ê°€ë‘ê³  í˜¹ë…í•œ í˜•ë²Œì„ ë‚´ë ¸ë‹¤.

ì´ì•¼ê¸°ëŠ” ì´ ë„ë ¹ì´ ê³ ìœ„ ê´€ì§ì— ì˜¤ë¥¸ í›„, ì¶˜ í–¥ì„ êµ¬í•´ ë‚´ëŠ” ê²ƒìœ¼ë¡œ ëë‚œë‹¤.

ë‘ ì‚¬ëžŒì€ ì˜¤ëžœ ì‹œë ¨ ëì— ë‹¤ì‹œ ë§Œë‚˜ê²Œ ë˜ê³ , ê·¸ë“¤ì˜ ì‚¬ëž‘ì€ ì˜¨ ì„¸ìƒì— ì „í•´ ì§€ë©° í›„ì„¸ì—ê¹Œì§€ ì´ì–´ì§„ë‹¤.

- ì¶˜í–¥ì „ (The Tale of Chunhyang)
```

## Hugging Face tokenizer[â€‹](#hugging-face-tokenizer "Direct link to Hugging Face tokenizer")

[Hugging Face](https://huggingface.co/docs/tokenizers/index) has many tokenizers.

We use Hugging Face tokenizer, the [GPT2TokenizerFast](https://huggingface.co/Ransaka/gpt2-tokenizer-fast) to count the text length in tokens.

1. How the text is split: by character passed in.
2. How the chunk size is measured: by number of tokens calculated by the `Hugging Face` tokenizer.

```python
from transformers import GPT2TokenizerFast

tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
```

```python
# This is a long document we can split up.
with open("state_of_the_union.txt") as f:
    state_of_the_union = f.read()
from langchain_text_splitters import CharacterTextSplitter
```

**API Reference:**[CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

```python
text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(
    tokenizer, chunk_size=100, chunk_overlap=0
)
texts = text_splitter.split_text(state_of_the_union)
```

```python
print(texts[0])
```

```output
Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  

Last year COVID-19 kept us apart. This year we are finally together again. 

Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. 

With a duty to one another to the American people to the Constitution.
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/split_by_token.ipynb)

* * *


- [tiktoken](#tiktoken)
- [spaCy](#spacy)
- [SentenceTransformers](#sentencetransformers)
- [NLTK](#nltk)
- [KoNLPY](#konlpy)
  
  - [Token splitting for Korean with KoNLPy's Kkma Analyzer](#token-splitting-for-korean-with-konlpys-kkma-analyzer)
  - [Usage Considerations](#usage-considerations)
- [Hugging Face tokenizer](#hugging-face-tokenizer)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/tutorials/agents.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/agents.ipynb)

# Build an Agent

By themselves, language models can't take actions - they just output text. A big use case for LangChain is creating **agents**. [Agents](/docs/concepts/agents/) are systems that use [LLMs](/docs/concepts/chat_models/) as reasoning engines to determine which actions to take and the inputs necessary to perform the action. After executing actions, the results can be fed back into the LLM to determine whether more actions are needed, or whether it is okay to finish. This is often achieved via [tool-calling](/docs/concepts/tool_calling/).

In this tutorial we will build an agent that can interact with a search engine. You will be able to ask this agent questions, watch it call the search tool, and have conversations with it.

## End-to-end agent[â€‹](#end-to-end-agent "Direct link to End-to-end agent")

The code snippet below represents a fully functional agent that uses an LLM to decide which tools to use. It is equipped with a generic search tool. It has conversational memory - meaning that it can be used as a multi-turn chatbot.

In the rest of the guide, we will walk through the individual components and what each part does - but if you want to just grab some code and get started, feel free to use this!

```python
# Import relevant functionality
from langchain_anthropic import ChatAnthropic
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

# Create the agent
memory = MemorySaver()
model = ChatAnthropic(model_name="claude-3-sonnet-20240229")
search = TavilySearchResults(max_results=2)
tools = [search]
agent_executor = create_react_agent(model, tools, checkpointer=memory)
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [TavilySearchResults](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```python
# Use the agent
config = {"configurable": {"thread_id": "abc123"}}
for step in agent_executor.stream(
    {"messages": [HumanMessage(content="hi im bob! and i live in sf")]},
    config,
    stream_mode="values",
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

hi im bob! and i live in sf
==================================[1m Ai Message [0m==================================

Hello Bob! Since you didn't ask a specific question, I don't need to use any tools right now. I'm an AI assistant created by Anthropic to be helpful, honest, and harmless. Feel free to ask me anything and I'll do my best to provide a useful response or look up information using my capabilities.
```

```python
for step in agent_executor.stream(
    {"messages": [HumanMessage(content="whats the weather where I live?")]},
    config,
    stream_mode="values",
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

whats the weather where I live?
==================================[1m Ai Message [0m==================================

[{'text': 'To get the current weather for your location in San Francisco, I can use the tavily_search_results_json tool:', 'type': 'text'}, {'id': 'toolu_01AKa2MErG1CU3zRiGsvpBud', 'input': {'query': 'san francisco weather'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls:
  tavily_search_results_json (toolu_01AKa2MErG1CU3zRiGsvpBud)
 Call ID: toolu_01AKa2MErG1CU3zRiGsvpBud
  Args:
    query: san francisco weather
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json

[{"url": "https://www.weatherapi.com/", "content": "{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1739994486, 'localtime': '2025-02-19 11:48'}, 'current': {'last_updated_epoch': 1739994300, 'last_updated': '2025-02-19 11:45', 'temp_c': 13.3, 'temp_f': 55.9, 'is_day': 1, 'condition': {'text': 'Light rain', 'icon': '//cdn.weatherapi.com/weather/64x64/day/296.png', 'code': 1183}, 'wind_mph': 5.8, 'wind_kph': 9.4, 'wind_degree': 195, 'wind_dir': 'SSW', 'pressure_mb': 1023.0, 'pressure_in': 30.2, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 87, 'cloud': 100, 'feelslike_c': 12.7, 'feelslike_f': 54.8, 'windchill_c': 9.1, 'windchill_f': 48.4, 'heatindex_c': 10.2, 'heatindex_f': 50.3, 'dewpoint_c': 9.8, 'dewpoint_f': 49.7, 'vis_km': 4.0, 'vis_miles': 2.0, 'uv': 1.4, 'gust_mph': 8.9, 'gust_kph': 14.4}}"}, {"url": "https://world-weather.info/forecast/usa/san_francisco/february-2025/", "content": "Weather in San Francisco in February 2025 (California) - Detailed Weather Forecast for a Month Weather World Weather in San Francisco Weather in San Francisco in February 2025 San Francisco Weather Forecast for February 2025, is based on previous years' statistical data. +59Â°+50Â° +59Â°+52Â° +59Â°+50Â° +61Â°+52Â° +59Â°+50Â° +61Â°+50Â° +61Â°+52Â° +63Â°+52Â° +61Â°+52Â° +61Â°+50Â° +61Â°+50Â° +61Â°+50Â° +59Â°+50Â° +59Â°+50Â° +61Â°+50Â° +61Â°+52Â° +59Â°+50Â° +59Â°+48Â° +57Â°+48Â° +59Â°+50Â° +59Â°+48Â° +59Â°+50Â° +57Â°+46Â° +61Â°+50Â° +61Â°+50Â° +59Â°+50Â° +59Â°+48Â° +59Â°+50Â° Extended weather forecast in San Francisco HourlyWeek10-Day14-Day30-DayYear Weather in large and nearby cities Weather in Washington, D.C.+41Â° Sacramento+55Â° Pleasanton+55Â° Redwood City+55Â° San Leandro+55Â° San Mateo+54Â° San Rafael+52Â° San Ramon+52Â° South San Francisco+54Â° Vallejo+50Â° Palo Alto+55Â° Pacifica+55Â° Berkeley+54Â° Castro Valley+55Â° Concord+52Â° Daly City+54Â° Noverd+52Â° Sign Hill+54Â° world's temperature today day day Temperature units"}]
==================================[1m Ai Message [0m==================================

The search results provide the current weather conditions and forecast for San Francisco. According to the data from WeatherAPI, the current temperature in San Francisco is around 55Â°F (13Â°C) with light rain and winds around 6 mph. The extended forecast shows temperatures ranging from the upper 40s to low 60s Fahrenheit over the next few weeks.

So in summary, it's a cool, rainy day currently in San Francisco where you live, Bob. Let me know if you need any other details about the weather there!
```

## Setup[â€‹](#setup "Direct link to Setup")

### Jupyter Notebook[â€‹](#jupyter-notebook "Direct link to Jupyter Notebook")

This guide (and most of the other guides in the documentation) uses [Jupyter notebooks](https://jupyter.org/) and assumes the reader is as well. Jupyter notebooks are perfect interactive environments for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc), and observing these cases is a great way to better understand building with LLMs.

This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See [here](https://jupyter.org/install) for instructions on how to install.

### Installation[â€‹](#installation "Direct link to Installation")

To install LangChain run:

```python
%pip install -U langchain-community langgraph langchain-anthropic tavily-python langgraph-checkpoint-sqlite
```

For more details, see our [Installation guide](/docs/how_to/installation/).

### LangSmith[â€‹](#langsmith "Direct link to LangSmith")

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
```

Or, if in a notebook, you can set them with:

```python
import getpass
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

### Tavily[â€‹](#tavily "Direct link to Tavily")

We will be using [Tavily](/docs/integrations/tools/tavily_search/) (a search engine) as a tool. In order to use it, you will need to get and set an API key:

```bash
export TAVILY_API_KEY="..."
```

Or, if in a notebook, you can set it with:

```python
import getpass
import os

os.environ["TAVILY_API_KEY"] = getpass.getpass()
```

## Define tools[â€‹](#define-tools "Direct link to Define tools")

We first need to create the tools we want to use. Our main tool of choice will be [Tavily](/docs/integrations/tools/tavily_search/) - a search engine. We have a built-in tool in LangChain to easily use Tavily search engine as tool.

```python
from langchain_community.tools.tavily_search import TavilySearchResults

search = TavilySearchResults(max_results=2)
search_results = search.invoke("what is the weather in SF")
print(search_results)
# If we want, we can create other tools.
# Once we have all the tools we want, we can put them in a list that we will reference later.
tools = [search]
```

**API Reference:**[TavilySearchResults](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html)

```output
[{'url': 'https://www.weatherapi.com/', 'content': "{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1739993250, 'localtime': '2025-02-19 11:27'}, 'current': {'last_updated_epoch': 1739992500, 'last_updated': '2025-02-19 11:15', 'temp_c': 13.3, 'temp_f': 55.9, 'is_day': 1, 'condition': {'text': 'Light rain', 'icon': '//cdn.weatherapi.com/weather/64x64/day/296.png', 'code': 1183}, 'wind_mph': 5.8, 'wind_kph': 9.4, 'wind_degree': 195, 'wind_dir': 'SSW', 'pressure_mb': 1023.0, 'pressure_in': 30.2, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 87, 'cloud': 100, 'feelslike_c': 12.7, 'feelslike_f': 54.8, 'windchill_c': 9.1, 'windchill_f': 48.4, 'heatindex_c': 10.2, 'heatindex_f': 50.3, 'dewpoint_c': 9.8, 'dewpoint_f': 49.7, 'vis_km': 4.0, 'vis_miles': 2.0, 'uv': 1.4, 'gust_mph': 8.9, 'gust_kph': 14.4}}"}, {'url': 'https://weathershogun.com/weather/usa/ca/san-francisco/480/february/2025-02-19', 'content': 'San Francisco, California Weather: Wednesday, February 19, 2025. Cloudy weather, overcast skies with clouds. Day 61Â°. Night 43Â°.'}]
```

## Using Language Models[â€‹](#using-language-models "Direct link to Using Language Models")

Next, let's learn how to use a language model to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4", model_provider="openai")
```

You can call the language model by passing in a list of messages. By default, the response is a `content` string.

```python
from langchain_core.messages import HumanMessage

response = model.invoke([HumanMessage(content="hi!")])
response.content
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

```output
'Hi there!'
```

We can now see what it is like to enable this model to do tool calling. In order to enable that we use `.bind_tools` to give the language model knowledge of these tools

```python
model_with_tools = model.bind_tools(tools)
```

We can now call the model. Let's first call it with a normal message, and see how it responds. We can look at both the `content` field as well as the `tool_calls` field.

```python
response = model_with_tools.invoke([HumanMessage(content="Hi!")])

print(f"ContentString: {response.content}")
print(f"ToolCalls: {response.tool_calls}")
```

```output
ContentString: Hello!
ToolCalls: []
```

Now, let's try calling it with some input that would expect a tool to be called.

```python
response = model_with_tools.invoke([HumanMessage(content="What's the weather in SF?")])

print(f"ContentString: {response.content}")
print(f"ToolCalls: {response.tool_calls}")
```

```output
ContentString: 
ToolCalls: [{'name': 'tavily_search_results_json', 'args': {'query': 'weather san francisco'}, 'id': 'toolu_01VTP7DUvSfgtYxsq9x4EwMp'}]
```

We can see that there's now no text content, but there is a tool call! It wants us to call the Tavily Search tool.

This isn't calling that tool yet - it's just telling us to. In order to actually call it, we'll want to create our agent.

## Create the agent[â€‹](#create-the-agent "Direct link to Create the agent")

Now that we have defined the tools and the LLM, we can create the agent. We will be using [LangGraph](/docs/concepts/architecture/#langgraph) to construct the agent. Currently, we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.

Now, we can initialize the agent with the LLM and the tools.

Note that we are passing in the `model`, not `model_with_tools`. That is because `create_react_agent` will call `.bind_tools` for us under the hood.

```python
from langgraph.prebuilt import create_react_agent

agent_executor = create_react_agent(model, tools)
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

## Run the agent[â€‹](#run-the-agent "Direct link to Run the agent")

We can now run the agent with a few queries! Note that for now, these are all **stateless** queries (it won't remember previous interactions). Note that the agent will return the **final** state at the end of the interaction (which includes any inputs, we will see later on how to get only the outputs).

First up, let's see how it responds when there's no need to call a tool:

```python
response = agent_executor.invoke({"messages": [HumanMessage(content="hi!")]})

response["messages"]
```

```output
[HumanMessage(content='hi!', id='a820fcc5-9b87-457a-9af0-f21768143ee3'),
 AIMessage(content='Hello!', response_metadata={'id': 'msg_01VbC493X1VEDyusgttiEr1z', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 264, 'output_tokens': 5}}, id='run-0e0ddae8-a85b-4bd6-947c-c36c857a4698-0', usage_metadata={'input_tokens': 264, 'output_tokens': 5, 'total_tokens': 269})]
```

In order to see exactly what is happening under the hood (and to make sure it's not calling a tool) we can take a look at the [LangSmith trace](https://smith.langchain.com/public/28311faa-e135-4d6a-ab6b-caecf6482aaa/r)

Let's now try it out on an example where it should be invoking the tool

```python
response = agent_executor.invoke(
    {"messages": [HumanMessage(content="whats the weather in sf?")]}
)
response["messages"]
```

```output
[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),
 AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),
 ToolMessage(content='[{"url": "https://www.weatherapi.com/", "content": "{\'location\': {\'name\': \'San Francisco\', \'region\': \'California\', \'country\': \'United States of America\', \'lat\': 37.78, \'lon\': -122.42, \'tz_id\': \'America/Los_Angeles\', \'localtime_epoch\': 1717238703, \'localtime\': \'2024-06-01 3:45\'}, \'current\': {\'last_updated_epoch\': 1717237800, \'last_updated\': \'2024-06-01 03:30\', \'temp_c\': 12.0, \'temp_f\': 53.6, \'is_day\': 0, \'condition\': {\'text\': \'Mist\', \'icon\': \'//cdn.weatherapi.com/weather/64x64/night/143.png\', \'code\': 1030}, \'wind_mph\': 5.6, \'wind_kph\': 9.0, \'wind_degree\': 310, \'wind_dir\': \'NW\', \'pressure_mb\': 1013.0, \'pressure_in\': 29.92, \'precip_mm\': 0.0, \'precip_in\': 0.0, \'humidity\': 88, \'cloud\': 100, \'feelslike_c\': 10.5, \'feelslike_f\': 50.8, \'windchill_c\': 9.3, \'windchill_f\': 48.7, \'heatindex_c\': 11.1, \'heatindex_f\': 51.9, \'dewpoint_c\': 8.8, \'dewpoint_f\': 47.8, \'vis_km\': 6.4, \'vis_miles\': 3.0, \'uv\': 1.0, \'gust_mph\': 12.5, \'gust_kph\': 20.1}}"}, {"url": "https://www.timeanddate.com/weather/usa/san-francisco/hourly", "content": "Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather."}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),
 AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\n\nThe weather in San Francisco is currently misty with a temperature of around 53Â°F (12Â°C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \n\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]
```

We can check out the [LangSmith trace](https://smith.langchain.com/public/f520839d-cd4d-4495-8764-e32b548e235d/r) to make sure it's calling the search tool effectively.

## Streaming Messages[â€‹](#streaming-messages "Direct link to Streaming Messages")

We've seen how the agent can be called with `.invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.

```python
for step in agent_executor.stream(
    {"messages": [HumanMessage(content="whats the weather in sf?")]},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

whats the weather in sf?
==================================[1m Ai Message [0m==================================

[{'text': 'Okay, let me look up the current weather for San Francisco using a search engine:', 'type': 'text'}, {'id': 'toolu_01H1brh5EZpZqtqHBxkosPtN', 'input': {'query': 'san francisco weather'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls:
  tavily_search_results_json (toolu_01H1brh5EZpZqtqHBxkosPtN)
 Call ID: toolu_01H1brh5EZpZqtqHBxkosPtN
  Args:
    query: san francisco weather
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json

[{"url": "https://www.weatherapi.com/", "content": "{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1739994486, 'localtime': '2025-02-19 11:48'}, 'current': {'last_updated_epoch': 1739994300, 'last_updated': '2025-02-19 11:45', 'temp_c': 13.3, 'temp_f': 55.9, 'is_day': 1, 'condition': {'text': 'Light rain', 'icon': '//cdn.weatherapi.com/weather/64x64/day/296.png', 'code': 1183}, 'wind_mph': 5.8, 'wind_kph': 9.4, 'wind_degree': 195, 'wind_dir': 'SSW', 'pressure_mb': 1023.0, 'pressure_in': 30.2, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 87, 'cloud': 100, 'feelslike_c': 12.7, 'feelslike_f': 54.8, 'windchill_c': 9.1, 'windchill_f': 48.4, 'heatindex_c': 10.2, 'heatindex_f': 50.3, 'dewpoint_c': 9.8, 'dewpoint_f': 49.7, 'vis_km': 4.0, 'vis_miles': 2.0, 'uv': 1.4, 'gust_mph': 8.9, 'gust_kph': 14.4}}"}, {"url": "https://world-weather.info/forecast/usa/san_francisco/february-2025/", "content": "Weather in San Francisco in February 2025 (California) - Detailed Weather Forecast for a Month Weather World Weather in San Francisco Weather in San Francisco in February 2025 San Francisco Weather Forecast for February 2025, is based on previous years' statistical data. +59Â°+50Â° +59Â°+52Â° +59Â°+50Â° +61Â°+52Â° +59Â°+50Â° +61Â°+50Â° +61Â°+52Â° +63Â°+52Â° +61Â°+52Â° +61Â°+50Â° +61Â°+50Â° +61Â°+50Â° +59Â°+50Â° +59Â°+50Â° +61Â°+50Â° +61Â°+52Â° +59Â°+50Â° +59Â°+48Â° +57Â°+48Â° +59Â°+50Â° +59Â°+48Â° +59Â°+50Â° +57Â°+46Â° +61Â°+50Â° +61Â°+50Â° +59Â°+50Â° +59Â°+48Â° +59Â°+50Â° Extended weather forecast in San Francisco HourlyWeek10-Day14-Day30-DayYear Weather in large and nearby cities Weather in Washington, D.C.+41Â° Sacramento+55Â° Pleasanton+55Â° Redwood City+55Â° San Leandro+55Â° San Mateo+54Â° San Rafael+52Â° San Ramon+52Â° South San Francisco+54Â° Vallejo+50Â° Palo Alto+55Â° Pacifica+55Â° Berkeley+54Â° Castro Valley+55Â° Concord+52Â° Daly City+54Â° Noverd+52Â° Sign Hill+54Â° world's temperature today day day Temperature units"}]
==================================[1m Ai Message [0m==================================

The search results provide details on the current weather conditions and forecast for San Francisco. Some key details:

- It is lightly raining in San Francisco right now, with a temperature around 55Â°F/13Â°C. 
- The forecast for the rest of February 2025 shows daytime highs mostly in the upper 50s to low 60s F, with night lows in the upper 40s to low 50s F. 
- Typical weather includes some rain, clouds, cool temperatures and breezy conditions.

So in summary, as is common for San Francisco in late winter, it is currently cool with light rain showers, and similar mild, unsettled weather is expected over the next couple weeks. Layers and a light jacket would be advisable for being outdoors. Let me know if you need any other details!
```

## Streaming tokens[â€‹](#streaming-tokens "Direct link to Streaming tokens")

In addition to streaming back messages, it is also useful to stream back tokens. We can do this by specifying `stream_mode="messages"`.

::: note

Below we use `message.text()`, which requires `langchain-core>=0.3.37`.

:::

```python
for step, metadata in agent_executor.stream(
    {"messages": [HumanMessage(content="whats the weather in sf?")]},
    stream_mode="messages",
):
    if metadata["langgraph_node"] == "agent" and (text := step.text()):
        print(text, end="|")
```

```output


Base|d on the weather| search| results, here| are the key details| about the weather in| San Francisco:|

- The current temperature| in| San Francisco is aroun|d 55|-|56|Â°F (13|Â°|C).| Light| rain is occurring with| |100|% clou|d cover. |

-| Winds| are aroun|d 5-9| mph from| the south|-southwest.|

- The| forecast| for| the rest| of February| 2025 |shows da|ytime highs mostly| in the upper| 50s to| low| 60sÂ°|F,| with overnight lows| in| the upper| 40s to| low| 50sÂ°|F.|

-| Overall|, typical| cool| an|d show|ery late| winter weather is| expected in San Francisco| for the remainder| of February,| with a| mix| of rain| and dry| periods|.| Temperatures will be| season|able| for| this| time of year.|

So| in summary, San| Francisco is| experiencing light| rain an|d cool| temperatures currently, but| the late| winter forecast| shows typical mil|d and show|ery conditions| pers|isting through the en|d of the| month.| Let| me know if you| need any other| details about| the weather in the| city!|
```

## Adding in memory[â€‹](#adding-in-memory "Direct link to Adding in memory")

As mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in a checkpointer. When passing in a checkpointer, we also have to pass in a `thread_id` when invoking the agent (so it knows which thread/conversation to resume from).

```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
```

**API Reference:**[MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver)

```python
agent_executor = create_react_agent(model, tools, checkpointer=memory)

config = {"configurable": {"thread_id": "abc123"}}
```

```python
for chunk in agent_executor.stream(
    {"messages": [HumanMessage(content="hi im bob!")]}, config
):
    print(chunk)
    print("----")
```

```output
{'agent': {'messages': [AIMessage(content="Hello Bob! It's nice to meet you again.", response_metadata={'id': 'msg_013C1z2ZySagEFwmU1EsysR2', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 1162, 'output_tokens': 14}}, id='run-f878acfd-d195-44e8-9166-e2796317e3f8-0', usage_metadata={'input_tokens': 1162, 'output_tokens': 14, 'total_tokens': 1176})]}}
----
```

```python
for chunk in agent_executor.stream(
    {"messages": [HumanMessage(content="whats my name?")]}, config
):
    print(chunk)
    print("----")
```

```output
{'agent': {'messages': [AIMessage(content='You mentioned your name is Bob when you introduced yourself earlier. So your name is Bob.', response_metadata={'id': 'msg_01WNwnRNGwGDRw6vRdivt6i1', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 1184, 'output_tokens': 21}}, id='run-f5c0b957-8878-405a-9d4b-a7cd38efe81f-0', usage_metadata={'input_tokens': 1184, 'output_tokens': 21, 'total_tokens': 1205})]}}
----
```

Example [LangSmith trace](https://smith.langchain.com/public/fa73960b-0f7d-4910-b73d-757a12f33b2b/r)

If you want to start a new conversation, all you have to do is change the `thread_id` used

```python
config = {"configurable": {"thread_id": "xyz123"}}
for chunk in agent_executor.stream(
    {"messages": [HumanMessage(content="whats my name?")]}, config
):
    print(chunk)
    print("----")
```

```output
{'agent': {'messages': [AIMessage(content="I'm afraid I don't actually know your name. As an AI assistant without personal information about you, I don't have a specific name associated with our conversation.", response_metadata={'id': 'msg_01NoaXNNYZKSoBncPcLkdcbo', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 267, 'output_tokens': 36}}, id='run-c9f7df3d-525a-4d8f-bbcf-a5b4a5d2e4b0-0', usage_metadata={'input_tokens': 267, 'output_tokens': 36, 'total_tokens': 303})]}}
----
```

## Conclusion[â€‹](#conclusion "Direct link to Conclusion")

That's a wrap! In this quick start we covered how to create a simple agent. We've then shown how to stream back a response - not only with the intermediate steps, but also tokens! We've also added in memory so you can have a conversation with them. Agents are a complex topic with lots to learn!

For more information on Agents, please check out the [LangGraph](/docs/concepts/architecture/#langgraph) documentation. This has it's own set of concepts, tutorials, and how-to guides.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/tutorials/agents.ipynb)

* * *


- [End-to-end agent](#end-to-end-agent)
- [Setup](#setup)
  
  - [Jupyter Notebook](#jupyter-notebook)
  - [Installation](#installation)
  - [LangSmith](#langsmith)
  - [Tavily](#tavily)
- [Define tools](#define-tools)
- [Using Language Models](#using-language-models)
- [Create the agent](#create-the-agent)
- [Run the agent](#run-the-agent)
- [Streaming Messages](#streaming-messages)
- [Streaming tokens](#streaming-tokens)
- [Adding in memory](#adding-in-memory)
- [Conclusion](#conclusion)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/prompts_composition.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/prompts_composition.ipynb)

# How to compose prompts together

Prerequisites

This guide assumes familiarity with the following concepts:

- [Prompt templates](/docs/concepts/prompt_templates/)

LangChain provides a user friendly interface for composing different parts of [prompts](/docs/concepts/prompt_templates/) together. You can do this with either string prompts or chat prompts. Constructing prompts this way allows for easy reuse of components.

## String prompt composition[â€‹](#string-prompt-composition "Direct link to String prompt composition")

When working with string prompts, each template is joined together. You can work with either prompts directly or strings (the first element in the list needs to be a prompt).

```python
from langchain_core.prompts import PromptTemplate

prompt = (
    PromptTemplate.from_template("Tell me a joke about {topic}")
    + ", make it funny"
    + "\n\nand in {language}"
)

prompt
```

**API Reference:**[PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

```output
PromptTemplate(input_variables=['language', 'topic'], template='Tell me a joke about {topic}, make it funny\n\nand in {language}')
```

```python
prompt.format(topic="sports", language="spanish")
```

```output
'Tell me a joke about sports, make it funny\n\nand in spanish'
```

## Chat prompt composition[â€‹](#chat-prompt-composition "Direct link to Chat prompt composition")

A chat prompt is made up a of a list of messages. Similarly to the above example, we can concatenate chat prompt templates. Each new element is a new message in the final prompt.

First, let's initialize the a [`ChatPromptTemplate`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) with a [`SystemMessage`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html).

```python
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

prompt = SystemMessage(content="You are a nice pirate")
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html)

You can then easily create a pipeline combining it with other messages *or* message templates. Use a `Message` when there is no variables to be formatted, use a `MessageTemplate` when there are variables to be formatted. You can also use just a string (note: this will automatically get inferred as a [`HumanMessagePromptTemplate`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.HumanMessagePromptTemplate.html).)

```python
new_prompt = (
    prompt + HumanMessage(content="hi") + AIMessage(content="what?") + "{input}"
)
```

Under the hood, this creates an instance of the ChatPromptTemplate class, so you can use it just as you did before!

```python
new_prompt.format_messages(input="i said hi")
```

```output
[SystemMessage(content='You are a nice pirate'),
 HumanMessage(content='hi'),
 AIMessage(content='what?'),
 HumanMessage(content='i said hi')]
```

## Using PipelinePrompt[â€‹](#using-pipelineprompt "Direct link to Using PipelinePrompt")

LangChain includes a class called [`PipelinePromptTemplate`](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.pipeline.PipelinePromptTemplate.html), which can be useful when you want to reuse parts of prompts. A PipelinePrompt consists of two main parts:

- Final prompt: The final prompt that is returned
- Pipeline prompts: A list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name.

```python
from langchain_core.prompts import PipelinePromptTemplate, PromptTemplate

full_template = """{introduction}

{example}

{start}"""
full_prompt = PromptTemplate.from_template(full_template)

introduction_template = """You are impersonating {person}."""
introduction_prompt = PromptTemplate.from_template(introduction_template)

example_template = """Here's an example of an interaction:

Q: {example_q}
A: {example_a}"""
example_prompt = PromptTemplate.from_template(example_template)

start_template = """Now, do this for real!

Q: {input}
A:"""
start_prompt = PromptTemplate.from_template(start_template)

input_prompts = [
    ("introduction", introduction_prompt),
    ("example", example_prompt),
    ("start", start_prompt),
]
pipeline_prompt = PipelinePromptTemplate(
    final_prompt=full_prompt, pipeline_prompts=input_prompts
)

pipeline_prompt.input_variables
```

**API Reference:**[PipelinePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.pipeline.PipelinePromptTemplate.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

```output
['person', 'example_a', 'example_q', 'input']
```

```python
print(
    pipeline_prompt.format(
        person="Elon Musk",
        example_q="What's your favorite car?",
        example_a="Tesla",
        input="What's your favorite social media site?",
    )
)
```

```output
You are impersonating Elon Musk.

Here's an example of an interaction:

Q: What's your favorite car?
A: Tesla

Now, do this for real!

Q: What's your favorite social media site?
A:
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to compose prompts together.

Next, check out the other how-to guides on prompt templates in this section, like [adding few-shot examples to your prompt templates](/docs/how_to/few_shot_examples_chat/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/prompts_composition.ipynb)

* * *


- [String prompt composition](#string-prompt-composition)
- [Chat prompt composition](#chat-prompt-composition)
- [Using PipelinePrompt](#using-pipelineprompt)
- [Next steps](#next-steps)









# How to disable parallel tool calling

OpenAI-specific

This API is currently only supported by OpenAI.

OpenAI tool calling performs tool calling in parallel by default. That means that if we ask a question like "What is the weather in Tokyo, New York, and Chicago?" and we have a tool for getting the weather, it will call the tool 3 times in parallel. We can force it to call only a single tool once by using the `parallel_tool_call` parameter.

First let's set up our tools and model:

```python
from langchain_core.tools import tool


@tool
def add(a: int, b: int) -> int:
    """Adds a and b."""
    return a + b


@tool
def multiply(a: int, b: int) -> int:
    """Multiplies a and b."""
    return a * b


tools = [add, multiply]
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

```python
import os
from getpass import getpass

from langchain_openai import ChatOpenAI

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

Now let's show a quick example of how disabling parallel tool calls work:

```python
llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)
llm_with_tools.invoke("Please call the first tool two times").tool_calls
```

```output
[{'name': 'add',
  'args': {'a': 2, 'b': 2},
  'id': 'call_Hh4JOTCDM85Sm9Pr84VKrWu5'}]
```

As we can see, even though we explicitly told the model to call a tool twice, by disabling parallel tool calls the model was constrained to only calling one.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tool_calling_parallel.ipynb)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_html.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_html.ipynb)

# How to load HTML

The HyperText Markup Language or [HTML](https://en.wikipedia.org/wiki/HTML) is the standard markup language for documents designed to be displayed in a web browser.

This covers how to load `HTML` documents into a LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects that we can use downstream.

Parsing HTML files often requires specialized tools. Here we demonstrate parsing via [Unstructured](https://docs.unstructured.io) and [BeautifulSoup4](https://beautiful-soup-4.readthedocs.io/en/latest/), which can be installed via pip. Head over to the integrations page to find integrations with additional services, such as [Azure AI Document Intelligence](/docs/integrations/document_loaders/azure_document_intelligence/) or [FireCrawl](/docs/integrations/document_loaders/firecrawl/).

## Loading HTML with Unstructured[â€‹](#loading-html-with-unstructured "Direct link to Loading HTML with Unstructured")

```python
%pip install unstructured
```

```python
from langchain_community.document_loaders import UnstructuredHTMLLoader

file_path = "../../docs/integrations/document_loaders/example_data/fake-content.html"

loader = UnstructuredHTMLLoader(file_path)
data = loader.load()

print(data)
```

**API Reference:**[UnstructuredHTMLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.html.UnstructuredHTMLLoader.html)

```output
[Document(page_content='My First Heading\n\nMy first paragraph.', metadata={'source': '../../docs/integrations/document_loaders/example_data/fake-content.html'})]
```

## Loading HTML with BeautifulSoup4[â€‹](#loading-html-with-beautifulsoup4 "Direct link to Loading HTML with BeautifulSoup4")

We can also use `BeautifulSoup4` to load HTML documents using the `BSHTMLLoader`. This will extract the text from the HTML into `page_content`, and the page title as `title` into `metadata`.

```python
%pip install bs4
```

```python
from langchain_community.document_loaders import BSHTMLLoader

loader = BSHTMLLoader(file_path)
data = loader.load()

print(data)
```

**API Reference:**[BSHTMLLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.html_bs.BSHTMLLoader.html)

```output
[Document(page_content='\nTest Title\n\n\nMy First Heading\nMy first paragraph.\n\n\n', metadata={'source': '../../docs/integrations/document_loaders/example_data/fake-content.html', 'title': 'Test Title'})]
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/document_loader_html.ipynb)

* * *


- [Loading HTML with Unstructured](#loading-html-with-unstructured)
- [Loading HTML with BeautifulSoup4](#loading-html-with-beautifulsoup4)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/streaming.mdx)

# Streaming

Prerequisites

- [Runnable Interface](/docs/concepts/runnables/)
- [Chat Models](/docs/concepts/chat_models/)

**Streaming** is crucial for enhancing the responsiveness of applications built on [LLMs](/docs/concepts/chat_models/). By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.

## Overview[â€‹](#overview "Direct link to Overview")

Generating full responses from [LLMs](/docs/concepts/chat_models/) often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.

In this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.

## What to stream in LLM applications[â€‹](#what-to-stream-in-llm-applications "Direct link to What to stream in LLM applications")

In applications involving LLMs, several types of data can be streamed to improve user experience by reducing perceived latency and increasing transparency. These include:

### 1. Streaming LLM outputs[â€‹](#1-streaming-llm-outputs "Direct link to 1. Streaming LLM outputs")

The most common and critical data to stream is the output generated by the LLM itself. LLMs often take time to generate full responses, and by streaming the output in real-time, users can see partial results as they are produced. This provides immediate feedback and helps reduce the wait time for users.

### 2. Streaming pipeline or workflow progress[â€‹](#2-streaming-pipeline-or-workflow-progress "Direct link to 2. Streaming pipeline or workflow progress")

Beyond just streaming LLM output, itâ€™s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:

- **In LangGraph Workflows:** With [LangGraph](/docs/concepts/architecture/#langgraph), workflows are composed of nodes and edges that represent various steps. Streaming here involves tracking changes to the **graph state** as individual **nodes** request updates. This allows for more granular monitoring of which node in the workflow is currently active, giving real-time updates about the status of the workflow as it progresses through different stages.
- **In LCEL Pipelines:** Streaming updates from an [LCEL](/docs/concepts/lcel/) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of the pipeline execute, you can stream which sub-runnable is currently running, providing real-time insight into the overall pipeline's progress.

Streaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.

### 3. Streaming custom data[â€‹](#3-streaming-custom-data "Direct link to 3. Streaming custom data")

In some cases, you may need to stream **custom data** that goes beyond the information provided by the pipeline or workflow structure. This custom information is injected within a specific step in the workflow, whether that step is a tool or a LangGraph node. For example, you could stream updates about what a tool is doing in real-time or the progress through a LangGraph node. This granular data, which is emitted directly from within the step, provides more detailed insights into the execution of the workflow and is especially useful in complex processes where more visibility is needed.

## Streaming APIs[â€‹](#streaming-apis "Direct link to Streaming APIs")

LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables/), including [LLMs](/docs/concepts/chat_models/), [compiled LangGraph graphs](https://langchain-ai.github.io/langgraph/concepts/low_level/), and any Runnable generated with [LCEL](/docs/concepts/lcel/).

1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream): Use to stream outputs from individual Runnables (e.g., a chat model) as they are generated or stream any workflow created with LangGraph.
2. The async only [astream\_events](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream_events): Use this API to get access to custom events and intermediate outputs from LLM applications built entirely with [LCEL](/docs/concepts/lcel/). Note that this API is available, but not needed when working with LangGraph.

note

In addition, there is a **legacy** async [astream\_log](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream_log) API. This API is not recommended for new projects it is more complex and less feature-rich than the other streaming APIs.

### `stream()` and `astream()`[â€‹](#stream-and-astream "Direct link to stream-and-astream")

The `stream()` method returns an iterator that yields chunks of output synchronously as they are produced. You can use a `for` loop to process each chunk in real-time. For example, when using an LLM, this allows the output to be streamed incrementally as it is generated, reducing the wait time for users.

The type of chunk yielded by the `stream()` and `astream()` methods depends on the component being streamed. For example, when streaming from an [LLM](/docs/concepts/chat_models/) each component will be an [AIMessageChunk](/docs/concepts/messages/#aimessagechunk); however, for other components, the chunk may be different.

The `stream()` method returns an iterator that yields these chunks as they are produced. For example,

```python
for chunk in component.stream(some_input):
    # IMPORTANT: Keep the processing of each chunk as efficient as possible.
    # While you're processing the current chunk, the upstream component is
    # waiting to produce the next one. For example, if working with LangGraph,
    # graph execution is paused while the current chunk is being processed.
    # In extreme cases, this could even result in timeouts (e.g., when llm outputs are
    # streamed from an API that has a timeout).
    print(chunk)
```

The [asynchronous version](/docs/concepts/async/), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming behavior.

#### Usage with chat models[â€‹](#usage-with-chat-models "Direct link to Usage with chat models")

When using `stream()` or `astream()` with chat models, the output is streamed as [AIMessageChunks](/docs/concepts/messages/#aimessagechunk) as it is generated by the LLM. This allows you to present or process the LLM's output incrementally as it's being produced, which is particularly useful in interactive applications or interfaces.

#### Usage with LangGraph[â€‹](#usage-with-langgraph "Direct link to Usage with LangGraph")

[LangGraph](/docs/concepts/architecture/#langgraph) compiled graphs are [Runnables](/docs/concepts/runnables/) and support the standard streaming APIs.

When using the *stream* and *astream* methods with LangGraph, you can choose **one or more** [streaming mode](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamMode) which allow you to control the type of output that is streamed. The available streaming modes are:

- **"values"**: Emit all values of the [state](https://langchain-ai.github.io/langgraph/concepts/low_level/) for each step.
- **"updates"**: Emit only the node name(s) and updates that were returned by the node(s) after each step.
- **"debug"**: Emit debug events for each step.
- **"messages"**: Emit LLM [messages](/docs/concepts/messages/) [token-by-token](/docs/concepts/tokens/).
- **"custom"**: Emit custom output written using [LangGraph's StreamWriter](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamWriter).

For more information, please see:

- [LangGraph streaming conceptual guide](https://langchain-ai.github.io/langgraph/concepts/streaming/) for more information on how to stream when working with LangGraph.
- [LangGraph streaming how-to guides](https://langchain-ai.github.io/langgraph/how-tos/#streaming) for specific examples of streaming in LangGraph.

#### Usage with LCEL[â€‹](#usage-with-lcel "Direct link to Usage with LCEL")

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel/), the `stream()` and `astream()` methods will, by convention, stream the output of the last step in the chain. This allows the final processed result to be streamed incrementally. **LCEL** tries to optimize streaming latency in pipelines so that the streaming results from the last step are available as soon as possible.

### `astream_events`[â€‹](#astream_events "Direct link to astream_events")

tip

Use the `astream_events` API to access custom data and intermediate outputs from LLM applications built entirely with [LCEL](/docs/concepts/lcel/).

While this API is available for use with [LangGraph](/docs/concepts/architecture/#langgraph) as well, it is usually not necessary when working with LangGraph, as the `stream` and `astream` methods provide comprehensive streaming capabilities for LangGraph graphs.

For chains constructed using **LCEL**, the `.stream()` method only streams the output of the final step from the chain. This might be sufficient for some applications, but as you build more complex chains of several LLM calls together, you may want to use the intermediate values of the chain alongside the final output. For example, you may want to return sources alongside the final generation when building a chat-over-documents app.

There are ways to do this [using callbacks](/docs/concepts/callbacks/), or by constructing your chain in such a way that it passes intermediate values to the end with something like chained [`.assign()`](/docs/how_to/passthrough/) calls, but LangChain also includes an `.astream_events()` method that combines the flexibility of callbacks with the ergonomics of `.stream()`. When called, it returns an iterator which yields [various types of events](/docs/how_to/streaming/#event-reference) that you can filter and process according to the needs of your project.

Here's one small example that prints just events containing streamed chat model output:

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for event in chain.astream_events({"topic": "parrot"}):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(event, end="|", flush=True)
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)

You can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!

See [this guide](/docs/how_to/streaming/#using-stream-events) for more detailed information on how to use `.astream_events()`, including a table listing available events.

## Writing custom data to the stream[â€‹](#writing-custom-data-to-the-stream "Direct link to Writing custom data to the stream")

To write custom data to the stream, you will need to choose one of the following methods based on the component you are working with:

1. LangGraph's [StreamWriter](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamWriter) can be used to write custom data that will surface through **stream** and **astream** APIs when working with LangGraph. **Important** this is a LangGraph feature, so it is not available when working with pure LCEL. See [how to streaming custom data](https://langchain-ai.github.io/langgraph/how-tos/streaming-content/) for more information.
2. [dispatch\_events](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.dispatch_custom_event.html) / [adispatch\_events](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.adispatch_custom_event.html) can be used to write custom data that will be surfaced through the **astream\_events** API. See [how to dispatch custom callback events](/docs/how_to/callbacks_custom_events/#astream-events-api) for more information.

## "Auto-Streaming" Chat Models[â€‹](#auto-streaming-chat-models 'Direct link to "Auto-Streaming" Chat Models')

LangChain simplifies streaming from [chat models](/docs/concepts/chat_models/) by automatically enabling streaming mode in certain cases, even when youâ€™re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming `invoke` method but still want to stream the entire application, including intermediate results from the chat model.

### How It Works[â€‹](#how-it-works "Direct link to How It Works")

When you call the `invoke` (or `ainvoke`) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application.

Under the hood, it'll have `invoke` (or `ainvoke`) use the `stream` (or `astream`) method to generate its output. The result of the invocation will be the same as far as the code that was using `invoke` is concerned; however, while the chat model is being streamed, LangChain will take care of invoking `on_llm_new_token` events in LangChain's [callback system](/docs/concepts/callbacks/). These callback events allow LangGraph `stream`/`astream` and `astream_events` to surface the chat model's output in real-time.

Example:

```python
def node(state):
    ...
    # The code below uses the invoke method, but LangChain will 
    # automatically switch to streaming mode
    # when it detects that the overall 
    # application is being streamed.
    ai_message = model.invoke(state["messages"])
    ...

for chunk in compiled_graph.stream(..., mode="messages"): 
    ...
```

## Async Programming[â€‹](#async-programming "Direct link to Async Programming")

LangChain offers both synchronous (sync) and asynchronous (async) versions of many of its methods. The async methods are typically prefixed with an "a" (e.g., `ainvoke`, `astream`). When writing async code, it's crucial to consistently use these asynchronous methods to ensure non-blocking behavior and optimal performance.

If streaming data fails to appear in real-time, please ensure that you are using the correct async methods for your workflow.

Please review the [async programming in LangChain guide](/docs/concepts/async/) for more information on writing async code with LangChain.

## Related Resources[â€‹](#related-resources "Direct link to Related Resources")

Please see the following how-to guides for specific examples of streaming in LangChain:

- [LangGraph conceptual guide on streaming](https://langchain-ai.github.io/langgraph/concepts/streaming/)
- [LangGraph streaming how-to guides](https://langchain-ai.github.io/langgraph/how-tos/#streaming)
- [How to stream runnables](/docs/how_to/streaming/): This how-to guide goes over common streaming patterns with LangChain components (e.g., chat models) and with [LCEL](/docs/concepts/lcel/).
- [How to stream chat models](/docs/how_to/chat_streaming/)
- [How to stream tool calls](/docs/how_to/tool_streaming/)

For writing custom data to the stream, please see the following resources:

- If using LangGraph, see [how to stream custom data](https://langchain-ai.github.io/langgraph/how-tos/streaming-content/).
- If using LCEL, see [how to dispatch custom callback events](/docs/how_to/callbacks_custom_events/#astream-events-api).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/streaming.mdx)

* * *


- [Overview](#overview)
- [What to stream in LLM applications](#what-to-stream-in-llm-applications)
  
  - [1. Streaming LLM outputs](#1-streaming-llm-outputs)
  - [2. Streaming pipeline or workflow progress](#2-streaming-pipeline-or-workflow-progress)
  - [3. Streaming custom data](#3-streaming-custom-data)
- [Streaming APIs](#streaming-apis)
  
  - [`stream()` and `astream()`](#stream-and-astream)
  - [`astream_events`](#astream_events)
- [Writing custom data to the stream](#writing-custom-data-to-the-stream)
- ["Auto-Streaming" Chat Models](#auto-streaming-chat-models)
  
  - [How It Works](#how-it-works)
- [Async Programming](#async-programming)
- [Related Resources](#related-resources)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/qa_chat_history_how_to.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/qa_chat_history_how_to.ipynb)

# How to add chat history

note

This guide previously used the [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) abstraction. You can access this version of the documentation in the [v0.2 docs](https://python.langchain.com/v0.2/docs/how_to/qa_chat_history_how_to/).

As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of [LangGraph persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/) to incorporate `memory` into new LangChain applications.

If your code is already relying on `RunnableWithMessageHistory` or `BaseChatMessageHistory`, you do **not** need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses `RunnableWithMessageHistory` will continue to work as expected.

Please see [How to migrate to LangGraph Memory](/docs/versions/migrating_memory/) for more details.

In many [Q&amp;A applications](/docs/concepts/rag/) we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of "memory" of past questions and answers, and some logic for incorporating those into its current thinking.

In this guide we focus on **adding logic for incorporating historical messages.**

This is largely a condensed version of the [Conversational RAG tutorial](/docs/tutorials/qa_chat_history/).

We will cover two approaches:

1. [Chains](/docs/how_to/qa_chat_history_how_to/#chains), in which we always execute a retrieval step;
2. [Agents](/docs/how_to/qa_chat_history_how_to/#agents), in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).

For the external knowledge source, we will use the same [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng from the [RAG tutorial](/docs/tutorials/rag/).

Both approaches leverage [LangGraph](https://langchain-ai.github.io/langgraph/) as an orchestration framework. LangGraph implements a built-in [persistence layer](https://langchain-ai.github.io/langgraph/concepts/persistence/), making it ideal for chat applications that support multiple conversational turns.

## Setup[â€‹](#setup "Direct link to Setup")

### Dependencies[â€‹](#dependencies "Direct link to Dependencies")

We'll use OpenAI embeddings and an InMemory vector store in this walkthrough, but everything shown here works with any [Embeddings](/docs/concepts/embedding_models/), and [VectorStore](/docs/concepts/vectorstores/) or [Retriever](/docs/concepts/retrievers/).

We'll use the following packages:

```python
%%capture --no-stderr
%pip install --upgrade --quiet langgraph langchain-community beautifulsoup4
```

### LangSmith[â€‹](#langsmith "Direct link to LangSmith")

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).

Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:

```python
os.environ["LANGSMITH_TRACING"] = "true"
if not os.environ.get("LANGSMITH_API_KEY"):
    os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

### Components[â€‹](#components "Direct link to Components")

We will need to select three components from LangChain's suite of integrations.

A [chat model](/docs/integrations/chat/):

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

An [embedding model](/docs/integrations/text_embedding/):

Select [embeddings model](/docs/integrations/text_embedding/):

OpenAIâ–¾

[OpenAI](#)

[Azure](#)

[Google](#)

[AWS](#)

[HuggingFace](#)

[Ollama](#)

[Cohere](#)

[MistralAI](#)

[Nomic](#)

[NVIDIA](#)

[Voyage AI](#)

[IBM watsonx](#)

[Fake](#)

```bash
pip install -qU langchain-openai
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
```

And a [vector store](/docs/integrations/vectorstores/):

Select [vector store](/docs/integrations/vectorstores/):

In-memoryâ–¾

[In-memory](#)

[AstraDB](#)

[Chroma](#)

[FAISS](#)

[Milvus](#)

[MongoDB](#)

[PGVector](#)

[Pinecone](#)

[Qdrant](#)

```bash
pip install -qU langchain-core
```

```python
from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)
```

## Chains[â€‹](#chains "Direct link to Chains")

The [RAG Tutorial](/docs/tutorials/rag/) indexes an [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng. We will repeat that here. Below we load the content of the page, split it into sub-documents, and embed the documents into our [vector store](/docs/concepts/vectorstores/):

```python
import bs4
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing_extensions import List, TypedDict

# Load and chunk contents of the blog
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html) | [WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

```python
# Index chunks
_ = vector_store.add_documents(documents=all_splits)
```

As detailed in [Part 2](/docs/tutorials/qa_chat_history/) of the RAG tutorial, we can naturally support a conversational experience by representing the flow of the RAG application as a sequence of [messages](/docs/concepts/messages/):

1. User input as a `HumanMessage`;
2. Vector store query as an `AIMessage` with tool calls;
3. Retrieved documents as a `ToolMessage`;
4. Final response as a `AIMessage`.

We will use [tool-calling](/docs/concepts/tool_calling/) to facilitate this, which additionally allows the query to be generated by the LLM. We can build a [tool](/docs/concepts/tools/) to execute the retrieval step:

```python
from langchain_core.tools import tool


@tool(response_format="content_and_artifact")
def retrieve(query: str):
    """Retrieve information related to a query."""
    retrieved_docs = vector_store.similarity_search(query, k=2)
    serialized = "\n\n".join(
        (f"Source: {doc.metadata}\n" f"Content: {doc.page_content}")
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

We can now build our LangGraph application.

Note that we compile it with a [checkpointer](https://langchain-ai.github.io/langgraph/concepts/persistence/) to support a back-and-forth conversation. LangGraph comes with a simple [in-memory checkpointer](https://langchain-ai.github.io/langgraph/reference/checkpoints/#memorysaver), which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).

```python
from langchain_core.messages import SystemMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, MessagesState, StateGraph
from langgraph.prebuilt import ToolNode, tools_condition


# Step 1: Generate an AIMessage that may include a tool-call to be sent.
def query_or_respond(state: MessagesState):
    """Generate tool call for retrieval or respond."""
    llm_with_tools = llm.bind_tools([retrieve])
    response = llm_with_tools.invoke(state["messages"])
    # MessagesState appends messages to state instead of overwriting
    return {"messages": [response]}


# Step 2: Execute the retrieval.
tools = ToolNode([retrieve])


# Step 3: Generate a response using the retrieved content.
def generate(state: MessagesState):
    """Generate answer."""
    # Get generated ToolMessages
    recent_tool_messages = []
    for message in reversed(state["messages"]):
        if message.type == "tool":
            recent_tool_messages.append(message)
        else:
            break
    tool_messages = recent_tool_messages[::-1]

    # Format into prompt
    docs_content = "\n\n".join(doc.content for doc in tool_messages)
    system_message_content = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you "
        "don't know. Use three sentences maximum and keep the "
        "answer concise."
        "\n\n"
        f"{docs_content}"
    )
    conversation_messages = [
        message
        for message in state["messages"]
        if message.type in ("human", "system")
        or (message.type == "ai" and not message.tool_calls)
    ]
    prompt = [SystemMessage(system_message_content)] + conversation_messages

    # Run
    response = llm.invoke(prompt)
    return {"messages": [response]}


# Build graph
graph_builder = StateGraph(MessagesState)

graph_builder.add_node(query_or_respond)
graph_builder.add_node(tools)
graph_builder.add_node(generate)

graph_builder.set_entry_point("query_or_respond")
graph_builder.add_conditional_edges(
    "query_or_respond",
    tools_condition,
    {END: END, "tools": "tools"},
)
graph_builder.add_edge("tools", "generate")
graph_builder.add_edge("generate", END)

memory = MemorySaver()
graph = graph_builder.compile(checkpointer=memory)
```

**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode) | [tools\_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.tools_condition)

```python
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

Let's test our application.

Note that it responds appropriately to messages that do not require an additional retrieval step:

```python
# Specify an ID for the thread
config = {"configurable": {"thread_id": "abc123"}}
```

```python
input_message = "Hello"

for step in graph.stream(
    {"messages": [{"role": "user", "content": input_message}]},
    stream_mode="values",
    config=config,
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

Hello
==================================[1m Ai Message [0m==================================

Hello! How can I assist you today?
```

And when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:

```python
input_message = "What is Task Decomposition?"

for step in graph.stream(
    {"messages": [{"role": "user", "content": input_message}]},
    stream_mode="values",
    config=config,
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

What is Task Decomposition?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_RntwX5GMt531biEE9MqSbgLV)
 Call ID: call_RntwX5GMt531biEE9MqSbgLV
  Args:
    query: Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
==================================[1m Ai Message [0m==================================

Task Decomposition is the process of breaking down a complicated task into smaller, more manageable steps. It often involves techniques like Chain of Thought (CoT), where the model is prompted to "think step by step," allowing for better handling of complex tasks. This approach enhances model performance and provides insight into the model's reasoning process.
```

Finally, because we have compiled our application with a [checkpointer](https://langchain-ai.github.io/langgraph/concepts/persistence/), historical messages are maintained in the state. This allows the model to contextualize user queries:

```python
input_message = "Can you look up some common ways of doing it?"

for step in graph.stream(
    {"messages": [{"role": "user", "content": input_message}]},
    stream_mode="values",
    config=config,
):
    step["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

Can you look up some common ways of doing it?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_kwO5rYPyJ0MftYKoKRFjKpZM)
 Call ID: call_kwO5rYPyJ0MftYKoKRFjKpZM
  Args:
    query: common methods for task decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.
==================================[1m Ai Message [0m==================================

Common ways of Task Decomposition include: (1) using large language models (LLMs) with simple prompts like "Steps for XYZ" or "What are the subgoals for achieving XYZ?"; (2) utilizing task-specific instructions, such as "Write a story outline" for creative tasks; and (3) incorporating human inputs to guide the decomposition process.
```

Note that we can observe the full sequence of messages sent to the chat model-- including tool calls and retrieved context-- in the [LangSmith trace](https://smith.langchain.com/public/3c85919e-9609-4a0d-8df1-21726f8f3e5c/r).

The conversation history can also be inspected via the state of the application:

```python
chat_history = graph.get_state(config).values["messages"]
for message in chat_history:
    message.pretty_print()
```

```output
================================[1m Human Message [0m=================================

Hello
==================================[1m Ai Message [0m==================================

Hello! How can I assist you today?
================================[1m Human Message [0m=================================

What is Task Decomposition?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_RntwX5GMt531biEE9MqSbgLV)
 Call ID: call_RntwX5GMt531biEE9MqSbgLV
  Args:
    query: Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
==================================[1m Ai Message [0m==================================

Task Decomposition is the process of breaking down a complicated task into smaller, more manageable steps. It often involves techniques like Chain of Thought (CoT), where the model is prompted to "think step by step," allowing for better handling of complex tasks. This approach enhances model performance and provides insight into the model's reasoning process.
================================[1m Human Message [0m=================================

Can you look up some common ways of doing it?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_kwO5rYPyJ0MftYKoKRFjKpZM)
 Call ID: call_kwO5rYPyJ0MftYKoKRFjKpZM
  Args:
    query: common methods for task decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.
==================================[1m Ai Message [0m==================================

Common ways of Task Decomposition include: (1) using large language models (LLMs) with simple prompts like "Steps for XYZ" or "What are the subgoals for achieving XYZ?"; (2) utilizing task-specific instructions, such as "Write a story outline" for creative tasks; and (3) incorporating human inputs to guide the decomposition process.
```

## Agents[â€‹](#agents "Direct link to Agents")

[Agents](/docs/concepts/agents/) leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above "chain", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.

Below we assemble a minimal RAG agent. Using LangGraph's [pre-built ReAct agent constructor](https://langchain-ai.github.io/langgraph/how-tos/#langgraph.prebuilt.chat_agent_executor.create_react_agent), we can do this in one line.

tip

Check out LangGraph's [Agentic RAG](https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/) tutorial for more advanced formulations.

```python
from langgraph.prebuilt import create_react_agent

agent_executor = create_react_agent(llm, [retrieve], checkpointer=memory)
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

Let's inspect the graph:

```python
display(Image(agent_executor.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

The key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.

Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:

```python
config = {"configurable": {"thread_id": "def234"}}

input_message = (
    "What is the standard method for Task Decomposition?\n\n"
    "Once you get the answer, look up common extensions of that method."
)

for event in agent_executor.stream(
    {"messages": [{"role": "user", "content": input_message}]},
    stream_mode="values",
    config=config,
):
    event["messages"][-1].pretty_print()
```

```output
================================[1m Human Message [0m=================================

What is the standard method for Task Decomposition?

Once you get the answer, look up common extensions of that method.
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_rxBqio7dxthnMuzjr4AIquSZ)
 Call ID: call_rxBqio7dxthnMuzjr4AIquSZ
  Args:
    query: standard method for Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_kmQMRWCKeBdtXdlJi8yZD9CO)
 Call ID: call_kmQMRWCKeBdtXdlJi8yZD9CO
  Args:
    query: common extensions of Task Decomposition methods
=================================[1m Tool Message [0m=================================
Name: retrieve

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.
==================================[1m Ai Message [0m==================================

The standard method for Task Decomposition involves breaking down complex tasks into smaller, manageable steps. Here are the main techniques:

1. **Chain of Thought (CoT)**: This prompting technique encourages a model to "think step by step," allowing it to utilize more computational resources during testing to decompose challenging tasks into simpler parts. CoT not only simplifies tasks but also provides insights into the model's reasoning process.

2. **Simple Prompting**: This can involve straightforward queries like "Steps for XYZ" or "What are the subgoals for achieving XYZ?" to guide the model in identifying the necessary steps.

3. **Task-specific Instructions**: Using specific prompts tailored to the task at hand, such as "Write a story outline" for creative writing, allows for more directed decomposition.

4. **Human Inputs**: Involving human expertise can also aid in breaking down tasks effectively.

### Common Extensions of Task Decomposition Methods

1. **Tree of Thoughts**: This method extends CoT by exploring multiple reasoning possibilities at each step. It decomposes the problem into various thought steps and generates multiple thoughts per step, forming a tree structure. This can utilize search processes like breadth-first search (BFS) or depth-first search (DFS) to evaluate states through classifiers or majority voting.

These extensions build on the basic principles of task decomposition, enhancing the depth and breadth of reasoning applied to complex tasks.
```

Note that the agent:

1. Generates a query to search for a standard method for task decomposition;
2. Receiving the answer, generates a second query to search for common extensions of it;
3. Having received all necessary context, answers the question.

We can see the full sequence of steps, along with latency and other metadata, in the [LangSmith trace](https://smith.langchain.com/public/48cbd35e-9ac1-49ab-8c09-500d54c06b81/r).

## Next steps[â€‹](#next-steps "Direct link to Next steps")

We've covered the steps to build a basic conversational Q&amp;A application:

- We used chains to build a predictable application that generates search queries for each user input;
- We used agents to build an application that "decides" when and how to generate search queries.

To explore different types of retrievers and retrieval strategies, visit the [retrievers](/docs/how_to/#retrievers) section of the how-to guides.

For a detailed walkthrough of LangChain's conversation memory abstractions, visit the [How to add message history (memory)](/docs/how_to/message_history/) LCEL page.

To learn more about agents, head to the [Agents Modules](/docs/tutorials/agents/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/qa_chat_history_how_to.ipynb)

* * *


- [Setup](#setup)
  
  - [Dependencies](#dependencies)
  - [LangSmith](#langsmith)
  - [Components](#components)
- [Chains](#chains)
- [Agents](#agents)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/tool_configure.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/tool_configure.ipynb)

# How to access the RunnableConfig from a tool

Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Tools](/docs/concepts/tools/)
- [Custom tools](/docs/how_to/custom_tools/)
- [LangChain Expression Language (LCEL)](/docs/concepts/lcel/)
- [Configuring runnable behavior](/docs/how_to/configure/)

If you have a [tool](/docs/concepts/tools/) that calls [chat models](/docs/concepts/chat_models/), [retrievers](/docs/concepts/retrievers/), or other [runnables](/docs/concepts/runnables/), you may want to access internal events from those runnables or configure them with additional properties. This guide shows you how to manually pass parameters properly so that you can do this using the `astream_events()` method.

Tools are [runnables](/docs/concepts/runnables/), and you can treat them the same way as any other runnable at the interface level - you can call `invoke()`, `batch()`, and `stream()` on them as normal. However, when writing custom tools, you may want to invoke other runnables like chat models or retrievers. In order to properly trace and configure those sub-invocations, you'll need to manually access and pass in the tool's current [`RunnableConfig`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) object. This guide show you some examples of how to do that.

Compatibility

This guide requires `langchain-core>=0.2.16`.

## Inferring by parameter type[â€‹](#inferring-by-parameter-type "Direct link to Inferring by parameter type")

To access reference the active config object from your custom tool, you'll need to add a parameter to your tool's signature typed as `RunnableConfig`. When you invoke your tool, LangChain will inspect your tool's signature, look for a parameter typed as `RunnableConfig`, and if it exists, populate that parameter with the correct value.

**Note:** The actual name of the parameter doesn't matter, only the typing.

To illustrate this, define a custom tool that takes a two parameters - one typed as a string, the other typed as `RunnableConfig`:

```python
%pip install -qU langchain_core
```

```python
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool


@tool
async def reverse_tool(text: str, special_config_param: RunnableConfig) -> str:
    """A test tool that combines input text with a configurable parameter."""
    return (text + special_config_param["configurable"]["additional_field"])[::-1]
```

**API Reference:**[RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

Then, if we invoke the tool with a `config` containing a `configurable` field, we can see that `additional_field` is passed through correctly:

```python
await reverse_tool.ainvoke(
    {"text": "abc"}, config={"configurable": {"additional_field": "123"}}
)
```

```output
'321cba'
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now seen how to configure and stream events from within a tool. Next, check out the following guides for more on using tools:

- [Stream events from child runs within a custom tool](/docs/how_to/tool_stream_events/)
- Pass [tool results back to a model](/docs/how_to/tool_results_pass_to_model/)

You can also check out some more specific uses of tool calling:

- Building [tool-using chains and agents](/docs/how_to/#tools)
- Getting [structured outputs](/docs/how_to/structured_output/) from models

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tool_configure.ipynb)

* * *


- [Inferring by parameter type](#inferring-by-parameter-type)
- [Next steps](#next-steps)








[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/index.mdx)

# Conceptual guide

This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.

We recommend that you go through at least one of the [Tutorials](/docs/tutorials/) before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.

The conceptual guide does not cover step-by-step instructions or specific implementation examples â€” those are found in the [How-to guides](/docs/how_to/) and [Tutorials](/docs/tutorials/). For detailed reference material, please see the [API reference](https://python.langchain.com/api_reference/).

## High level[â€‹](#high-level "Direct link to High level")

- [**Why LangChain?**](/docs/concepts/why_langchain/): Overview of the value that LangChain provides.
- [**Architecture**](/docs/concepts/architecture/): How packages are organized in the LangChain ecosystem.

## Concepts[â€‹](#concepts "Direct link to Concepts")

- [**Chat models**](/docs/concepts/chat_models/): LLMs exposed via a chat API that process sequences of messages as input and output a message.
- [**Messages**](/docs/concepts/messages/): The unit of communication in chat models, used to represent model input and output.
- [**Chat history**](/docs/concepts/chat_history/): A conversation represented as a sequence of messages, alternating between user messages and model responses.
- [**Tools**](/docs/concepts/tools/): A function with an associated schema defining the function's name, description, and the arguments it accepts.
- [**Tool calling**](/docs/concepts/tool_calling/): A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.
- [**Structured output**](/docs/concepts/structured_outputs/): A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.
- [**Memory**](https://langchain-ai.github.io/langgraph/concepts/memory/): Information about a conversation that is persisted so that it can be used in future conversations.
- [**Multimodality**](/docs/concepts/multimodality/): The ability to work with data that comes in different forms, such as text, audio, images, and video.
- [**Runnable interface**](/docs/concepts/runnables/): The base abstraction that many LangChain components and the LangChain Expression Language are built on.
- [**Streaming**](/docs/concepts/streaming/): LangChain streaming APIs for surfacing results as they are generated.
- [**LangChain Expression Language (LCEL)**](/docs/concepts/lcel/): A syntax for orchestrating LangChain components. Most useful for simpler applications.
- [**Document loaders**](/docs/concepts/document_loaders/): Load a source as a list of documents.
- [**Retrieval**](/docs/concepts/retrieval/): Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.
- [**Text splitters**](/docs/concepts/text_splitters/): Split long text into smaller chunks that can be individually indexed to enable granular retrieval.
- [**Embedding models**](/docs/concepts/embedding_models/): Models that represent data such as text or images in a vector space.
- [**Vector stores**](/docs/concepts/vectorstores/): Storage of and efficient search over vectors and associated metadata.
- [**Retriever**](/docs/concepts/retrievers/): A component that returns relevant documents from a knowledge base in response to a query.
- [**Retrieval Augmented Generation (RAG)**](/docs/concepts/rag/): A technique that enhances language models by combining them with external knowledge bases.
- [**Agents**](/docs/concepts/agents/): Use a [language model](/docs/concepts/chat_models/) to choose a sequence of actions to take. Agents can interact with external resources via [tool](/docs/concepts/tools/).
- [**Prompt templates**](/docs/concepts/prompt_templates/): Component for factoring out the static parts of a model "prompt" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.
- [**Output parsers**](/docs/concepts/output_parsers/): Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of [tool calling](/docs/concepts/tool_calling/) and [structured outputs](/docs/concepts/structured_outputs/).
- [**Few-shot prompting**](/docs/concepts/few_shot_prompting/): A technique for improving model performance by providing a few examples of the task to perform in the prompt.
- [**Example selectors**](/docs/concepts/example_selectors/): Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.
- [**Async programming**](/docs/concepts/async/): The basics that one should know to use LangChain in an asynchronous context.
- [**Callbacks**](/docs/concepts/callbacks/): Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.
- [**Tracing**](/docs/concepts/tracing/): The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.
- [**Evaluation**](/docs/concepts/evaluation/): The process of assessing the performance and effectiveness of AI applications. This involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.
- [**Testing**](/docs/concepts/testing/): The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.

## Glossary[â€‹](#glossary "Direct link to Glossary")

- [**AIMessageChunk**](/docs/concepts/messages/#aimessagechunk): A partial response from an AI message. Used when streaming responses from a chat model.
- [**AIMessage**](/docs/concepts/messages/#aimessage): Represents a complete response from an AI model.
- [**astream\_events**](/docs/concepts/chat_models/#key-methods): Stream granular information from [LCEL](/docs/concepts/lcel/) chains.
- [**BaseTool**](/docs/concepts/tools/#tool-interface): The base class for all tools in LangChain.
- [**batch**](/docs/concepts/runnables/): Use to execute a runnable with batch inputs.
- [**bind\_tools**](/docs/concepts/tool_calling/#tool-binding): Allows models to interact with tools.
- [**Caching**](/docs/concepts/chat_models/#caching): Storing results to avoid redundant calls to a chat model.
- [**Chat models**](/docs/concepts/multimodality/#multimodality-in-chat-models): Chat models that handle multiple data modalities.
- [**Configurable runnables**](/docs/concepts/runnables/#configurable-runnables): Creating configurable Runnables.
- [**Context window**](/docs/concepts/chat_models/#context-window): The maximum size of input a chat model can process.
- [**Conversation patterns**](/docs/concepts/chat_history/#conversation-patterns): Common patterns in chat interactions.
- [**Document**](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html): LangChain's representation of a document.
- [**Embedding models**](/docs/concepts/multimodality/#multimodality-in-embedding-models): Models that generate vector embeddings for various data types.
- [**HumanMessage**](/docs/concepts/messages/#humanmessage): Represents a message from a human user.
- [**InjectedState**](/docs/concepts/tools/#injectedstate): A state injected into a tool function.
- [**InjectedStore**](/docs/concepts/tools/#injectedstore): A store that can be injected into a tool for data persistence.
- [**InjectedToolArg**](/docs/concepts/tools/#injectedtoolarg): Mechanism to inject arguments into tool functions.
- [**input and output types**](/docs/concepts/runnables/#input-and-output-types): Types used for input and output in Runnables.
- [**Integration packages**](/docs/concepts/architecture/#integration-packages): Third-party packages that integrate with LangChain.
- [**Integration tests**](/docs/concepts/testing/#integration-tests): Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.
- [**invoke**](/docs/concepts/runnables/): A standard method to invoke a Runnable.
- [**JSON mode**](/docs/concepts/structured_outputs/#json-mode): Returning responses in JSON format.
- [**langchain-community**](/docs/concepts/architecture/#langchain-community): Community-driven components for LangChain.
- [**langchain-core**](/docs/concepts/architecture/#langchain-core): Core langchain package. Includes base interfaces and in-memory implementations.
- [**langchain**](/docs/concepts/architecture/#langchain): A package for higher level components (e.g., some pre-built chains).
- [**langgraph**](/docs/concepts/architecture/#langgraph): Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.
- [**langserve**](/docs/concepts/architecture/#langserve): Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.
- [**LLMs (legacy)**](/docs/concepts/text_llms/): Older language models that take a string as input and return a string as output.
- [**Managing chat history**](/docs/concepts/chat_history/#managing-chat-history): Techniques to maintain and manage the chat history.
- [**OpenAI format**](/docs/concepts/messages/#openai-format): OpenAI's message format for chat models.
- [**Propagation of RunnableConfig**](/docs/concepts/runnables/#propagation-of-runnableconfig): Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.
- [**rate-limiting**](/docs/concepts/chat_models/#rate-limiting): Client side rate limiting for chat models.
- [**RemoveMessage**](/docs/concepts/messages/#removemessage): An abstraction used to remove a message from chat history, used primarily in LangGraph.
- [**role**](/docs/concepts/messages/#role): Represents the role (e.g., user, assistant) of a chat message.
- [**RunnableConfig**](/docs/concepts/runnables/#runnableconfig): Use to pass run time information to Runnables (e.g., `run_name`, `run_id`, `tags`, `metadata`, `max_concurrency`, `recursion_limit`, `configurable`).
- [**Standard parameters for chat models**](/docs/concepts/chat_models/#standard-parameters): Parameters such as API key, `temperature`, and `max_tokens`.
- [**Standard tests**](/docs/concepts/testing/#standard-tests): A defined set of unit and integration tests that all integrations must pass.
- [**stream**](/docs/concepts/streaming/): Use to stream output from a Runnable or a graph.
- [**Tokenization**](/docs/concepts/tokens/): The process of converting data into tokens and vice versa.
- [**Tokens**](/docs/concepts/tokens/): The basic unit that a language model reads, processes, and generates under the hood.
- [**Tool artifacts**](/docs/concepts/tools/#tool-artifacts): Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.
- [**Tool binding**](/docs/concepts/tool_calling/#tool-binding): Binding tools to models.
- [**@tool**](/docs/concepts/tools/#create-tools-using-the-tool-decorator): Decorator for creating tools in LangChain.
- [**Toolkits**](/docs/concepts/tools/#toolkits): A collection of tools that can be used together.
- [**ToolMessage**](/docs/concepts/messages/#toolmessage): Represents a message that contains the results of a tool execution.
- [**Unit tests**](/docs/concepts/testing/#unit-tests): Tests that verify the correctness of individual components, run in isolation without access to the Internet.
- [**Vector stores**](/docs/concepts/vectorstores/): Datastores specialized for storing and efficiently searching vector embeddings.
- [**with\_structured\_output**](/docs/concepts/structured_outputs/#structured-output-method): A helper method for chat models that natively support [tool calling](/docs/concepts/tool_calling/) to get structured output matching a given schema specified via Pydantic, JSON schema or a function.
- [**with\_types**](/docs/concepts/runnables/#with_types): Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/index.mdx)

* * *


- [High level](#high-level)
- [Concepts](#concepts)
- [Glossary](#glossary)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/structured_output.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/structured_output.ipynb)

# How to return structured data from a model

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models/)
- [Function/tool calling](/docs/concepts/tool_calling/)

It is often useful to have a model return output that matches a specific [schema](/docs/concepts/structured_outputs/). One common use-case is extracting data from text to insert into a database or use with some other downstream system. This guide covers a few strategies for getting structured outputs from a model.

## The `.with_structured_output()` method[â€‹](#the-with_structured_output-method "Direct link to the-with_structured_output-method")

Supported models

You can find a [list of models that support this method here](/docs/integrations/chat/).

This is the easiest and most reliable way to get structured outputs. `with_structured_output()` is implemented for [models that provide native APIs for structuring outputs](/docs/integrations/chat/), like tool/function calling or JSON mode, and makes use of these capabilities under the hood.

This method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or [messages](/docs/concepts/messages/) it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, [JSON Schema](https://json-schema.org/) or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.

As an example, let's get a model to generate a joke and separate the setup from the punchline:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

### Pydantic class[â€‹](#pydantic-class "Direct link to Pydantic class")

If we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.

```python
from typing import Optional

from pydantic import BaseModel, Field


# Pydantic
class Joke(BaseModel):
    """Joke to tell user."""

    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline to the joke")
    rating: Optional[int] = Field(
        default=None, description="How funny the joke is, from 1 to 10"
    )


structured_llm = llm.with_structured_output(Joke)

structured_llm.invoke("Tell me a joke about cats")
```

```output
Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)
```

tip

Beyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time `with_structured_output` is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.

### TypedDict or JSON Schema[â€‹](#typeddict-or-json-schema "Direct link to TypedDict or JSON Schema")

If you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special `Annotated` syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is *not* filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.

Requirements

- Core: `langchain-core>=0.2.26`
- Typing extensions: It is highly recommended to import `Annotated` and `TypedDict` from `typing_extensions` instead of `typing` to ensure consistent behavior across Python versions.

```python
from typing import Optional

from typing_extensions import Annotated, TypedDict


# TypedDict
class Joke(TypedDict):
    """Joke to tell user."""

    setup: Annotated[str, ..., "The setup of the joke"]

    # Alternatively, we could have specified setup as:

    # setup: str                    # no default, no description
    # setup: Annotated[str, ...]    # no default, no description
    # setup: Annotated[str, "foo"]  # default, no description

    punchline: Annotated[str, ..., "The punchline of the joke"]
    rating: Annotated[Optional[int], None, "How funny the joke is, from 1 to 10"]


structured_llm = llm.with_structured_output(Joke)

structured_llm.invoke("Tell me a joke about cats")
```

```output
{'setup': 'Why was the cat sitting on the computer?',
 'punchline': 'Because it wanted to keep an eye on the mouse!',
 'rating': 7}
```

Equivalently, we can pass in a [JSON Schema](https://json-schema.org/) dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.

```python
json_schema = {
    "title": "joke",
    "description": "Joke to tell user.",
    "type": "object",
    "properties": {
        "setup": {
            "type": "string",
            "description": "The setup of the joke",
        },
        "punchline": {
            "type": "string",
            "description": "The punchline to the joke",
        },
        "rating": {
            "type": "integer",
            "description": "How funny the joke is, from 1 to 10",
            "default": None,
        },
    },
    "required": ["setup", "punchline"],
}
structured_llm = llm.with_structured_output(json_schema)

structured_llm.invoke("Tell me a joke about cats")
```

```output
{'setup': 'Why was the cat sitting on the computer?',
 'punchline': 'Because it wanted to keep an eye on the mouse!',
 'rating': 7}
```

### Choosing between multiple schemas[â€‹](#choosing-between-multiple-schemas "Direct link to Choosing between multiple schemas")

The simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.

#### Using Pydantic[â€‹](#using-pydantic "Direct link to Using Pydantic")

```python
from typing import Union


class Joke(BaseModel):
    """Joke to tell user."""

    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline to the joke")
    rating: Optional[int] = Field(
        default=None, description="How funny the joke is, from 1 to 10"
    )


class ConversationalResponse(BaseModel):
    """Respond in a conversational manner. Be kind and helpful."""

    response: str = Field(description="A conversational response to the user's query")


class FinalResponse(BaseModel):
    final_output: Union[Joke, ConversationalResponse]


structured_llm = llm.with_structured_output(FinalResponse)

structured_llm.invoke("Tell me a joke about cats")
```

```output
FinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))
```

```python
structured_llm.invoke("How are you today?")
```

```output
FinalResponse(final_output=ConversationalResponse(response="I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!"))
```

#### Using TypedDict[â€‹](#using-typeddict "Direct link to Using TypedDict")

```python
from typing import Optional, Union

from typing_extensions import Annotated, TypedDict


class Joke(TypedDict):
    """Joke to tell user."""

    setup: Annotated[str, ..., "The setup of the joke"]
    punchline: Annotated[str, ..., "The punchline of the joke"]
    rating: Annotated[Optional[int], None, "How funny the joke is, from 1 to 10"]


class ConversationalResponse(TypedDict):
    """Respond in a conversational manner. Be kind and helpful."""

    response: Annotated[str, ..., "A conversational response to the user's query"]


class FinalResponse(TypedDict):
    final_output: Union[Joke, ConversationalResponse]


structured_llm = llm.with_structured_output(FinalResponse)

structured_llm.invoke("Tell me a joke about cats")
```

```output
{'final_output': {'setup': 'Why was the cat sitting on the computer?',
  'punchline': 'Because it wanted to keep an eye on the mouse!',
  'rating': 7}}
```

```python
structured_llm.invoke("How are you today?")
```

```output
{'final_output': {'response': "I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!"}}
```

Responses shall be identical to the ones shown in the Pydantic example.

Alternatively, you can use tool calling directly to allow the model to choose between options, if your [chosen model supports it](/docs/integrations/chat/). This involves a bit more parsing and setup but in some instances leads to better performance because you don't have to use nested schemas. See [this how-to guide](/docs/how_to/tool_calling/) for more details.

### Streaming[â€‹](#streaming "Direct link to Streaming")

We can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or JSON Schema dict).

info

Note that what's yielded is already aggregated chunks, not deltas.

```python
from typing_extensions import Annotated, TypedDict


# TypedDict
class Joke(TypedDict):
    """Joke to tell user."""

    setup: Annotated[str, ..., "The setup of the joke"]
    punchline: Annotated[str, ..., "The punchline of the joke"]
    rating: Annotated[Optional[int], None, "How funny the joke is, from 1 to 10"]


structured_llm = llm.with_structured_output(Joke)

for chunk in structured_llm.stream("Tell me a joke about cats"):
    print(chunk)
```

```output
{}
{'setup': ''}
{'setup': 'Why'}
{'setup': 'Why was'}
{'setup': 'Why was the'}
{'setup': 'Why was the cat'}
{'setup': 'Why was the cat sitting'}
{'setup': 'Why was the cat sitting on'}
{'setup': 'Why was the cat sitting on the'}
{'setup': 'Why was the cat sitting on the computer'}
{'setup': 'Why was the cat sitting on the computer?'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': ''}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}
```

### Few-shot prompting[â€‹](#few-shot-prompting "Direct link to Few-shot prompting")

For more complex schemas it's very useful to add few-shot examples to the prompt. This can be done in a few ways.

The simplest and most universal way is to add examples to a system message in the prompt:

```python
from langchain_core.prompts import ChatPromptTemplate

system = """You are a hilarious comedian. Your specialty is knock-knock jokes. \
Return a joke which has the setup (the response to "Who's there?") and the final punchline (the response to "<setup> who?").

Here are some examples of jokes:

example_user: Tell me a joke about planes
example_assistant: {{"setup": "Why don't planes ever get tired?", "punchline": "Because they have rest wings!", "rating": 2}}

example_user: Tell me another joke about planes
example_assistant: {{"setup": "Cargo", "punchline": "Cargo 'vroom vroom', but planes go 'zoom zoom'!", "rating": 10}}

example_user: Now about caterpillars
example_assistant: {{"setup": "Caterpillar", "punchline": "Caterpillar really slow, but watch me turn into a butterfly and steal the show!", "rating": 5}}"""

prompt = ChatPromptTemplate.from_messages([("system", system), ("human", "{input}")])

few_shot_structured_llm = prompt | structured_llm
few_shot_structured_llm.invoke("what's something funny about woodpeckers")
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
{'setup': 'Woodpecker',
 'punchline': "Woodpecker you a joke, but I'm afraid it might be too 'hole-some'!",
 'rating': 7}
```

When the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you're using makes use of tool calling in its API reference.

```python
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage

examples = [
    HumanMessage("Tell me a joke about planes", name="example_user"),
    AIMessage(
        "",
        name="example_assistant",
        tool_calls=[
            {
                "name": "joke",
                "args": {
                    "setup": "Why don't planes ever get tired?",
                    "punchline": "Because they have rest wings!",
                    "rating": 2,
                },
                "id": "1",
            }
        ],
    ),
    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.
    ToolMessage("", tool_call_id="1"),
    # Some models also expect an AIMessage to follow any ToolMessages,
    # so you may need to add an AIMessage here.
    HumanMessage("Tell me another joke about planes", name="example_user"),
    AIMessage(
        "",
        name="example_assistant",
        tool_calls=[
            {
                "name": "joke",
                "args": {
                    "setup": "Cargo",
                    "punchline": "Cargo 'vroom vroom', but planes go 'zoom zoom'!",
                    "rating": 10,
                },
                "id": "2",
            }
        ],
    ),
    ToolMessage("", tool_call_id="2"),
    HumanMessage("Now about caterpillars", name="example_user"),
    AIMessage(
        "",
        tool_calls=[
            {
                "name": "joke",
                "args": {
                    "setup": "Caterpillar",
                    "punchline": "Caterpillar really slow, but watch me turn into a butterfly and steal the show!",
                    "rating": 5,
                },
                "id": "3",
            }
        ],
    ),
    ToolMessage("", tool_call_id="3"),
]
system = """You are a hilarious comedian. Your specialty is knock-knock jokes. \
Return a joke which has the setup (the response to "Who's there?") \
and the final punchline (the response to "<setup> who?")."""

prompt = ChatPromptTemplate.from_messages(
    [("system", system), ("placeholder", "{examples}"), ("human", "{input}")]
)
few_shot_structured_llm = prompt | structured_llm
few_shot_structured_llm.invoke({"input": "crocodiles", "examples": examples})
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html)

```output
{'setup': 'Crocodile',
 'punchline': 'Crocodile be seeing you later, alligator!',
 'rating': 6}
```

For more on few shot prompting when using tool calling, see [here](/docs/how_to/tools_few_shot/).

### (Advanced) Specifying the method for structuring outputs[â€‹](#advanced-specifying-the-method-for-structuring-outputs "Direct link to (Advanced) Specifying the method for structuring outputs")

For models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the `method=` argument.

JSON mode

If using JSON mode you'll have to still specify the desired schema in the model prompt. The schema you pass to `with_structured_output` will only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.

To see if the model you're using supports JSON mode, check its entry in the [API reference](https://python.langchain.com/api_reference/langchain/index.html).

```python
structured_llm = llm.with_structured_output(None, method="json_mode")

structured_llm.invoke(
    "Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys"
)
```

```output
{'setup': 'Why was the cat sitting on the computer?',
 'punchline': 'Because it wanted to keep an eye on the mouse!'}
```

### (Advanced) Raw outputs[â€‹](#advanced-raw-outputs "Direct link to (Advanced) Raw outputs")

LLMs aren't perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing `include_raw=True`. This changes the output format to contain the raw message output, the `parsed` value (if successful), and any resulting errors:

```python
structured_llm = llm.with_structured_output(Joke, include_raw=True)

structured_llm.invoke("Tell me a joke about cats")
```

```output
{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'function': {'arguments': '{"setup":"Why was the cat sitting on the computer?","punchline":"Because it wanted to keep an eye on the mouse!","rating":7}', 'name': 'Joke'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 93, 'total_tokens': 126}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518', 'finish_reason': 'stop', 'logprobs': None}, id='run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0', tool_calls=[{'name': 'Joke', 'args': {'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}, 'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 33, 'total_tokens': 126}),
 'parsed': {'setup': 'Why was the cat sitting on the computer?',
  'punchline': 'Because it wanted to keep an eye on the mouse!',
  'rating': 7},
 'parsing_error': None}
```

## Prompting and parsing model outputs directly[â€‹](#prompting-and-parsing-model-outputs-directly "Direct link to Prompting and parsing model outputs directly")

Not all models support `.with_structured_output()`, since not all models have tool calling or JSON mode support. For such models you'll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.

### Using `PydanticOutputParser`[â€‹](#using-pydanticoutputparser "Direct link to using-pydanticoutputparser")

The following example uses the built-in [`PydanticOutputParser`](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html) to parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding `format_instructions` directly to the prompt from a method on the parser:

```python
from typing import List

from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field


class Person(BaseModel):
    """Information about a person."""

    name: str = Field(..., description="The name of the person")
    height_in_meters: float = Field(
        ..., description="The height of the person expressed in meters."
    )


class People(BaseModel):
    """Identifying information about all people in a text."""

    people: List[Person]


# Set up a parser
parser = PydanticOutputParser(pydantic_object=People)

# Prompt
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Answer the user query. Wrap the output in `json` tags\n{format_instructions}",
        ),
        ("human", "{query}"),
    ]
).partial(format_instructions=parser.get_format_instructions())
```

**API Reference:**[PydanticOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

Letâ€™s take a look at what information is sent to the model:

```python
query = "Anna is 23 years old and she is 6 feet tall"

print(prompt.invoke({"query": query}).to_string())
```

```output
System: Answer the user query. Wrap the output in `json` tags
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
\`\`\`
{"description": "Identifying information about all people in a text.", "properties": {"people": {"title": "People", "type": "array", "items": {"$ref": "#/definitions/Person"}}}, "required": ["people"], "definitions": {"Person": {"title": "Person", "description": "Information about a person.", "type": "object", "properties": {"name": {"title": "Name", "description": "The name of the person", "type": "string"}, "height_in_meters": {"title": "Height In Meters", "description": "The height of the person expressed in meters.", "type": "number"}}, "required": ["name", "height_in_meters"]}}}
\`\`\`
Human: Anna is 23 years old and she is 6 feet tall
```

And now let's invoke it:

```python
chain = prompt | llm | parser

chain.invoke({"query": query})
```

```output
People(people=[Person(name='Anna', height_in_meters=1.8288)])
```

For a deeper dive into using output parsers with prompting techniques for structured output, see [this guide](/docs/how_to/output_parser_structured/).

### Custom Parsing[â€‹](#custom-parsing "Direct link to Custom Parsing")

You can also create a custom prompt and parser with [LangChain Expression Language (LCEL)](/docs/concepts/lcel/), using a plain function to parse the output from the model:

```python
import json
import re
from typing import List

from langchain_core.messages import AIMessage
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field


class Person(BaseModel):
    """Information about a person."""

    name: str = Field(..., description="The name of the person")
    height_in_meters: float = Field(
        ..., description="The height of the person expressed in meters."
    )


class People(BaseModel):
    """Identifying information about all people in a text."""

    people: List[Person]


# Prompt
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Answer the user query. Output your answer as JSON that  "
            "matches the given schema: \`\`\`json\n{schema}\n\`\`\`. "
            "Make sure to wrap the answer in \`\`\`json and \`\`\` tags",
        ),
        ("human", "{query}"),
    ]
).partial(schema=People.schema())


# Custom parser
def extract_json(message: AIMessage) -> List[dict]:
    """Extracts JSON content from a string where JSON is embedded between \`\`\`json and \`\`\` tags.

    Parameters:
        text (str): The text containing the JSON content.

    Returns:
        list: A list of extracted JSON strings.
    """
    text = message.content
    # Define the regular expression pattern to match JSON blocks
    pattern = r"\`\`\`json(.*?)\`\`\`"

    # Find all non-overlapping matches of the pattern in the string
    matches = re.findall(pattern, text, re.DOTALL)

    # Return the list of matched JSON strings, stripping any leading or trailing whitespace
    try:
        return [json.loads(match.strip()) for match in matches]
    except Exception:
        raise ValueError(f"Failed to parse: {message}")
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

Here is the prompt sent to the model:

```python
query = "Anna is 23 years old and she is 6 feet tall"

print(prompt.format_prompt(query=query).to_string())
```

```output
System: Answer the user query. Output your answer as JSON that  matches the given schema: \`\`\`json
{'title': 'People', 'description': 'Identifying information about all people in a text.', 'type': 'object', 'properties': {'people': {'title': 'People', 'type': 'array', 'items': {'$ref': '#/definitions/Person'}}}, 'required': ['people'], 'definitions': {'Person': {'title': 'Person', 'description': 'Information about a person.', 'type': 'object', 'properties': {'name': {'title': 'Name', 'description': 'The name of the person', 'type': 'string'}, 'height_in_meters': {'title': 'Height In Meters', 'description': 'The height of the person expressed in meters.', 'type': 'number'}}, 'required': ['name', 'height_in_meters']}}}
\`\`\`. Make sure to wrap the answer in \`\`\`json and \`\`\` tags
Human: Anna is 23 years old and she is 6 feet tall
```

And here's what it looks like when we invoke it:

```python
chain = prompt | llm | extract_json

chain.invoke({"query": query})
```

```output
[{'people': [{'name': 'Anna', 'height_in_meters': 1.8288}]}]
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/structured_output.ipynb)

* * *


- [The `.with_structured_output()` method](#the-with_structured_output-method)
  
  - [Pydantic class](#pydantic-class)
  - [TypedDict or JSON Schema](#typeddict-or-json-schema)
  - [Choosing between multiple schemas](#choosing-between-multiple-schemas)
  - [Streaming](#streaming)
  - [Few-shot prompting](#few-shot-prompting)
  - [(Advanced) Specifying the method for structuring outputs](#advanced-specifying-the-method-for-structuring-outputs)
  - [(Advanced) Raw outputs](#advanced-raw-outputs)
- [Prompting and parsing model outputs directly](#prompting-and-parsing-model-outputs-directly)
  
  - [Using `PydanticOutputParser`](#using-pydanticoutputparser)
  - [Custom Parsing](#custom-parsing)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/example_selectors_langsmith.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/example_selectors_langsmith.ipynb)

# How to select examples from a LangSmith dataset

ðŸ“šPrerequisites

- [Chat models](/docs/concepts/chat_models)
- [Few-shot-prompting](/docs/concepts/few-shot-prompting)
- [LangSmith](https://docs.smith.langchain.com/)

ðŸ“¦Compatibility

The code in this guide requires `langsmith>=0.1.101`, `langchain-core>=0.2.34`. Please ensure you have the correct packages installed.

[LangSmith](https://docs.smith.langchain.com/) datasets have built-in support for similarity search, making them a great tool for building and querying few-shot examples.

In this guide we'll see how to use an indexed LangSmith dataset as a few-shot example selector.

## Setup[â€‹](#setup "Direct link to Setup")

Before getting started make sure you've [created a LangSmith account](https://smith.langchain.com/) and set your credentials:

```python
import getpass
import os

if not os.environ.get("LANGSMITH_API_KEY"):
    os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Set LangSmith API key:\n\n")

os.environ["LANGSMITH_TRACING"] = "true"
```

```output
Set LangSmith API key:

Â·Â·Â·Â·Â·Â·Â·Â·
```

We'll need to install the `langsmith` SDK. In this example we'll also make use of `langchain`, `langchain-openai`, and `langchain-benchmarks`:

```python
%pip install -qU "langsmith>=0.1.101" "langchain-core>=0.2.34" langchain langchain-openai langchain-benchmarks
```

Now we'll clone a public dataset and turn on indexing for the dataset. We can also turn on indexing via the [LangSmith UI](https://docs.smith.langchain.com/how_to_guides/datasets/index_datasets_for_dynamic_few_shot_example_selection).

We'll clone the [Multiverse math few shot example dataset](https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/).

This enables searching over the dataset and will make sure that anytime we update/add examples they are also indexed.

```python
from langsmith import Client as LangSmith

ls_client = LangSmith()

dataset_name = "multiverse-math-few-shot-examples-v2"
dataset_public_url = (
    "https://smith.langchain.com/public/620596ee-570b-4d2b-8c8f-f828adbe5242/d"
)

ls_client.clone_public_dataset(dataset_public_url)

dataset_id = ls_client.read_dataset(dataset_name=dataset_name).id

ls_client.index_dataset(dataset_id=dataset_id)
```

## Querying dataset[â€‹](#querying-dataset "Direct link to Querying dataset")

Indexing can take a few seconds. Once the dataset is indexed, we can search for similar examples. Note that the input to the `similar_examples` method must have the same schema as the examples inputs. In this case our example inputs are a dictionary with a "question" key:

```python
examples = ls_client.similar_examples(
    {"question": "whats the negation of the negation of the negation of 3"},
    limit=3,
    dataset_id=dataset_id,
)
len(examples)
```

```output
3
```

```python
examples[0].inputs["question"]
```

```output
'evaluate the negation of -100'
```

For this dataset, the outputs are the conversation that followed the question in OpenAI message format:

```python
examples[0].outputs["conversation"]
```

```output
[{'role': 'assistant',
  'content': None,
  'tool_calls': [{'id': 'toolu_01HTpq4cYNUac6F7omUc2Wz3',
    'type': 'function',
    'function': {'name': 'negate', 'arguments': '{"a": -100}'}}]},
 {'role': 'tool',
  'content': '-100.0',
  'tool_call_id': 'toolu_01HTpq4cYNUac6F7omUc2Wz3'},
 {'role': 'assistant', 'content': 'So the answer is 100.'},
 {'role': 'user',
  'content': '100 is incorrect. Please refer to the output of your tool call.'},
 {'role': 'assistant',
  'content': [{'text': "You're right, my previous answer was incorrect. Let me re-evaluate using the tool output:",
    'type': 'text'}],
  'tool_calls': [{'id': 'toolu_01XsJQboYghGDygQpPjJkeRq',
    'type': 'function',
    'function': {'name': 'negate', 'arguments': '{"a": -100}'}}]},
 {'role': 'tool',
  'content': '-100.0',
  'tool_call_id': 'toolu_01XsJQboYghGDygQpPjJkeRq'},
 {'role': 'assistant', 'content': 'The answer is -100.0'},
 {'role': 'user',
  'content': 'You have the correct numerical answer but are returning additional text. Please only respond with the numerical answer.'},
 {'role': 'assistant', 'content': '-100.0'}]
```

## Creating dynamic few-shot prompts[â€‹](#creating-dynamic-few-shot-prompts "Direct link to Creating dynamic few-shot prompts")

The search returns the examples whose inputs are most similar to the query input. We can use this for few-shot prompting a model like so:

```python
from langchain.chat_models import init_chat_model
from langchain_benchmarks.tool_usage.tasks.multiverse_math import (
    add,
    cos,
    divide,
    log,
    multiply,
    negate,
    pi,
    power,
    sin,
    subtract,
)
from langchain_core.runnables import RunnableLambda
from langsmith import AsyncClient as AsyncLangSmith

async_ls_client = AsyncLangSmith()


def similar_examples(input_: dict) -> dict:
    examples = ls_client.similar_examples(input_, limit=5, dataset_id=dataset_id)
    return {**input_, "examples": examples}


async def asimilar_examples(input_: dict) -> dict:
    examples = await async_ls_client.similar_examples(
        input_, limit=5, dataset_id=dataset_id
    )
    return {**input_, "examples": examples}


def construct_prompt(input_: dict) -> list:
    instructions = """You are great at using mathematical tools."""
    examples = []
    for ex in input_["examples"]:
        examples.append({"role": "user", "content": ex.inputs["question"]})
        for msg in ex.outputs["conversation"]:
            if msg["role"] == "assistant":
                msg["name"] = "example_assistant"
            if msg["role"] == "user":
                msg["name"] = "example_user"
            examples.append(msg)
    return [
        {"role": "system", "content": instructions},
        *examples,
        {"role": "user", "content": input_["question"]},
    ]


tools = [add, cos, divide, log, multiply, negate, pi, power, sin, subtract]
llm = init_chat_model("gpt-4o-2024-08-06")
llm_with_tools = llm.bind_tools(tools)

example_selector = RunnableLambda(func=similar_examples, afunc=asimilar_examples)

chain = example_selector | construct_prompt | llm_with_tools
```

**API Reference:**[init\_chat\_model](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html) | [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)

```python
ai_msg = await chain.ainvoke({"question": "whats the negation of the negation of 3"})
ai_msg.tool_calls
```

```output
[{'name': 'negate',
  'args': {'a': 3},
  'id': 'call_uMSdoTl6ehfHh5a6JQUb2NoZ',
  'type': 'tool_call'}]
```

Looking at the LangSmith trace, we can see that relevant examples were pulled in in the `similar_examples` step and passed as messages to ChatOpenAI: [https://smith.langchain.com/public/9585e30f-765a-4ed9-b964-2211420cd2f8/r/fdea98d6-e90f-49d4-ac22-dfd012e9e0d9](https://smith.langchain.com/public/9585e30f-765a-4ed9-b964-2211420cd2f8/r/fdea98d6-e90f-49d4-ac22-dfd012e9e0d9).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/example_selectors_langsmith.ipynb)

* * *


- [Setup](#setup)
- [Querying dataset](#querying-dataset)
- [Creating dynamic few-shot prompts](#creating-dynamic-few-shot-prompts)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/callbacks.mdx)

# Callbacks

Prerequisites

- [Runnable interface](/docs/concepts/runnables/)

LangChain provides a callback system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.

You can subscribe to these events by using the `callbacks` argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.

## Callback events[â€‹](#callback-events "Direct link to Callback events")

| Event            | Event Trigger                               | Associated Method     |
|------------------|---------------------------------------------|-----------------------|
| Chat model start | When a chat model starts                    | `on_chat_model_start` |
| LLM start        | When a llm starts                           | `on_llm_start`        |
| LLM new token    | When an llm OR chat model emits a new token | `on_llm_new_token`    |
| LLM ends         | When an llm OR chat model ends              | `on_llm_end`          |
| LLM errors       | When an llm OR chat model errors            | `on_llm_error`        |
| Chain start      | When a chain starts running                 | `on_chain_start`      |
| Chain end        | When a chain ends                           | `on_chain_end`        |
| Chain error      | When a chain errors                         | `on_chain_error`      |
| Tool start       | When a tool starts running                  | `on_tool_start`       |
| Tool end         | When a tool ends                            | `on_tool_end`         |
| Tool error       | When a tool errors                          | `on_tool_error`       |
| Agent action     | When an agent takes an action               | `on_agent_action`     |
| Agent finish     | When an agent ends                          | `on_agent_finish`     |
| Retriever start  | When a retriever starts                     | `on_retriever_start`  |
| Retriever end    | When a retriever ends                       | `on_retriever_end`    |
| Retriever error  | When a retriever errors                     | `on_retriever_error`  |
| Text             | When arbitrary text is run                  | `on_text`             |
| Retry            | When a retry event is run                   | `on_retry`            |

## Callback handlers[â€‹](#callback-handlers "Direct link to Callback handlers")

Callback handlers can either be `sync` or `async`:

- Sync callback handlers implement the [BaseCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) interface.
- Async callback handlers implement the [AsyncCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.AsyncCallbackHandler.html) interface.

During run-time LangChain configures an appropriate callback manager (e.g., [CallbackManager](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.CallbackManager.html) or [AsyncCallbackManager](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.AsyncCallbackManager.html) which will be responsible for calling the appropriate method on each "registered" callback handler when the event is triggered.

## Passing callbacks[â€‹](#passing-callbacks "Direct link to Passing callbacks")

The `callbacks` property is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:

- **Request time callbacks**: Passed at the time of the request in addition to the input data. Available on all standard `Runnable` objects. These callbacks are INHERITED by all children of the object they are defined on. For example, `chain.invoke({"number": 25}, {"callbacks": [handler]})`.
- **Constructor callbacks**: `chain = TheNameOfSomeChain(callbacks=[handler])`. These callbacks are passed as arguments to the constructor of the object. The callbacks are scoped only to the object they are defined on, and are **not** inherited by any children of the object.

warning

Constructor callbacks are scoped only to the object they are defined on. They are **not** inherited by children of the object.

If you're creating a custom chain or runnable, you need to remember to propagate request time callbacks to any child objects.

Async in Python&lt;=3.10

Any `RunnableLambda`, a `RunnableGenerator`, or `Tool` that invokes other runnables and is running `async` in python&lt;=3.10, will have to propagate callbacks to child objects manually. This is because LangChain cannot automatically propagate callbacks to child objects in this case.

This is a common reason why you may fail to see events being emitted from custom runnables or tools.

For specifics on how to use callbacks, see the [relevant how-to guides here](/docs/how_to/#callbacks).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/callbacks.mdx)

* * *


- [Callback events](#callback-events)
- [Callback handlers](#callback-handlers)
- [Passing callbacks](#passing-callbacks)









# How to split by character

This is the simplest method. This [splits](/docs/concepts/text_splitters/) based on a given character sequence, which defaults to `"\n\n"`. Chunk length is measured by number of characters.

1. How the text is split: by single character separator.
2. How the chunk size is measured: by number of characters.

To obtain the string content directly, use `.split_text`.

To create LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects (e.g., for use in downstream tasks), use `.create_documents`.

```python
%pip install -qU langchain-text-splitters
```

```python
from langchain_text_splitters import CharacterTextSplitter

# Load an example document
with open("state_of_the_union.txt") as f:
    state_of_the_union = f.read()

text_splitter = CharacterTextSplitter(
    separator="\n\n",
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    is_separator_regex=False,
)
texts = text_splitter.create_documents([state_of_the_union])
print(texts[0])
```

**API Reference:**[CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

```output
page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n\nLast year COVID-19 kept us apart. This year we are finally together again. \n\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n\nWith a duty to one another to the American people to the Constitution. \n\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \n\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n\nHe met the Ukrainian people. \n\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.'
```

Use `.create_documents` to propagate metadata associated with each document to the output chunks:

```python
metadatas = [{"document": 1}, {"document": 2}]
documents = text_splitter.create_documents(
    [state_of_the_union, state_of_the_union], metadatas=metadatas
)
print(documents[0])
```

```output
page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n\nLast year COVID-19 kept us apart. This year we are finally together again. \n\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n\nWith a duty to one another to the American people to the Constitution. \n\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \n\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n\nHe met the Ukrainian people. \n\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.' metadata={'document': 1}
```

Use `.split_text` to obtain the string content directly:

```python
text_splitter.split_text(state_of_the_union)[0]
```

```output
'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n\nLast year COVID-19 kept us apart. This year we are finally together again. \n\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n\nWith a duty to one another to the American people to the Constitution. \n\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \n\nSix days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n\nHe met the Ukrainian people. \n\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.'
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/character_text_splitter.ipynb)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_csv.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/document_loader_csv.ipynb)

# How to load CSVs

A [comma-separated values (CSV)](https://en.wikipedia.org/wiki/Comma-separated_values) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.

LangChain implements a [CSV Loader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.csv_loader.CSVLoader.html) that will load CSV files into a sequence of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects. Each row of the CSV file is translated to one document.

```python
from langchain_community.document_loaders.csv_loader import CSVLoader

file_path = "../integrations/document_loaders/example_data/mlb_teams_2012.csv"

loader = CSVLoader(file_path=file_path)
data = loader.load()

for record in data[:2]:
    print(record)
```

**API Reference:**[CSVLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.csv_loader.CSVLoader.html)

```output
page_content='Team: Nationals\n"Payroll (millions)": 81.34\n"Wins": 98' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 0}
page_content='Team: Reds\n"Payroll (millions)": 82.20\n"Wins": 97' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 1}
```

## Customizing the CSV parsing and loading[â€‹](#customizing-the-csv-parsing-and-loading "Direct link to Customizing the CSV parsing and loading")

`CSVLoader` will accept a `csv_args` kwarg that supports customization of arguments passed to Python's `csv.DictReader`. See the [csv module](https://docs.python.org/3/library/csv.html) documentation for more information of what csv args are supported.

```python
loader = CSVLoader(
    file_path=file_path,
    csv_args={
        "delimiter": ",",
        "quotechar": '"',
        "fieldnames": ["MLB Team", "Payroll in millions", "Wins"],
    },
)

data = loader.load()
for record in data[:2]:
    print(record)
```

```output
page_content='MLB Team: Team\nPayroll in millions: "Payroll (millions)"\nWins: "Wins"' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 0}
page_content='MLB Team: Nationals\nPayroll in millions: 81.34\nWins: 98' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 1}
```

## Specify a column to identify the document source[â€‹](#specify-a-column-to-identify-the-document-source "Direct link to Specify a column to identify the document source")

The `"source"` key on [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) metadata can be set using a column of the CSV. Use the `source_column` argument to specify a source for the document created from each row. Otherwise `file_path` will be used as the source for all documents created from the CSV file.

This is useful when using documents loaded from CSV files for chains that answer questions using sources.

```python
loader = CSVLoader(file_path=file_path, source_column="Team")

data = loader.load()
for record in data[:2]:
    print(record)
```

```output
page_content='Team: Nationals\n"Payroll (millions)": 81.34\n"Wins": 98' metadata={'source': 'Nationals', 'row': 0}
page_content='Team: Reds\n"Payroll (millions)": 82.20\n"Wins": 97' metadata={'source': 'Reds', 'row': 1}
```

## Load from a string[â€‹](#load-from-a-string "Direct link to Load from a string")

Python's `tempfile` can be used when working with CSV strings directly.

```python
import tempfile
from io import StringIO

string_data = """
"Team", "Payroll (millions)", "Wins"
"Nationals",     81.34, 98
"Reds",          82.20, 97
"Yankees",      197.96, 95
"Giants",       117.62, 94
""".strip()


with tempfile.NamedTemporaryFile(delete=False, mode="w+") as temp_file:
    temp_file.write(string_data)
    temp_file_path = temp_file.name

loader = CSVLoader(file_path=temp_file_path)
data = loader.load()
for record in data[:2]:
    print(record)
```

```output
page_content='Team: Nationals\n"Payroll (millions)": 81.34\n"Wins": 98' metadata={'source': 'Nationals', 'row': 0}
page_content='Team: Reds\n"Payroll (millions)": 82.20\n"Wins": 97' metadata={'source': 'Reds', 'row': 1}
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/document_loader_csv.ipynb)

* * *


- [Customizing the CSV parsing and loading](#customizing-the-csv-parsing-and-loading)
- [Specify a column to identify the document source](#specify-a-column-to-identify-the-document-source)
- [Load from a string](#load-from-a-string)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/tool_runtime.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/tool_runtime.ipynb)

# How to pass run time values to tools

ðŸ“šPrerequisites

- [Chat models](/docs/concepts/chat_models)
- [LangChain Tools](/docs/concepts/tools)
- [How to create tools](/docs/how_to/custom_tools)
- [How to use a model to call tools](/docs/how_to/tool_calling)

ðŸ“¦Compatibility

The code in this guide requires `langchain-core>=0.2.21`. Please ensure you have the correct packages installed.

You may need to bind values to a [tool](/docs/concepts/tools/) that are only known at runtime. For example, the tool logic may require using the ID of the user who made the request.

Most of the time, such values should not be controlled by the LLM. In fact, allowing the LLM to control the user ID may lead to a security risk.

Instead, the LLM should only control the parameters of the tool that are meant to be controlled by the LLM, while other parameters (such as user ID) should be fixed by the application logic.

This how-to guide shows you how to prevent the model from generating certain tool arguments and injecting them in directly at runtime.

Using with LangGraph

If you're using LangGraph, please refer to [this how-to guide](https://langchain-ai.github.io/langgraph/how-tos/pass-run-time-values-to-tools/) which shows how to create an agent that keeps track of a given user's favorite pets.

We can bind them to chat models as follows:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

## Hiding arguments from the model[â€‹](#hiding-arguments-from-the-model "Direct link to Hiding arguments from the model")

We can use the InjectedToolArg annotation to mark certain parameters of our Tool, like `user_id` as being injected at runtime, meaning they shouldn't be generated by the model

```python
from typing import List

from langchain_core.tools import InjectedToolArg, tool
from typing_extensions import Annotated

user_to_pets = {}


@tool(parse_docstring=True)
def update_favorite_pets(
    pets: List[str], user_id: Annotated[str, InjectedToolArg]
) -> None:
    """Add the list of favorite pets.

    Args:
        pets: List of favorite pets to set.
        user_id: User's ID.
    """
    user_to_pets[user_id] = pets


@tool(parse_docstring=True)
def delete_favorite_pets(user_id: Annotated[str, InjectedToolArg]) -> None:
    """Delete the list of favorite pets.

    Args:
        user_id: User's ID.
    """
    if user_id in user_to_pets:
        del user_to_pets[user_id]


@tool(parse_docstring=True)
def list_favorite_pets(user_id: Annotated[str, InjectedToolArg]) -> None:
    """List favorite pets if any.

    Args:
        user_id: User's ID.
    """
    return user_to_pets.get(user_id, [])
```

**API Reference:**[InjectedToolArg](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.InjectedToolArg.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

If we look at the input schemas for these tools, we'll see that user\_id is still listed:

```python
update_favorite_pets.get_input_schema().schema()
```

```output
{'description': 'Add the list of favorite pets.',
 'properties': {'pets': {'description': 'List of favorite pets to set.',
   'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'},
  'user_id': {'description': "User's ID.",
   'title': 'User Id',
   'type': 'string'}},
 'required': ['pets', 'user_id'],
 'title': 'update_favorite_petsSchema',
 'type': 'object'}
```

But if we look at the tool call schema, which is what is passed to the model for tool-calling, user\_id has been removed:

```python
update_favorite_pets.tool_call_schema.schema()
```

```output
{'description': 'Add the list of favorite pets.',
 'properties': {'pets': {'description': 'List of favorite pets to set.',
   'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'}},
 'required': ['pets'],
 'title': 'update_favorite_pets',
 'type': 'object'}
```

So when we invoke our tool, we need to pass in user\_id:

```python
user_id = "123"
update_favorite_pets.invoke({"pets": ["lizard", "dog"], "user_id": user_id})
print(user_to_pets)
print(list_favorite_pets.invoke({"user_id": user_id}))
```

```output
{'123': ['lizard', 'dog']}
['lizard', 'dog']
```

But when the model calls the tool, no user\_id argument will be generated:

```python
tools = [
    update_favorite_pets,
    delete_favorite_pets,
    list_favorite_pets,
]
llm_with_tools = llm.bind_tools(tools)
ai_msg = llm_with_tools.invoke("my favorite animals are cats and parrots")
ai_msg.tool_calls
```

```output
[{'name': 'update_favorite_pets',
  'args': {'pets': ['cats', 'parrots']},
  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',
  'type': 'tool_call'}]
```

## Injecting arguments at runtime[â€‹](#injecting-arguments-at-runtime "Direct link to Injecting arguments at runtime")

If we want to actually execute our tools using the model-generated tool call, we'll need to inject the user\_id ourselves:

```python
from copy import deepcopy

from langchain_core.runnables import chain


@chain
def inject_user_id(ai_msg):
    tool_calls = []
    for tool_call in ai_msg.tool_calls:
        tool_call_copy = deepcopy(tool_call)
        tool_call_copy["args"]["user_id"] = user_id
        tool_calls.append(tool_call_copy)
    return tool_calls


inject_user_id.invoke(ai_msg)
```

**API Reference:**[chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)

```output
[{'name': 'update_favorite_pets',
  'args': {'pets': ['cats', 'parrots'], 'user_id': '123'},
  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',
  'type': 'tool_call'}]
```

And now we can chain together our model, injection code, and the actual tools to create a tool-executing chain:

```python
tool_map = {tool.name: tool for tool in tools}


@chain
def tool_router(tool_call):
    return tool_map[tool_call["name"]]


chain = llm_with_tools | inject_user_id | tool_router.map()
chain.invoke("my favorite animals are cats and parrots")
```

```output
[ToolMessage(content='null', name='update_favorite_pets', tool_call_id='call_oYCD0THSedHTbwNAY3NW6uUj')]
```

Looking at the user\_to\_pets dict, we can see that it's been updated to include cats and parrots:

```python
user_to_pets
```

```output
{'123': ['cats', 'parrots']}
```

## Other ways of annotating args[â€‹](#other-ways-of-annotating-args "Direct link to Other ways of annotating args")

Here are a few other ways of annotating our tool args:

```python
from langchain_core.tools import BaseTool
from pydantic import BaseModel, Field


class UpdateFavoritePetsSchema(BaseModel):
    """Update list of favorite pets"""

    pets: List[str] = Field(..., description="List of favorite pets to set.")
    user_id: Annotated[str, InjectedToolArg] = Field(..., description="User's ID.")


@tool(args_schema=UpdateFavoritePetsSchema)
def update_favorite_pets(pets, user_id):
    user_to_pets[user_id] = pets


update_favorite_pets.get_input_schema().schema()
```

**API Reference:**[BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html)

```output
{'description': 'Update list of favorite pets',
 'properties': {'pets': {'description': 'List of favorite pets to set.',
   'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'},
  'user_id': {'description': "User's ID.",
   'title': 'User Id',
   'type': 'string'}},
 'required': ['pets', 'user_id'],
 'title': 'UpdateFavoritePetsSchema',
 'type': 'object'}
```

```python
update_favorite_pets.tool_call_schema.schema()
```

```output
{'description': 'Update list of favorite pets',
 'properties': {'pets': {'description': 'List of favorite pets to set.',
   'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'}},
 'required': ['pets'],
 'title': 'update_favorite_pets',
 'type': 'object'}
```

```python
from typing import Optional, Type


class UpdateFavoritePets(BaseTool):
    name: str = "update_favorite_pets"
    description: str = "Update list of favorite pets"
    args_schema: Optional[Type[BaseModel]] = UpdateFavoritePetsSchema

    def _run(self, pets, user_id):
        user_to_pets[user_id] = pets


UpdateFavoritePets().get_input_schema().schema()
```

```output
{'description': 'Update list of favorite pets',
 'properties': {'pets': {'description': 'List of favorite pets to set.',
   'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'},
  'user_id': {'description': "User's ID.",
   'title': 'User Id',
   'type': 'string'}},
 'required': ['pets', 'user_id'],
 'title': 'UpdateFavoritePetsSchema',
 'type': 'object'}
```

```python
UpdateFavoritePets().tool_call_schema.schema()
```

```output
{'description': 'Update list of favorite pets',
 'properties': {'pets': {'description': 'List of favorite pets to set.',
   'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'}},
 'required': ['pets'],
 'title': 'update_favorite_pets',
 'type': 'object'}
```

```python
class UpdateFavoritePets2(BaseTool):
    name: str = "update_favorite_pets"
    description: str = "Update list of favorite pets"

    def _run(self, pets: List[str], user_id: Annotated[str, InjectedToolArg]) -> None:
        user_to_pets[user_id] = pets


UpdateFavoritePets2().get_input_schema().schema()
```

```output
{'description': 'Use the tool.\n\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\nto child implementations to enable tracing.',
 'properties': {'pets': {'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'},
  'user_id': {'title': 'User Id', 'type': 'string'}},
 'required': ['pets', 'user_id'],
 'title': 'update_favorite_petsSchema',
 'type': 'object'}
```

```python
UpdateFavoritePets2().tool_call_schema.schema()
```

```output
{'description': 'Update list of favorite pets',
 'properties': {'pets': {'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'}},
 'required': ['pets'],
 'title': 'update_favorite_pets',
 'type': 'object'}
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tool_runtime.ipynb)

* * *


- [Hiding arguments from the model](#hiding-arguments-from-the-model)
- [Injecting arguments at runtime](#injecting-arguments-at-runtime)
- [Other ways of annotating args](#other-ways-of-annotating-args)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_llm.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_llm.ipynb)

# How to create a custom LLM class

This notebook goes over how to create a custom LLM wrapper, in case you want to use your own LLM or a different wrapper than one that is supported in LangChain.

Wrapping your LLM with the standard `LLM` interface allow you to use your LLM in existing LangChain programs with minimal code modifications.

As an bonus, your LLM will automatically become a LangChain `Runnable` and will benefit from some optimizations out of the box, async support, the `astream_events` API, etc.

caution

You are currently on a page documenting the use of [text completion models](/docs/concepts/text_llms/). Many of the latest and most popular models are [chat completion models](/docs/concepts/chat_models/).

Unless you are specifically using more advanced prompting techniques, you are probably looking for [this page instead](/docs/how_to/custom_chat_model/).

## Implementation[â€‹](#implementation "Direct link to Implementation")

There are only two required things that a custom LLM needs to implement:

| Method      | Description                                                                             |
|-------------|-----------------------------------------------------------------------------------------|
| `_call`     | Takes in a string and some optional stop words, and returns a string. Used by `invoke`. |
| `_llm_type` | A property that returns a string, used for logging purposes only.                       |

Optional implementations:

| Method                | Description                                                                                                        |
|-----------------------|--------------------------------------------------------------------------------------------------------------------|
| `_identifying_params` | Used to help with identifying the model and printing the LLM; should return a dictionary. This is a **@property**. |
| `_acall`              | Provides an async native implementation of `_call`, used by `ainvoke`.                                             |
| `_stream`             | Method to stream the output token by token.                                                                        |
| `_astream`            | Provides an async native implementation of `_stream`; in newer LangChain versions, defaults to `_stream`.          |

Let's implement a simple custom LLM that just returns the first n characters of the input.

```python
from typing import Any, Dict, Iterator, List, Mapping, Optional

from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import GenerationChunk


class CustomLLM(LLM):
    """A custom chat model that echoes the first `n` characters of the input.

    When contributing an implementation to LangChain, carefully document
    the model including the initialization parameters, include
    an example of how to initialize the model and include any relevant
    links to the underlying models documentation or API.

    Example:

        .. code-block:: python

            model = CustomChatModel(n=2)
            result = model.invoke([HumanMessage(content="hello")])
            result = model.batch([[HumanMessage(content="hello")],
                                 [HumanMessage(content="world")]])
    """

    n: int
    """The number of characters from the last message of the prompt to be echoed."""

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Run the LLM on the given input.

        Override this method to implement the LLM logic.

        Args:
            prompt: The prompt to generate from.
            stop: Stop words to use when generating. Model output is cut off at the
                first occurrence of any of the stop substrings.
                If stop tokens are not supported consider raising NotImplementedError.
            run_manager: Callback manager for the run.
            **kwargs: Arbitrary additional keyword arguments. These are usually passed
                to the model provider API call.

        Returns:
            The model output as a string. Actual completions SHOULD NOT include the prompt.
        """
        if stop is not None:
            raise ValueError("stop kwargs are not permitted.")
        return prompt[: self.n]

    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        """Stream the LLM on the given prompt.

        This method should be overridden by subclasses that support streaming.

        If not implemented, the default behavior of calls to stream will be to
        fallback to the non-streaming version of the model and return
        the output as a single chunk.

        Args:
            prompt: The prompt to generate from.
            stop: Stop words to use when generating. Model output is cut off at the
                first occurrence of any of these substrings.
            run_manager: Callback manager for the run.
            **kwargs: Arbitrary additional keyword arguments. These are usually passed
                to the model provider API call.

        Returns:
            An iterator of GenerationChunks.
        """
        for char in prompt[: self.n]:
            chunk = GenerationChunk(text=char)
            if run_manager:
                run_manager.on_llm_new_token(chunk.text, chunk=chunk)

            yield chunk

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Return a dictionary of identifying parameters."""
        return {
            # The model name allows users to specify custom token counting
            # rules in LLM monitoring applications (e.g., in LangSmith users
            # can provide per token pricing for their model and monitor
            # costs for the given LLM.)
            "model_name": "CustomChatModel",
        }

    @property
    def _llm_type(self) -> str:
        """Get the type of language model used by this chat model. Used for logging purposes only."""
        return "custom"
```

**API Reference:**[CallbackManagerForLLMRun](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.CallbackManagerForLLMRun.html) | [LLM](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.LLM.html) | [GenerationChunk](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.generation.GenerationChunk.html)

### Let's test it ðŸ§ª[â€‹](#lets-test-it- "Direct link to Let's test it ðŸ§ª")

This LLM will implement the standard `Runnable` interface of LangChain which many of the LangChain abstractions support!

```python
llm = CustomLLM(n=5)
print(llm)
```

```output
[1mCustomLLM[0m
Params: {'model_name': 'CustomChatModel'}
```

```python
llm.invoke("This is a foobar thing")
```

```output
'This '
```

```python
await llm.ainvoke("world")
```

```output
'world'
```

```python
llm.batch(["woof woof woof", "meow meow meow"])
```

```output
['woof ', 'meow ']
```

```python
await llm.abatch(["woof woof woof", "meow meow meow"])
```

```output
['woof ', 'meow ']
```

```python
async for token in llm.astream("hello"):
    print(token, end="|", flush=True)
```

```output
h|e|l|l|o|
```

Let's confirm that in integrates nicely with other `LangChain` APIs.

```python
from langchain_core.prompts import ChatPromptTemplate
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```python
prompt = ChatPromptTemplate.from_messages(
    [("system", "you are a bot"), ("human", "{input}")]
)
```

```python
llm = CustomLLM(n=7)
chain = prompt | llm
```

```python
idx = 0
async for event in chain.astream_events({"input": "hello there!"}, version="v1"):
    print(event)
    idx += 1
    if idx > 7:
        # Truncate
        break
```

```output
{'event': 'on_chain_start', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}
{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}
{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}, 'output': ChatPromptValue(messages=[SystemMessage(content='you are a bot'), HumanMessage(content='hello there!')])}}
{'event': 'on_llm_start', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'input': {'prompts': ['System: you are a bot\nHuman: hello there!']}}}
{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'S'}}
{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'S'}}
{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'y'}}
{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'y'}}
```

## Contributing[â€‹](#contributing "Direct link to Contributing")

We appreciate all chat model integration contributions.

Here's a checklist to help make sure your contribution gets added to LangChain:

Documentation:

- The model contains doc-strings for all initialization arguments, as these will be surfaced in the [APIReference](https://python.langchain.com/api_reference/langchain/index.html).
- The class doc-string for the model contains a link to the model API if the model is powered by a service.

Tests:

- Add unit or integration tests to the overridden methods. Verify that `invoke`, `ainvoke`, `batch`, `stream` work if you've over-ridden the corresponding code.

Streaming (if you're implementing it):

- Make sure to invoke the `on_llm_new_token` callback
- `on_llm_new_token` is invoked BEFORE yielding the chunk

Stop Token Behavior:

- Stop token should be respected
- Stop token should be INCLUDED as part of the response

Secret API Keys:

- If your model connects to an API it will likely accept API keys as part of its initialization. Use Pydantic's `SecretStr` type for secrets, so they don't get accidentally printed out when folks print the model.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/custom_llm.ipynb)

* * *


- [Implementation](#implementation)
  
  - [Let's test it ðŸ§ª](#lets-test-it-)
- [Contributing](#contributing)








[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/index.mdx)

# Tutorials

New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.

## Get started[â€‹](#get-started "Direct link to Get started")

Familiarize yourself with LangChain's open-source components by building simple applications.

If you're looking to get started with [chat models](/docs/integrations/chat/), [vector stores](/docs/integrations/vectorstores/), or other LangChain components from a specific provider, check out our supported [integrations](/docs/integrations/providers/).

- [Chat models and prompts](/docs/tutorials/llm_chain/): Build a simple LLM application with [prompt templates](/docs/concepts/prompt_templates/) and [chat models](/docs/concepts/chat_models/).
- [Semantic search](/docs/tutorials/retrievers/): Build a semantic search engine over a PDF with [document loaders](/docs/concepts/document_loaders/), [embedding models](/docs/concepts/embedding_models/), and [vector stores](/docs/concepts/vectorstores/).
- [Classification](/docs/tutorials/classification/): Classify text into categories or labels using [chat models](/docs/concepts/chat_models/) with [structured outputs](/docs/concepts/structured_outputs/).
- [Extraction](/docs/tutorials/extraction/): Extract structured data from text and other unstructured media using [chat models](/docs/concepts/chat_models/) and [few-shot examples](/docs/concepts/few_shot_prompting/).

Refer to the [how-to guides](/docs/how_to/) for more detail on using all LangChain components.

## Orchestration[â€‹](#orchestration "Direct link to Orchestration")

Get started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble LangChain components into full-featured applications.

- [Chatbots](/docs/tutorials/chatbot/): Build a chatbot that incorporates memory.
- [Agents](/docs/tutorials/agents/): Build an agent that interacts with external tools.
- [Retrieval Augmented Generation (RAG) Part 1](/docs/tutorials/rag/): Build an application that uses your own documents to inform its responses.
- [Retrieval Augmented Generation (RAG) Part 2](/docs/tutorials/qa_chat_history/): Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.
- [Question-Answering with SQL](/docs/tutorials/sql_qa/): Build a question-answering system that executes SQL queries to inform its responses.
- [Summarization](/docs/tutorials/summarization/): Generate summaries of (potentially long) texts.
- [Question-Answering with Graph Databases](/docs/tutorials/graph/): Build a question-answering system that queries a graph database to inform its responses.

## LangSmith[â€‹](#langsmith "Direct link to LangSmith")

LangSmith allows you to closely trace, monitor and evaluate your LLM application. It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.

LangSmith documentation is hosted on a separate site. You can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/).

### Evaluation[â€‹](#evaluation "Direct link to Evaluation")

LangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:

- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/tutorials/index.mdx)

* * *


- [Get started](#get-started)
- [Orchestration](#orchestration)
- [LangSmith](#langsmith)
  
  - [Evaluation](#evaluation)









# How to select examples by similarity

This object selects [examples](/docs/concepts/example_selectors/) based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.

```python
from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain_openai import OpenAIEmbeddings

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]
```

**API Reference:**[SemanticSimilarityExampleSelector](https://python.langchain.com/api_reference/core/example_selectors/langchain_core.example_selectors.semantic_similarity.SemanticSimilarityExampleSelector.html) | [FewShotPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```python
example_selector = SemanticSimilarityExampleSelector.from_examples(
    # The list of examples available to select from.
    examples,
    # The embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # The VectorStore class that is used to store the embeddings and do a similarity search over.
    Chroma,
    # The number of examples to produce.
    k=1,
)
similar_prompt = FewShotPromptTemplate(
    # We provide an ExampleSelector instead of examples.
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="Give the antonym of every input",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)
```

```python
# Input is a feeling, so should select the happy/sad example
print(similar_prompt.format(adjective="worried"))
```

```output
Give the antonym of every input

Input: happy
Output: sad

Input: worried
Output:
```

```python
# Input is a measurement, so should select the tall/short example
print(similar_prompt.format(adjective="large"))
```

```output
Give the antonym of every input

Input: tall
Output: short

Input: large
Output:
```

```python
# You can add new examples to the SemanticSimilarityExampleSelector as well
similar_prompt.example_selector.add_example(
    {"input": "enthusiastic", "output": "apathetic"}
)
print(similar_prompt.format(adjective="passionate"))
```

```output
Give the antonym of every input

Input: enthusiastic
Output: apathetic

Input: passionate
Output:
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/example_selectors_similarity.ipynb)

* * *









- [LangSmith](https://docs.smith.langchain.com)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [LangChain Hub](https://smith.langchain.com/hub)
- [LangChain JS/TS](https://js.langchain.com)

[v0.3](#)

- [v0.3](/docs/introduction/)
- [v0.2](https://python.langchain.com/v0.2/docs/introduction)
- [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)

[ðŸ’¬](https://chat.langchain.com)

Search

- [Providers](/docs/integrations/providers/)
  
  - [Anthropic](/docs/integrations/providers/anthropic/)
  - [AWS](/docs/integrations/providers/aws/)
  - [Google](/docs/integrations/providers/google/)
  - [Hugging Face](/docs/integrations/providers/huggingface/)
  - [Microsoft](/docs/integrations/providers/microsoft/)
  - [OpenAI](/docs/integrations/providers/openai/)
  - [More](/docs/integrations/providers/all/)
    
    - [Providers](/docs/integrations/providers/)
    - [Abso](/docs/integrations/providers/abso/)
    - [Acreom](/docs/integrations/providers/acreom/)
    - [Activeloop Deep Lake](/docs/integrations/providers/activeloop_deeplake/)
    - [ADS4GPTs](/docs/integrations/providers/ads4gpts/)
    - [Aerospike](/docs/integrations/providers/aerospike/)
    - [AgentQL](/docs/integrations/providers/agentql/)
    - [AI21 Labs](/docs/integrations/providers/ai21/)
    - [Aim](/docs/integrations/providers/aim_tracking/)
    - [AINetwork](/docs/integrations/providers/ainetwork/)
    - [Airbyte](/docs/integrations/providers/airbyte/)
    - [Airtable](/docs/integrations/providers/airtable/)
    - [Alchemy](/docs/integrations/providers/alchemy/)
    - [Aleph Alpha](/docs/integrations/providers/aleph_alpha/)
    - [Alibaba Cloud](/docs/integrations/providers/alibaba_cloud/)
    - [AnalyticDB](/docs/integrations/providers/analyticdb/)
    - [Annoy](/docs/integrations/providers/annoy/)
    - [Anthropic](/docs/integrations/providers/anthropic/)
    - [Anyscale](/docs/integrations/providers/anyscale/)
    - [Apache Software Foundation](/docs/integrations/providers/apache/)
    - [Apache Doris](/docs/integrations/providers/apache_doris/)
    - [Apify](/docs/integrations/providers/apify/)
    - [Apple](/docs/integrations/providers/apple/)
    - [ArangoDB](/docs/integrations/providers/arangodb/)
    - [Arcee](/docs/integrations/providers/arcee/)
    - [ArcGIS](/docs/integrations/providers/arcgis/)
    - [Argilla](/docs/integrations/providers/argilla/)
    - [Arize](/docs/integrations/providers/arize/)
    - [Arthur](/docs/integrations/providers/arthur_tracking/)
    - [Arxiv](/docs/integrations/providers/arxiv/)
    - [Ascend](/docs/integrations/providers/ascend/)
    - [AskNews](/docs/integrations/providers/asknews/)
    - [AssemblyAI](/docs/integrations/providers/assemblyai/)
    - [Astra DB](/docs/integrations/providers/astradb/)
    - [Atlas](/docs/integrations/providers/atlas/)
    - [AwaDB](/docs/integrations/providers/awadb/)
    - [AWS](/docs/integrations/providers/aws/)
    - [AZLyrics](/docs/integrations/providers/azlyrics/)
    - [Azure AI](/docs/integrations/providers/azure_ai/)
    - [BAAI](/docs/integrations/providers/baai/)
    - [Bagel](/docs/integrations/providers/bagel/)
    - [BagelDB](/docs/integrations/providers/bageldb/)
    - [Baichuan](/docs/integrations/providers/baichuan/)
    - [Baidu](/docs/integrations/providers/baidu/)
    - [Banana](/docs/integrations/providers/bananadev/)
    - [Baseten](/docs/integrations/providers/baseten/)
    - [Beam](/docs/integrations/providers/beam/)
    - [Beautiful Soup](/docs/integrations/providers/beautiful_soup/)
    - [BibTeX](/docs/integrations/providers/bibtex/)
    - [BiliBili](/docs/integrations/providers/bilibili/)
    - [Bittensor](/docs/integrations/providers/bittensor/)
    - [Blackboard](/docs/integrations/providers/blackboard/)
    - [bookend.ai](/docs/integrations/providers/bookendai/)
    - [Box](/docs/integrations/providers/box/)
    - [Brave Search](/docs/integrations/providers/brave_search/)
    - [Breebs (Open Knowledge)](/docs/integrations/providers/breebs/)
    - [Browserbase](/docs/integrations/providers/browserbase/)
    - [Browserless](/docs/integrations/providers/browserless/)
    - [ByteDance](/docs/integrations/providers/byte_dance/)
    - [Cassandra](/docs/integrations/providers/cassandra/)
    - [Cerebras](/docs/integrations/providers/cerebras/)
    - [CerebriumAI](/docs/integrations/providers/cerebriumai/)
    - [Chaindesk](/docs/integrations/providers/chaindesk/)
    - [Chroma](/docs/integrations/providers/chroma/)
    - [Clarifai](/docs/integrations/providers/clarifai/)
    - [ClearML](/docs/integrations/providers/clearml_tracking/)
    - [ClickHouse](/docs/integrations/providers/clickhouse/)
    - [ClickUp](/docs/integrations/providers/clickup/)
    - [Cloudflare](/docs/integrations/providers/cloudflare/)
    - [Clova](/docs/integrations/providers/clova/)
    - [CnosDB](/docs/integrations/providers/cnosdb/)
    - [Cognee](/docs/integrations/providers/cognee/)
    - [CogniSwitch](/docs/integrations/providers/cogniswitch/)
    - [Cohere](/docs/integrations/providers/cohere/)
    - [College Confidential](/docs/integrations/providers/college_confidential/)
    - [Comet](/docs/integrations/providers/comet_tracking/)
    - [Confident AI](/docs/integrations/providers/confident/)
    - [Confluence](/docs/integrations/providers/confluence/)
    - [Connery](/docs/integrations/providers/connery/)
    - [Context](/docs/integrations/providers/context/)
    - [Contextual AI](/docs/integrations/providers/contextual/)
    - [Couchbase](/docs/integrations/providers/couchbase/)
    - [Coze](/docs/integrations/providers/coze/)
    - [CrateDB](/docs/integrations/providers/cratedb/)
    - [C Transformers](/docs/integrations/providers/ctransformers/)
    - [CTranslate2](/docs/integrations/providers/ctranslate2/)
    - [Cube](/docs/integrations/providers/cube/)
    - [Dappier](/docs/integrations/providers/dappier/)
    - [DashVector](/docs/integrations/providers/dashvector/)
    - [Databricks](/docs/integrations/providers/databricks/)
    - [Datadog Tracing](/docs/integrations/providers/datadog/)
    - [Datadog Logs](/docs/integrations/providers/datadog_logs/)
    - [DataForSEO](/docs/integrations/providers/dataforseo/)
    - [Dataherald](/docs/integrations/providers/dataherald/)
    - [Dedoc](/docs/integrations/providers/dedoc/)
    - [DeepInfra](/docs/integrations/providers/deepinfra/)
    - [Deeplake](/docs/integrations/providers/deeplake/)
    - [DeepSeek](/docs/integrations/providers/deepseek/)
    - [DeepSparse](/docs/integrations/providers/deepsparse/)
    - [Dell](/docs/integrations/providers/dell/)
    - [Diffbot](/docs/integrations/providers/diffbot/)
    - [DingoDB](/docs/integrations/providers/dingo/)
    - [Discord](/docs/integrations/providers/discord-shikenso/)
    - [Discord (community loader)](/docs/integrations/providers/discord/)
    - [DocArray](/docs/integrations/providers/docarray/)
    - [Docling](/docs/integrations/providers/docling/)
    - [Doctran](/docs/integrations/providers/doctran/)
    - [Docugami](/docs/integrations/providers/docugami/)
    - [Docusaurus](/docs/integrations/providers/docusaurus/)
    - [Dria](/docs/integrations/providers/dria/)
    - [Dropbox](/docs/integrations/providers/dropbox/)
    - [DSPy](/docs/integrations/providers/dspy/)
    - [DuckDB](/docs/integrations/providers/duckdb/)
    - [DuckDuckGo Search](/docs/integrations/providers/duckduckgo_search/)
    - [E2B](/docs/integrations/providers/e2b/)
    - [Eden AI](/docs/integrations/providers/edenai/)
    - [Elasticsearch](/docs/integrations/providers/elasticsearch/)
    - [ElevenLabs](/docs/integrations/providers/elevenlabs/)
    - [Embedchain](/docs/integrations/providers/embedchain/)
    - [Epsilla](/docs/integrations/providers/epsilla/)
    - [Etherscan](/docs/integrations/providers/etherscan/)
    - [Everly AI](/docs/integrations/providers/everlyai/)
    - [EverNote](/docs/integrations/providers/evernote/)
    - [Exa](/docs/integrations/providers/exa_search/)
    - [Facebook - Meta](/docs/integrations/providers/facebook/)
    - [FalkorDB](/docs/integrations/providers/falkordb/)
    - [Fauna](/docs/integrations/providers/fauna/)
    - [Fiddler](/docs/integrations/providers/fiddler/)
    - [Figma](/docs/integrations/providers/figma/)
    - [FireCrawl](/docs/integrations/providers/firecrawl/)
    - [Fireworks AI](/docs/integrations/providers/fireworks/)
    - [Flyte](/docs/integrations/providers/flyte/)
    - [FMP Data (Financial Data Prep)](/docs/integrations/providers/fmp-data/)
    - [Forefront AI](/docs/integrations/providers/forefrontai/)
    - [Friendli AI](/docs/integrations/providers/friendli/)
    - [Smabbler](/docs/integrations/providers/galaxia/)
    - [Geopandas](/docs/integrations/providers/geopandas/)
    - [Git](/docs/integrations/providers/git/)
    - [GitBook](/docs/integrations/providers/gitbook/)
    - [GitHub](/docs/integrations/providers/github/)
    - [GitLab](/docs/integrations/providers/gitlab/)
    - [GOAT](/docs/integrations/providers/goat/)
    - [Golden](/docs/integrations/providers/golden/)
    - [Goodfire](/docs/integrations/providers/goodfire/)
    - [Google](/docs/integrations/providers/google/)
    - [Serper - Google Search API](/docs/integrations/providers/google_serper/)
    - [GooseAI](/docs/integrations/providers/gooseai/)
    - [GPT4All](/docs/integrations/providers/gpt4all/)
    - [Gradient](/docs/integrations/providers/gradient/)
    - [Graph RAG](/docs/integrations/providers/graph_rag/)
    - [Graphsignal](/docs/integrations/providers/graphsignal/)
    - [Grobid](/docs/integrations/providers/grobid/)
    - [Groq](/docs/integrations/providers/groq/)
    - [Gutenberg](/docs/integrations/providers/gutenberg/)
    - [Hacker News](/docs/integrations/providers/hacker_news/)
    - [Hazy Research](/docs/integrations/providers/hazy_research/)
    - [Helicone](/docs/integrations/providers/helicone/)
    - [Hologres](/docs/integrations/providers/hologres/)
    - [HTML to text](/docs/integrations/providers/html2text/)
    - [Huawei](/docs/integrations/providers/huawei/)
    - [Hugging Face](/docs/integrations/providers/huggingface/)
    - [Hyperbrowser](/docs/integrations/providers/hyperbrowser/)
    - [IBM](/docs/integrations/providers/ibm/)
    - [IEIT Systems](/docs/integrations/providers/ieit_systems/)
    - [iFixit](/docs/integrations/providers/ifixit/)
    - [iFlytek](/docs/integrations/providers/iflytek/)
    - [IMSDb](/docs/integrations/providers/imsdb/)
    - [Infinispan VS](/docs/integrations/providers/infinispanvs/)
    - [Infinity](/docs/integrations/providers/infinity/)
    - [Infino](/docs/integrations/providers/infino/)
    - [Intel](/docs/integrations/providers/intel/)
    - [Iugu](/docs/integrations/providers/iugu/)
    - [Jaguar](/docs/integrations/providers/jaguar/)
    - [Javelin AI Gateway](/docs/integrations/providers/javelin_ai_gateway/)
    - [Jenkins](/docs/integrations/providers/jenkins/)
    - [Jina AI](/docs/integrations/providers/jina/)
    - [Johnsnowlabs](/docs/integrations/providers/johnsnowlabs/)
    - [Joplin](/docs/integrations/providers/joplin/)
    - [KDB.AI](/docs/integrations/providers/kdbai/)
    - [Kinetica](/docs/integrations/providers/kinetica/)
    - [KoboldAI](/docs/integrations/providers/koboldai/)
    - [Konko](/docs/integrations/providers/konko/)
    - [KoNLPY](/docs/integrations/providers/konlpy/)
    - [KÃ¹zu](/docs/integrations/providers/kuzu/)
    - [Label Studio](/docs/integrations/providers/labelstudio/)
    - [lakeFS](/docs/integrations/providers/lakefs/)
    - [LanceDB](/docs/integrations/providers/lancedb/)
    - [LangChain Decorators âœ¨](/docs/integrations/providers/langchain_decorators/)
    - [LangFair: Use-Case Level LLM Bias and Fairness Assessments](/docs/integrations/providers/langfair/)
    - [Langfuse ðŸª¢](/docs/integrations/providers/langfuse/)
    - [Lantern](/docs/integrations/providers/lantern/)
    - [Lindorm](/docs/integrations/providers/lindorm/)
    - [Linkup](/docs/integrations/providers/linkup/)
    - [LiteLLM](/docs/integrations/providers/litellm/)
    - [LlamaIndex](/docs/integrations/providers/llama_index/)
    - [Llama.cpp](/docs/integrations/providers/llamacpp/)
    - [LlamaEdge](/docs/integrations/providers/llamaedge/)
    - [llamafile](/docs/integrations/providers/llamafile/)
    - [LLMonitor](/docs/integrations/providers/llmonitor/)
    - [LocalAI](/docs/integrations/providers/localai/)
    - [Log10](/docs/integrations/providers/log10/)
    - [MariaDB](/docs/integrations/providers/mariadb/)
    - [MariTalk](/docs/integrations/providers/maritalk/)
    - [Marqo](/docs/integrations/providers/marqo/)
    - [MediaWikiDump](/docs/integrations/providers/mediawikidump/)
    - [Meilisearch](/docs/integrations/providers/meilisearch/)
    - [Memcached](/docs/integrations/providers/memcached/)
    - [Memgraph](/docs/integrations/providers/memgraph/)
    - [Metal](/docs/integrations/providers/metal/)
    - [Microsoft](/docs/integrations/providers/microsoft/)
    - [Milvus](/docs/integrations/providers/milvus/)
    - [MindsDB](/docs/integrations/providers/mindsdb/)
    - [Minimax](/docs/integrations/providers/minimax/)
    - [MistralAI](/docs/integrations/providers/mistralai/)
    - [MLflow AI Gateway for LLMs](/docs/integrations/providers/mlflow/)
    - [MLflow](/docs/integrations/providers/mlflow_tracking/)
    - [MLX](/docs/integrations/providers/mlx/)
    - [Modal](/docs/integrations/providers/modal/)
    - [ModelScope](/docs/integrations/providers/modelscope/)
    - [Modern Treasury](/docs/integrations/providers/modern_treasury/)
    - [Momento](/docs/integrations/providers/momento/)
    - [MongoDB](/docs/integrations/providers/mongodb/)
    - [MongoDB Atlas](/docs/integrations/providers/mongodb_atlas/)
    - [Motherduck](/docs/integrations/providers/motherduck/)
    - [MotÃ¶rhead](/docs/integrations/providers/motorhead/)
    - [MyScale](/docs/integrations/providers/myscale/)
    - [NAVER](/docs/integrations/providers/naver/)
    - [Neo4j](/docs/integrations/providers/neo4j/)
    - [Netmind](/docs/integrations/providers/netmind/)
    - [Nimble](/docs/integrations/providers/nimble/)
    - [NLPCloud](/docs/integrations/providers/nlpcloud/)
    - [Nomic](/docs/integrations/providers/nomic/)
    - [Notion DB](/docs/integrations/providers/notion/)
    - [Nuclia](/docs/integrations/providers/nuclia/)
    - [NVIDIA](/docs/integrations/providers/nvidia/)
    - [Obsidian](/docs/integrations/providers/obsidian/)
    - [OceanBase](/docs/integrations/providers/oceanbase/)
    - [Oracle Cloud Infrastructure (OCI)](/docs/integrations/providers/oci/)
    - [OctoAI](/docs/integrations/providers/octoai/)
    - [Ollama](/docs/integrations/providers/ollama/)
    - [Ontotext GraphDB](/docs/integrations/providers/ontotext_graphdb/)
    - [OpenAI](/docs/integrations/providers/openai/)
    - [OpenGradient](/docs/integrations/providers/opengradient/)
    - [OpenLLM](/docs/integrations/providers/openllm/)
    - [OpenSearch](/docs/integrations/providers/opensearch/)
    - [OpenWeatherMap](/docs/integrations/providers/openweathermap/)
    - [OracleAI Vector Search](/docs/integrations/providers/oracleai/)
    - [Outline](/docs/integrations/providers/outline/)
    - [Outlines](/docs/integrations/providers/outlines/)
    - [Oxylabs](/docs/integrations/providers/oxylabs/)
    - [Pandas](/docs/integrations/providers/pandas/)
    - [PaymanAI](/docs/integrations/providers/payman-tool/)
    - [Pebblo](/docs/integrations/providers/pebblo/)
    - [Permit](/docs/integrations/providers/permit/)
    - [Perplexity](/docs/integrations/providers/perplexity/)
    - [Petals](/docs/integrations/providers/petals/)
    - [Postgres Embedding](/docs/integrations/providers/pg_embedding/)
    - [PGVector](/docs/integrations/providers/pgvector/)
    - [Pinecone](/docs/integrations/providers/pinecone/)
    - [PipelineAI](/docs/integrations/providers/pipelineai/)
    - [Pipeshift](/docs/integrations/providers/pipeshift/)
    - [Portkey](/docs/integrations/providers/portkey/)
    - [Predibase](/docs/integrations/providers/predibase/)
    - [Prediction Guard](/docs/integrations/providers/predictionguard/)
    - [PremAI](/docs/integrations/providers/premai/)
    - [SWI-Prolog](/docs/integrations/providers/prolog/)
    - [PromptLayer](/docs/integrations/providers/promptlayer/)
    - [Psychic](/docs/integrations/providers/psychic/)
    - [PubMed](/docs/integrations/providers/pubmed/)
    - [PullMd Loader](/docs/integrations/providers/pull-md/)
    - [PygmalionAI](/docs/integrations/providers/pygmalionai/)
    - [PyMuPDF4LLM](/docs/integrations/providers/pymupdf4llm/)
    - [Qdrant](/docs/integrations/providers/qdrant/)
    - [RAGatouille](/docs/integrations/providers/ragatouille/)
    - [rank\_bm25](/docs/integrations/providers/rank_bm25/)
    - [Ray Serve](/docs/integrations/providers/ray_serve/)
    - [Rebuff](/docs/integrations/providers/rebuff/)
    - [Reddit](/docs/integrations/providers/reddit/)
    - [Redis](/docs/integrations/providers/redis/)
    - [Remembrall](/docs/integrations/providers/remembrall/)
    - [Replicate](/docs/integrations/providers/replicate/)
    - [Roam](/docs/integrations/providers/roam/)
    - [Sema4 (fka Robocorp)](/docs/integrations/providers/robocorp/)
    - [Rockset](/docs/integrations/providers/rockset/)
    - [Runhouse](/docs/integrations/providers/runhouse/)
    - [Runpod](/docs/integrations/providers/runpod/)
    - [RWKV-4](/docs/integrations/providers/rwkv/)
    - [Salesforce](/docs/integrations/providers/salesforce/)
    - [Salute Devices](/docs/integrations/providers/salute_devices/)
    - [SambaNova](/docs/integrations/providers/sambanova/)
    - [SAP](/docs/integrations/providers/sap/)
    - [ScrapeGraph AI](/docs/integrations/providers/scrapegraph/)
    - [SearchApi](/docs/integrations/providers/searchapi/)
    - [SearxNG Search API](/docs/integrations/providers/searx/)
    - [SemaDB](/docs/integrations/providers/semadb/)
    - [SerpAPI](/docs/integrations/providers/serpapi/)
    - [Shale Protocol](/docs/integrations/providers/shaleprotocol/)
    - [SingleStore Integration](/docs/integrations/providers/singlestore/)
    - [scikit-learn](/docs/integrations/providers/sklearn/)
    - [Slack](/docs/integrations/providers/slack/)
    - [Snowflake](/docs/integrations/providers/snowflake/)
    - [spaCy](/docs/integrations/providers/spacy/)
    - [Spark](/docs/integrations/providers/spark/)
    - [SparkLLM](/docs/integrations/providers/sparkllm/)
    - [Spreedly](/docs/integrations/providers/spreedly/)
    - [SQLite](/docs/integrations/providers/sqlite/)
    - [Stack Exchange](/docs/integrations/providers/stackexchange/)
    - [StarRocks](/docs/integrations/providers/starrocks/)
    - [StochasticAI](/docs/integrations/providers/stochasticai/)
    - [Streamlit](/docs/integrations/providers/streamlit/)
    - [Stripe](/docs/integrations/providers/stripe/)
    - [Supabase (Postgres)](/docs/integrations/providers/supabase/)
    - [Nebula](/docs/integrations/providers/symblai_nebula/)
    - [Tableau](/docs/integrations/providers/tableau/)
    - [Taiga](/docs/integrations/providers/taiga/)
    - [Tair](/docs/integrations/providers/tair/)
    - [Tavily](/docs/integrations/providers/tavily/)
    - [Telegram](/docs/integrations/providers/telegram/)
    - [Tencent](/docs/integrations/providers/tencent/)
    - [TensorFlow Datasets](/docs/integrations/providers/tensorflow_datasets/)
    - [TiDB](/docs/integrations/providers/tidb/)
    - [TigerGraph](/docs/integrations/providers/tigergraph/)
    - [Tigris](/docs/integrations/providers/tigris/)
    - [Tilores](/docs/integrations/providers/tilores/)
    - [Together AI](/docs/integrations/providers/together/)
    - [2Markdown](/docs/integrations/providers/tomarkdown/)
    - [Transwarp](/docs/integrations/providers/transwarp/)
    - [Trello](/docs/integrations/providers/trello/)
    - [Trubrics](/docs/integrations/providers/trubrics/)
    - [TruLens](/docs/integrations/providers/trulens/)
    - [Twitter](/docs/integrations/providers/twitter/)
    - [Typesense](/docs/integrations/providers/typesense/)
    - [Unstructured](/docs/integrations/providers/unstructured/)
    - [Upstage](/docs/integrations/providers/upstage/)
    - [upstash](/docs/integrations/providers/upstash/)
    - [UpTrain](/docs/integrations/providers/uptrain/)
    - [USearch](/docs/integrations/providers/usearch/)
    - [Valthera](/docs/integrations/providers/valthera/)
    - [VDMS](/docs/integrations/providers/vdms/)
    - [Vearch](/docs/integrations/providers/vearch/)
    - [Vectara](/docs/integrations/providers/vectara/)
    - [Vectorize](/docs/integrations/providers/vectorize/)
    - [Vespa](/docs/integrations/providers/vespa/)
    - [vlite](/docs/integrations/providers/vlite/)
    - [VoyageAI](/docs/integrations/providers/voyageai/)
    - [Weights &amp; Biases](/docs/integrations/providers/wandb/)
    - [Weights &amp; Biases tracing](/docs/integrations/providers/wandb_tracing/)
    - [Weights &amp; Biases tracking](/docs/integrations/providers/wandb_tracking/)
    - [Weather](/docs/integrations/providers/weather/)
    - [Weaviate](/docs/integrations/providers/weaviate/)
    - [WhatsApp](/docs/integrations/providers/whatsapp/)
    - [WhyLabs](/docs/integrations/providers/whylabs_profiling/)
    - [Wikipedia](/docs/integrations/providers/wikipedia/)
    - [Wolfram Alpha](/docs/integrations/providers/wolfram_alpha/)
    - [Writer, Inc.](/docs/integrations/providers/writer/)
    - [xAI](/docs/integrations/providers/xai/)
    - [Xata](/docs/integrations/providers/xata/)
    - [Xorbits Inference (Xinference)](/docs/integrations/providers/xinference/)
    - [Yahoo](/docs/integrations/providers/yahoo/)
    - [Yandex](/docs/integrations/providers/yandex/)
    - [YDB](/docs/integrations/providers/ydb/)
    - [Yeager.ai](/docs/integrations/providers/yeagerai/)
    - [Yellowbrick](/docs/integrations/providers/yellowbrick/)
    - [01.AI](/docs/integrations/providers/yi/)
    - [You](/docs/integrations/providers/you/)
    - [YouTube](/docs/integrations/providers/youtube/)
    - [Zep](/docs/integrations/providers/zep/)
    - [Zhipu AI](/docs/integrations/providers/zhipuai/)
    - [Zilliz](/docs/integrations/providers/zilliz/)
    - [Zotero](/docs/integrations/providers/zotero/)
- [Components](/docs/integrations/components/)
  
  - [Chat models](/docs/integrations/chat/)
    
    - [Chat models](/docs/integrations/chat/)
    - [Abso](/docs/integrations/chat/abso/)
    - [AI21 Labs](/docs/integrations/chat/ai21/)
    - [Alibaba Cloud PAI EAS](/docs/integrations/chat/alibaba_cloud_pai_eas/)
    - [Anthropic](/docs/integrations/chat/anthropic/)
    - [\[Deprecated\] Experimental Anthropic Tools Wrapper](/docs/integrations/chat/anthropic_functions/)
    - [Anyscale](/docs/integrations/chat/anyscale/)
    - [AzureAIChatCompletionsModel](/docs/integrations/chat/azure_ai/)
    - [Azure OpenAI](/docs/integrations/chat/azure_chat_openai/)
    - [Azure ML Endpoint](/docs/integrations/chat/azureml_chat_endpoint/)
    - [Baichuan Chat](/docs/integrations/chat/baichuan/)
    - [Baidu Qianfan](/docs/integrations/chat/baidu_qianfan_endpoint/)
    - [AWS Bedrock](/docs/integrations/chat/bedrock/)
    - [Cerebras](/docs/integrations/chat/cerebras/)
    - [CloudflareWorkersAI](/docs/integrations/chat/cloudflare_workersai/)
    - [Cohere](/docs/integrations/chat/cohere/)
    - [ContextualAI](/docs/integrations/chat/contextual/)
    - [Coze Chat](/docs/integrations/chat/coze/)
    - [Dappier AI](/docs/integrations/chat/dappier/)
    - [Databricks](/docs/integrations/chat/databricks/)
    - [DeepInfra](/docs/integrations/chat/deepinfra/)
    - [DeepSeek](/docs/integrations/chat/deepseek/)
    - [Eden AI](/docs/integrations/chat/edenai/)
    - [Ernie Bot Chat](/docs/integrations/chat/ernie/)
    - [EverlyAI](/docs/integrations/chat/everlyai/)
    - [Fireworks](/docs/integrations/chat/fireworks/)
    - [ChatFriendli](/docs/integrations/chat/friendli/)
    - [GigaChat](/docs/integrations/chat/gigachat/)
    - [Goodfire](/docs/integrations/chat/goodfire/)
    - [Google AI](/docs/integrations/chat/google_generative_ai/)
    - [Google Cloud Vertex AI](/docs/integrations/chat/google_vertex_ai_palm/)
    - [GPTRouter](/docs/integrations/chat/gpt_router/)
    - [Groq](/docs/integrations/chat/groq/)
    - [ChatHuggingFace](/docs/integrations/chat/huggingface/)
    - [IBM watsonx.ai](/docs/integrations/chat/ibm_watsonx/)
    - [JinaChat](/docs/integrations/chat/jinachat/)
    - [Kinetica](/docs/integrations/chat/kinetica/)
    - [Konko](/docs/integrations/chat/konko/)
    - [LiteLLM](/docs/integrations/chat/litellm/)
    - [LiteLLM Router](/docs/integrations/chat/litellm_router/)
    - [Llama 2 Chat](/docs/integrations/chat/llama2_chat/)
    - [Llama API](/docs/integrations/chat/llama_api/)
    - [LlamaEdge](/docs/integrations/chat/llama_edge/)
    - [Llama.cpp](/docs/integrations/chat/llamacpp/)
    - [maritalk](/docs/integrations/chat/maritalk/)
    - [MiniMax](/docs/integrations/chat/minimax/)
    - [MistralAI](/docs/integrations/chat/mistralai/)
    - [MLX](/docs/integrations/chat/mlx/)
    - [ModelScope](/docs/integrations/chat/modelscope_chat_endpoint/)
    - [Moonshot](/docs/integrations/chat/moonshot/)
    - [Naver](/docs/integrations/chat/naver/)
    - [Netmind](/docs/integrations/chat/netmind/)
    - [NVIDIA AI Endpoints](/docs/integrations/chat/nvidia_ai_endpoints/)
    - [ChatOCIModelDeployment](/docs/integrations/chat/oci_data_science/)
    - [OCIGenAI](/docs/integrations/chat/oci_generative_ai/)
    - [ChatOctoAI](/docs/integrations/chat/octoai/)
    - [Ollama](/docs/integrations/chat/ollama/)
    - [OpenAI](/docs/integrations/chat/openai/)
    - [Outlines](/docs/integrations/chat/outlines/)
    - [Perplexity](/docs/integrations/chat/perplexity/)
    - [Pipeshift](/docs/integrations/chat/pipeshift/)
    - [ChatPredictionGuard](/docs/integrations/chat/predictionguard/)
    - [PremAI](/docs/integrations/chat/premai/)
    - [PromptLayer ChatOpenAI](/docs/integrations/chat/promptlayer_chatopenai/)
    - [Qwen QwQ](/docs/integrations/chat/qwq/)
    - [Reka](/docs/integrations/chat/reka/)
    - [RunPod Chat Model](/docs/integrations/chat/runpod/)
    - [SambaNovaCloud](/docs/integrations/chat/sambanova/)
    - [SambaStudio](/docs/integrations/chat/sambastudio/)
    - [ChatSeekrFlow](/docs/integrations/chat/seekrflow/)
    - [Snowflake Cortex](/docs/integrations/chat/snowflake/)
    - [solar](/docs/integrations/chat/solar/)
    - [SparkLLM Chat](/docs/integrations/chat/sparkllm/)
    - [Nebula (Symbl.ai)](/docs/integrations/chat/symblai_nebula/)
    - [Tencent Hunyuan](/docs/integrations/chat/tencent_hunyuan/)
    - [Together](/docs/integrations/chat/together/)
    - [Tongyi Qwen](/docs/integrations/chat/tongyi/)
    - [Upstage](/docs/integrations/chat/upstage/)
    - [vectara](/docs/integrations/chat/vectara/)
    - [vLLM Chat](/docs/integrations/chat/vllm/)
    - [Volc Enging Maas](/docs/integrations/chat/volcengine_maas/)
    - [Chat Writer](/docs/integrations/chat/writer/)
    - [xAI](/docs/integrations/chat/xai/)
    - [Xinference](/docs/integrations/chat/xinference/)
    - [YandexGPT](/docs/integrations/chat/yandex/)
    - [ChatYI](/docs/integrations/chat/yi/)
    - [Yuan2.0](/docs/integrations/chat/yuan2/)
    - [ZHIPU AI](/docs/integrations/chat/zhipuai/)
  - [Retrievers](/docs/integrations/retrievers/)
    
    - [Retrievers](/docs/integrations/retrievers/)
    - [Activeloop Deep Memory](/docs/integrations/retrievers/activeloop/)
    - [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/)
    - [Arcee](/docs/integrations/retrievers/arcee/)
    - [Arxiv](/docs/integrations/retrievers/arxiv/)
    - [AskNews](/docs/integrations/retrievers/asknews/)
    - [Azure AI Search](/docs/integrations/retrievers/azure_ai_search/)
    - [Bedrock (Knowledge Bases)](/docs/integrations/retrievers/bedrock/)
    - [BM25](/docs/integrations/retrievers/bm25/)
    - [Box](/docs/integrations/retrievers/box/)
    - [BREEBS (Open Knowledge)](/docs/integrations/retrievers/breebs/)
    - [Chaindesk](/docs/integrations/retrievers/chaindesk/)
    - [ChatGPT plugin](/docs/integrations/retrievers/chatgpt-plugin/)
    - [Cognee](/docs/integrations/retrievers/cognee/)
    - [Cohere reranker](/docs/integrations/retrievers/cohere-reranker/)
    - [Cohere RAG](/docs/integrations/retrievers/cohere/)
    - [Contextual AI Reranker](/docs/integrations/retrievers/contextual/)
    - [Dappier](/docs/integrations/retrievers/dappier/)
    - [DocArray](/docs/integrations/retrievers/docarray_retriever/)
    - [Dria](/docs/integrations/retrievers/dria_index/)
    - [ElasticSearch BM25](/docs/integrations/retrievers/elastic_search_bm25/)
    - [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/)
    - [Embedchain](/docs/integrations/retrievers/embedchain/)
    - [FlashRank reranker](/docs/integrations/retrievers/flashrank-reranker/)
    - [Fleet AI Context](/docs/integrations/retrievers/fleet_context/)
    - [Galaxia](/docs/integrations/retrievers/galaxia-retriever/)
    - [Google Drive](/docs/integrations/retrievers/google_drive/)
    - [Google Vertex AI Search](/docs/integrations/retrievers/google_vertex_ai_search/)
    - [Graph RAG](/docs/integrations/retrievers/graph_rag/)
    - [IBM watsonx.ai](/docs/integrations/retrievers/ibm_watsonx_ranker/)
    - [JaguarDB Vector Database](/docs/integrations/retrievers/jaguar/)
    - [Kay.ai](/docs/integrations/retrievers/kay/)
    - [Kinetica Vectorstore based Retriever](/docs/integrations/retrievers/kinetica/)
    - [kNN](/docs/integrations/retrievers/knn/)
    - [LinkupSearchRetriever](/docs/integrations/retrievers/linkup_search/)
    - [LLMLingua Document Compressor](/docs/integrations/retrievers/llmlingua/)
    - [LOTR (Merger Retriever)](/docs/integrations/retrievers/merger_retriever/)
    - [Metal](/docs/integrations/retrievers/metal/)
    - [Milvus Hybrid Search](/docs/integrations/retrievers/milvus_hybrid_search/)
    - [NanoPQ (Product Quantization)](/docs/integrations/retrievers/nanopq/)
    - [needle](/docs/integrations/retrievers/needle/)
    - [Nimble](/docs/integrations/retrievers/nimble/)
    - [Outline](/docs/integrations/retrievers/outline/)
    - [Permit](/docs/integrations/retrievers/permit/)
    - [Pinecone Hybrid Search](/docs/integrations/retrievers/pinecone_hybrid_search/)
    - [PubMed](/docs/integrations/retrievers/pubmed/)
    - [Qdrant Sparse Vector](/docs/integrations/retrievers/qdrant-sparse/)
    - [RAGatouille](/docs/integrations/retrievers/ragatouille/)
    - [RePhraseQuery](/docs/integrations/retrievers/re_phrase/)
    - [Rememberizer](/docs/integrations/retrievers/rememberizer/)
    - [SEC filing](/docs/integrations/retrievers/sec_filings/)
    - [Self-querying retrievers](/docs/integrations/retrievers/self_query/)
    - [SVM](/docs/integrations/retrievers/svm/)
    - [TavilySearchAPI](/docs/integrations/retrievers/tavily/)
    - [TF-IDF](/docs/integrations/retrievers/tf_idf/)
    - [\*\*NeuralDB\*\*](/docs/integrations/retrievers/thirdai_neuraldb/)
    - [Vectorize](/docs/integrations/retrievers/vectorize/)
    - [Vespa](/docs/integrations/retrievers/vespa/)
    - [Wikipedia](/docs/integrations/retrievers/wikipedia/)
    - [You.com](/docs/integrations/retrievers/you-retriever/)
    - [Zep Cloud](/docs/integrations/retrievers/zep_cloud_memorystore/)
    - [Zep Open Source](/docs/integrations/retrievers/zep_memorystore/)
    - [Zilliz Cloud Pipeline](/docs/integrations/retrievers/zilliz_cloud_pipeline/)
    - [Zotero](/docs/integrations/retrievers/zotero/)
  - [Tools/Toolkits](/docs/integrations/tools/)
    
    - [Tools](/docs/integrations/tools/)
    - [ADS4GPTs](/docs/integrations/tools/ads4gpts/)
    - [AgentQL](/docs/integrations/tools/agentql/)
    - [AINetwork Toolkit](/docs/integrations/tools/ainetwork/)
    - [Alpha Vantage](/docs/integrations/tools/alpha_vantage/)
    - [Amadeus Toolkit](/docs/integrations/tools/amadeus/)
    - [Apify Actor](/docs/integrations/tools/apify_actors/)
    - [ArXiv](/docs/integrations/tools/arxiv/)
    - [AskNews](/docs/integrations/tools/asknews/)
    - [AWS Lambda](/docs/integrations/tools/awslambda/)
    - [Azure AI Services Toolkit](/docs/integrations/tools/azure_ai_services/)
    - [Azure Cognitive Services Toolkit](/docs/integrations/tools/azure_cognitive_services/)
    - [Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions/)
    - [Shell (bash)](/docs/integrations/tools/bash/)
    - [Bearly Code Interpreter](/docs/integrations/tools/bearly/)
    - [Bing Search](/docs/integrations/tools/bing_search/)
    - [Brave Search](/docs/integrations/tools/brave_search/)
    - [Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database/)
    - [CDP](/docs/integrations/tools/cdp_agentkit/)
    - [ChatGPT Plugins](/docs/integrations/tools/chatgpt_plugins/)
    - [ClickUp Toolkit](/docs/integrations/tools/clickup/)
    - [Cogniswitch Toolkit](/docs/integrations/tools/cogniswitch/)
    - [Connery Toolkit and Tools](/docs/integrations/tools/connery/)
    - [Dall-E Image Generator](/docs/integrations/tools/dalle_image_generator/)
    - [Dappier](/docs/integrations/tools/dappier/)
    - [Databricks Unity Catalog (UC)](/docs/integrations/tools/databricks/)
    - [DataForSEO](/docs/integrations/tools/dataforseo/)
    - [Dataherald](/docs/integrations/tools/dataherald/)
    - [DuckDuckGo Search](/docs/integrations/tools/ddg/)
    - [Discord](/docs/integrations/tools/discord/)
    - [E2B Data Analysis](/docs/integrations/tools/e2b_data_analysis/)
    - [Eden AI](/docs/integrations/tools/edenai_tools/)
    - [ElevenLabs Text2Speech](/docs/integrations/tools/eleven_labs_tts/)
    - [Exa Search](/docs/integrations/tools/exa_search/)
    - [File System](/docs/integrations/tools/filesystem/)
    - [FinancialDatasets Toolkit](/docs/integrations/tools/financial_datasets/)
    - [FMP Data](/docs/integrations/tools/fmp-data/)
    - [Github Toolkit](/docs/integrations/tools/github/)
    - [Gitlab Toolkit](/docs/integrations/tools/gitlab/)
    - [Gmail Toolkit](/docs/integrations/tools/gmail/)
    - [GOAT](/docs/integrations/tools/goat/)
    - [Golden Query](/docs/integrations/tools/golden_query/)
    - [Google Books](/docs/integrations/tools/google_books/)
    - [Google Calendar Toolkit](/docs/integrations/tools/google_calendar/)
    - [Google Cloud Text-to-Speech](/docs/integrations/tools/google_cloud_texttospeech/)
    - [Google Drive](/docs/integrations/tools/google_drive/)
    - [Google Finance](/docs/integrations/tools/google_finance/)
    - [Google Imagen](/docs/integrations/tools/google_imagen/)
    - [Google Jobs](/docs/integrations/tools/google_jobs/)
    - [Google Lens](/docs/integrations/tools/google_lens/)
    - [Google Places](/docs/integrations/tools/google_places/)
    - [Google Scholar](/docs/integrations/tools/google_scholar/)
    - [Google Search](/docs/integrations/tools/google_search/)
    - [Google Serper](/docs/integrations/tools/google_serper/)
    - [Google Trends](/docs/integrations/tools/google_trends/)
    - [Gradio](/docs/integrations/tools/gradio_tools/)
    - [GraphQL](/docs/integrations/tools/graphql/)
    - [HuggingFace Hub Tools](/docs/integrations/tools/huggingface_tools/)
    - [Human as a tool](/docs/integrations/tools/human_tools/)
    - [Hyperbrowser Browser Agent Tools](/docs/integrations/tools/hyperbrowser_browser_agent_tools/)
    - [Hyperbrowser Web Scraping Tools](/docs/integrations/tools/hyperbrowser_web_scraping_tools/)
    - [IBM watsonx.ai](/docs/integrations/tools/ibm_watsonx/)
    - [IFTTT WebHooks](/docs/integrations/tools/ifttt/)
    - [Infobip](/docs/integrations/tools/infobip/)
    - [Ionic Shopping Tool](/docs/integrations/tools/ionic_shopping/)
    - [Jenkins](/docs/integrations/tools/jenkins/)
    - [Jina Search](/docs/integrations/tools/jina_search/)
    - [Jira Toolkit](/docs/integrations/tools/jira/)
    - [JSON Toolkit](/docs/integrations/tools/json/)
    - [Lemon Agent](/docs/integrations/tools/lemonai/)
    - [LinkupSearchTool](/docs/integrations/tools/linkup_search/)
    - [Memgraph](/docs/integrations/tools/memgraph/)
    - [Memorize](/docs/integrations/tools/memorize/)
    - [Mojeek Search](/docs/integrations/tools/mojeek_search/)
    - [MultiOn Toolkit](/docs/integrations/tools/multion/)
    - [NASA Toolkit](/docs/integrations/tools/nasa/)
    - [Naver Search](/docs/integrations/tools/naver_search/)
    - [Nuclia Understanding](/docs/integrations/tools/nuclia/)
    - [NVIDIA Riva: ASR and TTS](/docs/integrations/tools/nvidia_riva/)
    - [Office365 Toolkit](/docs/integrations/tools/office365/)
    - [OpenAPI Toolkit](/docs/integrations/tools/openapi/)
    - [Natural Language API Toolkits](/docs/integrations/tools/openapi_nla/)
    - [OpenGradient](/docs/integrations/tools/opengradient_toolkit/)
    - [OpenWeatherMap](/docs/integrations/tools/openweathermap/)
    - [Oracle AI Vector Search: Generate Summary](/docs/integrations/tools/oracleai/)
    - [Oxylabs](/docs/integrations/tools/oxylabs/)
    - [Pandas Dataframe](/docs/integrations/tools/pandas/)
    - [Passio NutritionAI](/docs/integrations/tools/passio_nutrition_ai/)
    - [PaymanAI](/docs/integrations/tools/payman-tool/)
    - [Permit](/docs/integrations/tools/permit/)
    - [PlayWright Browser Toolkit](/docs/integrations/tools/playwright/)
    - [Polygon IO Toolkit and Tools](/docs/integrations/tools/polygon/)
    - [PowerBI Toolkit](/docs/integrations/tools/powerbi/)
    - [Prolog](/docs/integrations/tools/prolog_tool/)
    - [PubMed](/docs/integrations/tools/pubmed/)
    - [Python REPL](/docs/integrations/tools/python/)
    - [Reddit Search](/docs/integrations/tools/reddit_search/)
    - [Requests Toolkit](/docs/integrations/tools/requests/)
    - [Riza Code Interpreter](/docs/integrations/tools/riza/)
    - [Robocorp Toolkit](/docs/integrations/tools/robocorp/)
    - [Salesforce](/docs/integrations/tools/salesforce/)
    - [SceneXplain](/docs/integrations/tools/sceneXplain/)
    - [ScrapeGraph](/docs/integrations/tools/scrapegraph/)
    - [SearchApi](/docs/integrations/tools/searchapi/)
    - [SearxNG Search](/docs/integrations/tools/searx_search/)
    - [Semantic Scholar API Tool](/docs/integrations/tools/semanticscholar/)
    - [SerpAPI](/docs/integrations/tools/serpapi/)
    - [Slack Toolkit](/docs/integrations/tools/slack/)
    - [Spark SQL Toolkit](/docs/integrations/tools/spark_sql/)
    - [SQLDatabase Toolkit](/docs/integrations/tools/sql_database/)
    - [StackExchange](/docs/integrations/tools/stackexchange/)
    - [Steam Toolkit](/docs/integrations/tools/steam/)
    - [Stripe](/docs/integrations/tools/stripe/)
    - [Tableau](/docs/integrations/tools/tableau/)
    - [Taiga](/docs/integrations/tools/taiga/)
    - [Tavily Extract](/docs/integrations/tools/tavily_extract/)
    - [Tavily Search](/docs/integrations/tools/tavily_search/)
    - [Tilores](/docs/integrations/tools/tilores/)
    - [Twilio](/docs/integrations/tools/twilio/)
    - [Upstage](/docs/integrations/tools/upstage_groundedness_check/)
    - [Valthera](/docs/integrations/tools/valthera/)
    - [Wikidata](/docs/integrations/tools/wikidata/)
    - [Wikipedia](/docs/integrations/tools/wikipedia/)
    - [Wolfram Alpha](/docs/integrations/tools/wolfram_alpha/)
    - [Writer Tools](/docs/integrations/tools/writer/)
    - [Yahoo Finance News](/docs/integrations/tools/yahoo_finance_news/)
    - [You.com Search](/docs/integrations/tools/you/)
    - [YouTube](/docs/integrations/tools/youtube/)
    - [Zapier Natural Language Actions](/docs/integrations/tools/zapier/)
    - [ZenGuard AI](/docs/integrations/tools/zenguard/)
  - [Document loaders](/docs/integrations/document_loaders/)
    
    - [Document loaders](/docs/integrations/document_loaders/)
    - [acreom](/docs/integrations/document_loaders/acreom/)
    - [AgentQLLoader](/docs/integrations/document_loaders/agentql/)
    - [AirbyteLoader](/docs/integrations/document_loaders/airbyte/)
    - [Airbyte CDK (Deprecated)](/docs/integrations/document_loaders/airbyte_cdk/)
    - [Airbyte Gong (Deprecated)](/docs/integrations/document_loaders/airbyte_gong/)
    - [Airbyte Hubspot (Deprecated)](/docs/integrations/document_loaders/airbyte_hubspot/)
    - [Airbyte JSON (Deprecated)](/docs/integrations/document_loaders/airbyte_json/)
    - [Airbyte Salesforce (Deprecated)](/docs/integrations/document_loaders/airbyte_salesforce/)
    - [Airbyte Shopify (Deprecated)](/docs/integrations/document_loaders/airbyte_shopify/)
    - [Airbyte Stripe (Deprecated)](/docs/integrations/document_loaders/airbyte_stripe/)
    - [Airbyte Typeform (Deprecated)](/docs/integrations/document_loaders/airbyte_typeform/)
    - [Airbyte Zendesk Support (Deprecated)](/docs/integrations/document_loaders/airbyte_zendesk_support/)
    - [Airtable](/docs/integrations/document_loaders/airtable/)
    - [Alibaba Cloud MaxCompute](/docs/integrations/document_loaders/alibaba_cloud_maxcompute/)
    - [Amazon Textract](/docs/integrations/document_loaders/amazon_textract/)
    - [Apify Dataset](/docs/integrations/document_loaders/apify_dataset/)
    - [ArcGIS](/docs/integrations/document_loaders/arcgis/)
    - [ArxivLoader](/docs/integrations/document_loaders/arxiv/)
    - [AssemblyAI Audio Transcripts](/docs/integrations/document_loaders/assemblyai/)
    - [AstraDB](/docs/integrations/document_loaders/astradb/)
    - [Async Chromium](/docs/integrations/document_loaders/async_chromium/)
    - [AsyncHtml](/docs/integrations/document_loaders/async_html/)
    - [Athena](/docs/integrations/document_loaders/athena/)
    - [AWS S3 Directory](/docs/integrations/document_loaders/aws_s3_directory/)
    - [AWS S3 File](/docs/integrations/document_loaders/aws_s3_file/)
    - [AZLyrics](/docs/integrations/document_loaders/azlyrics/)
    - [Azure AI Data](/docs/integrations/document_loaders/azure_ai_data/)
    - [Azure Blob Storage Container](/docs/integrations/document_loaders/azure_blob_storage_container/)
    - [Azure Blob Storage File](/docs/integrations/document_loaders/azure_blob_storage_file/)
    - [Azure AI Document Intelligence](/docs/integrations/document_loaders/azure_document_intelligence/)
    - [BibTeX](/docs/integrations/document_loaders/bibtex/)
    - [BiliBili](/docs/integrations/document_loaders/bilibili/)
    - [Blackboard](/docs/integrations/document_loaders/blackboard/)
    - [Blockchain](/docs/integrations/document_loaders/blockchain/)
    - [Box](/docs/integrations/document_loaders/box/)
    - [Brave Search](/docs/integrations/document_loaders/brave_search/)
    - [Browserbase](/docs/integrations/document_loaders/browserbase/)
    - [Browserless](/docs/integrations/document_loaders/browserless/)
    - [BSHTMLLoader](/docs/integrations/document_loaders/bshtml/)
    - [Cassandra](/docs/integrations/document_loaders/cassandra/)
    - [ChatGPT Data](/docs/integrations/document_loaders/chatgpt_loader/)
    - [College Confidential](/docs/integrations/document_loaders/college_confidential/)
    - [Concurrent Loader](/docs/integrations/document_loaders/concurrent/)
    - [Confluence](/docs/integrations/document_loaders/confluence/)
    - [CoNLL-U](/docs/integrations/document_loaders/conll-u/)
    - [Copy Paste](/docs/integrations/document_loaders/copypaste/)
    - [Couchbase](/docs/integrations/document_loaders/couchbase/)
    - [CSV](/docs/integrations/document_loaders/csv/)
    - [Cube Semantic Layer](/docs/integrations/document_loaders/cube_semantic/)
    - [Datadog Logs](/docs/integrations/document_loaders/datadog_logs/)
    - [Dedoc](/docs/integrations/document_loaders/dedoc/)
    - [Diffbot](/docs/integrations/document_loaders/diffbot/)
    - [Discord](/docs/integrations/document_loaders/discord/)
    - [Docling](/docs/integrations/document_loaders/docling/)
    - [Docugami](/docs/integrations/document_loaders/docugami/)
    - [Docusaurus](/docs/integrations/document_loaders/docusaurus/)
    - [Dropbox](/docs/integrations/document_loaders/dropbox/)
    - [DuckDB](/docs/integrations/document_loaders/duckdb/)
    - [Email](/docs/integrations/document_loaders/email/)
    - [EPub](/docs/integrations/document_loaders/epub/)
    - [Etherscan](/docs/integrations/document_loaders/etherscan/)
    - [EverNote](/docs/integrations/document_loaders/evernote/)
    - [example\_data](/docs/integrations/document_loaders/example_data/example/)
    - [Facebook Chat](/docs/integrations/document_loaders/facebook_chat/)
    - [Fauna](/docs/integrations/document_loaders/fauna/)
    - [Figma](/docs/integrations/document_loaders/figma/)
    - [FireCrawl](/docs/integrations/document_loaders/firecrawl/)
    - [Geopandas](/docs/integrations/document_loaders/geopandas/)
    - [Git](/docs/integrations/document_loaders/git/)
    - [GitBook](/docs/integrations/document_loaders/gitbook/)
    - [GitHub](/docs/integrations/document_loaders/github/)
    - [Glue Catalog](/docs/integrations/document_loaders/glue_catalog/)
    - [Google AlloyDB for PostgreSQL](/docs/integrations/document_loaders/google_alloydb/)
    - [Google BigQuery](/docs/integrations/document_loaders/google_bigquery/)
    - [Google Bigtable](/docs/integrations/document_loaders/google_bigtable/)
    - [Google Cloud SQL for SQL server](/docs/integrations/document_loaders/google_cloud_sql_mssql/)
    - [Google Cloud SQL for MySQL](/docs/integrations/document_loaders/google_cloud_sql_mysql/)
    - [Google Cloud SQL for PostgreSQL](/docs/integrations/document_loaders/google_cloud_sql_pg/)
    - [Google Cloud Storage Directory](/docs/integrations/document_loaders/google_cloud_storage_directory/)
    - [Google Cloud Storage File](/docs/integrations/document_loaders/google_cloud_storage_file/)
    - [Google Firestore in Datastore Mode](/docs/integrations/document_loaders/google_datastore/)
    - [Google Drive](/docs/integrations/document_loaders/google_drive/)
    - [Google El Carro for Oracle Workloads](/docs/integrations/document_loaders/google_el_carro/)
    - [Google Firestore (Native Mode)](/docs/integrations/document_loaders/google_firestore/)
    - [Google Memorystore for Redis](/docs/integrations/document_loaders/google_memorystore_redis/)
    - [Google Spanner](/docs/integrations/document_loaders/google_spanner/)
    - [Google Speech-to-Text Audio Transcripts](/docs/integrations/document_loaders/google_speech_to_text/)
    - [Grobid](/docs/integrations/document_loaders/grobid/)
    - [Gutenberg](/docs/integrations/document_loaders/gutenberg/)
    - [Hacker News](/docs/integrations/document_loaders/hacker_news/)
    - [Huawei OBS Directory](/docs/integrations/document_loaders/huawei_obs_directory/)
    - [Huawei OBS File](/docs/integrations/document_loaders/huawei_obs_file/)
    - [HuggingFace dataset](/docs/integrations/document_loaders/hugging_face_dataset/)
    - [HyperbrowserLoader](/docs/integrations/document_loaders/hyperbrowser/)
    - [iFixit](/docs/integrations/document_loaders/ifixit/)
    - [Images](/docs/integrations/document_loaders/image/)
    - [Image captions](/docs/integrations/document_loaders/image_captions/)
    - [IMSDb](/docs/integrations/document_loaders/imsdb/)
    - [Iugu](/docs/integrations/document_loaders/iugu/)
    - [Joplin](/docs/integrations/document_loaders/joplin/)
    - [JSONLoader](/docs/integrations/document_loaders/json/)
    - [Jupyter Notebook](/docs/integrations/document_loaders/jupyter_notebook/)
    - [Kinetica](/docs/integrations/document_loaders/kinetica/)
    - [lakeFS](/docs/integrations/document_loaders/lakefs/)
    - [LangSmith](/docs/integrations/document_loaders/langsmith/)
    - [LarkSuite (FeiShu)](/docs/integrations/document_loaders/larksuite/)
    - [LLM Sherpa](/docs/integrations/document_loaders/llmsherpa/)
    - [Mastodon](/docs/integrations/document_loaders/mastodon/)
    - [MathPixPDFLoader](/docs/integrations/document_loaders/mathpix/)
    - [MediaWiki Dump](/docs/integrations/document_loaders/mediawikidump/)
    - [Merge Documents Loader](/docs/integrations/document_loaders/merge_doc/)
    - [mhtml](/docs/integrations/document_loaders/mhtml/)
    - [Microsoft Excel](/docs/integrations/document_loaders/microsoft_excel/)
    - [Microsoft OneDrive](/docs/integrations/document_loaders/microsoft_onedrive/)
    - [Microsoft OneNote](/docs/integrations/document_loaders/microsoft_onenote/)
    - [Microsoft PowerPoint](/docs/integrations/document_loaders/microsoft_powerpoint/)
    - [Microsoft SharePoint](/docs/integrations/document_loaders/microsoft_sharepoint/)
    - [Microsoft Word](/docs/integrations/document_loaders/microsoft_word/)
    - [Near Blockchain](/docs/integrations/document_loaders/mintbase/)
    - [Modern Treasury](/docs/integrations/document_loaders/modern_treasury/)
    - [MongoDB](/docs/integrations/document_loaders/mongodb/)
    - [Needle Document Loader](/docs/integrations/document_loaders/needle/)
    - [News URL](/docs/integrations/document_loaders/news/)
    - [Notion DB 2/2](/docs/integrations/document_loaders/notion/)
    - [Nuclia](/docs/integrations/document_loaders/nuclia/)
    - [Obsidian](/docs/integrations/document_loaders/obsidian/)
    - [Open Document Format (ODT)](/docs/integrations/document_loaders/odt/)
    - [Open City Data](/docs/integrations/document_loaders/open_city_data/)
    - [Oracle Autonomous Database](/docs/integrations/document_loaders/oracleadb_loader/)
    - [Oracle AI Vector Search: Document Processing](/docs/integrations/document_loaders/oracleai/)
    - [Org-mode](/docs/integrations/document_loaders/org_mode/)
    - [Pandas DataFrame](/docs/integrations/document_loaders/pandas_dataframe/)
    - [parsers](/docs/integrations/document_loaders/parsers/azure_openai_whisper_parser/)
    - [PDFMinerLoader](/docs/integrations/document_loaders/pdfminer/)
    - [PDFPlumber](/docs/integrations/document_loaders/pdfplumber/)
    - [Pebblo Safe DocumentLoader](/docs/integrations/document_loaders/pebblo/)
    - [Polars DataFrame](/docs/integrations/document_loaders/polars_dataframe/)
    - [Dell PowerScale Document Loader](/docs/integrations/document_loaders/powerscale/)
    - [Psychic](/docs/integrations/document_loaders/psychic/)
    - [PubMed](/docs/integrations/document_loaders/pubmed/)
    - [PullMdLoader](/docs/integrations/document_loaders/pull_md/)
    - [PyMuPDFLoader](/docs/integrations/document_loaders/pymupdf/)
    - [PyMuPDF4LLM](/docs/integrations/document_loaders/pymupdf4llm/)
    - [PyPDFDirectoryLoader](/docs/integrations/document_loaders/pypdfdirectory/)
    - [PyPDFium2Loader](/docs/integrations/document_loaders/pypdfium2/)
    - [PyPDFLoader](/docs/integrations/document_loaders/pypdfloader/)
    - [PySpark](/docs/integrations/document_loaders/pyspark_dataframe/)
    - [Quip](/docs/integrations/document_loaders/quip/)
    - [ReadTheDocs Documentation](/docs/integrations/document_loaders/readthedocs_documentation/)
    - [Recursive URL](/docs/integrations/document_loaders/recursive_url/)
    - [Reddit](/docs/integrations/document_loaders/reddit/)
    - [Roam](/docs/integrations/document_loaders/roam/)
    - [Rockset](/docs/integrations/document_loaders/rockset/)
    - [rspace](/docs/integrations/document_loaders/rspace/)
    - [RSS Feeds](/docs/integrations/document_loaders/rss/)
    - [RST](/docs/integrations/document_loaders/rst/)
    - [scrapfly](/docs/integrations/document_loaders/scrapfly/)
    - [ScrapingAnt](/docs/integrations/document_loaders/scrapingant/)
    - [SingleStore](/docs/integrations/document_loaders/singlestore/)
    - [Sitemap](/docs/integrations/document_loaders/sitemap/)
    - [Slack](/docs/integrations/document_loaders/slack/)
    - [Snowflake](/docs/integrations/document_loaders/snowflake/)
    - [Source Code](/docs/integrations/document_loaders/source_code/)
    - [Spider](/docs/integrations/document_loaders/spider/)
    - [Spreedly](/docs/integrations/document_loaders/spreedly/)
    - [Stripe](/docs/integrations/document_loaders/stripe/)
    - [Subtitle](/docs/integrations/document_loaders/subtitle/)
    - [SurrealDB](/docs/integrations/document_loaders/surrealdb/)
    - [Telegram](/docs/integrations/document_loaders/telegram/)
    - [Tencent COS Directory](/docs/integrations/document_loaders/tencent_cos_directory/)
    - [Tencent COS File](/docs/integrations/document_loaders/tencent_cos_file/)
    - [TensorFlow Datasets](/docs/integrations/document_loaders/tensorflow_datasets/)
    - [TiDB](/docs/integrations/document_loaders/tidb/)
    - [2Markdown](/docs/integrations/document_loaders/tomarkdown/)
    - [TOML](/docs/integrations/document_loaders/toml/)
    - [Trello](/docs/integrations/document_loaders/trello/)
    - [TSV](/docs/integrations/document_loaders/tsv/)
    - [Twitter](/docs/integrations/document_loaders/twitter/)
    - [Unstructured](/docs/integrations/document_loaders/unstructured_file/)
    - [UnstructuredMarkdownLoader](/docs/integrations/document_loaders/unstructured_markdown/)
    - [UnstructuredPDFLoader](/docs/integrations/document_loaders/unstructured_pdfloader/)
    - [Upstage](/docs/integrations/document_loaders/upstage/)
    - [URL](/docs/integrations/document_loaders/url/)
    - [Vsdx](/docs/integrations/document_loaders/vsdx/)
    - [Weather](/docs/integrations/document_loaders/weather/)
    - [WebBaseLoader](/docs/integrations/document_loaders/web_base/)
    - [WhatsApp Chat](/docs/integrations/document_loaders/whatsapp_chat/)
    - [Wikipedia](/docs/integrations/document_loaders/wikipedia/)
    - [UnstructuredXMLLoader](/docs/integrations/document_loaders/xml/)
    - [Xorbits Pandas DataFrame](/docs/integrations/document_loaders/xorbits/)
    - [YouTube audio](/docs/integrations/document_loaders/youtube_audio/)
    - [YouTube transcripts](/docs/integrations/document_loaders/youtube_transcript/)
    - [YoutubeLoaderDL](/docs/integrations/document_loaders/yt_dlp/)
    - [Yuque](/docs/integrations/document_loaders/yuque/)
    - [ZeroxPDFLoader](/docs/integrations/document_loaders/zeroxpdfloader/)
  - [Vector stores](/docs/integrations/vectorstores/)
    
    - [Vector stores](/docs/integrations/vectorstores/)
    - [Activeloop Deep Lake](/docs/integrations/vectorstores/activeloop_deeplake/)
    - [Aerospike](/docs/integrations/vectorstores/aerospike/)
    - [Alibaba Cloud OpenSearch](/docs/integrations/vectorstores/alibabacloud_opensearch/)
    - [AnalyticDB](/docs/integrations/vectorstores/analyticdb/)
    - [Annoy](/docs/integrations/vectorstores/annoy/)
    - [Apache Doris](/docs/integrations/vectorstores/apache_doris/)
    - [ApertureDB](/docs/integrations/vectorstores/aperturedb/)
    - [Astra DB Vector Store](/docs/integrations/vectorstores/astradb/)
    - [Atlas](/docs/integrations/vectorstores/atlas/)
    - [AwaDB](/docs/integrations/vectorstores/awadb/)
    - [Azure Cosmos DB Mongo vCore](/docs/integrations/vectorstores/azure_cosmos_db/)
    - [Azure Cosmos DB No SQL](/docs/integrations/vectorstores/azure_cosmos_db_no_sql/)
    - [Azure AI Search](/docs/integrations/vectorstores/azuresearch/)
    - [Bagel](/docs/integrations/vectorstores/bagel/)
    - [BagelDB](/docs/integrations/vectorstores/bageldb/)
    - [Baidu Cloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/baiducloud_vector_search/)
    - [Baidu VectorDB](/docs/integrations/vectorstores/baiduvectordb/)
    - [Apache Cassandra](/docs/integrations/vectorstores/cassandra/)
    - [Chroma](/docs/integrations/vectorstores/chroma/)
    - [Clarifai](/docs/integrations/vectorstores/clarifai/)
    - [ClickHouse](/docs/integrations/vectorstores/clickhouse/)
    - [CloudflareVectorize](/docs/integrations/vectorstores/cloudflare_vectorize/)
    - [Couchbase](/docs/integrations/vectorstores/couchbase/)
    - [DashVector](/docs/integrations/vectorstores/dashvector/)
    - [Databricks](/docs/integrations/vectorstores/databricks_vector_search/)
    - [DingoDB](/docs/integrations/vectorstores/dingo/)
    - [DocArray HnswSearch](/docs/integrations/vectorstores/docarray_hnsw/)
    - [DocArray InMemorySearch](/docs/integrations/vectorstores/docarray_in_memory/)
    - [Amazon Document DB](/docs/integrations/vectorstores/documentdb/)
    - [DuckDB](/docs/integrations/vectorstores/duckdb/)
    - [China Mobile ECloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/ecloud_vector_search/)
    - [Elasticsearch](/docs/integrations/vectorstores/elasticsearch/)
    - [Epsilla](/docs/integrations/vectorstores/epsilla/)
    - [Faiss](/docs/integrations/vectorstores/faiss/)
    - [Faiss (Async)](/docs/integrations/vectorstores/faiss_async/)
    - [FalkorDBVectorStore](/docs/integrations/vectorstores/falkordbvector/)
    - [Google AlloyDB for PostgreSQL](/docs/integrations/vectorstores/google_alloydb/)
    - [Google BigQuery Vector Search](/docs/integrations/vectorstores/google_bigquery_vector_search/)
    - [Google Cloud SQL for MySQL](/docs/integrations/vectorstores/google_cloud_sql_mysql/)
    - [Google Cloud SQL for PostgreSQL](/docs/integrations/vectorstores/google_cloud_sql_pg/)
    - [Firestore](/docs/integrations/vectorstores/google_firestore/)
    - [Google Memorystore for Redis](/docs/integrations/vectorstores/google_memorystore_redis/)
    - [Google Spanner](/docs/integrations/vectorstores/google_spanner/)
    - [Google Vertex AI Feature Store](/docs/integrations/vectorstores/google_vertex_ai_feature_store/)
    - [Google Vertex AI Vector Search](/docs/integrations/vectorstores/google_vertex_ai_vector_search/)
    - [Hippo](/docs/integrations/vectorstores/hippo/)
    - [Hologres](/docs/integrations/vectorstores/hologres/)
    - [Infinispan](/docs/integrations/vectorstores/infinispanvs/)
    - [Jaguar Vector Database](/docs/integrations/vectorstores/jaguar/)
    - [KDB.AI](/docs/integrations/vectorstores/kdbai/)
    - [Kinetica](/docs/integrations/vectorstores/kinetica/)
    - [LanceDB](/docs/integrations/vectorstores/lancedb/)
    - [Lantern](/docs/integrations/vectorstores/lantern/)
    - [Lindorm](/docs/integrations/vectorstores/lindorm/)
    - [LLMRails](/docs/integrations/vectorstores/llm_rails/)
    - [ManticoreSearch VectorStore](/docs/integrations/vectorstores/manticore_search/)
    - [MariaDB](/docs/integrations/vectorstores/mariadb/)
    - [Marqo](/docs/integrations/vectorstores/marqo/)
    - [Meilisearch](/docs/integrations/vectorstores/meilisearch/)
    - [Amazon MemoryDB](/docs/integrations/vectorstores/memorydb/)
    - [Milvus](/docs/integrations/vectorstores/milvus/)
    - [Momento Vector Index (MVI)](/docs/integrations/vectorstores/momento_vector_index/)
    - [MongoDB Atlas](/docs/integrations/vectorstores/mongodb_atlas/)
    - [MyScale](/docs/integrations/vectorstores/myscale/)
    - [Neo4j Vector Index](/docs/integrations/vectorstores/neo4jvector/)
    - [NucliaDB](/docs/integrations/vectorstores/nucliadb/)
    - [Oceanbase](/docs/integrations/vectorstores/oceanbase/)
    - [openGauss](/docs/integrations/vectorstores/opengauss/)
    - [OpenSearch](/docs/integrations/vectorstores/opensearch/)
    - [Oracle AI Vector Search: Vector Store](/docs/integrations/vectorstores/oracle/)
    - [Pathway](/docs/integrations/vectorstores/pathway/)
    - [Postgres Embedding](/docs/integrations/vectorstores/pgembedding/)
    - [PGVecto.rs](/docs/integrations/vectorstores/pgvecto_rs/)
    - [PGVector](/docs/integrations/vectorstores/pgvector/)
    - [Pinecone](/docs/integrations/vectorstores/pinecone/)
    - [Qdrant](/docs/integrations/vectorstores/qdrant/)
    - [Redis](/docs/integrations/vectorstores/redis/)
    - [Relyt](/docs/integrations/vectorstores/relyt/)
    - [Rockset](/docs/integrations/vectorstores/rockset/)
    - [SAP HANA Cloud Vector Engine](/docs/integrations/vectorstores/sap_hanavector/)
    - [ScaNN](/docs/integrations/vectorstores/scann/)
    - [SemaDB](/docs/integrations/vectorstores/semadb/)
    - [SingleStore](/docs/integrations/vectorstores/singlestore/)
    - [scikit-learn](/docs/integrations/vectorstores/sklearn/)
    - [SQLiteVec](/docs/integrations/vectorstores/sqlitevec/)
    - [SQLite-VSS](/docs/integrations/vectorstores/sqlitevss/)
    - [SQLServer](/docs/integrations/vectorstores/sqlserver/)
    - [StarRocks](/docs/integrations/vectorstores/starrocks/)
    - [Supabase (Postgres)](/docs/integrations/vectorstores/supabase/)
    - [SurrealDB](/docs/integrations/vectorstores/surrealdb/)
    - [Tablestore](/docs/integrations/vectorstores/tablestore/)
    - [Tair](/docs/integrations/vectorstores/tair/)
    - [Tencent Cloud VectorDB](/docs/integrations/vectorstores/tencentvectordb/)
    - [ThirdAI NeuralDB](/docs/integrations/vectorstores/thirdai_neuraldb/)
    - [TiDB Vector](/docs/integrations/vectorstores/tidb_vector/)
    - [Tigris](/docs/integrations/vectorstores/tigris/)
    - [TileDB](/docs/integrations/vectorstores/tiledb/)
    - [Timescale Vector (Postgres)](/docs/integrations/vectorstores/timescalevector/)
    - [Typesense](/docs/integrations/vectorstores/typesense/)
    - [Upstash Vector](/docs/integrations/vectorstores/upstash/)
    - [USearch](/docs/integrations/vectorstores/usearch/)
    - [Vald](/docs/integrations/vectorstores/vald/)
    - [VDMS](/docs/integrations/vectorstores/vdms/)
    - [Vearch](/docs/integrations/vectorstores/vearch/)
    - [Vectara](/docs/integrations/vectorstores/vectara/)
    - [Vespa](/docs/integrations/vectorstores/vespa/)
    - [viking DB](/docs/integrations/vectorstores/vikingdb/)
    - [vlite](/docs/integrations/vectorstores/vlite/)
    - [Weaviate](/docs/integrations/vectorstores/weaviate/)
    - [Xata](/docs/integrations/vectorstores/xata/)
    - [YDB](/docs/integrations/vectorstores/ydb/)
    - [Yellowbrick](/docs/integrations/vectorstores/yellowbrick/)
    - [Zep](/docs/integrations/vectorstores/zep/)
    - [Zep Cloud](/docs/integrations/vectorstores/zep_cloud/)
    - [Zilliz](/docs/integrations/vectorstores/zilliz/)
  - [Embedding models](/docs/integrations/text_embedding/)
    
    - [Embedding models](/docs/integrations/text_embedding/)
    - [AI21](/docs/integrations/text_embedding/ai21/)
    - [Aleph Alpha](/docs/integrations/text_embedding/aleph_alpha/)
    - [Anyscale](/docs/integrations/text_embedding/anyscale/)
    - [ascend](/docs/integrations/text_embedding/ascend/)
    - [AwaDB](/docs/integrations/text_embedding/awadb/)
    - [AzureOpenAI](/docs/integrations/text_embedding/azureopenai/)
    - [Baichuan Text Embeddings](/docs/integrations/text_embedding/baichuan/)
    - [Baidu Qianfan](/docs/integrations/text_embedding/baidu_qianfan_endpoint/)
    - [Bedrock](/docs/integrations/text_embedding/bedrock/)
    - [BGE on Hugging Face](/docs/integrations/text_embedding/bge_huggingface/)
    - [Bookend AI](/docs/integrations/text_embedding/bookend/)
    - [Clarifai](/docs/integrations/text_embedding/clarifai/)
    - [Cloudflare Workers AI](/docs/integrations/text_embedding/cloudflare_workersai/)
    - [Clova Embeddings](/docs/integrations/text_embedding/clova/)
    - [Cohere](/docs/integrations/text_embedding/cohere/)
    - [DashScope](/docs/integrations/text_embedding/dashscope/)
    - [Databricks](/docs/integrations/text_embedding/databricks/)
    - [DeepInfra](/docs/integrations/text_embedding/deepinfra/)
    - [EDEN AI](/docs/integrations/text_embedding/edenai/)
    - [Elasticsearch](/docs/integrations/text_embedding/elasticsearch/)
    - [Embaas](/docs/integrations/text_embedding/embaas/)
    - [ERNIE](/docs/integrations/text_embedding/ernie/)
    - [Fake Embeddings](/docs/integrations/text_embedding/fake/)
    - [FastEmbed by Qdrant](/docs/integrations/text_embedding/fastembed/)
    - [Fireworks](/docs/integrations/text_embedding/fireworks/)
    - [GigaChat](/docs/integrations/text_embedding/gigachat/)
    - [Google Generative AI Embeddings](/docs/integrations/text_embedding/google_generative_ai/)
    - [Google Vertex AI](/docs/integrations/text_embedding/google_vertex_ai_palm/)
    - [GPT4All](/docs/integrations/text_embedding/gpt4all/)
    - [Gradient](/docs/integrations/text_embedding/gradient/)
    - [Hugging Face](/docs/integrations/text_embedding/huggingfacehub/)
    - [IBM watsonx.ai](/docs/integrations/text_embedding/ibm_watsonx/)
    - [Infinity](/docs/integrations/text_embedding/infinity/)
    - [Instruct Embeddings on Hugging Face](/docs/integrations/text_embedding/instruct_embeddings/)
    - [IPEX-LLM: Local BGE Embeddings on Intel CPU](/docs/integrations/text_embedding/ipex_llm/)
    - [IPEX-LLM: Local BGE Embeddings on Intel GPU](/docs/integrations/text_embedding/ipex_llm_gpu/)
    - [IntelÂ® Extension for Transformers Quantized Text Embeddings](/docs/integrations/text_embedding/itrex/)
    - [Jina](/docs/integrations/text_embedding/jina/)
    - [John Snow Labs](/docs/integrations/text_embedding/johnsnowlabs_embedding/)
    - [LASER Language-Agnostic SEntence Representations Embeddings by Meta AI](/docs/integrations/text_embedding/laser/)
    - [Lindorm](/docs/integrations/text_embedding/lindorm/)
    - [Llama.cpp](/docs/integrations/text_embedding/llamacpp/)
    - [llamafile](/docs/integrations/text_embedding/llamafile/)
    - [LLMRails](/docs/integrations/text_embedding/llm_rails/)
    - [LocalAI](/docs/integrations/text_embedding/localai/)
    - [MiniMax](/docs/integrations/text_embedding/minimax/)
    - [MistralAI](/docs/integrations/text_embedding/mistralai/)
    - [model2vec](/docs/integrations/text_embedding/model2vec/)
    - [ModelScope](/docs/integrations/text_embedding/modelscope_embedding/)
    - [MosaicML](/docs/integrations/text_embedding/mosaicml/)
    - [Naver](/docs/integrations/text_embedding/naver/)
    - [Netmind](/docs/integrations/text_embedding/netmind/)
    - [NLP Cloud](/docs/integrations/text_embedding/nlp_cloud/)
    - [Nomic](/docs/integrations/text_embedding/nomic/)
    - [NVIDIA NIMs](/docs/integrations/text_embedding/nvidia_ai_endpoints/)
    - [Oracle Cloud Infrastructure Generative AI](/docs/integrations/text_embedding/oci_generative_ai/)
    - [Ollama](/docs/integrations/text_embedding/ollama/)
    - [OpenClip](/docs/integrations/text_embedding/open_clip/)
    - [OpenAI](/docs/integrations/text_embedding/openai/)
    - [OpenVINO](/docs/integrations/text_embedding/openvino/)
    - [Embedding Documents using Optimized and Quantized Embedders](/docs/integrations/text_embedding/optimum_intel/)
    - [Oracle AI Vector Search: Generate Embeddings](/docs/integrations/text_embedding/oracleai/)
    - [OVHcloud](/docs/integrations/text_embedding/ovhcloud/)
    - [Pinecone Embeddings](/docs/integrations/text_embedding/pinecone/)
    - [PredictionGuardEmbeddings](/docs/integrations/text_embedding/predictionguard/)
    - [PremAI](/docs/integrations/text_embedding/premai/)
    - [SageMaker](/docs/integrations/text_embedding/sagemaker-endpoint/)
    - [SambaNovaCloud](/docs/integrations/text_embedding/sambanova/)
    - [SambaStudio](/docs/integrations/text_embedding/sambastudio/)
    - [Self Hosted](/docs/integrations/text_embedding/self-hosted/)
    - [Sentence Transformers on Hugging Face](/docs/integrations/text_embedding/sentence_transformers/)
    - [Solar](/docs/integrations/text_embedding/solar/)
    - [SpaCy](/docs/integrations/text_embedding/spacy_embedding/)
    - [SparkLLM Text Embeddings](/docs/integrations/text_embedding/sparkllm/)
    - [TensorFlow Hub](/docs/integrations/text_embedding/tensorflowhub/)
    - [Text Embeddings Inference](/docs/integrations/text_embedding/text_embeddings_inference/)
    - [TextEmbed - Embedding Inference Server](/docs/integrations/text_embedding/textembed/)
    - [Titan Takeoff](/docs/integrations/text_embedding/titan_takeoff/)
    - [Together AI](/docs/integrations/text_embedding/together/)
    - [Upstage](/docs/integrations/text_embedding/upstage/)
    - [Volc Engine](/docs/integrations/text_embedding/volcengine/)
    - [Voyage AI](/docs/integrations/text_embedding/voyageai/)
    - [Xorbits inference (Xinference)](/docs/integrations/text_embedding/xinference/)
    - [YandexGPT](/docs/integrations/text_embedding/yandex/)
    - [ZhipuAI](/docs/integrations/text_embedding/zhipuai/)
  - [Other](/docs/integrations/llms/)

<!--THE END-->

- [Components](/docs/integrations/components/)
- Embedding models


[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/text_embedding/index.mdx)

# Embedding models

[Embedding models](/docs/concepts/embedding_models/) create a vector representation of a piece of text.

This page documents integrations with various model providers that allow you to use embeddings in LangChain.

Select [embeddings model](/docs/integrations/text_embedding/):

OpenAIâ–¾

[OpenAI](#)

[Azure](#)

[Google](#)

[AWS](#)

[HuggingFace](#)

[Ollama](#)

[Cohere](#)

[MistralAI](#)

[Nomic](#)

[NVIDIA](#)

[Voyage AI](#)

[IBM watsonx](#)

[Fake](#)

```bash
pip install -qU langchain-openai
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
```

```python
embeddings.embed_query("Hello, world!")
```

| Provider                      | Package                                                                                                                                                        |
|-------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [AzureOpenAI](azureopenai)    | [langchain-openai](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.azure.AzureOpenAIEmbeddings.html)                  |
| [Ollama](ollama)              | [langchain-ollama](https://python.langchain.com/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html)                             |
| [AI21](ai21)                  | [langchain-ai21](https://python.langchain.com/api_reference/ai21/embeddings/langchain_ai21.embeddings.AI21Embeddings.html)                                     |
| [Fake](fake)                  | [langchain-core](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.fake.FakeEmbeddings.html)                                |
| [OpenAI](openai)              | [langchain-openai](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)                            |
| [Together](together)          | [langchain-together](https://python.langchain.com/api_reference/together/embeddings/langchain_together.embeddings.TogetherEmbeddings.html)                     |
| [Fireworks](fireworks)        | [langchain-fireworks](https://python.langchain.com/api_reference/fireworks/embeddings/langchain_fireworks.embeddings.FireworksEmbeddings.html)                 |
| [MistralAI](mistralai)        | [langchain-mistralai](https://python.langchain.com/api_reference/mistralai/embeddings/langchain_mistralai.embeddings.MistralAIEmbeddings.html)                 |
| [Cohere](cohere)              | [langchain-cohere](https://python.langchain.com/api_reference/cohere/embeddings/langchain_cohere.embeddings.CohereEmbeddings.html)                             |
| [Nomic](nomic)                | [langchain-nomic](https://python.langchain.com/api_reference/nomic/embeddings/langchain_nomic.embeddings.NomicEmbeddings.html)                                 |
| [Databricks](databricks)      | [databricks-langchain](https://api-docs.databricks.com/python/databricks-ai-bridge/latest/databricks_langchain.html#databricks_langchain.DatabricksEmbeddings) |
| [VoyageAI](voyageai)          | [langchain-voyageai](https://python.langchain.com/api_reference/voyageai/embeddings/langchain_voyageai.embeddings.VoyageAIEmbeddings.html)                     |
| [IBM](ibm_watsonx)            | [langchain-ibm](https://python.langchain.com/api_reference/ibm/embeddings/langchain_ibm.embeddings.WatsonxEmbeddings.html)                                     |
| [NVIDIA](nvidia_ai_endpoints) | [langchain-nvidia](https://python.langchain.com/api_reference/nvidia_ai_endpoints/embeddings/langchain_nvidia_ai_endpoints.embeddings.NVIDIAEmbeddings.html)   |

## All embedding models[â€‹](#all-embedding-models "Direct link to All embedding models")

| Name                                                                                                              | Description                                                                  |
|-------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| [AI21](/docs/integrations/text_embedding/ai21)                                                                    | This will help you get started with AI21 embedding models using LangC...     |
| [Aleph Alpha](/docs/integrations/text_embedding/aleph_alpha)                                                      | There are two possible ways to use Aleph Alpha's semantic embeddings....     |
| [Anyscale](/docs/integrations/text_embedding/anyscale)                                                            | Let's load the Anyscale Embedding class.                                     |
| [ascend](/docs/integrations/text_embedding/ascend)                                                                | [[-0.00348254 0.03098977 -0.00203087 ... 0.08492374 0.03970494               |
| [AwaDB](/docs/integrations/text_embedding/awadb)                                                                  | AwaDB is an AI Native database for the search and storage of embeddin...     |
| [AzureOpenAI](/docs/integrations/text_embedding/azureopenai)                                                      | This will help you get started with AzureOpenAI embedding models usin...     |
| [Baichuan Text Embeddings](/docs/integrations/text_embedding/baichuan)                                            | As of today (Jan 25th, 2024) BaichuanTextEmbeddings ranks #1 in C-MTE...     |
| [Baidu Qianfan](/docs/integrations/text_embedding/baidu_qianfan_endpoint)                                         | Baidu AI Cloud Qianfan Platform is a one-stop large model development...     |
| [Bedrock](/docs/integrations/text_embedding/bedrock)                                                              | Amazon Bedrock is a fully managed service that offers a choice of            |
| [BGE on Hugging Face](/docs/integrations/text_embedding/bge_huggingface)                                          | BGE models on the HuggingFace are one of the best open-source embeddi...     |
| [Bookend AI](/docs/integrations/text_embedding/bookend)                                                           | Let's load the Bookend AI Embeddings class.                                  |
| [Clarifai](/docs/integrations/text_embedding/clarifai)                                                            | Clarifai is an AI Platform that provides the full AI lifecycle rangin...     |
| [Cloudflare Workers AI](/docs/integrations/text_embedding/cloudflare_workersai)                                   | Cloudflare, Inc. (Wikipedia) is an American company that provides con...     |
| [Clova Embeddings](/docs/integrations/text_embedding/clova)                                                       | Clova offers an embeddings service                                           |
| [Cohere](/docs/integrations/text_embedding/cohere)                                                                | This will help you get started with Cohere embedding models using Lan...     |
| [DashScope](/docs/integrations/text_embedding/dashscope)                                                          | Let's load the DashScope Embedding class.                                    |
| [Databricks](/docs/integrations/text_embedding/databricks)                                                        | Databricks Lakehouse Platform unifies data, analytics, and AI on one ...     |
| [DeepInfra](/docs/integrations/text_embedding/deepinfra)                                                          | DeepInfra is a serverless inference as a service that provides access...     |
| [EDEN AI](/docs/integrations/text_embedding/edenai)                                                               | Eden AI is revolutionizing the AI landscape by uniting the best AI pr...     |
| [Elasticsearch](/docs/integrations/text_embedding/elasticsearch)                                                  | Walkthrough of how to generate embeddings using a hosted embedding mo...     |
| [Embaas](/docs/integrations/text_embedding/embaas)                                                                | embaas is a fully managed NLP API service that offers features like e...     |
| [Fake Embeddings](/docs/integrations/text_embedding/fake)                                                         | LangChain also provides a fake embedding class. You can use this to t...     |
| [FastEmbed by Qdrant](/docs/integrations/text_embedding/fastembed)                                                | FastEmbed from Qdrant is a lightweight, fast, Python library built fo...     |
| [Fireworks](/docs/integrations/text_embedding/fireworks)                                                          | This will help you get started with Fireworks embedding models using ...     |
| [GigaChat](/docs/integrations/text_embedding/gigachat)                                                            | This notebook shows how to use LangChain with GigaChat embeddings.           |
| [Google Generative AI Embeddings](/docs/integrations/text_embedding/google_generative_ai)                         | Connect to Google's generative AI embeddings service using the Google...     |
| [Google Vertex AI](/docs/integrations/text_embedding/google_vertex_ai_palm)                                       | This will help you get started with Google Vertex AI Embeddings model...     |
| [GPT4All](/docs/integrations/text_embedding/gpt4all)                                                              | GPT4All is a free-to-use, locally running, privacy-aware chatbot. The...     |
| [Gradient](/docs/integrations/text_embedding/gradient)                                                            | Gradient allows to create Embeddings as well fine tune and get comple...     |
| [Hugging Face](/docs/integrations/text_embedding/huggingfacehub)                                                  | Let's load the Hugging Face Embedding class.                                 |
| [IBM watsonx.ai](/docs/integrations/text_embedding/ibm_watsonx)                                                   | WatsonxEmbeddings is a wrapper for IBM watsonx.ai foundation models.         |
| [Infinity](/docs/integrations/text_embedding/infinity)                                                            | Infinity allows to create Embeddings using a MIT-licensed Embedding S...     |
| [Instruct Embeddings on Hugging Face](/docs/integrations/text_embedding/instruct_embeddings)                      | Hugging Face sentence-transformers is a Python framework for state-of...     |
| [IPEX-LLM: Local BGE Embeddings on Intel CPU](/docs/integrations/text_embedding/ipex_llm)                         | IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e...     |
| [IPEX-LLM: Local BGE Embeddings on Intel GPU](/docs/integrations/text_embedding/ipex_llm_gpu)                     | IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e...     |
| [IntelÂ® Extension for Transformers Quantized Text Embeddings](/docs/integrations/text_embedding/itrex)            | Load quantized BGE embedding models generated by IntelÂ® Extension for...     |
| [Jina](/docs/integrations/text_embedding/jina)                                                                    | You can check the list of available models from here.                        |
| [John Snow Labs](/docs/integrations/text_embedding/johnsnowlabs_embedding)                                        | John Snow Labs NLP &amp; LLM ecosystem includes software libraries for st... |
| [LASER Language-Agnostic SEntence Representations Embeddings by Meta AI](/docs/integrations/text_embedding/laser) | LASER is a Python library developed by the Meta AI Research team and ...     |
| [Lindorm](/docs/integrations/text_embedding/lindorm)                                                              | This will help you get started with Lindorm embedding models using La...     |
| [Llama.cpp](/docs/integrations/text_embedding/llamacpp)                                                           | llama.cpp python library is a simple Python bindings for @ggerganov          |
| [llamafile](/docs/integrations/text_embedding/llamafile)                                                          | Let's load the llamafile Embeddings class.                                   |
| [LLMRails](/docs/integrations/text_embedding/llm_rails)                                                           | Let's load the LLMRails Embeddings class.                                    |
| [LocalAI](/docs/integrations/text_embedding/localai)                                                              | langchain-localai is a 3rd party integration package for LocalAI. It ...     |
| [MiniMax](/docs/integrations/text_embedding/minimax)                                                              | MiniMax offers an embeddings service.                                        |
| [MistralAI](/docs/integrations/text_embedding/mistralai)                                                          | This will help you get started with MistralAI embedding models using ...     |
| [model2vec](/docs/integrations/text_embedding/model2vec)                                                          | Overview                                                                     |
| [ModelScope](/docs/integrations/text_embedding/modelscope_embedding)                                              | ModelScope (Home \| GitHub) is built upon the notion of â€œModel-as-a-Se...    |
| [MosaicML](/docs/integrations/text_embedding/mosaicml)                                                            | MosaicML offers a managed inference service. You can either use a var...     |
| [Naver](/docs/integrations/text_embedding/naver)                                                                  | This notebook covers how to get started with embedding models provide...     |
| [Netmind](/docs/integrations/text_embedding/netmind)                                                              | This will help you get started with Netmind embedding models using La...     |
| [NLP Cloud](/docs/integrations/text_embedding/nlp_cloud)                                                          | NLP Cloud is an artificial intelligence platform that allows you to u...     |
| [Nomic](/docs/integrations/text_embedding/nomic)                                                                  | This will help you get started with Nomic embedding models using Lang...     |
| [NVIDIA NIMs](/docs/integrations/text_embedding/nvidia_ai_endpoints)                                              | The langchain-nvidia-ai-endpoints package contains LangChain integrat...     |
| [Oracle Cloud Infrastructure Generative AI](/docs/integrations/text_embedding/oci_generative_ai)                  | Oracle Cloud Infrastructure (OCI) Generative AI is a fully managed se...     |
| [Ollama](/docs/integrations/text_embedding/ollama)                                                                | This will help you get started with Ollama embedding models using Lan...     |
| [OpenClip](/docs/integrations/text_embedding/open_clip)                                                           | OpenClip is an source implementation of OpenAI's CLIP.                       |
| [OpenAI](/docs/integrations/text_embedding/openai)                                                                | This will help you get started with OpenAI embedding models using Lan...     |
| [OpenVINO](/docs/integrations/text_embedding/openvino)                                                            | OpenVINOâ„¢ is an open-source toolkit for optimizing and deploying AI i...     |
| [Embedding Documents using Optimized and Quantized Embedders](/docs/integrations/text_embedding/optimum_intel)    | Embedding all documents using Quantized Embedders.                           |
| [Oracle AI Vector Search: Generate Embeddings](/docs/integrations/text_embedding/oracleai)                        | Oracle AI Vector Search is designed for Artificial Intelligence (AI) ...     |
| [OVHcloud](/docs/integrations/text_embedding/ovhcloud)                                                            | In order to use this model you need to create a new token on the AI E...     |
| [Pinecone Embeddings](/docs/integrations/text_embedding/pinecone)                                                 | Pinecone's inference API can be accessed via PineconeEmbeddings. Prov...     |
| [PredictionGuardEmbeddings](/docs/integrations/text_embedding/predictionguard)                                    | Prediction Guard is a secure, scalable GenAI platform that safeguards...     |
| [PremAI](/docs/integrations/text_embedding/premai)                                                                | PremAI is an all-in-one platform that simplifies the creation of robu...     |
| [SageMaker](/docs/integrations/text_embedding/sagemaker-endpoint)                                                 | Let's load the SageMaker Endpoints Embeddings class. The class can be...     |
| [SambaNovaCloud](/docs/integrations/text_embedding/sambanova)                                                     | This will help you getting started with SambaNovaCloud embedding mode...     |
| [SambaStudio](/docs/integrations/text_embedding/sambastudio)                                                      | This will help you get started with SambaNova's SambaStudio embedding...     |
| [Self Hosted](/docs/integrations/text_embedding/self-hosted)                                                      | Let's load the SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings,...     |
| [Sentence Transformers on Hugging Face](/docs/integrations/text_embedding/sentence_transformers)                  | Hugging Face sentence-transformers is a Python framework for state-of...     |
| [Solar](/docs/integrations/text_embedding/solar)                                                                  | Solar offers an embeddings service.                                          |
| [SpaCy](/docs/integrations/text_embedding/spacy_embedding)                                                        | spaCy is an open-source software library for advanced natural languag...     |
| [SparkLLM Text Embeddings](/docs/integrations/text_embedding/sparkllm)                                            | Official Website//www.xfyun.cn/doc/spark/Embeddingnewapi.html                |
| [TensorFlow Hub](/docs/integrations/text_embedding/tensorflowhub)                                                 | TensorFlow Hub is a repository of trained machine learning models rea...     |
| [Text Embeddings Inference](/docs/integrations/text_embedding/text_embeddings_inference)                          | Hugging Face Text Embeddings Inference (TEI) is a toolkit for deployi...     |
| [TextEmbed - Embedding Inference Server](/docs/integrations/text_embedding/textembed)                             | TextEmbed is a high-throughput, low-latency REST API designed for ser...     |
| [Titan Takeoff](/docs/integrations/text_embedding/titan_takeoff)                                                  | TitanML helps businesses build and deploy better, smaller, cheaper, a...     |
| [Together AI](/docs/integrations/text_embedding/together)                                                         | This will help you get started with Together embedding models using L...     |
| [Upstage](/docs/integrations/text_embedding/upstage)                                                              | This notebook covers how to get started with Upstage embedding models.       |
| [Volc Engine](/docs/integrations/text_embedding/volcengine)                                                       | This notebook provides you with a guide on how to load the Volcano Em...     |
| [Voyage AI](/docs/integrations/text_embedding/voyageai)                                                           | Voyage AI provides cutting-edge embedding/vectorizations models.             |
| [Xorbits inference (Xinference)](/docs/integrations/text_embedding/xinference)                                    | This notebook goes over how to use Xinference embeddings within LangC...     |
| [YandexGPT](/docs/integrations/text_embedding/yandex)                                                             | This notebook goes over how to use Langchain with YandexGPT embedding...     |
| [ZhipuAI](/docs/integrations/text_embedding/zhipuai)                                                              | This will help you get started with ZhipuAI embedding models using La...     |

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/integrations/text_embedding/index.mdx)

* * *


- [All embedding models](#all-embedding-models)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/extraction_parse.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/extraction_parse.ipynb)

# How to use prompting alone (no tool calling) to do extraction

[Tool calling](/docs/concepts/tool_calling/) features are not required for generating structured output from LLMs. LLMs that are able to follow prompt instructions well can be tasked with outputting information in a given format.

This approach relies on designing good prompts and then parsing the output of the LLMs to make them extract information well.

To extract data without tool-calling features:

1. Instruct the LLM to generate text following an expected format (e.g., JSON with a certain schema);
2. Use [output parsers](/docs/concepts/output_parsers/) to structure the model response into a desired Python object.

First we select a LLM:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4o-mini", model_provider="openai")
```

tip

This tutorial is meant to be simple, but generally should really include reference examples to squeeze out performance!

## Using PydanticOutputParser[â€‹](#using-pydanticoutputparser "Direct link to Using PydanticOutputParser")

The following example uses the built-in `PydanticOutputParser` to parse the output of a chat model.

```python
from typing import List, Optional

from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field, validator


class Person(BaseModel):
    """Information about a person."""

    name: str = Field(..., description="The name of the person")
    height_in_meters: float = Field(
        ..., description="The height of the person expressed in meters."
    )


class People(BaseModel):
    """Identifying information about all people in a text."""

    people: List[Person]


# Set up a parser
parser = PydanticOutputParser(pydantic_object=People)

# Prompt
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Answer the user query. Wrap the output in `json` tags\n{format_instructions}",
        ),
        ("human", "{query}"),
    ]
).partial(format_instructions=parser.get_format_instructions())
```

**API Reference:**[PydanticOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

Let's take a look at what information is sent to the model

```python
query = "Anna is 23 years old and she is 6 feet tall"
```

```python
print(prompt.format_prompt(query=query).to_string())
```

```output
System: Answer the user query. Wrap the output in `json` tags
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
\`\`\`
{"$defs": {"Person": {"description": "Information about a person.", "properties": {"name": {"description": "The name of the person", "title": "Name", "type": "string"}, "height_in_meters": {"description": "The height of the person expressed in meters.", "title": "Height In Meters", "type": "number"}}, "required": ["name", "height_in_meters"], "title": "Person", "type": "object"}}, "description": "Identifying information about all people in a text.", "properties": {"people": {"items": {"$ref": "#/$defs/Person"}, "title": "People", "type": "array"}}, "required": ["people"]}
\`\`\`
Human: Anna is 23 years old and she is 6 feet tall
```

Having defined our prompt, we simply chain together the prompt, model and output parser:

```python
chain = prompt | model | parser
chain.invoke({"query": query})
```

```output
People(people=[Person(name='Anna', height_in_meters=1.83)])
```

Check out the associated [Langsmith trace](https://smith.langchain.com/public/92ed52a3-92b9-45af-a663-0a9c00e5e396/r).

Note that the schema shows up in two places:

1. In the prompt, via `parser.get_format_instructions()`;
2. In the chain, to receive the formatted output and structure it into a Python object (in this case, the Pydantic object `People`).

## Custom Parsing[â€‹](#custom-parsing "Direct link to Custom Parsing")

If desired, it's easy to create a custom prompt and parser with `LangChain` and `LCEL`.

To create a custom parser, define a function to parse the output from the model (typically an [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html)) into an object of your choice.

See below for a simple implementation of a JSON parser.

```python
import json
import re
from typing import List, Optional

from langchain_anthropic.chat_models import ChatAnthropic
from langchain_core.messages import AIMessage
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field, validator


class Person(BaseModel):
    """Information about a person."""

    name: str = Field(..., description="The name of the person")
    height_in_meters: float = Field(
        ..., description="The height of the person expressed in meters."
    )


class People(BaseModel):
    """Identifying information about all people in a text."""

    people: List[Person]


# Prompt
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Answer the user query. Output your answer as JSON that  "
            "matches the given schema: \`\`\`json\n{schema}\n\`\`\`. "
            "Make sure to wrap the answer in \`\`\`json and \`\`\` tags",
        ),
        ("human", "{query}"),
    ]
).partial(schema=People.schema())


# Custom parser
def extract_json(message: AIMessage) -> List[dict]:
    """Extracts JSON content from a string where JSON is embedded between \`\`\`json and \`\`\` tags.

    Parameters:
        text (str): The text containing the JSON content.

    Returns:
        list: A list of extracted JSON strings.
    """
    text = message.content
    # Define the regular expression pattern to match JSON blocks
    pattern = r"\`\`\`json(.*?)\`\`\`"

    # Find all non-overlapping matches of the pattern in the string
    matches = re.findall(pattern, text, re.DOTALL)

    # Return the list of matched JSON strings, stripping any leading or trailing whitespace
    try:
        return [json.loads(match.strip()) for match in matches]
    except Exception:
        raise ValueError(f"Failed to parse: {message}")
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```python
query = "Anna is 23 years old and she is 6 feet tall"
print(prompt.format_prompt(query=query).to_string())
```

```output
System: Answer the user query. Output your answer as JSON that  matches the given schema: \`\`\`json
{'$defs': {'Person': {'description': 'Information about a person.', 'properties': {'name': {'description': 'The name of the person', 'title': 'Name', 'type': 'string'}, 'height_in_meters': {'description': 'The height of the person expressed in meters.', 'title': 'Height In Meters', 'type': 'number'}}, 'required': ['name', 'height_in_meters'], 'title': 'Person', 'type': 'object'}}, 'description': 'Identifying information about all people in a text.', 'properties': {'people': {'items': {'$ref': '#/$defs/Person'}, 'title': 'People', 'type': 'array'}}, 'required': ['people'], 'title': 'People', 'type': 'object'}
\`\`\`. Make sure to wrap the answer in \`\`\`json and \`\`\` tags
Human: Anna is 23 years old and she is 6 feet tall
```

```python
chain = prompt | model | extract_json
chain.invoke({"query": query})
```

```output
/Users/bagatur/langchain/.venv/lib/python3.11/site-packages/pydantic/_internal/_fields.py:201: UserWarning: Field name "schema" in "PromptInput" shadows an attribute in parent "BaseModel"
  warnings.warn(
```

```output
[{'people': [{'name': 'Anna', 'height_in_meters': 1.83}]}]
```

## Other Libraries[â€‹](#other-libraries "Direct link to Other Libraries")

If you're looking at extracting using a parsing approach, check out the [Kor](https://eyurtsev.github.io/kor/) library. It's written by one of the `LangChain` maintainers and it helps to craft a prompt that takes examples into account, allows controlling formats (e.g., JSON or CSV) and expresses the schema in TypeScript. It seems to work pretty!

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/extraction_parse.ipynb)

* * *


- [Using PydanticOutputParser](#using-pydanticoutputparser)
- [Custom Parsing](#custom-parsing)
- [Other Libraries](#other-libraries)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/vectorstores.mdx)

# Vector stores

Prerequisites

- [Embeddings](/docs/concepts/embedding_models/)
- [Text splitters](/docs/concepts/text_splitters/)

Note

This conceptual overview focuses on text-based indexing and retrieval for simplicity. However, embedding models can be [multi-modal](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings) and vector stores can be used to store and retrieve a variety of data types beyond text.

## Overview[â€‹](#overview "Direct link to Overview")

Vector stores are specialized data stores that enable indexing and retrieving information based on vector representations.

These vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.

Vector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.

![Vector stores](/assets/images/vectorstores-2540b4bc355b966c99b0f02cfdddb273.png)

## Integrations[â€‹](#integrations "Direct link to Integrations")

LangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.

Please see the [full list of LangChain vectorstore integrations](/docs/integrations/vectorstores/).

## Interface[â€‹](#interface "Direct link to Interface")

LangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations.

The interface consists of basic methods for writing, deleting and searching for documents in the vector store.

The key methods are:

- `add_documents`: Add a list of texts to the vector store.
- `delete`: Delete a list of documents from the vector store.
- `similarity_search`: Search for similar documents to a given query.

## Initialization[â€‹](#initialization "Direct link to Initialization")

Most vectors in LangChain accept an embedding model as an argument when initializing the vector store.

We will use LangChain's [InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) implementation to illustrate the API.

```python
from langchain_core.vectorstores import InMemoryVectorStore
# Initialize with an embedding model
vector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())
```

**API Reference:**[InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html)

## Adding documents[â€‹](#adding-documents "Direct link to Adding documents")

To add documents, use the `add_documents` method.

This API works with a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects. `Document` objects all have `page_content` and `metadata` attributes, making them a universal way to store unstructured text and associated metadata.

```python
from langchain_core.documents import Document

document_1 = Document(
    page_content="I had chocalate chip pancakes and scrambled eggs for breakfast this morning.",
    metadata={"source": "tweet"},
)

document_2 = Document(
    page_content="The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.",
    metadata={"source": "news"},
)

documents = [document_1, document_2]

vector_store.add_documents(documents=documents)
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

You should usually provide IDs for the documents you add to the vector store, so that instead of adding the same document multiple times, you can update the existing document.

```python
vector_store.add_documents(documents=documents, ids=["doc1", "doc2"])
```

## Delete[â€‹](#delete "Direct link to Delete")

To delete documents, use the `delete` method which takes a list of document IDs to delete.

```python
vector_store.delete(ids=["doc1"])
```

## Search[â€‹](#search "Direct link to Search")

Vector stores embed and store the documents that added. If we pass in a query, the vectorstore will embed the query, perform a similarity search over the embedded documents, and return the most similar ones. This captures two important concepts: first, there needs to be a way to measure the similarity between the query and *any* [embedded](/docs/concepts/embedding_models/) document. Second, there needs to be an algorithm to efficiently perform this similarity search across *all* embedded documents.

### Similarity metrics[â€‹](#similarity-metrics "Direct link to Similarity metrics")

A critical advantage of embeddings vectors is they can be compared using many simple mathematical operations:

- **Cosine Similarity**: Measures the cosine of the angle between two vectors.
- **Euclidean Distance**: Measures the straight-line distance between two points.
- **Dot Product**: Measures the projection of one vector onto another.

The choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer to the documentation of the specific vectorstore you are using to see what similarity metrics are supported.

Further reading

- See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with embeddings.
- See Pinecone's [blog post](https://www.pinecone.io/learn/vector-similarity/) on similarity metrics.
- See OpenAI's [FAQ](https://platform.openai.com/docs/guides/embeddings/faq) on what similarity metric to use with OpenAI embeddings.

### Similarity search[â€‹](#similarity-search "Direct link to Similarity search")

Given a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over *all* the embedded documents to find the most similar ones. There are various ways to do this. As an example, many vectorstores implement [HNSW (Hierarchical Navigable Small World)](https://www.pinecone.io/learn/series/faiss/hnsw/), a graph-based index structure that allows for efficient similarity search. Regardless of the search algorithm used under the hood, the LangChain vectorstore interface has a `similarity_search` method for all integrations. This will take the search query, create an embedding, find similar documents, and return them as a list of [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html).

```python
query = "my query"
docs = vectorstore.similarity_search(query)
```

Many vectorstores support search parameters to be passed with the `similarity_search` method. See the documentation for the specific vectorstore you are using to see what parameters are supported. As an example [Pinecone](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.similarity_search) several parameters that are important general concepts: Many vectorstores support [the `k`](/docs/integrations/vectorstores/pinecone/#query-directly), which controls the number of Documents to return, and `filter`, which allows for filtering documents by metadata.

- `query (str) â€“ Text to look up documents similar to.`
- `k (int) â€“ Number of Documents to return. Defaults to 4.`
- `filter (dict | None) â€“ Dictionary of argument(s) to filter on metadata`

Further reading

- See the [how-to guide](/docs/how_to/vectorstores/) for more details on how to use the `similarity_search` method.
- See the [integrations page](/docs/integrations/vectorstores/) for more details on arguments that can be passed in to the `similarity_search` method for specific vectorstores.

### Metadata filtering[â€‹](#metadata-filtering "Direct link to Metadata filtering")

While vectorstore implement a search algorithm to efficiently search over *all* the embedded documents to find the most similar ones, many also support filtering on metadata. Metadata filtering helps narrow down the search by applying specific conditions such as retrieving documents from a particular source or date range. These two concepts work well together:

1. **Semantic search**: Query the unstructured data directly, often via embedding or keyword similarity.
2. **Metadata search**: Apply structured query to the metadata, filtering specific documents.

Vector store support for metadata filtering is typically dependent on the underlying vector store implementation.

Here is example usage with [Pinecone](/docs/integrations/vectorstores/pinecone/#query-directly), showing that we filter for all documents that have the metadata key `source` with value `tweet`.

```python
vectorstore.similarity_search(
    "LangChain provides abstractions to make working with LLMs easy",
    k=2,
    filter={"source": "tweet"},
)
```

Further reading

- See Pinecone's [documentation](https://docs.pinecone.io/guides/data/filter-with-metadata) on filtering with metadata.
- See the [list of LangChain vectorstore integrations](/docs/integrations/retrievers/self_query/) that support metadata filtering.

## Advanced search and retrieval techniques[â€‹](#advanced-search-and-retrieval-techniques "Direct link to Advanced search and retrieval techniques")

While algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity. For example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results. As a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches. At the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`. See this [how-to guide on hybrid search](/docs/how_to/hybrid/) for more details.

| Name                                                                                                                                                                                                                                            | When to use                                           | Description                                                                                                                                  |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| [Hybrid search](/docs/integrations/retrievers/pinecone_hybrid_search/)                                                                                                                                                                          | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. [Paper](https://arxiv.org/abs/2210.11934). |
| [Maximal Marginal Relevance (MMR)](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.max_marginal_relevance_search) | When needing to diversify search results.             | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.                                        |

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/vectorstores.mdx)

* * *


- [Overview](#overview)
- [Integrations](#integrations)
- [Interface](#interface)
- [Initialization](#initialization)
- [Adding documents](#adding-documents)
- [Delete](#delete)
- [Search](#search)
  
  - [Similarity metrics](#similarity-metrics)
  - [Similarity search](#similarity-search)
  - [Metadata filtering](#metadata-filtering)
- [Advanced search and retrieval techniques](#advanced-search-and-retrieval-techniques)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_callbacks.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_callbacks.ipynb)

# How to create custom callback handlers

Prerequisites

This guide assumes familiarity with the following concepts:

- [Callbacks](/docs/concepts/callbacks/)

LangChain has some built-in callback handlers, but you will often want to create your own handlers with custom logic.

To create a custom callback handler, we need to determine the [event(s)](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html#langchain-core-callbacks-base-basecallbackhandler) we want our callback handler to handle as well as what we want our callback handler to do when the event is triggered. Then all we need to do is attach the callback handler to the object, for example via [the constructor](/docs/how_to/callbacks_constructor/) or [at runtime](/docs/how_to/callbacks_runtime/).

In the example below, we'll implement streaming with a custom handler.

In our custom callback handler `MyCustomHandler`, we implement the `on_llm_new_token` handler to print the token we have just received. We then attach our custom handler to the model object as a constructor callback.

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.prompts import ChatPromptTemplate


class MyCustomHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(f"My custom handler, token: {token}")


prompt = ChatPromptTemplate.from_messages(["Tell me a joke about {animal}"])

# To enable streaming, we pass in `streaming=True` to the ChatModel constructor
# Additionally, we pass in our custom handler as a list to the callbacks parameter
model = ChatAnthropic(
    model="claude-3-sonnet-20240229", streaming=True, callbacks=[MyCustomHandler()]
)

chain = prompt | model

response = chain.invoke({"animal": "bears"})
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [BaseCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
My custom handler, token: Here
My custom handler, token: 's
My custom handler, token:  a
My custom handler, token:  bear
My custom handler, token:  joke
My custom handler, token:  for
My custom handler, token:  you
My custom handler, token: :
My custom handler, token: 

Why
My custom handler, token:  di
My custom handler, token: d the
My custom handler, token:  bear
My custom handler, token:  dissol
My custom handler, token: ve
My custom handler, token:  in
My custom handler, token:  water
My custom handler, token: ?
My custom handler, token: 
Because
My custom handler, token:  it
My custom handler, token:  was
My custom handler, token:  a
My custom handler, token:  polar
My custom handler, token:  bear
My custom handler, token: !
```

You can see [this reference page](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html#langchain-core-callbacks-base-basecallbackhandler) for a list of events you can handle. Note that the `handle_chain_*` events run for most LCEL runnables.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to create your own custom callback handlers.

Next, check out the other how-to guides in this section, such as [how to attach callbacks to a runnable](/docs/how_to/callbacks_attach/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/custom_callbacks.ipynb)

* * *


- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/llm_chain.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/llm_chain.ipynb)

# Migrating from LLMChain

[`LLMChain`](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) combined a prompt template, LLM, and output parser into a class.

Some advantages of switching to the LCEL implementation are:

- Clarity around contents and parameters. The legacy `LLMChain` contains a default output parser and other options.
- Easier streaming. `LLMChain` only supports streaming via callbacks.
- Easier access to raw message outputs if desired. `LLMChain` only exposes these via a parameter or via callback.

```python
%pip install --upgrade --quiet langchain-openai
```

```python
import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

## Legacy[â€‹](#legacy "Direct link to Legacy")

Details

```python
from langchain.chains import LLMChain
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_messages(
    [("user", "Tell me a {adjective} joke")],
)

legacy_chain = LLMChain(llm=ChatOpenAI(), prompt=prompt)

legacy_result = legacy_chain({"adjective": "funny"})
legacy_result
```

**API Reference:**[LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
{'adjective': 'funny',
 'text': "Why couldn't the bicycle stand up by itself?\n\nBecause it was two tired!"}
```

Note that `LLMChain` by default returned a `dict` containing both the input and the output from `StrOutputParser`, so to extract the output, you need to access the `"text"` key.

```python
legacy_result["text"]
```

```output
"Why couldn't the bicycle stand up by itself?\n\nBecause it was two tired!"
```

## LCEL[â€‹](#lcel "Direct link to LCEL")

Details

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_messages(
    [("user", "Tell me a {adjective} joke")],
)

chain = prompt | ChatOpenAI() | StrOutputParser()

chain.invoke({"adjective": "funny"})
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
'Why was the math book sad?\n\nBecause it had too many problems.'
```

If you'd like to mimic the `dict` packaging of input and output in `LLMChain`, you can use a [`RunnablePassthrough.assign`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) like:

```python
from langchain_core.runnables import RunnablePassthrough

outer_chain = RunnablePassthrough().assign(text=chain)

outer_chain.invoke({"adjective": "funny"})
```

**API Reference:**[RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)

```output
{'adjective': 'funny',
 'text': 'Why did the scarecrow win an award? Because he was outstanding in his field!'}
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

See [this tutorial](/docs/tutorials/llm_chain/) for more detail on building with prompt templates, LLMs, and output parsers.

Check out the [LCEL conceptual docs](/docs/concepts/lcel/) for more background information.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_chains/llm_chain.ipynb)

* * *


- [Legacy](#legacy)
- [LCEL](#lcel)
- [Next steps](#next-steps)








- [LangGraph](.)
- [Agents](agents/overview/)
- [API reference](reference/)

[![logo](static/wordmark_dark.svg) ![logo](static/wordmark_light.svg)](.)

[GitHub](https://github.com/langchain-ai/langgraph "Go to repository")

- [LangGraph](.)
  
  LangGraph
  
  - Get started
    
    Get started
    
    - [Learn the basics](tutorials/introduction/)
    - [Deployment](tutorials/deployment/)
  - Guides
    
    Guides
    
    - [How-to Guides](how-tos/)
    - [Concepts](concepts/)
    - [Tutorials](tutorials/)
  - Resources
    
    Resources
    
    - [Companies using LangGraph](adopters/)
    - [LLMS-txt](llms-txt-overview/)
    - [FAQ](concepts/faq/)
    - [Troubleshooting](troubleshooting/errors/)
    - [LangGraph Academy Course](https://academy.langchain.com/courses/intro-to-langgraph)
- [Agents](agents/overview/)
- [API reference](reference/)

Table of contents

- [Why use LangGraph?](#why-use-langgraph)
- [LangGraphâ€™s ecosystem](#langgraphs-ecosystem)
- [Pairing with LangGraph Platform](#pairing-with-langgraph-platform)
- [Additional resources](#additional-resources)
- [Acknowledgements](#acknowledgements)

[Edit this page](https://github.com/langchain-ai/langgraph/edit/main/docs/docs/index.md "Edit this page")

# LangGraph

![LangGraph Logo](static/wordmark_dark.svg) ![LangGraph Logo](static/wordmark_light.svg)

![LangGraph Logo](https://langchain-ai.github.io/langgraph/static/wordmark_dark.svg)

[![Version](https://img.shields.io/pypi/v/langgraph.svg)](https://pypi.org/project/langgraph/) [![Downloads](https://static.pepy.tech/badge/langgraph/month)](https://pepy.tech/project/langgraph) [![Open Issues](https://img.shields.io/github/issues-raw/langchain-ai/langgraph)](https://github.com/langchain-ai/langgraph/issues) [![Docs](https://img.shields.io/badge/docs-latest-blue)](https://langchain-ai.github.io/langgraph/)

Note

Looking for the JS version? See the [JS repo](https://github.com/langchain-ai/langgraphjs) and the [JS docs](https://langchain-ai.github.io/langgraphjs/).

LangGraph â€” used by Replit, Uber, LinkedIn, GitLab and more â€” is a low-level orchestration framework for building controllable agents. While langchain provides integrations and composable components to streamline LLM application development, the LangGraph library enables agent orchestration â€” offering customizable architectures, long-term memory, and human-in-the-loop to reliably handle complex tasks.

```
pip install -U langgraph
```

To learn more about how to use LangGraph, check out [the docs](https://langchain-ai.github.io/langgraph/). We show a simple example below of how to create a ReAct agent.

```
# This code depends on pip install langchain[anthropic]
from langgraph.prebuilt import create_react_agent

def search(query: str):
    """Call to surf the web."""
    if "sf" in query.lower() or "san francisco" in query.lower():
        return "It's 60 degrees and foggy."
    return "It's 90 degrees and sunny."

agent = create_react_agent("anthropic:claude-3-7-sonnet-latest", tools=[search])
agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)
```

Tip

Check out [this guide](https://langchain-ai.github.io/langgraph/tutorials/workflows/) that walks through implementing common patterns (workflows and agents) in LangGraph.

## Why use LangGraph?[Â¶](#why-use-langgraph "Permanent link")

LangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:

- **Reliability and controllability.** Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.
- **Low-level and extensible.** Build custom agents with fully descriptive, low-level primitives â€“ free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.
- **First-class streaming support.** With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.

LangGraph is trusted in production and powering agents for companies like:

- [Klarna](https://blog.langchain.dev/customers-klarna/): Customer support bot for 85 million active users
- [Elastic](https://www.elastic.co/blog/elastic-security-generative-ai-features): Security AI assistant for threat detection
- [Uber](https://dpe.org/sessions/ty-smith-adam-huda/this-year-in-ubers-ai-driven-developer-productivity-revolution/): Automated unit test generation
- [Replit](https://www.langchain.com/breakoutagents/replit): Code generation
- And many more ([see list here](https://www.langchain.com/built-with-langgraph))

## LangGraphâ€™s ecosystem[Â¶](#langgraphs-ecosystem "Permanent link")

While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:

- [LangSmith](http://www.langchain.com/langsmith) â€” Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.
- [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/#langgraph-platform) â€” Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in [LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/).

## Pairing with LangGraph Platform[Â¶](#pairing-with-langgraph-platform "Permanent link")

While LangGraph is our open-source agent orchestration framework, enterprises that need scalable agent deployment can benefit from [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/).

LangGraph Platform can help engineering teams:

- **Accelerate agent development**: Quickly create agent UXs with configurable templates and [LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/) for visualizing and debugging agent interactions.
- **Deploy seamlessly**: We handle the complexity of deploying your agent. LangGraph Platform includes robust APIs for memory, threads, and cron jobs plus auto-scaling task queues &amp; servers.
- **Centralize agent management &amp; reusability**: Discover, reuse, and manage agents across the organization. Business users can also modify agents without coding.

## Additional resources[Â¶](#additional-resources "Permanent link")

- [LangChain Academy](https://academy.langchain.com/courses/intro-to-langgraph): Learn the basics of LangGraph in our free, structured course.
- [Tutorials](https://langchain-ai.github.io/langgraph/tutorials/): Simple walkthroughs with guided examples on getting started with LangGraph.
- [Templates](https://langchain-ai.github.io/langgraph/concepts/template_applications/): Pre-built reference apps for common agentic workflows (e.g. ReAct agent, memory, retrieval etc.) that can be cloned and adapted.
- [How-to Guides](https://langchain-ai.github.io/langgraph/how-tos/): Quick, actionable code snippets for topics such as streaming, adding memory &amp; persistence, and design patterns (e.g. branching, subgraphs, etc.).
- [API Reference](https://langchain-ai.github.io/langgraph/reference/graphs/): Detailed reference on core classes, methods, how to use the graph and checkpointing APIs, and higher-level prebuilt components.
- [Built with LangGraph](https://www.langchain.com/built-with-langgraph): Hear how industry leaders use LangGraph to ship powerful, production-ready AI applications.

## Acknowledgements[Â¶](#acknowledgements "Permanent link")

LangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.

Was this page helpful?

Thanks for your feedback!

Thanks for your feedback! Please help us improve this page by adding to the discussion below.

Back to top

[Next  
\
Learn the basics](tutorials/introduction/)

Copyright Â© 2025 LangChain, Inc | [Consent Preferences](#__consent)

Made with [Material for MkDocs Insiders](https://squidfunk.github.io/mkdocs-material/)

[langchain-ai.github.io](https://langchain-ai.github.io/langgraphjs/ "langchain-ai.github.io")[github.com](https://github.com/langchain-ai/langgraph "github.com")[twitter.com](https://twitter.com/LangChainAI "twitter.com")

#### Cookie consent

We use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they're searching for. **Clicking "Accept" makes our documentation better. Thank you!** â¤ï¸

- Google Analytics
- GitHub

Accept Reject

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/agent_executor.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/agent_executor.ipynb)

# Build an Agent with AgentExecutor (Legacy)

important

This section will cover building with the legacy LangChain AgentExecutor. These are fine for getting started, but past a certain point, you will likely want flexibility and control that they do not offer. For working with more advanced agents, we'd recommend checking out [LangGraph Agents](/docs/concepts/architecture/#langgraph) or the [migration guide](/docs/how_to/migrate_agent/)

By themselves, language models can't take actions - they just output text. A big use case for LangChain is creating [**agents**](/docs/concepts/agents/). Agents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be. The results of those actions can then be fed back into the agent and it determines whether more actions are needed, or whether it is okay to finish.

In this tutorial, we will build an agent that can interact with multiple different tools: one being a local database, the other being a search engine. You will be able to ask this agent questions, watch it call tools, and have conversations with it.

## Concepts[â€‹](#concepts "Direct link to Concepts")

Concepts we will cover are:

- Using [language models](/docs/concepts/chat_models/), in particular their tool calling ability
- Creating a [Retriever](/docs/concepts/retrievers/) to expose specific information to our agent
- Using a Search [Tool](/docs/concepts/tools/) to look up things online
- [`Chat History`](/docs/concepts/chat_history/), which allows a chatbot to "remember" past interactions and take them into account when responding to follow-up questions.
- Debugging and tracing your application using [LangSmith](https://docs.smith.langchain.com/)

## Setup[â€‹](#setup "Direct link to Setup")

### Jupyter Notebook[â€‹](#jupyter-notebook "Direct link to Jupyter Notebook")

This guide (and most of the other guides in the documentation) uses [Jupyter notebooks](https://jupyter.org/) and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.

This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See [here](https://jupyter.org/install) for instructions on how to install.

### Installation[â€‹](#installation "Direct link to Installation")

To install LangChain run:

- Pip
- Conda

```bash
pip install langchain
```

```bash
conda install langchain -c conda-forge
```

For more details, see our [Installation guide](/docs/how_to/installation/).

### LangSmith[â€‹](#langsmith "Direct link to LangSmith")

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
```

Or, if in a notebook, you can set them with:

```python
import getpass
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

## Define tools[â€‹](#define-tools "Direct link to Define tools")

We first need to create the tools we want to use. We will use two tools: [Tavily](/docs/integrations/tools/tavily_search/) (to search online) and then a retriever over a local index we will create

### [Tavily](/docs/integrations/tools/tavily_search/)[â€‹](#tavily "Direct link to tavily")

We have a built-in tool in LangChain to easily use Tavily search engine as tool. Note that this requires an API key - they have a free tier, but if you don't have one or don't want to create one, you can always ignore this step.

Once you create your API key, you will need to export that as:

```bash
export TAVILY_API_KEY="..."
```

```python
from langchain_community.tools.tavily_search import TavilySearchResults
```

**API Reference:**[TavilySearchResults](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html)

```python
search = TavilySearchResults(max_results=2)
```

```python
search.invoke("what is the weather in SF")
```

```output
[{'url': 'https://www.weatherapi.com/',
  'content': "{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1714000492, 'localtime': '2024-04-24 16:14'}, 'current': {'last_updated_epoch': 1713999600, 'last_updated': '2024-04-24 16:00', 'temp_c': 15.6, 'temp_f': 60.1, 'is_day': 1, 'condition': {'text': 'Overcast', 'icon': '//cdn.weatherapi.com/weather/64x64/day/122.png', 'code': 1009}, 'wind_mph': 10.5, 'wind_kph': 16.9, 'wind_degree': 330, 'wind_dir': 'NNW', 'pressure_mb': 1018.0, 'pressure_in': 30.06, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 72, 'cloud': 100, 'feelslike_c': 15.6, 'feelslike_f': 60.1, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 14.8, 'gust_kph': 23.8}}"},
 {'url': 'https://www.weathertab.com/en/c/e/04/united-states/california/san-francisco/',
  'content': 'San Francisco Weather Forecast for Apr 2024 - Risk of Rain Graph. Rain Risk Graph: Monthly Overview. Bar heights indicate rain risk percentages. Yellow bars mark low-risk days, while black and grey bars signal higher risks. Grey-yellow bars act as buffers, advising to keep at least one day clear from the riskier grey and black days, guiding ...'}]
```

### Retriever[â€‹](#retriever "Direct link to Retriever")

We will also create a retriever over some data of our own. For a deeper explanation of each step here, see [this tutorial](/docs/tutorials/rag/).

```python
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

loader = WebBaseLoader("https://docs.smith.langchain.com/overview")
docs = loader.load()
documents = RecursiveCharacterTextSplitter(
    chunk_size=1000, chunk_overlap=200
).split_documents(docs)
vector = FAISS.from_documents(documents, OpenAIEmbeddings())
retriever = vector.as_retriever()
```

**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

```python
retriever.invoke("how to upload a dataset")[0]
```

```output
Document(page_content='# The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix="sample-experiment", # The name of the experiment    metadata={      "version": "1.0.0",      "revision_id": "beta"    },)import { Client, Run, Example } from \'langsmith\';import { runOnDataset } from \'langchain/smith\';import { EvaluationResult } from \'langsmith/evaluation\';const client = new Client();// Define dataset: these are your test casesconst datasetName = "Sample Dataset";const dataset = await client.createDataset(datasetName, {    description: "A sample dataset in LangSmith."});await client.createExamples({    inputs: [        { postfix: "to LangSmith" },        { postfix: "to Evaluations in LangSmith" },    ],    outputs: [        { output: "Welcome to LangSmith" },        { output: "Welcome to Evaluations in LangSmith" },    ],    datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async ({ run, example }: { run: Run; example?:', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | \uf8ffÃ¼Â¶ÃºÃ”âˆÃ¨\uf8ffÃ¼Ãµâ€ Ã”âˆÃ¨ LangSmith', 'description': 'Introduction', 'language': 'en'})
```

Now that we have populated our index that we will do doing retrieval over, we can easily turn it into a tool (the format needed for an agent to properly use it)

```python
from langchain.tools.retriever import create_retriever_tool
```

**API Reference:**[create\_retriever\_tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.retriever.create_retriever_tool.html)

```python
retriever_tool = create_retriever_tool(
    retriever,
    "langsmith_search",
    "Search for information about LangSmith. For any questions about LangSmith, you must use this tool!",
)
```

### Tools[â€‹](#tools "Direct link to Tools")

Now that we have created both, we can create a list of tools that we will use downstream.

```python
tools = [search, retriever_tool]
```

## Using Language Models[â€‹](#using-language-models "Direct link to Using Language Models")

Next, let's learn how to use a language model by to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4", model_provider="openai")
```

You can call the language model by passing in a list of messages. By default, the response is a `content` string.

```python
from langchain_core.messages import HumanMessage

response = model.invoke([HumanMessage(content="hi!")])
response.content
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

```output
'Hello! How can I assist you today?'
```

We can now see what it is like to enable this model to do tool calling. In order to enable that we use `.bind_tools` to give the language model knowledge of these tools

```python
model_with_tools = model.bind_tools(tools)
```

We can now call the model. Let's first call it with a normal message, and see how it responds. We can look at both the `content` field as well as the `tool_calls` field.

```python
response = model_with_tools.invoke([HumanMessage(content="Hi!")])

print(f"ContentString: {response.content}")
print(f"ToolCalls: {response.tool_calls}")
```

```output
ContentString: Hello! How can I assist you today?
ToolCalls: []
```

Now, let's try calling it with some input that would expect a tool to be called.

```python
response = model_with_tools.invoke([HumanMessage(content="What's the weather in SF?")])

print(f"ContentString: {response.content}")
print(f"ToolCalls: {response.tool_calls}")
```

```output
ContentString: 
ToolCalls: [{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_4HteVahXkRAkWjp6dGXryKZX'}]
```

We can see that there's now no content, but there is a tool call! It wants us to call the Tavily Search tool.

This isn't calling that tool yet - it's just telling us to. In order to actually calll it, we'll want to create our agent.

## Create the agent[â€‹](#create-the-agent "Direct link to Create the agent")

Now that we have defined the tools and the LLM, we can create the agent. We will be using a tool calling agent - for more information on this type of agent, as well as other options, see [this guide](/docs/concepts/agents/).

We can first choose the prompt we want to use to guide the agent.

If you want to see the contents of this prompt and have access to LangSmith, you can go to:

[https://smith.langchain.com/hub/hwchase17/openai-functions-agent](https://smith.langchain.com/hub/hwchase17/openai-functions-agent)

```python
from langchain import hub

# Get the prompt to use - you can modify this!
prompt = hub.pull("hwchase17/openai-functions-agent")
prompt.messages
```

**API Reference:**[hub](https://python.langchain.com/api_reference/langchain/hub/langchain.hub.hub.html)

```output
[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),
 MessagesPlaceholder(variable_name='chat_history', optional=True),
 HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),
 MessagesPlaceholder(variable_name='agent_scratchpad')]
```

Now, we can initialize the agent with the LLM, the prompt, and the tools. The agent is responsible for taking in input and deciding what actions to take. Crucially, the Agent does not execute those actions - that is done by the AgentExecutor (next step). For more information about how to think about these components, see our [conceptual guide](/docs/concepts/agents/).

Note that we are passing in the `model`, not `model_with_tools`. That is because `create_tool_calling_agent` will call `.bind_tools` for us under the hood.

```python
from langchain.agents import create_tool_calling_agent

agent = create_tool_calling_agent(model, tools, prompt)
```

**API Reference:**[create\_tool\_calling\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html)

Finally, we combine the agent (the brains) with the tools inside the AgentExecutor (which will repeatedly call the agent and execute tools).

```python
from langchain.agents import AgentExecutor

agent_executor = AgentExecutor(agent=agent, tools=tools)
```

**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html)

## Run the agent[â€‹](#run-the-agent "Direct link to Run the agent")

We can now run the agent on a few queries! Note that for now, these are all **stateless** queries (it won't remember previous interactions).

First up, let's how it responds when there's no need to call a tool:

```python
agent_executor.invoke({"input": "hi!"})
```

```output
{'input': 'hi!', 'output': 'Hello! How can I assist you today?'}
```

In order to see exactly what is happening under the hood (and to make sure it's not calling a tool) we can take a look at the [LangSmith trace](https://smith.langchain.com/public/8441812b-94ce-4832-93ec-e1114214553a/r)

Let's now try it out on an example where it should be invoking the retriever

```python
agent_executor.invoke({"input": "how can langsmith help with testing?"})
```

```output
{'input': 'how can langsmith help with testing?',
 'output': 'LangSmith is a platform that aids in building production-grade Language Learning Model (LLM) applications. It can assist with testing in several ways:\n\n1. **Monitoring and Evaluation**: LangSmith allows close monitoring and evaluation of your application. This helps you to ensure the quality of your application and deploy it with confidence.\n\n2. **Tracing**: LangSmith has tracing capabilities that can be beneficial for debugging and understanding the behavior of your application.\n\n3. **Evaluation Capabilities**: LangSmith has built-in tools for evaluating the performance of your LLM. \n\n4. **Prompt Hub**: This is a prompt management tool built into LangSmith that can help in testing different prompts and their responses.\n\nPlease note that to use LangSmith, you would need to install it and create an API key. The platform offers Python and Typescript SDKs for utilization. It works independently and does not require the use of LangChain.'}
```

Let's take a look at the [LangSmith trace](https://smith.langchain.com/public/762153f6-14d4-4c98-8659-82650f860c62/r) to make sure it's actually calling that.

Now let's try one where it needs to call the search tool:

```python
agent_executor.invoke({"input": "whats the weather in sf?"})
```

```output
{'input': 'whats the weather in sf?',
 'output': 'The current weather in San Francisco is partly cloudy with a temperature of 16.1Â°C (61.0Â°F). The wind is coming from the WNW at a speed of 10.5 mph. The humidity is at 67%. [source](https://www.weatherapi.com/)'}
```

We can check out the [LangSmith trace](https://smith.langchain.com/public/36df5b1a-9a0b-4185-bae2-964e1d53c665/r) to make sure it's calling the search tool effectively.

## Adding in memory[â€‹](#adding-in-memory "Direct link to Adding in memory")

As mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in previous `chat_history`. Note: it needs to be called `chat_history` because of the prompt we are using. If we use a different prompt, we could change the variable name

```python
# Here we pass in an empty list of messages for chat_history because it is the first message in the chat
agent_executor.invoke({"input": "hi! my name is bob", "chat_history": []})
```

```output
{'input': 'hi! my name is bob',
 'chat_history': [],
 'output': 'Hello Bob! How can I assist you today?'}
```

```python
from langchain_core.messages import AIMessage, HumanMessage
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

```python
agent_executor.invoke(
    {
        "chat_history": [
            HumanMessage(content="hi! my name is bob"),
            AIMessage(content="Hello Bob! How can I assist you today?"),
        ],
        "input": "what's my name?",
    }
)
```

```output
{'chat_history': [HumanMessage(content='hi! my name is bob'),
  AIMessage(content='Hello Bob! How can I assist you today?')],
 'input': "what's my name?",
 'output': 'Your name is Bob. How can I assist you further?'}
```

If we want to keep track of these messages automatically, we can wrap this in a RunnableWithMessageHistory. For more information on how to use this, see [this guide](/docs/how_to/message_history/).

```python
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

store = {}


def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]
```

**API Reference:**[ChatMessageHistory](https://python.langchain.com/api_reference/core/chat_history/langchain_core.chat_history.ChatMessageHistory.html) | [BaseChatMessageHistory](https://python.langchain.com/api_reference/core/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html)

Because we have multiple inputs, we need to specify two things:

- `input_messages_key`: The input key to use to add to the conversation history.
- `history_messages_key`: The key to add the loaded messages into.

```python
agent_with_chat_history = RunnableWithMessageHistory(
    agent_executor,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)
```

```python
agent_with_chat_history.invoke(
    {"input": "hi! I'm bob"},
    config={"configurable": {"session_id": "<foo>"}},
)
```

```output
{'input': "hi! I'm bob",
 'chat_history': [],
 'output': 'Hello Bob! How can I assist you today?'}
```

```python
agent_with_chat_history.invoke(
    {"input": "what's my name?"},
    config={"configurable": {"session_id": "<foo>"}},
)
```

```output
{'input': "what's my name?",
 'chat_history': [HumanMessage(content="hi! I'm bob"),
  AIMessage(content='Hello Bob! How can I assist you today?')],
 'output': 'Your name is Bob.'}
```

Example LangSmith trace: [https://smith.langchain.com/public/98c8d162-60ae-4493-aa9f-992d87bd0429/r](https://smith.langchain.com/public/98c8d162-60ae-4493-aa9f-992d87bd0429/r)

## Conclusion[â€‹](#conclusion "Direct link to Conclusion")

That's a wrap! In this quick start we covered how to create a simple agent. Agents are a complex topic, and there's lot to learn!

important

This section covered building with LangChain Agents. They are fine for getting started, but past a certain point you will likely want flexibility and control which they do not offer. To develop more advanced agents, we recommend checking out [LangGraph](/docs/concepts/architecture/#langgraph)

If you want to continue using LangChain agents, some good advanced guides are:

- [How to use LangGraph's built-in versions of `AgentExecutor`](/docs/how_to/migrate_agent/)
- [How to create a custom agent](https://python.langchain.com/v0.1/docs/modules/agents/how_to/custom_agent/)
- [How to stream responses from an agent](https://python.langchain.com/v0.1/docs/modules/agents/how_to/streaming/)
- [How to return structured output from an agent](https://python.langchain.com/v0.1/docs/modules/agents/how_to/agent_structured/)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/agent_executor.ipynb)

* * *


- [Concepts](#concepts)
- [Setup](#setup)
  
  - [Jupyter Notebook](#jupyter-notebook)
  - [Installation](#installation)
  - [LangSmith](#langsmith)
- [Define tools](#define-tools)
  
  - [Tavily](#tavily)
  - [Retriever](#retriever)
  - [Tools](#tools)
- [Using Language Models](#using-language-models)
- [Create the agent](#create-the-agent)
- [Run the agent](#run-the-agent)
- [Adding in memory](#adding-in-memory)
- [Conclusion](#conclusion)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/testing.mdx)

# Testing

Testing is a critical part of the development process that ensures your code works as expected and meets the desired quality standards.

In the LangChain ecosystem, we have 2 main types of tests: **unit tests** and **integration tests**.

For integrations that implement standard LangChain abstractions, we have a set of **standard tests** (both unit and integration) that help maintain compatibility between different components and ensure reliability of high-usage ones.

## Unit Tests[â€‹](#unit-tests "Direct link to Unit Tests")

**Definition**: Unit tests are designed to validate the smallest parts of your codeâ€”individual functions or methodsâ€”ensuring they work as expected in isolation. They do not rely on external systems or integrations.

**Example**: Testing the `convert_langchain_aimessage_to_dict` function to confirm it correctly converts an AI message to a dictionary format:

```python
from langchain_core.messages import AIMessage, ToolCall, convert_to_openai_messages

def test_convert_to_openai_messages():
    ai_message = AIMessage(
        content="Let me call that tool for you!",
        tool_calls=[
            ToolCall(name='parrot_multiply_tool', id='1', args={'a': 2, 'b': 3}),
        ]
    )
    
    result = convert_to_openai_messages(ai_message)
    
    expected = {
        "role": "assistant",
        "tool_calls": [
            {
                "type": "function",
                "id": "1",
                "function": {
                    "name": "parrot_multiply_tool",
                    "arguments": '{"a": 2, "b": 3}',
                },
            }
        ],
        "content": "Let me call that tool for you!",
    }
    assert result == expected  # Ensure conversion matches expected output
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [ToolCall](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolCall.html) | [convert\_to\_openai\_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.convert_to_openai_messages.html)

* * *

## Integration Tests[â€‹](#integration-tests "Direct link to Integration Tests")

**Definition**: Integration tests validate that multiple components or systems work together as expected. For tools or integrations relying on external services, these tests often ensure end-to-end functionality.

**Example**: Testing `ParrotMultiplyTool` with access to an API service that multiplies two numbers and adds 80:

```python
def test_integration_with_service():
    tool = ParrotMultiplyTool()
    result = tool.invoke({"a": 2, "b": 3})
    assert result == 86
```

* * *

## Standard Tests[â€‹](#standard-tests "Direct link to Standard Tests")

**Definition**: Standard tests are pre-defined tests provided by LangChain to ensure consistency and reliability across all tools and integrations. They include both unit and integration test templates tailored for LangChain components.

**Example**: Subclassing LangChain's `ToolsUnitTests` or `ToolsIntegrationTests` to automatically run standard tests:

```python
from langchain_tests.unit_tests import ToolsUnitTests

class TestParrotMultiplyToolUnit(ToolsUnitTests):
    @property
    def tool_constructor(self):
        return ParrotMultiplyTool

    def tool_invoke_params_example(self):
        return {"a": 2, "b": 3}
```

**API Reference:**[ToolsUnitTests](https://python.langchain.com/api_reference/tests/unit_tests/langchain_tests.unit_tests.tools.ToolsUnitTests.html)

To learn more, check out our guide on [how to add standard tests to an integration](/docs/contributing/how_to/integrations/standard_tests/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/testing.mdx)

* * *


- [Unit Tests](#unit-tests)
- [Integration Tests](#integration-tests)
- [Standard Tests](#standard-tests)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/recursive_json_splitter.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/recursive_json_splitter.ipynb)

# How to split JSON data

This json splitter [splits](/docs/concepts/text_splitters/) json data while allowing control over chunk sizes. It traverses json data depth first and builds smaller json chunks. It attempts to keep nested json objects whole but will split them if needed to keep chunks between a min\_chunk\_size and the max\_chunk\_size.

If the value is not a nested json, but rather a very large string the string will not be split. If you need a hard cap on the chunk size consider composing this with a Recursive Text splitter on those chunks. There is an optional pre-processing step to split lists, by first converting them to json (dict) and then splitting them as such.

1. How the text is split: json value.
2. How the chunk size is measured: by number of characters.

```python
%pip install -qU langchain-text-splitters
```

First we load some json data:

```python
import json

import requests

# This is a large nested json object and will be loaded as a python dict
json_data = requests.get("https://api.smith.langchain.com/openapi.json").json()
```

## Basic usage[â€‹](#basic-usage "Direct link to Basic usage")

Specify `max_chunk_size` to constrain chunk sizes:

```python
from langchain_text_splitters import RecursiveJsonSplitter

splitter = RecursiveJsonSplitter(max_chunk_size=300)
```

**API Reference:**[RecursiveJsonSplitter](https://python.langchain.com/api_reference/text_splitters/json/langchain_text_splitters.json.RecursiveJsonSplitter.html)

To obtain json chunks, use the `.split_json` method:

```python
# Recursively split json data - If you need to access/manipulate the smaller json chunks
json_chunks = splitter.split_json(json_data=json_data)

for chunk in json_chunks[:3]:
    print(chunk)
```

```output
{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}, 'servers': [{'url': 'https://api.smith.langchain.com', 'description': 'LangSmith API endpoint.'}]}
{'paths': {'/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions'], 'summary': 'Read Tracer Session', 'description': 'Get a specific session.', 'operationId': 'read_tracer_session_api_v1_sessions__session_id__get'}}}}
{'paths': {'/api/v1/sessions/{session_id}': {'get': {'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}
```

To obtain LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects, use the `.create_documents` method:

```python
# The splitter can also output documents
docs = splitter.create_documents(texts=[json_data])

for doc in docs[:3]:
    print(doc)
```

```output
page_content='{"openapi": "3.1.0", "info": {"title": "LangSmith", "version": "0.1.0"}, "servers": [{"url": "https://api.smith.langchain.com", "description": "LangSmith API endpoint."}]}'
page_content='{"paths": {"/api/v1/sessions/{session_id}": {"get": {"tags": ["tracer-sessions"], "summary": "Read Tracer Session", "description": "Get a specific session.", "operationId": "read_tracer_session_api_v1_sessions__session_id__get"}}}}'
page_content='{"paths": {"/api/v1/sessions/{session_id}": {"get": {"security": [{"API Key": []}, {"Tenant ID": []}, {"Bearer Auth": []}]}}}}'
```

Or use `.split_text` to obtain string content directly:

```python
texts = splitter.split_text(json_data=json_data)

print(texts[0])
print(texts[1])
```

```output
{"openapi": "3.1.0", "info": {"title": "LangSmith", "version": "0.1.0"}, "servers": [{"url": "https://api.smith.langchain.com", "description": "LangSmith API endpoint."}]}
{"paths": {"/api/v1/sessions/{session_id}": {"get": {"tags": ["tracer-sessions"], "summary": "Read Tracer Session", "description": "Get a specific session.", "operationId": "read_tracer_session_api_v1_sessions__session_id__get"}}}}
```

## How to manage chunk sizes from list content[â€‹](#how-to-manage-chunk-sizes-from-list-content "Direct link to How to manage chunk sizes from list content")

Note that one of the chunks in this example is larger than the specified `max_chunk_size` of 300. Reviewing one of these chunks that was bigger we see there is a list object there:

```python
print([len(text) for text in texts][:10])
print()
print(texts[3])
```

```output
[171, 231, 126, 469, 210, 213, 237, 271, 191, 232]

{"paths": {"/api/v1/sessions/{session_id}": {"get": {"parameters": [{"name": "session_id", "in": "path", "required": true, "schema": {"type": "string", "format": "uuid", "title": "Session Id"}}, {"name": "include_stats", "in": "query", "required": false, "schema": {"type": "boolean", "default": false, "title": "Include Stats"}}, {"name": "accept", "in": "header", "required": false, "schema": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Accept"}}]}}}}
```

The json splitter by default does not split lists.

Specify `convert_lists=True` to preprocess the json, converting list content to dicts with `index:item` as `key:val` pairs:

```python
texts = splitter.split_text(json_data=json_data, convert_lists=True)
```

Let's look at the size of the chunks. Now they are all under the max

```python
print([len(text) for text in texts][:10])
```

```output
[176, 236, 141, 203, 212, 221, 210, 213, 242, 291]
```

The list has been converted to a dict, but retains all the needed contextual information even if split into many chunks:

```python
print(texts[1])
```

```output
{"paths": {"/api/v1/sessions/{session_id}": {"get": {"tags": {"0": "tracer-sessions"}, "summary": "Read Tracer Session", "description": "Get a specific session.", "operationId": "read_tracer_session_api_v1_sessions__session_id__get"}}}}
```

```python
# We can also look at the documents
docs[1]
```

```output
Document(page_content='{"paths": {"/api/v1/sessions/{session_id}": {"get": {"tags": ["tracer-sessions"], "summary": "Read Tracer Session", "description": "Get a specific session.", "operationId": "read_tracer_session_api_v1_sessions__session_id__get"}}}}')
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/recursive_json_splitter.ipynb)

* * *


- [Basic usage](#basic-usage)
- [How to manage chunk sizes from list content](#how-to-manage-chunk-sizes-from-list-content)








Redirecting...
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/qa_citations.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/qa_citations.ipynb)

# How to get a RAG application to add citations

This guide reviews methods to get a model to cite which parts of the source documents it referenced in generating its response.

We will cover five methods:

1. Using tool-calling to cite document IDs;
2. Using tool-calling to cite documents IDs and provide text snippets;
3. Direct prompting;
4. Retrieval post-processing (i.e., compressing the retrieved context to make it more relevant);
5. Generation post-processing (i.e., issuing a second LLM call to annotate a generated answer with citations).

We generally suggest using the first item of the list that works for your use-case. That is, if your model supports tool-calling, try methods 1 or 2; otherwise, or if those fail, advance down the list.

Let's first create a simple [RAG](/docs/concepts/rag/) chain. To start we'll just retrieve from Wikipedia using the [WikipediaRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.wikipedia.WikipediaRetriever.html). We will use the same [LangGraph](/docs/concepts/architecture/#langgraph) implementation from the [RAG Tutorial](/docs/tutorials/rag/).

## Setup[â€‹](#setup "Direct link to Setup")

First we'll need to install some dependencies:

```python
%pip install -qU langchain-community wikipedia
```

Let's first select a LLM:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

We can now load a [retriever](/docs/concepts/retrievers/) and construct our [prompt](/docs/concepts/prompt_templates/):

```python
from langchain_community.retrievers import WikipediaRetriever
from langchain_core.prompts import ChatPromptTemplate

system_prompt = (
    "You're a helpful AI assistant. Given a user question "
    "and some Wikipedia article snippets, answer the user "
    "question. If none of the articles answer the question, "
    "just say you don't know."
    "\n\nHere are the Wikipedia articles: "
    "{context}"
)

retriever = WikipediaRetriever(top_k_results=6, doc_content_chars_max=2000)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{question}"),
    ]
)
prompt.pretty_print()
```

**API Reference:**[WikipediaRetriever](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.wikipedia.WikipediaRetriever.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
================================[1m System Message [0m================================

You're a helpful AI assistant. Given a user question and some Wikipedia article snippets, answer the user question. If none of the articles answer the question, just say you don't know.

Here are the Wikipedia articles: [33;1m[1;3m{context}[0m

================================[1m Human Message [0m=================================

[33;1m[1;3m{question}[0m
```

Now that we've got a [model](/docs/concepts/chat_models/), [retriever](/docs/concepts/retrievers/) and [prompt](/docs/concepts/prompt_templates/), let's chain them all together. Following the how-to guide on [adding citations](/docs/how_to/qa_citations/) to a RAG application, we'll make it so our chain returns both the answer and the retrieved Documents. This uses the same [LangGraph](/docs/concepts/architecture/#langgraph) implementation as in the [RAG Tutorial](/docs/tutorials/rag/).

```python
from langchain_core.documents import Document
from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict


# Define state for application
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str


# Define application steps
def retrieve(state: State):
    retrieved_docs = retriever.invoke(state["question"])
    return {"context": retrieved_docs}


def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}


# Compile application and test
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```python
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

```python
result = graph.invoke({"question": "How fast are cheetahs?"})

sources = [doc.metadata["source"] for doc in result["context"]]
print(f"Sources: {sources}\n\n")
print(f'Answer: {result["answer"]}')
```

```output
Sources: ['https://en.wikipedia.org/wiki/Cheetah', 'https://en.wikipedia.org/wiki/Southeast_African_cheetah', 'https://en.wikipedia.org/wiki/Footspeed', 'https://en.wikipedia.org/wiki/Fastest_animals', 'https://en.wikipedia.org/wiki/Pursuit_predation', 'https://en.wikipedia.org/wiki/Gepard-class_fast_attack_craft']


Answer: Cheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph).
```

Check out the [LangSmith trace](https://smith.langchain.com/public/ed043789-8599-44de-b88e-ba463ea454a3/r).

## Tool-calling[â€‹](#tool-calling "Direct link to Tool-calling")

If your LLM of choice implements a [tool-calling](/docs/concepts/tool_calling/) feature, you can use it to make the model specify which of the provided documents it's referencing when generating its answer. LangChain tool-calling models implement a `.with_structured_output` method which will force generation adhering to a desired schema (see details [here](/docs/how_to/structured_output/)).

### Cite documents[â€‹](#cite-documents "Direct link to Cite documents")

To cite documents using an identifier, we format the identifiers into the prompt, then use `.with_structured_output` to coerce the LLM to reference these identifiers in its output.

First we define a schema for the output. The `.with_structured_output` supports multiple formats, including JSON schema and Pydantic. Here we will use Pydantic:

```python
from pydantic import BaseModel, Field


class CitedAnswer(BaseModel):
    """Answer the user question based only on the given sources, and cite the sources used."""

    answer: str = Field(
        ...,
        description="The answer to the user question, which is based only on the given sources.",
    )
    citations: List[int] = Field(
        ...,
        description="The integer IDs of the SPECIFIC sources which justify the answer.",
    )
```

Let's see what the model output is like when we pass in our functions and a user input:

```python
structured_llm = llm.with_structured_output(CitedAnswer)

example_q = """What Brian's height?

Source: 1
Information: Suzy is 6'2"

Source: 2
Information: Jeremiah is blonde

Source: 3
Information: Brian is 3 inches shorter than Suzy"""
result = structured_llm.invoke(example_q)

result
```

```output
CitedAnswer(answer='Brian is 5\'11".', citations=[1, 3])
```

Or as a dict:

```python
result.dict()
```

```output
{'answer': 'Brian is 5\'11".', 'citations': [1, 3]}
```

Now we structure the source identifiers into the prompt to replicate with our chain. We will make three changes:

1. Update the prompt to include source identifiers;
2. Use the `structured_llm` (i.e., `llm.with_structured_output(CitedAnswer)`);
3. Return the Pydantic object in the output.

```python
def format_docs_with_id(docs: List[Document]) -> str:
    formatted = [
        f"Source ID: {i}\nArticle Title: {doc.metadata['title']}\nArticle Snippet: {doc.page_content}"
        for i, doc in enumerate(docs)
    ]
    return "\n\n" + "\n\n".join(formatted)


class State(TypedDict):
    question: str
    context: List[Document]
    answer: CitedAnswer


def generate(state: State):
    formatted_docs = format_docs_with_id(state["context"])
    messages = prompt.invoke({"question": state["question"], "context": formatted_docs})
    structured_llm = llm.with_structured_output(CitedAnswer)
    response = structured_llm.invoke(messages)
    return {"answer": response}


graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
```

```python
result = graph.invoke({"question": "How fast are cheetahs?"})

result["answer"]
```

```output
CitedAnswer(answer='Cheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph).', citations=[0, 3])
```

We can inspect the document at index 0, which the model cited:

```python
print(result["context"][0])
```

```output
page_content='The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67â€“94 cm (26â€“37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in). Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail.
The cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran. It lives in a variety of habitats such as savannahs in the Serengeti, arid mountain ranges in the Sahara, and hilly desert terrain.
The cheetah lives in three main social groups: females and their cubs, male "coalitions", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk. It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson's gazelles. The cheetah typically stalks its prey within 60â€“100 m (200â€“330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year. After a gestation of nearly three months, females give birth to a litter of three or four cubs. Cheetah cubs are highly vulnerable to predation by other large carnivores. They are weaned a' metadata={'title': 'Cheetah', 'summary': 'The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67â€“94 cm (26â€“37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in). Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail.\nThe cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran. It lives in a variety of habitats such as savannahs in the Serengeti, arid mountain ranges in the Sahara, and hilly desert terrain.\nThe cheetah lives in three main social groups: females and their cubs, male "coalitions", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk. It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson\'s gazelles. The cheetah typically stalks its prey within 60â€“100 m (200â€“330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year. After a gestation of nearly three months, females give birth to a litter of three or four cubs. Cheetah cubs are highly vulnerable to predation by other large carnivores. They are weaned at around four months and are independent by around 20 months of age.\nThe cheetah is threatened by habitat loss, conflict with humans, poaching and high susceptibility to diseases. The global cheetah population was estimated in 2021 at 6,517; it is listed as Vulnerable on the IUCN Red List. It has been widely depicted in art, literature, advertising, and animation. It was tamed in ancient Egypt and trained for hunting ungulates in the Arabian Peninsula and India. It has been kept in zoos since the early 19th century.', 'source': 'https://en.wikipedia.org/wiki/Cheetah'}
```

LangSmith trace: [https://smith.langchain.com/public/6f34d136-451d-4625-90c8-2d8decebc21a/r](https://smith.langchain.com/public/6f34d136-451d-4625-90c8-2d8decebc21a/r)

### Cite snippets[â€‹](#cite-snippets "Direct link to Cite snippets")

To return text spans (perhaps in addition to source identifiers), we can use the same approach. The only change will be to build a more complex output schema, here using Pydantic, that includes a "quote" alongside a source identifier.

*Aside: Note that if we break up our documents so that we have many documents with only a sentence or two instead of a few long documents, citing documents becomes roughly equivalent to citing snippets, and may be easier for the model because the model just needs to return an identifier for each snippet instead of the actual text. Probably worth trying both approaches and evaluating.*

```python
class Citation(BaseModel):
    source_id: int = Field(
        ...,
        description="The integer ID of a SPECIFIC source which justifies the answer.",
    )
    quote: str = Field(
        ...,
        description="The VERBATIM quote from the specified source that justifies the answer.",
    )


class QuotedAnswer(BaseModel):
    """Answer the user question based only on the given sources, and cite the sources used."""

    answer: str = Field(
        ...,
        description="The answer to the user question, which is based only on the given sources.",
    )
    citations: List[Citation] = Field(
        ..., description="Citations from the given sources that justify the answer."
    )
```

```python
class State(TypedDict):
    question: str
    context: List[Document]
    answer: QuotedAnswer


def generate(state: State):
    formatted_docs = format_docs_with_id(state["context"])
    messages = prompt.invoke({"question": state["question"], "context": formatted_docs})
    structured_llm = llm.with_structured_output(QuotedAnswer)
    response = structured_llm.invoke(messages)
    return {"answer": response}


graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
```

Here we see that the model has extracted a relevant snippet of text from source 0:

```python
result = graph.invoke({"question": "How fast are cheetahs?"})

result["answer"]
```

```output
QuotedAnswer(answer='Cheetahs are capable of running at speeds of 93 to 104 km/h (58 to 65 mph).', citations=[Citation(source_id=0, quote='The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed.')])
```

LangSmith trace: [https://smith.langchain.com/public/e16dc72f-4261-4f25-a9a7-906238737283/r](https://smith.langchain.com/public/e16dc72f-4261-4f25-a9a7-906238737283/r)

## Direct prompting[â€‹](#direct-prompting "Direct link to Direct prompting")

Some models don't support function-calling. We can achieve similar results with direct prompting. Let's try instructing a model to generate structured XML for its output:

```python
xml_system = """You're a helpful AI assistant. Given a user question and some Wikipedia article snippets, \
answer the user question and provide citations. If none of the articles answer the question, just say you don't know.

Remember, you must return both an answer and citations. A citation consists of a VERBATIM quote that \
justifies the answer and the ID of the quote article. Return a citation for every quote across all articles \
that justify the answer. Use the following format for your final output:

<cited_answer>
    <answer></answer>
    <citations>
        <citation><source_id></source_id><quote></quote></citation>
        <citation><source_id></source_id><quote></quote></citation>
        ...
    </citations>
</cited_answer>

Here are the Wikipedia articles:{context}"""
xml_prompt = ChatPromptTemplate.from_messages(
    [("system", xml_system), ("human", "{question}")]
)
```

We now make similar small updates to our chain:

1. We update the formatting function to wrap the retrieved context in XML tags;
2. We do not use `.with_structured_output` (e.g., because it does not exist for a model);
3. We use [XMLOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.xml.XMLOutputParser.html) to parse the answer into a dict.

```python
from langchain_core.output_parsers import XMLOutputParser


def format_docs_xml(docs: List[Document]) -> str:
    formatted = []
    for i, doc in enumerate(docs):
        doc_str = f"""\
    <source id=\"{i}\">
        <title>{doc.metadata['title']}</title>
        <article_snippet>{doc.page_content}</article_snippet>
    </source>"""
        formatted.append(doc_str)
    return "\n\n<sources>" + "\n".join(formatted) + "</sources>"


class State(TypedDict):
    question: str
    context: List[Document]
    answer: dict


def generate(state: State):
    formatted_docs = format_docs_xml(state["context"])
    messages = xml_prompt.invoke(
        {"question": state["question"], "context": formatted_docs}
    )
    response = llm.invoke(messages)
    parsed_response = XMLOutputParser().invoke(response)
    return {"answer": parsed_response}


graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
```

**API Reference:**[XMLOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.xml.XMLOutputParser.html)

Note that citations are again structured into the answer:

```python
result = graph.invoke({"question": "How fast are cheetahs?"})

result["answer"]
```

```output
{'cited_answer': [{'answer': 'Cheetahs can run at speeds of 93 to 104 km/h (58 to 65 mph).'},
  {'citations': [{'citation': [{'source_id': '0'},
      {'quote': 'The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph);'}]},
    {'citation': [{'source_id': '3'},
      {'quote': 'The fastest land animal is the cheetah.'}]}]}]}
```

LangSmith trace: [https://smith.langchain.com/public/0c45f847-c640-4b9a-a5fa-63559e413527/r](https://smith.langchain.com/public/0c45f847-c640-4b9a-a5fa-63559e413527/r)

## Retrieval post-processing[â€‹](#retrieval-post-processing "Direct link to Retrieval post-processing")

Another approach is to post-process our retrieved documents to compress the content, so that the source content is already minimal enough that we don't need the model to cite specific sources or spans. For example, we could break up each document into a sentence or two, embed those and keep only the most relevant ones. LangChain has some built-in components for this. Here we'll use a [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/text_splitter/langchain_text_splitters.RecursiveCharacterTextSplitter.html#langchain_text_splitters.RecursiveCharacterTextSplitter), which creates chunks of a specified size by splitting on separator substrings, and an [EmbeddingsFilter](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.embeddings_filter.EmbeddingsFilter.html#langchain.retrievers.document_compressors.embeddings_filter.EmbeddingsFilter), which keeps only the texts with the most relevant embeddings.

This approach effectively updates our `retrieve` step to compress the documents. Let's first select an [embedding model](/docs/integrations/text_embedding/):

Select [embeddings model](/docs/integrations/text_embedding/):

OpenAIâ–¾

[OpenAI](#)

[Azure](#)

[Google](#)

[AWS](#)

[HuggingFace](#)

[Ollama](#)

[Cohere](#)

[MistralAI](#)

[Nomic](#)

[NVIDIA](#)

[Voyage AI](#)

[IBM watsonx](#)

[Fake](#)

```bash
pip install -qU langchain-openai
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
```

We can now rewrite the `retrieve` step:

```python
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain_core.runnables import RunnableParallel
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=0,
    separators=["\n\n", "\n", ".", " "],
    keep_separator=False,
)
compressor = EmbeddingsFilter(embeddings=embeddings, k=10)


class State(TypedDict):
    question: str
    context: List[Document]
    answer: str


def retrieve(state: State):
    retrieved_docs = retriever.invoke(state["question"])
    split_docs = splitter.split_documents(retrieved_docs)
    stateful_docs = compressor.compress_documents(split_docs, state["question"])
    return {"context": stateful_docs}
```

**API Reference:**[EmbeddingsFilter](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.embeddings_filter.EmbeddingsFilter.html) | [RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

Let's test this out:

```python
retrieval_result = retrieve({"question": "How fast are cheetahs?"})

for doc in retrieval_result["context"]:
    print(f"{doc.page_content}\n\n")
```

```output
Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail


The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67â€“94 cm (26â€“37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in)


2 mph), or 171 body lengths per second. The cheetah, the fastest land mammal, scores at only 16 body lengths per second


It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson's gazelles. The cheetah typically stalks its prey within 60â€“100 m (200â€“330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year


The cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran


The cheetah lives in three main social groups: females and their cubs, male "coalitions", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk


The Southeast African cheetah (Acinonyx jubatus jubatus) is the nominate cheetah subspecies native to East and Southern Africa. The Southern African cheetah lives mainly in the lowland areas and deserts of the Kalahari, the savannahs of Okavango Delta, and the grasslands of the Transvaal region in South Africa. In Namibia, cheetahs are mostly found in farmlands


Subpopulations have been called "South African cheetah" and "Namibian cheetah."


In India, four cheetahs of the subspecies are living in Kuno National Park in Madhya Pradesh after having been introduced there


Acinonyx jubatus velox proposed in 1913 by Edmund Heller on basis of a cheetah that was shot by Kermit Roosevelt in June 1909 in the Kenyan highlands.
Acinonyx rex proposed in 1927 by Reginald Innes Pocock on basis of a specimen from the Umvukwe Range in Rhodesia.
```

Next, we assemble it into our chain as before:

```python
# This step is unchanged from our original RAG implementation
def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}


graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
```

```python
result = graph.invoke({"question": "How fast are cheetahs?"})

print(result["answer"])
```

```output
Cheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph). They are known as the fastest land animals.
```

Note that the document content is now compressed, although the document objects retain the original content in a "summary" key in their metadata. These summaries are not passed to the model; only the condensed content is.

```python
result["context"][0].page_content  # passed to model
```

```output
'Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail'
```

```python
result["context"][0].metadata["summary"]  # original document  # original document
```

```output
'The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67â€“94 cm (26â€“37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in). Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail.\nThe cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran. It lives in a variety of habitats such as savannahs in the Serengeti, arid mountain ranges in the Sahara, and hilly desert terrain.\nThe cheetah lives in three main social groups: females and their cubs, male "coalitions", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk. It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson\'s gazelles. The cheetah typically stalks its prey within 60â€“100 m (200â€“330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year. After a gestation of nearly three months, females give birth to a litter of three or four cubs. Cheetah cubs are highly vulnerable to predation by other large carnivores. They are weaned at around four months and are independent by around 20 months of age.\nThe cheetah is threatened by habitat loss, conflict with humans, poaching and high susceptibility to diseases. The global cheetah population was estimated in 2021 at 6,517; it is listed as Vulnerable on the IUCN Red List. It has been widely depicted in art, literature, advertising, and animation. It was tamed in ancient Egypt and trained for hunting ungulates in the Arabian Peninsula and India. It has been kept in zoos since the early 19th century.'
```

LangSmith trace: [https://smith.langchain.com/public/21b0dc15-d70a-4293-9402-9c70f9178e66/r](https://smith.langchain.com/public/21b0dc15-d70a-4293-9402-9c70f9178e66/r)

## Generation post-processing[â€‹](#generation-post-processing "Direct link to Generation post-processing")

Another approach is to post-process our model generation. In this example we'll first generate just an answer, and then we'll ask the model to annotate it's own answer with citations. The downside of this approach is of course that it is slower and more expensive, because two model calls need to be made.

Let's apply this to our initial chain. If desired, we can implement this via a third step in our application.

```python
class Citation(BaseModel):
    source_id: int = Field(
        ...,
        description="The integer ID of a SPECIFIC source which justifies the answer.",
    )
    quote: str = Field(
        ...,
        description="The VERBATIM quote from the specified source that justifies the answer.",
    )


class AnnotatedAnswer(BaseModel):
    """Annotate the answer to the user question with quote citations that justify the answer."""

    citations: List[Citation] = Field(
        ..., description="Citations from the given sources that justify the answer."
    )


structured_llm = llm.with_structured_output(AnnotatedAnswer)
```

```python
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str
    annotations: AnnotatedAnswer


def retrieve(state: State):
    retrieved_docs = retriever.invoke(state["question"])
    return {"context": retrieved_docs}


def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    response = llm.invoke(messages)
    return {"answer": response.content}


def annotate(state: State):
    formatted_docs = format_docs_with_id(state["context"])
    messages = [
        ("system", system_prompt.format(context=formatted_docs)),
        ("human", state["question"]),
        ("ai", state["answer"]),
        ("human", "Annotate your answer with citations."),
    ]
    response = structured_llm.invoke(messages)
    return {"annotations": response}


graph_builder = StateGraph(State).add_sequence([retrieve, generate, annotate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
```

```python
display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/png;base64,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)

```python
result = graph.invoke({"question": "How fast are cheetahs?"})

print(result["answer"])
```

```output
Cheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph).
```

```python
result["annotations"]
```

```output
AnnotatedAnswer(citations=[Citation(source_id=0, quote='The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph)')])
```

LangSmith trace: [https://smith.langchain.com/public/b8257417-573b-47c4-a750-74e542035f19/r](https://smith.langchain.com/public/b8257417-573b-47c4-a750-74e542035f19/r)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/qa_citations.ipynb)

* * *


- [Setup](#setup)
- [Tool-calling](#tool-calling)
  
  - [Cite documents](#cite-documents)
  - [Cite snippets](#cite-snippets)
- [Direct prompting](#direct-prompting)
- [Retrieval post-processing](#retrieval-post-processing)
- [Generation post-processing](#generation-post-processing)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_embeddings.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_embeddings.ipynb)

# Custom Embeddings

LangChain is integrated with many [3rd party embedding models](/docs/integrations/text_embedding/). In this guide we'll show you how to create a custom Embedding class, in case a built-in one does not already exist. Embeddings are critical in natural language processing applications as they convert text into a numerical form that algorithms can understand, thereby enabling a wide range of applications such as similarity search, text classification, and clustering.

Implementing embeddings using the standard [Embeddings](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.embeddings.Embeddings.html) interface will allow your embeddings to be utilized in existing `LangChain` abstractions (e.g., as the embeddings powering a [VectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html) or cached using [CacheBackedEmbeddings](/docs/how_to/caching_embeddings/)).

## Interface[â€‹](#interface "Direct link to Interface")

The current `Embeddings` abstraction in LangChain is designed to operate on text data. In this implementation, the inputs are either single strings or lists of strings, and the outputs are lists of numerical arrays (vectors), where each vector represents an embedding of the input text into some n-dimensional space.

Your custom embedding must implement the following methods:

| Method/Property           | Description                                                    | Required/Optional |
|---------------------------|----------------------------------------------------------------|-------------------|
| `embed_documents(texts)`  | Generates embeddings for a list of strings.                    | Required          |
| `embed_query(text)`       | Generates an embedding for a single text query.                | Required          |
| `aembed_documents(texts)` | Asynchronously generates embeddings for a list of strings.     | Optional          |
| `aembed_query(text)`      | Asynchronously generates an embedding for a single text query. | Optional          |

These methods ensure that your embedding model can be integrated seamlessly into the LangChain framework, providing both synchronous and asynchronous capabilities for scalability and performance optimization.

note

`Embeddings` do not currently implement the [Runnable](/docs/concepts/runnables/) interface and are also **not** instances of pydantic `BaseModel`.

### Embedding queries vs documents[â€‹](#embedding-queries-vs-documents "Direct link to Embedding queries vs documents")

The `embed_query` and `embed_documents` methods are required. These methods both operate on string inputs. The accessing of `Document.page_content` attributes is handled by the vector store using the embedding model for legacy reasons.

`embed_query` takes in a single string and returns a single embedding as a list of floats. If your model has different modes for embedding queries vs the underlying documents, you can implement this method to handle that.

`embed_documents` takes in a list of strings and returns a list of embeddings as a list of lists of floats.

note

`embed_documents` takes in a list of plain text, not a list of LangChain `Document` objects. The name of this method may change in future versions of LangChain.

## Implementation[â€‹](#implementation "Direct link to Implementation")

As an example, we'll implement a simple embeddings model that returns a constant vector. This model is for illustrative purposes only.

```python
from typing import List

from langchain_core.embeddings import Embeddings


class ParrotLinkEmbeddings(Embeddings):
    """ParrotLink embedding model integration.

    # TODO: Populate with relevant params.
    Key init args â€” completion params:
        model: str
            Name of ParrotLink model to use.

    See full list of supported init args and their descriptions in the params section.

    # TODO: Replace with relevant init params.
    Instantiate:
        .. code-block:: python

            from langchain_parrot_link import ParrotLinkEmbeddings

            embed = ParrotLinkEmbeddings(
                model="...",
                # api_key="...",
                # other params...
            )

    Embed single text:
        .. code-block:: python

            input_text = "The meaning of life is 42"
            embed.embed_query(input_text)

        .. code-block:: python

            # TODO: Example output.

    # TODO: Delete if token-level streaming isn't supported.
    Embed multiple text:
        .. code-block:: python

             input_texts = ["Document 1...", "Document 2..."]
            embed.embed_documents(input_texts)

        .. code-block:: python

            # TODO: Example output.

    # TODO: Delete if native async isn't supported.
    Async:
        .. code-block:: python

            await embed.aembed_query(input_text)

            # multiple:
            # await embed.aembed_documents(input_texts)

        .. code-block:: python

            # TODO: Example output.

    """

    def __init__(self, model: str):
        self.model = model

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed search docs."""
        return [[0.5, 0.6, 0.7] for _ in texts]

    def embed_query(self, text: str) -> List[float]:
        """Embed query text."""
        return self.embed_documents([text])[0]

    # optional: add custom async implementations here
    # you can also delete these, and the base class will
    # use the default implementation, which calls the sync
    # version in an async executor:

    # async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
    #     """Asynchronous Embed search docs."""
    #     ...

    # async def aembed_query(self, text: str) -> List[float]:
    #     """Asynchronous Embed query text."""
    #     ...
```

**API Reference:**[Embeddings](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.embeddings.Embeddings.html)

### Let's test it ðŸ§ª[â€‹](#lets-test-it- "Direct link to Let's test it ðŸ§ª")

```python
embeddings = ParrotLinkEmbeddings("test-model")
print(embeddings.embed_documents(["Hello", "world"]))
print(embeddings.embed_query("Hello"))
```

```output
[[0.5, 0.6, 0.7], [0.5, 0.6, 0.7]]
[0.5, 0.6, 0.7]
```

## Contributing[â€‹](#contributing "Direct link to Contributing")

We welcome contributions of Embedding models to the LangChain code base.

If you aim to contribute an embedding model for a new provider (e.g., with a new set of dependencies or SDK), we encourage you to publish your implementation in a separate `langchain-*` integration package. This will enable you to appropriately manage dependencies and version your package. Please refer to our [contributing guide](/docs/contributing/how_to/integrations/) for a walkthrough of this process.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/custom_embeddings.ipynb)

* * *


- [Interface](#interface)
  
  - [Embedding queries vs documents](#embedding-queries-vs-documents)
- [Implementation](#implementation)
  
  - [Let's test it ðŸ§ª](#lets-test-it-)
- [Contributing](#contributing)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/merge_message_runs.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/merge_message_runs.ipynb)

# How to merge consecutive messages of the same type

Certain models do not support passing in consecutive [messages](/docs/concepts/messages/) of the same type (a.k.a. "runs" of the same message type).

The `merge_message_runs` utility makes it easy to merge consecutive messages of the same type.

### Setup[â€‹](#setup "Direct link to Setup")

```python
%pip install -qU langchain-core langchain-anthropic
```

## Basic usage[â€‹](#basic-usage "Direct link to Basic usage")

```python
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    merge_message_runs,
)

messages = [
    SystemMessage("you're a good assistant."),
    SystemMessage("you always respond with a joke."),
    HumanMessage([{"type": "text", "text": "i wonder why it's called langchain"}]),
    HumanMessage("and who is harrison chasing anyways"),
    AIMessage(
        'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
    ),
    AIMessage("Why, he's probably chasing after the last cup of coffee in the office!"),
]

merged = merge_message_runs(messages)
print("\n\n".join([repr(x) for x in merged]))
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [merge\_message\_runs](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.merge_message_runs.html)

```output
SystemMessage(content="you're a good assistant.\nyou always respond with a joke.", additional_kwargs={}, response_metadata={})

HumanMessage(content=[{'type': 'text', 'text': "i wonder why it's called langchain"}, 'and who is harrison chasing anyways'], additional_kwargs={}, response_metadata={})

AIMessage(content='Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!\nWhy, he\'s probably chasing after the last cup of coffee in the office!', additional_kwargs={}, response_metadata={})
```

Notice that if the contents of one of the messages to merge is a list of content blocks then the merged message will have a list of content blocks. And if both messages to merge have string contents then those are concatenated with a newline character.

## Chaining[â€‹](#chaining "Direct link to Chaining")

`merge_message_runs` can be used in an imperatively (like above) or declaratively, making it easy to compose with other components in a chain:

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-sonnet-20240229", temperature=0)
# Notice we don't pass in messages. This creates
# a RunnableLambda that takes messages as input
merger = merge_message_runs()
chain = merger | llm
chain.invoke(messages)
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)

```output
Note: you may need to restart the kernel to use updated packages.
```

```output
AIMessage(content=[], additional_kwargs={}, response_metadata={'id': 'msg_01KNGUMTuzBVfwNouLDpUMwf', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 84, 'output_tokens': 3}}, id='run-b908b198-9c24-450b-9749-9d4a8182937b-0', usage_metadata={'input_tokens': 84, 'output_tokens': 3, 'total_tokens': 87})
```

Looking at the LangSmith trace we can see that before the messages are passed to the model they are merged: [https://smith.langchain.com/public/ab558677-cac9-4c59-9066-1ecce5bcd87c/r](https://smith.langchain.com/public/ab558677-cac9-4c59-9066-1ecce5bcd87c/r)

Looking at just the merger, we can see that it's a Runnable object that can be invoked like all Runnables:

```python
merger.invoke(messages)
```

```output
[SystemMessage(content="you're a good assistant.\nyou always respond with a joke.", additional_kwargs={}, response_metadata={}),
 HumanMessage(content=[{'type': 'text', 'text': "i wonder why it's called langchain"}, 'and who is harrison chasing anyways'], additional_kwargs={}, response_metadata={}),
 AIMessage(content='Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!\nWhy, he\'s probably chasing after the last cup of coffee in the office!', additional_kwargs={}, response_metadata={})]
```

`merge_message_runs` can also be placed after a prompt:

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate(
    [
        ("system", "You're great a {skill}"),
        ("system", "You're also great at explaining things"),
        ("human", "{query}"),
    ]
)
chain = prompt | merger | llm
chain.invoke({"skill": "math", "query": "what's the definition of a convergent series"})
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
AIMessage(content='A convergent series is an infinite series whose partial sums approach a finite value as more terms are added. In other words, the sequence of partial sums has a limit.\n\nMore formally, an infinite series Î£ an (where an are the terms of the series) is said to be convergent if the sequence of partial sums:\n\nS1 = a1\nS2 = a1 + a2  \nS3 = a1 + a2 + a3\n...\nSn = a1 + a2 + a3 + ... + an\n...\n\nconverges to some finite number S as n goes to infinity. We write:\n\nlim nâ†’âˆž Sn = S\n\nThe finite number S is called the sum of the convergent infinite series.\n\nIf the sequence of partial sums does not approach any finite limit, the infinite series is said to be divergent.\n\nSome key properties:\n- A series converges if and only if the sequence of its partial sums is a Cauchy sequence.\n- Absolute/conditional convergence criteria help determine if a given series converges.\n- Convergent series have many important applications in mathematics, physics, engineering etc.', additional_kwargs={}, response_metadata={'id': 'msg_01MfV6y2hep7ZNvDz24A36U4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 29, 'output_tokens': 267}}, id='run-9d925f58-021e-4bd0-94fc-f8f5e91010a4-0', usage_metadata={'input_tokens': 29, 'output_tokens': 267, 'total_tokens': 296})
```

LangSmith Trace: [https://smith.langchain.com/public/432150b6-9909-40a7-8ae7-944b7e657438/r/f4ad5fb2-4d38-42a6-b780-25f62617d53f](https://smith.langchain.com/public/432150b6-9909-40a7-8ae7-944b7e657438/r/f4ad5fb2-4d38-42a6-b780-25f62617d53f)

## API reference[â€‹](#api-reference "Direct link to API reference")

For a complete description of all arguments head to the API reference: [https://python.langchain.com/api\_reference/core/messages/langchain\_core.messages.utils.merge\_message\_runs.html](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.merge_message_runs.html)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/merge_message_runs.ipynb)

* * *


- [Setup](#setup)
- [Basic usage](#basic-usage)
- [Chaining](#chaining)
- [API reference](#api-reference)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/multi_prompt_chain.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/multi_prompt_chain.ipynb)

# Migrating from MultiPromptChain

The [`MultiPromptChain`](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html) routed an input query to one of multiple LLMChains-- that is, given an input query, it used a LLM to select from a list of prompts, formatted the query into the prompt, and generated a response.

`MultiPromptChain` does not support common [chat model](/docs/concepts/chat_models/) features, such as message roles and [tool calling](/docs/concepts/tool_calling/).

A [LangGraph](https://langchain-ai.github.io/langgraph/) implementation confers a number of advantages for this problem:

- Supports chat prompt templates, including messages with `system` and other roles;
- Supports the use of tool calling for the routing step;
- Supports streaming of both individual steps and output tokens.

Now let's look at them side-by-side. Note that for this guide we will `langchain-openai >= 0.1.20`

```python
%pip install -qU langchain-core langchain-openai
```

```python
import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

## Legacy[â€‹](#legacy "Direct link to Legacy")

Details

```python
from langchain.chains.router.multi_prompt import MultiPromptChain
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

prompt_1_template = """
You are an expert on animals. Please answer the below query:

{input}
"""

prompt_2_template = """
You are an expert on vegetables. Please answer the below query:

{input}
"""

prompt_infos = [
    {
        "name": "animals",
        "description": "prompt for an animal expert",
        "prompt_template": prompt_1_template,
    },
    {
        "name": "vegetables",
        "description": "prompt for a vegetable expert",
        "prompt_template": prompt_2_template,
    },
]

chain = MultiPromptChain.from_prompts(llm, prompt_infos)
```

**API Reference:**[MultiPromptChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```python
chain.invoke({"input": "What color are carrots?"})
```

```output
{'input': 'What color are carrots?',
 'text': 'Carrots are most commonly orange, but they can also be found in a variety of other colors including purple, yellow, white, and red. The orange variety is the most popular and widely recognized.'}
```

In the [LangSmith trace](https://smith.langchain.com/public/e935238b-0b63-4984-abc8-873b2170a32d/r) we can see the two steps of this process, including the prompts for routing the query and the final selected prompt.

## LangGraph[â€‹](#langgraph "Direct link to LangGraph")

Details

```python
pip install -qU langgraph
```

```python
from operator import itemgetter
from typing import Literal

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from typing_extensions import TypedDict

llm = ChatOpenAI(model="gpt-4o-mini")

# Define the prompts we will route to
prompt_1 = ChatPromptTemplate.from_messages(
    [
        ("system", "You are an expert on animals."),
        ("human", "{input}"),
    ]
)
prompt_2 = ChatPromptTemplate.from_messages(
    [
        ("system", "You are an expert on vegetables."),
        ("human", "{input}"),
    ]
)

# Construct the chains we will route to. These format the input query
# into the respective prompt, run it through a chat model, and cast
# the result to a string.
chain_1 = prompt_1 | llm | StrOutputParser()
chain_2 = prompt_2 | llm | StrOutputParser()


# Next: define the chain that selects which branch to route to.
# Here we will take advantage of tool-calling features to force
# the output to select one of two desired branches.
route_system = "Route the user's query to either the animal or vegetable expert."
route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", route_system),
        ("human", "{input}"),
    ]
)


# Define schema for output:
class RouteQuery(TypedDict):
    """Route query to destination expert."""

    destination: Literal["animal", "vegetable"]


route_chain = route_prompt | llm.with_structured_output(RouteQuery)


# For LangGraph, we will define the state of the graph to hold the query,
# destination, and final answer.
class State(TypedDict):
    query: str
    destination: RouteQuery
    answer: str


# We define functions for each node, including routing the query:
async def route_query(state: State, config: RunnableConfig):
    destination = await route_chain.ainvoke(state["query"], config)
    return {"destination": destination}


# And one node for each prompt
async def prompt_1(state: State, config: RunnableConfig):
    return {"answer": await chain_1.ainvoke(state["query"], config)}


async def prompt_2(state: State, config: RunnableConfig):
    return {"answer": await chain_2.ainvoke(state["query"], config)}


# We then define logic that selects the prompt based on the classification
def select_node(state: State) -> Literal["prompt_1", "prompt_2"]:
    if state["destination"] == "animal":
        return "prompt_1"
    else:
        return "prompt_2"


# Finally, assemble the multi-prompt chain. This is a sequence of two steps:
# 1) Select "animal" or "vegetable" via the route_chain, and collect the answer
# alongside the input query.
# 2) Route the input query to chain_1 or chain_2, based on the
# selection.
graph = StateGraph(State)
graph.add_node("route_query", route_query)
graph.add_node("prompt_1", prompt_1)
graph.add_node("prompt_2", prompt_2)

graph.add_edge(START, "route_query")
graph.add_conditional_edges("route_query", select_node)
graph.add_edge("prompt_1", END)
graph.add_edge("prompt_2", END)
app = graph.compile()
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```python
from IPython.display import Image

Image(app.get_graph().draw_mermaid_png())
```

![](data:image/jpg;base64,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)

We can invoke the chain as follows:

```python
state = await app.ainvoke({"query": "what color are carrots"})
print(state["destination"])
print(state["answer"])
```

```output
{'destination': 'vegetable'}
Carrots are most commonly orange, but they can also come in a variety of other colors, including purple, red, yellow, and white. The different colors often indicate varying flavors and nutritional profiles. For example, purple carrots contain anthocyanins, while orange carrots are rich in beta-carotene, which is converted to vitamin A in the body.
```

In the [LangSmith trace](https://smith.langchain.com/public/1017a9d2-2d2a-4954-a5fd-5689632b4c5f/r) we can see the tool call that routed the query and the prompt that was selected to generate the answer.

## Overview:[â€‹](#overview "Direct link to Overview:")

- Under the hood, `MultiPromptChain` routed the query by instructing the LLM to generate JSON-formatted text, and parses out the intended destination. It took a registry of string prompt templates as input.
- The LangGraph implementation, implemented above via lower-level primitives, uses tool-calling to route to arbitrary chains. In this example, the chains include chat model templates and chat models.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

See [this tutorial](/docs/tutorials/llm_chain/) for more detail on building with prompt templates, LLMs, and output parsers.

Check out the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) for detail on building with LangGraph.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_chains/multi_prompt_chain.ipynb)

* * *


- [Legacy](#legacy)
- [LangGraph](#langgraph)
- [Overview:](#overview)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/split_html.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/split_html.ipynb)

# How to split HTML

Splitting HTML documents into manageable chunks is essential for various text processing tasks such as natural language processing, search indexing, and more. In this guide, we will explore three different text splitters provided by LangChain that you can use to split HTML content effectively:

- [**HTMLHeaderTextSplitter**](#using-htmlheadertextsplitter)
- [**HTMLSectionSplitter**](#using-htmlsectionsplitter)
- [**HTMLSemanticPreservingSplitter**](#using-htmlsemanticpreservingsplitter)

Each of these splitters has unique features and use cases. This guide will help you understand the differences between them, why you might choose one over the others, and how to use them effectively.

```text
%pip install -qU langchain-text-splitters
```

## Overview of the Splitters[â€‹](#overview-of-the-splitters "Direct link to Overview of the Splitters")

### [HTMLHeaderTextSplitter](#using-htmlheadertextsplitter)[â€‹](#htmlheadertextsplitter "Direct link to htmlheadertextsplitter")

info

Useful when you want to preserve the hierarchical structure of a document based on its headings.

**Description**: Splits HTML text based on header tags (e.g., `<h1>`, `<h2>`, `<h3>`, etc.), and adds metadata for each header relevant to any given chunk.

**Capabilities**:

- Splits text at the HTML element level.
- Preserves context-rich information encoded in document structures.
- Can return chunks element by element or combine elements with the same metadata.

* * *

### [HTMLSectionSplitter](#using-htmlsectionsplitter)[â€‹](#htmlsectionsplitter "Direct link to htmlsectionsplitter")

info

Useful when you want to split HTML documents into larger sections, such as `<section>`, `<div>`, or custom-defined sections.

**Description**: Similar to HTMLHeaderTextSplitter but focuses on splitting HTML into sections based on specified tags.

**Capabilities**:

- Uses XSLT transformations to detect and split sections.
- Internally uses `RecursiveCharacterTextSplitter` for large sections.
- Considers font sizes to determine sections.

* * *

### [HTMLSemanticPreservingSplitter](#using-htmlsemanticpreservingsplitter)[â€‹](#htmlsemanticpreservingsplitter "Direct link to htmlsemanticpreservingsplitter")

info

Ideal when you need to ensure that structured elements are not split across chunks, preserving contextual relevancy.

**Description**: Splits HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components.

**Capabilities**:

- Preserves tables, lists, and other specified HTML elements.
- Allows custom handlers for specific HTML tags.
- Ensures that the semantic meaning of the document is maintained.
- Built in normalization &amp; stopword removal

* * *

### Choosing the Right Splitter[â€‹](#choosing-the-right-splitter "Direct link to Choosing the Right Splitter")

- **Use `HTMLHeaderTextSplitter` when**: You need to split an HTML document based on its header hierarchy and maintain metadata about the headers.
- **Use `HTMLSectionSplitter` when**: You need to split the document into larger, more general sections, possibly based on custom tags or font sizes.
- **Use `HTMLSemanticPreservingSplitter` when**: You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained.

| Feature                                     | HTMLHeaderTextSplitter | HTMLSectionSplitter | HTMLSemanticPreservingSplitter |
|---------------------------------------------|------------------------|---------------------|--------------------------------|
| Splits based on headers                     | Yes                    | Yes                 | Yes                            |
| Preserves semantic elements (tables, lists) | No                     | No                  | Yes                            |
| Adds metadata for headers                   | Yes                    | Yes                 | Yes                            |
| Custom handlers for HTML tags               | No                     | No                  | Yes                            |
| Preserves media (images, videos)            | No                     | No                  | Yes                            |
| Considers font sizes                        | No                     | Yes                 | No                             |
| Uses XSLT transformations                   | No                     | Yes                 | No                             |

## Example HTML Document[â€‹](#example-html-document "Direct link to Example HTML Document")

Let's use the following HTML document as an example:

```python
html_string = """
<!DOCTYPE html>
  <html lang='en'>
  <head>
    <meta charset='UTF-8'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0'>
    <title>Fancy Example HTML Page</title>
  </head>
  <body>
    <h1>Main Title</h1>
    <p>This is an introductory paragraph with some basic content.</p>
    
    <h2>Section 1: Introduction</h2>
    <p>This section introduces the topic. Below is a list:</p>
    <ul>
      <li>First item</li>
      <li>Second item</li>
      <li>Third item with <strong>bold text</strong> and <a href='#'>a link</a></li>
    </ul>
    
    <h3>Subsection 1.1: Details</h3>
    <p>This subsection provides additional details. Here's a table:</p>
    <table border='1'>
      <thead>
        <tr>
          <th>Header 1</th>
          <th>Header 2</th>
          <th>Header 3</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Row 1, Cell 1</td>
          <td>Row 1, Cell 2</td>
          <td>Row 1, Cell 3</td>
        </tr>
        <tr>
          <td>Row 2, Cell 1</td>
          <td>Row 2, Cell 2</td>
          <td>Row 2, Cell 3</td>
        </tr>
      </tbody>
    </table>
    
    <h2>Section 2: Media Content</h2>
    <p>This section contains an image and a video:</p>
      <img src='example_image_link.mp4' alt='Example Image'>
      <video controls width='250' src='example_video_link.mp4' type='video/mp4'>
      Your browser does not support the video tag.
    </video>

    <h2>Section 3: Code Example</h2>
    <p>This section contains a code block:</p>
    <pre><code data-lang="html">
    &lt;div&gt;
      &lt;p&gt;This is a paragraph inside a div.&lt;/p&gt;
    &lt;/div&gt;
    </code></pre>

    <h2>Conclusion</h2>
    <p>This is the conclusion of the document.</p>
  </body>
  </html>
"""
```

## Using HTMLHeaderTextSplitter[â€‹](#using-htmlheadertextsplitter "Direct link to Using HTMLHeaderTextSplitter")

[HTMLHeaderTextSplitter](https://python.langchain.com/api_reference/text_splitters/html/langchain_text_splitters.html.HTMLHeaderTextSplitter.html) is a "structure-aware" [text splitter](/docs/concepts/text_splitters/) that splits text at the HTML element level and adds metadata for each header "relevant" to any given chunk. It can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures. It can be used with other text splitters as part of a chunking pipeline.

It is analogous to the [MarkdownHeaderTextSplitter](/docs/how_to/markdown_header_metadata_splitter/) for markdown files.

To specify what headers to split on, specify `headers_to_split_on` when instantiating `HTMLHeaderTextSplitter` as shown below.

```python
from langchain_text_splitters import HTMLHeaderTextSplitter

headers_to_split_on = [
    ("h1", "Header 1"),
    ("h2", "Header 2"),
    ("h3", "Header 3"),
]

html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)
html_header_splits = html_splitter.split_text(html_string)
html_header_splits
```

**API Reference:**[HTMLHeaderTextSplitter](https://python.langchain.com/api_reference/text_splitters/html/langchain_text_splitters.html.HTMLHeaderTextSplitter.html)

```output
[Document(metadata={'Header 1': 'Main Title'}, page_content='This is an introductory paragraph with some basic content.'),
 Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}, page_content='This section introduces the topic. Below is a list:  \nFirst item Second item Third item with bold text and a link'),
 Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction', 'Header 3': 'Subsection 1.1: Details'}, page_content="This subsection provides additional details. Here's a table:"),
 Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 2: Media Content'}, page_content='This section contains an image and a video:'),
 Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 3: Code Example'}, page_content='This section contains a code block:'),
 Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Conclusion'}, page_content='This is the conclusion of the document.')]
```

To return each element together with their associated headers, specify `return_each_element=True` when instantiating `HTMLHeaderTextSplitter`:

```python
html_splitter = HTMLHeaderTextSplitter(
    headers_to_split_on,
    return_each_element=True,
)
html_header_splits_elements = html_splitter.split_text(html_string)
```

Comparing with the above, where elements are aggregated by their headers:

```python
for element in html_header_splits[:2]:
    print(element)
```

```output
page_content='This is an introductory paragraph with some basic content.' metadata={'Header 1': 'Main Title'}
page_content='This section introduces the topic. Below is a list:  
First item Second item Third item with bold text and a link' metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}
```

Now each element is returned as a distinct `Document`:

```python
for element in html_header_splits_elements[:3]:
    print(element)
```

```output
page_content='This is an introductory paragraph with some basic content.' metadata={'Header 1': 'Main Title'}
page_content='This section introduces the topic. Below is a list:' metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}
page_content='First item Second item Third item with bold text and a link' metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}
```

### How to split from a URL or HTML file:[â€‹](#how-to-split-from-a-url-or-html-file "Direct link to How to split from a URL or HTML file:")

To read directly from a URL, pass the URL string into the `split_text_from_url` method.

Similarly, a local HTML file can be passed to the `split_text_from_file` method.

```python
url = "https://plato.stanford.edu/entries/goedel/"

headers_to_split_on = [
    ("h1", "Header 1"),
    ("h2", "Header 2"),
    ("h3", "Header 3"),
    ("h4", "Header 4"),
]

html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)

# for local file use html_splitter.split_text_from_file(<path_to_file>)
html_header_splits = html_splitter.split_text_from_url(url)
```

### How to constrain chunk sizes:[â€‹](#how-to-constrain-chunk-sizes "Direct link to How to constrain chunk sizes:")

`HTMLHeaderTextSplitter`, which splits based on HTML headers, can be composed with another splitter which constrains splits based on character lengths, such as `RecursiveCharacterTextSplitter`.

This can be done using the `.split_documents` method of the second splitter:

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

chunk_size = 500
chunk_overlap = 30
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size, chunk_overlap=chunk_overlap
)

# Split
splits = text_splitter.split_documents(html_header_splits)
splits[80:85]
```

**API Reference:**[RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

```output
[Document(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='We see that GÃ¶del first tried to reduce the consistency problem for analysis to that of arithmetic. This seemed to require a truth definition for arithmetic, which in turn led to paradoxes, such as the Liar paradox (â€œThis sentence is falseâ€) and Berryâ€™s paradox (â€œThe least number not defined by an expression consisting of just fourteen English wordsâ€). GÃ¶del then noticed that such paradoxes would not necessarily arise if truth were replaced by provability. But this means that arithmetic truth'),
 Document(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='means that arithmetic truth and arithmetic provability are not co-extensive â€” whence the First Incompleteness Theorem.'),
 Document(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='This account of GÃ¶delâ€™s discovery was told to Hao Wang very much after the fact; but in GÃ¶delâ€™s contemporary correspondence with Bernays and Zermelo, essentially the same description of his path to the theorems is given. (See GÃ¶del 2003a and GÃ¶del 2003b respectively.) From those accounts we see that the undefinability of truth in arithmetic, a result credited to Tarski, was likely obtained in some form by GÃ¶del by 1931. But he neither publicized nor published the result; the biases logicians'),
 Document(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='result; the biases logicians had expressed at the time concerning the notion of truth, biases which came vehemently to the fore when Tarski announced his results on the undefinability of truth in formal systems 1935, may have served as a deterrent to GÃ¶delâ€™s publication of that theorem.'),
 Document(metadata={'Header 1': 'Kurt GÃ¶del', 'Header 2': '2. GÃ¶delâ€™s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}, page_content='We now describe the proof of the two theorems, formulating GÃ¶delâ€™s results in Peano arithmetic. GÃ¶del himself used a system related to that defined in Principia Mathematica, but containing Peano arithmetic. In our presentation of the First and Second Incompleteness Theorems we refer to Peano arithmetic as P, following GÃ¶delâ€™s notation.')]
```

### Limitations[â€‹](#limitations "Direct link to Limitations")

There can be quite a bit of structural variation from one HTML document to another, and while `HTMLHeaderTextSplitter` will attempt to attach all "relevant" headers to any given chunk, it can sometimes miss certain headers. For example, the algorithm assumes an informational hierarchy in which headers are always at nodes "above" associated text, i.e. prior siblings, ancestors, and combinations thereof. In the following news article (as of the writing of this document), the document is structured such that the text of the top-level headline, while tagged "h1", is in a *distinct* subtree from the text elements that we'd expect it to be *"above"*â€”so we can observe that the "h1" element and its associated text do not show up in the chunk metadata (but, where applicable, we do see "h2" and its associated text):

```python
url = "https://www.cnn.com/2023/09/25/weather/el-nino-winter-us-climate/index.html"

headers_to_split_on = [
    ("h1", "Header 1"),
    ("h2", "Header 2"),
]

html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)
html_header_splits = html_splitter.split_text_from_url(url)
print(html_header_splits[1].page_content[:500])
```

```output
No two El NiÃ±o winters are the same, but many have temperature and precipitation trends in common.  
Average conditions during an El NiÃ±o winter across the continental US.  
One of the major reasons is the position of the jet stream, which often shifts south during an El NiÃ±o winter. This shift typically brings wetter and cooler weather to the South while the North becomes drier and warmer, according to NOAA.  
Because the jet stream is essentially a river of air that storms flow through, they c
```

## Using HTMLSectionSplitter[â€‹](#using-htmlsectionsplitter "Direct link to Using HTMLSectionSplitter")

Similar in concept to the [HTMLHeaderTextSplitter](#using-htmlheadertextsplitter), the `HTMLSectionSplitter` is a "structure-aware" [text splitter](/docs/concepts/text_splitters/) that splits text at the element level and adds metadata for each header "relevant" to any given chunk. It lets you split HTML by sections.

It can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures.

Use `xslt_path` to provide an absolute path to transform the HTML so that it can detect sections based on provided tags. The default is to use the `converting_to_header.xslt` file in the `data_connection/document_transformers` directory. This is for converting the html to a format/layout that is easier to detect sections. For example, `span` based on their font size can be converted to header tags to be detected as a section.

### How to split HTML strings:[â€‹](#how-to-split-html-strings "Direct link to How to split HTML strings:")

```python
from langchain_text_splitters import HTMLSectionSplitter

headers_to_split_on = [
    ("h1", "Header 1"),
    ("h2", "Header 2"),
]

html_splitter = HTMLSectionSplitter(headers_to_split_on)
html_header_splits = html_splitter.split_text(html_string)
html_header_splits
```

**API Reference:**[HTMLSectionSplitter](https://python.langchain.com/api_reference/text_splitters/html/langchain_text_splitters.html.HTMLSectionSplitter.html)

```output
[Document(metadata={'Header 1': 'Main Title'}, page_content='Main Title \n This is an introductory paragraph with some basic content.'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content="Section 1: Introduction \n This section introduces the topic. Below is a list: \n \n First item \n Second item \n Third item with  bold text  and  a link \n \n \n Subsection 1.1: Details \n This subsection provides additional details. Here's a table: \n \n \n \n Header 1 \n Header 2 \n Header 3 \n \n \n \n \n Row 1, Cell 1 \n Row 1, Cell 2 \n Row 1, Cell 3 \n \n \n Row 2, Cell 1 \n Row 2, Cell 2 \n Row 2, Cell 3"),
 Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='Section 2: Media Content \n This section contains an image and a video: \n \n \n      Your browser does not support the video tag.'),
 Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='Section 3: Code Example \n This section contains a code block: \n \n    <div>\n      <p>This is a paragraph inside a div.</p>\n    </div>'),
 Document(metadata={'Header 2': 'Conclusion'}, page_content='Conclusion \n This is the conclusion of the document.')]
```

### How to constrain chunk sizes:[â€‹](#how-to-constrain-chunk-sizes-1 "Direct link to How to constrain chunk sizes:")

`HTMLSectionSplitter` can be used with other text splitters as part of a chunking pipeline. Internally, it uses the `RecursiveCharacterTextSplitter` when the section size is larger than the chunk size. It also considers the font size of the text to determine whether it is a section or not based on the determined font size threshold.

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

headers_to_split_on = [
    ("h1", "Header 1"),
    ("h2", "Header 2"),
    ("h3", "Header 3"),
]

html_splitter = HTMLSectionSplitter(headers_to_split_on)

html_header_splits = html_splitter.split_text(html_string)

chunk_size = 50
chunk_overlap = 5
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size, chunk_overlap=chunk_overlap
)

# Split
splits = text_splitter.split_documents(html_header_splits)
splits
```

**API Reference:**[RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

```output
[Document(metadata={'Header 1': 'Main Title'}, page_content='Main Title'),
 Document(metadata={'Header 1': 'Main Title'}, page_content='This is an introductory paragraph with some'),
 Document(metadata={'Header 1': 'Main Title'}, page_content='some basic content.'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='Section 1: Introduction'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='This section introduces the topic. Below is a'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='is a list:'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='First item \n Second item'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='Third item with  bold text  and  a link'),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Subsection 1.1: Details'),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='This subsection provides additional details.'),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content="Here's a table:"),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Header 1 \n Header 2 \n Header 3'),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Row 1, Cell 1 \n Row 1, Cell 2'),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Row 1, Cell 3 \n \n \n Row 2, Cell 1'),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Row 2, Cell 2 \n Row 2, Cell 3'),
 Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='Section 2: Media Content'),
 Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='This section contains an image and a video:'),
 Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='Your browser does not support the video'),
 Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='tag.'),
 Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='Section 3: Code Example'),
 Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='This section contains a code block: \n \n    <div>'),
 Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='<p>This is a paragraph inside a div.</p>'),
 Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='</div>'),
 Document(metadata={'Header 2': 'Conclusion'}, page_content='Conclusion'),
 Document(metadata={'Header 2': 'Conclusion'}, page_content='This is the conclusion of the document.')]
```

## Using HTMLSemanticPreservingSplitter[â€‹](#using-htmlsemanticpreservingsplitter "Direct link to Using HTMLSemanticPreservingSplitter")

The `HTMLSemanticPreservingSplitter` is designed to split HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components. This ensures that such elements are not split across chunks, causing loss of contextual relevancy such as table headers, list headers etc.

This splitter is designed at its heart, to create contextually relevant chunks. General Recursive splitting with `HTMLHeaderTextSplitter` can cause tables, lists and other structured elements to be split in the middle, losing significant context and creating bad chunks.

The `HTMLSemanticPreservingSplitter` is essential for splitting HTML content that includes structured elements like tables and lists, especially when it's critical to preserve these elements intact. Additionally, its ability to define custom handlers for specific HTML tags makes it a versatile tool for processing complex HTML documents.

**IMPORTANT**: `max_chunk_size` is not a definite maximum size of a chunk, the calculation of max size, occurs when the preserved content is not apart of the chunk, to ensure it is not split. When we add the preserved data back in to the chunk, there is a chance the chunk size will exceed the `max_chunk_size`. This is crucial to ensure we maintain the structure of the original document

info

Notes:

1. We have defined a custom handler to re-format the contents of code blocks
2. We defined a deny list for specific html elements, to decompose them and their contents pre-processing
3. We have intentionally set a small chunk size to demonstrate the non-splitting of elements

```python
# BeautifulSoup is required to use the custom handlers
from bs4 import Tag
from langchain_text_splitters import HTMLSemanticPreservingSplitter

headers_to_split_on = [
    ("h1", "Header 1"),
    ("h2", "Header 2"),
]


def code_handler(element: Tag) -> str:
    data_lang = element.get("data-lang")
    code_format = f"<code:{data_lang}>{element.get_text()}</code>"

    return code_format


splitter = HTMLSemanticPreservingSplitter(
    headers_to_split_on=headers_to_split_on,
    separators=["\n\n", "\n", ". ", "! ", "? "],
    max_chunk_size=50,
    preserve_images=True,
    preserve_videos=True,
    elements_to_preserve=["table", "ul", "ol", "code"],
    denylist_tags=["script", "style", "head"],
    custom_handlers={"code": code_handler},
)

documents = splitter.split_text(html_string)
documents
```

**API Reference:**[HTMLSemanticPreservingSplitter](https://python.langchain.com/api_reference/text_splitters/html/langchain_text_splitters.html.HTMLSemanticPreservingSplitter.html)

```output
[Document(metadata={'Header 1': 'Main Title'}, page_content='This is an introductory paragraph with some basic content.'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='This section introduces the topic'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='. Below is a list: First item Second item Third item with bold text and a link Subsection 1.1: Details This subsection provides additional details'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content=". Here's a table: Header 1 Header 2 Header 3 Row 1, Cell 1 Row 1, Cell 2 Row 1, Cell 3 Row 2, Cell 1 Row 2, Cell 2 Row 2, Cell 3"),
 Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='This section contains an image and a video: ![image:example_image_link.mp4](example_image_link.mp4) ![video:example_video_link.mp4](example_video_link.mp4)'),
 Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='This section contains a code block: <code:html> <div> <p>This is a paragraph inside a div.</p> </div> </code>'),
 Document(metadata={'Header 2': 'Conclusion'}, page_content='This is the conclusion of the document.')]
```

### Preserving Tables and Lists[â€‹](#preserving-tables-and-lists "Direct link to Preserving Tables and Lists")

In this example, we will demonstrate how the `HTMLSemanticPreservingSplitter` can preserve a table and a large list within an HTML document. The chunk size will be set to 50 characters to illustrate how the splitter ensures that these elements are not split, even when they exceed the maximum defined chunk size.

```python
from langchain_text_splitters import HTMLSemanticPreservingSplitter

html_string = """
<!DOCTYPE html>
<html>
    <body>
        <div>
            <h1>Section 1</h1>
            <p>This section contains an important table and list that should not be split across chunks.</p>
            <table>
                <tr>
                    <th>Item</th>
                    <th>Quantity</th>
                    <th>Price</th>
                </tr>
                <tr>
                    <td>Apples</td>
                    <td>10</td>
                    <td>$1.00</td>
                </tr>
                <tr>
                    <td>Oranges</td>
                    <td>5</td>
                    <td>$0.50</td>
                </tr>
                <tr>
                    <td>Bananas</td>
                    <td>50</td>
                    <td>$1.50</td>
                </tr>
            </table>
            <h2>Subsection 1.1</h2>
            <p>Additional text in subsection 1.1 that is separated from the table and list.</p>
            <p>Here is a detailed list:</p>
            <ul>
                <li>Item 1: Description of item 1, which is quite detailed and important.</li>
                <li>Item 2: Description of item 2, which also contains significant information.</li>
                <li>Item 3: Description of item 3, another item that we don't want to split across chunks.</li>
            </ul>
        </div>
    </body>
</html>
"""

headers_to_split_on = [("h1", "Header 1"), ("h2", "Header 2")]

splitter = HTMLSemanticPreservingSplitter(
    headers_to_split_on=headers_to_split_on,
    max_chunk_size=50,
    elements_to_preserve=["table", "ul"],
)

documents = splitter.split_text(html_string)
print(documents)
```

**API Reference:**[HTMLSemanticPreservingSplitter](https://python.langchain.com/api_reference/text_splitters/html/langchain_text_splitters.html.HTMLSemanticPreservingSplitter.html)

```output
[Document(metadata={'Header 1': 'Section 1'}, page_content='This section contains an important table and list'), Document(metadata={'Header 1': 'Section 1'}, page_content='that should not be split across chunks.'), Document(metadata={'Header 1': 'Section 1'}, page_content='Item Quantity Price Apples 10 $1.00 Oranges 5 $0.50 Bananas 50 $1.50'), Document(metadata={'Header 2': 'Subsection 1.1'}, page_content='Additional text in subsection 1.1 that is'), Document(metadata={'Header 2': 'Subsection 1.1'}, page_content='separated from the table and list. Here is a'), Document(metadata={'Header 2': 'Subsection 1.1'}, page_content="detailed list: Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don't want to split across chunks.")]
```

#### Explanation[â€‹](#explanation "Direct link to Explanation")

In this example, the `HTMLSemanticPreservingSplitter` ensures that the entire table and the unordered list (`<ul>`) are preserved within their respective chunks. Even though the chunk size is set to 50 characters, the splitter recognizes that these elements should not be split and keeps them intact.

This is particularly important when dealing with data tables or lists, where splitting the content could lead to loss of context or confusion. The resulting `Document` objects retain the full structure of these elements, ensuring that the contextual relevance of the information is maintained.

### Using a Custom Handler[â€‹](#using-a-custom-handler "Direct link to Using a Custom Handler")

The `HTMLSemanticPreservingSplitter` allows you to define custom handlers for specific HTML elements. Some platforms, have custom HTML tags that are not natively parsed by `BeautifulSoup`, when this occurs, you can utilize custom handlers to add the formatting logic easily.

This can be particularly useful for elements that require special processing, such as `<iframe>` tags or specific 'data-' elements. In this example, we'll create a custom handler for `iframe` tags that converts them into Markdown-like links.

```python
def custom_iframe_extractor(iframe_tag):
    iframe_src = iframe_tag.get("src", "")
    return f"[iframe:{iframe_src}]({iframe_src})"


splitter = HTMLSemanticPreservingSplitter(
    headers_to_split_on=headers_to_split_on,
    max_chunk_size=50,
    separators=["\n\n", "\n", ". "],
    elements_to_preserve=["table", "ul", "ol"],
    custom_handlers={"iframe": custom_iframe_extractor},
)

html_string = """
<!DOCTYPE html>
<html>
    <body>
        <div>
            <h1>Section with Iframe</h1>
            <iframe src="https://example.com/embed"></iframe>
            <p>Some text after the iframe.</p>
            <ul>
                <li>Item 1: Description of item 1, which is quite detailed and important.</li>
                <li>Item 2: Description of item 2, which also contains significant information.</li>
                <li>Item 3: Description of item 3, another item that we don't want to split across chunks.</li>
            </ul>
        </div>
    </body>
</html>
"""

documents = splitter.split_text(html_string)
print(documents)
```

```output
[Document(metadata={'Header 1': 'Section with Iframe'}, page_content='[iframe:https://example.com/embed](https://example.com/embed) Some text after the iframe'), Document(metadata={'Header 1': 'Section with Iframe'}, page_content=". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don't want to split across chunks.")]
```

#### Explanation[â€‹](#explanation-1 "Direct link to Explanation")

In this example, we defined a custom handler for `iframe` tags that converts them into Markdown-like links. When the splitter processes the HTML content, it uses this custom handler to transform the `iframe` tags while preserving other elements like tables and lists. The resulting `Document` objects show how the iframe is handled according to the custom logic you provided.

**Important**: When presvering items such as links, you should be mindful not to include `.` in your separators, or leave separators blank. `RecursiveCharacterTextSplitter` splits on full stop, which will cut links in half. Ensure you provide a separator list with `.` instead.

### Using a custom handler to analyze an image with an LLM[â€‹](#using-a-custom-handler-to-analyze-an-image-with-an-llm "Direct link to Using a custom handler to analyze an image with an LLM")

With custom handler's, we can also override the default processing for any element. A great example of this, is inserting semantic analysis of an image within a document, directly in the chunking flow.

Since our function is called when the tag is discovered, we can override the `<img>` tag and turn off `preserve_images` to insert any content we would like to embed in our chunks.

```python
"""This example assumes you have helper methods `load_image_from_url` and an LLM agent `llm` that can process image data."""

from langchain.agents import AgentExecutor

# This example needs to be replaced with your own agent
llm = AgentExecutor(...)


# This method is a placeholder for loading image data from a URL and is not implemented here
def load_image_from_url(image_url: str) -> bytes:
    # Assuming this method fetches the image data from the URL
    return b"image_data"


html_string = """
<!DOCTYPE html>
<html>
    <body>
        <div>
            <h1>Section with Image and Link</h1>
            <p>
                <img src="https://example.com/image.jpg" alt="An example image" />
                Some text after the image.
            </p>
            <ul>
                <li>Item 1: Description of item 1, which is quite detailed and important.</li>
                <li>Item 2: Description of item 2, which also contains significant information.</li>
                <li>Item 3: Description of item 3, another item that we don't want to split across chunks.</li>
            </ul>
        </div>
    </body>
</html>
"""


def custom_image_handler(img_tag) -> str:
    img_src = img_tag.get("src", "")
    img_alt = img_tag.get("alt", "No alt text provided")

    image_data = load_image_from_url(img_src)
    semantic_meaning = llm.invoke(image_data)

    markdown_text = f"[Image Alt Text: {img_alt} | Image Source: {img_src} | Image Semantic Meaning: {semantic_meaning}]"

    return markdown_text


splitter = HTMLSemanticPreservingSplitter(
    headers_to_split_on=headers_to_split_on,
    max_chunk_size=50,
    separators=["\n\n", "\n", ". "],
    elements_to_preserve=["ul"],
    preserve_images=False,
    custom_handlers={"img": custom_image_handler},
)

documents = splitter.split_text(html_string)

print(documents)
```

**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html)

```text
[Document(metadata={'Header 1': 'Section with Image and Link'}, page_content='[Image Alt Text: An example image | Image Source: https://example.com/image.jpg | Image Semantic Meaning: semantic-meaning] Some text after the image'), 
Document(metadata={'Header 1': 'Section with Image and Link'}, page_content=". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don't want to split across chunks.")]
```

#### Explanation:[â€‹](#explanation-2 "Direct link to Explanation:")

With our custom handler written to extract the specific fields from a `<img>` element in HTML, we can further process the data with our agent, and insert the result directly into our chunk. It is important to ensure `preserve_images` is set to `False` otherwise the default processing of `<img>` fields will take place.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/split_html.ipynb)

* * *


- [Overview of the Splitters](#overview-of-the-splitters)
  
  - [HTMLHeaderTextSplitter](#htmlheadertextsplitter)
  - [HTMLSectionSplitter](#htmlsectionsplitter)
  - [HTMLSemanticPreservingSplitter](#htmlsemanticpreservingsplitter)
  - [Choosing the Right Splitter](#choosing-the-right-splitter)
- [Example HTML Document](#example-html-document)
- [Using HTMLHeaderTextSplitter](#using-htmlheadertextsplitter)
  
  - [How to split from a URL or HTML file:](#how-to-split-from-a-url-or-html-file)
  - [How to constrain chunk sizes:](#how-to-constrain-chunk-sizes)
  - [Limitations](#limitations)
- [Using HTMLSectionSplitter](#using-htmlsectionsplitter)
  
  - [How to split HTML strings:](#how-to-split-html-strings)
  - [How to constrain chunk sizes:](#how-to-constrain-chunk-sizes-1)
- [Using HTMLSemanticPreservingSplitter](#using-htmlsemanticpreservingsplitter)
  
  - [Preserving Tables and Lists](#preserving-tables-and-lists)
  - [Using a Custom Handler](#using-a-custom-handler)
  - [Using a custom handler to analyze an image with an LLM](#using-a-custom-handler-to-analyze-an-image-with-an-llm)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/index.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/index.ipynb)

# How to migrate from v0.0 chains

LangChain has evolved since its initial release, and many of the original "Chain" classes have been deprecated in favor of the more flexible and powerful frameworks of LCEL and LangGraph.

This guide will help you migrate your existing v0.0 chains to the new abstractions.

How deprecated implementations work

Even though many of these implementations are deprecated, they are **still supported** in the codebase. However, they are not recommended for new development, and we recommend re-implementing them using the following guides!

To see the planned removal version for each deprecated implementation, check their API reference.

Prerequisites

These guides assume some familiarity with the following concepts:

- [LangChain Expression Language](/docs/concepts/lcel/)
- [LangGraph](https://langchain-ai.github.io/langgraph/)

LangChain maintains a number of legacy abstractions. Many of these can be reimplemented via short combinations of LCEL and LangGraph primitives.

### LCEL[â€‹](#lcel "Direct link to LCEL")

[LCEL](/docs/concepts/lcel/) is designed to streamline the process of building useful apps with LLMs and combining related components. It does this by providing:

1. **A unified interface**: Every LCEL object implements the `Runnable` interface, which defines a common set of invocation methods (`invoke`, `batch`, `stream`, `ainvoke`, ...). This makes it possible to also automatically and consistently support useful operations like streaming of intermediate steps and batching, since every chain composed of LCEL objects is itself an LCEL object.
2. **Composition primitives**: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internals, and more.

### LangGraph[â€‹](#langgraph "Direct link to LangGraph")

[LangGraph](https://langchain-ai.github.io/langgraph/), built on top of LCEL, allows for performant orchestrations of application components while maintaining concise and readable code. It includes built-in persistence, support for cycles, and prioritizes controllability. If LCEL grows unwieldy for larger or more complex chains, they may benefit from a LangGraph implementation.

### Advantages[â€‹](#advantages "Direct link to Advantages")

Using these frameworks for existing v0.0 chains confers some advantages:

- The resulting chains typically implement the full `Runnable` interface, including streaming and asynchronous support where appropriate;
- The chains may be more easily extended or modified;
- The parameters of the chain are typically surfaced for easier customization (e.g., prompts) over previous versions, which tended to be subclasses and had opaque parameters and internals.
- If using LangGraph, the chain supports built-in persistence, allowing for conversational experiences via a "memory" of the chat history.
- If using LangGraph, the steps of the chain can be streamed, allowing for greater control and customizability.

The below pages assist with migration from various specific chains to LCEL and LangGraph:

- [LLMChain](/docs/versions/migrating_chains/llm_chain/)
- [ConversationChain](/docs/versions/migrating_chains/conversation_chain/)
- [RetrievalQA](/docs/versions/migrating_chains/retrieval_qa/)
- [ConversationalRetrievalChain](/docs/versions/migrating_chains/conversation_retrieval_chain/)
- [StuffDocumentsChain](/docs/versions/migrating_chains/stuff_docs_chain/)
- [MapReduceDocumentsChain](/docs/versions/migrating_chains/map_reduce_chain/)
- [MapRerankDocumentsChain](/docs/versions/migrating_chains/map_rerank_docs_chain/)
- [RefineDocumentsChain](/docs/versions/migrating_chains/refine_docs_chain/)
- [LLMRouterChain](/docs/versions/migrating_chains/llm_router_chain/)
- [MultiPromptChain](/docs/versions/migrating_chains/multi_prompt_chain/)
- [LLMMathChain](/docs/versions/migrating_chains/llm_math_chain/)
- [ConstitutionalChain](/docs/versions/migrating_chains/constitutional_chain/)

Check out the [LCEL conceptual docs](/docs/concepts/lcel/) and [LangGraph docs](https://langchain-ai.github.io/langgraph/) for more background information.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_chains/index.ipynb)

* * *


- [LCEL](#lcel)
- [LangGraph](#langgraph)
- [Advantages](#advantages)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/retrievers.mdx)

# Retrievers

Prerequisites

- [Vector stores](/docs/concepts/vectorstores/)
- [Embeddings](/docs/concepts/embedding_models/)
- [Text splitters](/docs/concepts/text_splitters/)

## Overview[â€‹](#overview "Direct link to Overview")

Many different types of retrieval systems exist, including vectorstores, graph databases, and relational databases. With the rise on popularity of large language models, retrieval systems have become an important component in AI application (e.g., [RAG](/docs/concepts/rag/)). Because of their importance and variability, LangChain provides a uniform interface for interacting with different types of retrieval systems. The LangChain [retriever](/docs/concepts/retrievers/) interface is straightforward:

1. Input: A query (string)
2. Output: A list of documents (standardized LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects)

## Key concept[â€‹](#key-concept "Direct link to Key concept")

![Retriever](/assets/images/retriever_concept-1093f15a8f63ddb90bd23decbd249ea5.png)

All retrievers implement a simple interface for retrieving documents using natural language queries.

## Interface[â€‹](#interface "Direct link to Interface")

The only requirement for a retriever is the ability to accepts a query and return documents. In particular, [LangChain's retriever class](https://python.langchain.com/api_reference/core/retrievers/langchain_core.retrievers.BaseRetriever.html) only requires that the `_get_relevant_documents` method is implemented, which takes a `query: str` and returns a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects that are most relevant to the query. The underlying logic used to get relevant documents is specified by the retriever and can be whatever is most useful for the application.

A LangChain retriever is a [runnable](/docs/how_to/lcel_cheatsheet/), which is a standard interface is for LangChain components. This means that it has a few common methods, including `invoke`, that are used to interact with it. A retriever can be invoked with a query:

```python
docs = retriever.invoke(query)
```

Retrievers return a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects, which have two attributes:

- `page_content`: The content of this document. Currently is a string.
- `metadata`: Arbitrary metadata associated with this document (e.g., document id, file name, source, etc).

Further reading

- See our [how-to guide](/docs/how_to/custom_retriever/) on building your own custom retriever.

## Common types[â€‹](#common-types "Direct link to Common types")

Despite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.

### Search apis[â€‹](#search-apis "Direct link to Search apis")

It's important to note that retrievers don't need to actually *store* documents. For example, we can be built retrievers on top of search APIs that simply return search results! See our retriever integrations with [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/) or [Wikipedia Search](/docs/integrations/retrievers/wikipedia/).

### Relational or graph database[â€‹](#relational-or-graph-database "Direct link to Relational or graph database")

Retrievers can be built on top of relational or graph databases. In these cases, [query analysis](/docs/concepts/retrieval/) techniques to construct a structured query from natural language is critical. For example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) retriever to be transformed into a SQL query behind the scenes.

Further reading

- See our [tutorial](/docs/tutorials/sql_qa/) for context on how to build a retreiver using a SQL database and text-to-SQL.
- See our [tutorial](/docs/tutorials/graph/) for context on how to build a retreiver using a graph database and text-to-Cypher.

### Lexical search[â€‹](#lexical-search "Direct link to Lexical search")

As discussed in our conceptual review of [retrieval](/docs/concepts/retrieval/), many search engines are based upon matching words in a query to the words in each document. [BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2). LangChain has retrievers for many popular lexical search algorithms / engines.

Further reading

- See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
- See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.
- See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.

### Vector store[â€‹](#vector-store "Direct link to Vector store")

[Vector stores](/docs/concepts/vectorstores/) are a powerful and efficient way to index and retrieve unstructured data. A vectorstore can be used as a retriever by calling the `as_retriever()` method.

```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```

## Advanced retrieval patterns[â€‹](#advanced-retrieval-patterns "Direct link to Advanced retrieval patterns")

### Ensemble[â€‹](#ensemble "Direct link to Ensemble")

Because the retriever interface is so simple, returning a list of `Document` objects given a search query, it is possible to combine multiple retrievers using ensembling. This is particularly useful when you have multiple retrievers that are good at finding different types of relevant documents. It is easy to create an [ensemble retriever](/docs/how_to/ensemble_retriever/) that combines multiple retrievers with linear weighted scores:

```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we combine search results from many retrievers? This motivates the concept of re-ranking, which takes the output of multiple retrievers and combines them using a more sophisticated algorithm such as [Reciprocal Rank Fusion (RRF)](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf).

### Source document retention[â€‹](#source-document-retention "Direct link to Source document retention")

Many retrievers utilize some kind of index to make documents easily searchable. The process of indexing can include a transformation step (e.g., vectorstores often use document splitting). Whatever transformation is used, can be very useful to retain a link between the *transformed document* and the original, giving the retriever the ability to return the *original* document.

![Retrieval with full docs](/assets/images/retriever_full_docs-e6282aafd63f69ab3fcb26b2cfc46b5c.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model. For example, you may use small chunk size for indexing documents in a vectorstore. If you return *only* the chunks as the retrieval result, then the model will have lost the original document context for the chunks.

LangChain has two different retrievers that can be used to address this challenge. The [Multi-Vector](/docs/how_to/multi_vector/) retriever allows the user to use any document transformation (e.g., use an LLM to write a summary of the document) for indexing while retaining linkage to the source document. The [ParentDocument](/docs/how_to/parent_document_retriever/) retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.

| Name                                                      | Index Type                    | Uses an LLM               | When to Use                                                                                                                             | Description                                                                                                                                                                                                              |
|-----------------------------------------------------------|-------------------------------|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [ParentDocument](/docs/how_to/parent_document_retriever/) | Vector store + Document Store | No                        | If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together. | This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks). |
| [Multi Vector](/docs/how_to/multi_vector/)                | Vector store + Document Store | Sometimes during indexing | If you are able to extract information from documents that you think is more relevant to index than the text itself.                    | This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.                                         |

Further reading

- See our [how-to guide](/docs/how_to/parent_document_retriever/) on using the ParentDocument retriever.
- See our [how-to guide](/docs/how_to/multi_vector/) on using the MultiVector retriever.
- See our RAG from Scratch video on the [multi vector retriever](https://youtu.be/gTCU9I6QqCE?feature=shared).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/retrievers.mdx)

* * *


- [Overview](#overview)
- [Key concept](#key-concept)
- [Interface](#interface)
- [Common types](#common-types)
  
  - [Search apis](#search-apis)
  - [Relational or graph database](#relational-or-graph-database)
  - [Lexical search](#lexical-search)
  - [Vector store](#vector-store)
- [Advanced retrieval patterns](#advanced-retrieval-patterns)
  
  - [Ensemble](#ensemble)
  - [Source document retention](#source-document-retention)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/summarize_map_reduce.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/summarize_map_reduce.ipynb)

# How to summarize text through parallelization

LLMs can summarize and otherwise distill desired information from text, including large volumes of text. In many cases, especially when the amount of text is large compared to the size of the model's context window, it can be helpful (or necessary) to break up the summarization task into smaller components.

Map-reduce represents one class of strategies for accomplishing this. The idea is to break the text into "sub-documents", and first map each sub-document to an individual summary using an LLM. Then, we reduce or consolidate those summaries into a single global summary.

Note that the map step is typically parallelized over the input documents. This strategy is especially effective when understanding of a sub-document does not rely on preceeding context. For example, when summarizing a corpus of many, shorter documents.

[LangGraph](https://langchain-ai.github.io/langgraph/), built on top of `langchain-core`, supports [map-reduce](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) workflows and is well-suited to this problem:

- LangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;
- LangGraph's [checkpointing](https://langchain-ai.github.io/langgraph/how-tos/persistence/) supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.
- The LangGraph implementation is straightforward to modify and extend.

Below, we demonstrate how to summarize text via a map-reduce strategy.

## Load chat model[â€‹](#load-chat-model "Direct link to Load chat model")

Let's first load a chat model:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

## Load documents[â€‹](#load-documents "Direct link to Load documents")

First we load in our documents. We will use [WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) to load a blog post, and split the documents into smaller sub-documents.

```python
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000, chunk_overlap=0
)

loader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")
docs = loader.load()

split_docs = text_splitter.split_documents(docs)
print(f"Generated {len(split_docs)} documents.")
```

**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

```````output
Created a chunk of size 1003, which is longer than the specified 1000
``````output
Generated 14 documents.
```````

## Create graph[â€‹](#create-graph "Direct link to Create graph")

### Map step[â€‹](#map-step "Direct link to Map step")

Let's first define the prompt associated with the map step, and associated it with the LLM via a [chain](/docs/how_to/sequence/):

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

map_prompt = ChatPromptTemplate.from_messages(
    [("human", "Write a concise summary of the following:\\n\\n{context}")]
)

map_chain = map_prompt | llm | StrOutputParser()
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

### Reduce step[â€‹](#reduce-step "Direct link to Reduce step")

We also define a chain that takes the document mapping results and reduces them into a single output.

```python
reduce_template = """
The following is a set of summaries:
{docs}
Take these and distill it into a final, consolidated summary
of the main themes.
"""

reduce_prompt = ChatPromptTemplate([("human", reduce_template)])

reduce_chain = reduce_prompt | llm | StrOutputParser()
```

### Orchestration via LangGraph[â€‹](#orchestration-via-langgraph "Direct link to Orchestration via LangGraph")

Below we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts.

Map-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model's context window size. Here we implement a recursive "collapsing" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.

We will need to install `langgraph`:

```python
pip install -qU langgraph
```

```python
import operator
from typing import Annotated, List, Literal, TypedDict

from langchain.chains.combine_documents.reduce import (
    acollapse_docs,
    split_list_of_docs,
)
from langchain_core.documents import Document
from langgraph.constants import Send
from langgraph.graph import END, START, StateGraph

token_max = 1000


def length_function(documents: List[Document]) -> int:
    """Get number of tokens for input contents."""
    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)


# This will be the overall state of the main graph.
# It will contain the input document contents, corresponding
# summaries, and a final summary.
class OverallState(TypedDict):
    # Notice here we use the operator.add
    # This is because we want combine all the summaries we generate
    # from individual nodes back into one list - this is essentially
    # the "reduce" part
    contents: List[str]
    summaries: Annotated[list, operator.add]
    collapsed_summaries: List[Document]
    final_summary: str


# This will be the state of the node that we will "map" all
# documents to in order to generate summaries
class SummaryState(TypedDict):
    content: str


# Here we generate a summary, given a document
async def generate_summary(state: SummaryState):
    response = await map_chain.ainvoke(state["content"])
    return {"summaries": [response]}


# Here we define the logic to map out over the documents
# We will use this an edge in the graph
def map_summaries(state: OverallState):
    # We will return a list of `Send` objects
    # Each `Send` object consists of the name of a node in the graph
    # as well as the state to send to that node
    return [
        Send("generate_summary", {"content": content}) for content in state["contents"]
    ]


def collect_summaries(state: OverallState):
    return {
        "collapsed_summaries": [Document(summary) for summary in state["summaries"]]
    }


# Add node to collapse summaries
async def collapse_summaries(state: OverallState):
    doc_lists = split_list_of_docs(
        state["collapsed_summaries"], length_function, token_max
    )
    results = []
    for doc_list in doc_lists:
        results.append(await acollapse_docs(doc_list, reduce_chain.ainvoke))

    return {"collapsed_summaries": results}


# This represents a conditional edge in the graph that determines
# if we should collapse the summaries or not
def should_collapse(
    state: OverallState,
) -> Literal["collapse_summaries", "generate_final_summary"]:
    num_tokens = length_function(state["collapsed_summaries"])
    if num_tokens > token_max:
        return "collapse_summaries"
    else:
        return "generate_final_summary"


# Here we will generate the final summary
async def generate_final_summary(state: OverallState):
    response = await reduce_chain.ainvoke(state["collapsed_summaries"])
    return {"final_summary": response}


# Construct the graph
# Nodes:
graph = StateGraph(OverallState)
graph.add_node("generate_summary", generate_summary)  # same as before
graph.add_node("collect_summaries", collect_summaries)
graph.add_node("collapse_summaries", collapse_summaries)
graph.add_node("generate_final_summary", generate_final_summary)

# Edges:
graph.add_conditional_edges(START, map_summaries, ["generate_summary"])
graph.add_edge("generate_summary", "collect_summaries")
graph.add_conditional_edges("collect_summaries", should_collapse)
graph.add_conditional_edges("collapse_summaries", should_collapse)
graph.add_edge("generate_final_summary", END)

app = graph.compile()
```

**API Reference:**[acollapse\_docs](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.reduce.acollapse_docs.html) | [split\_list\_of\_docs](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.reduce.split_list_of_docs.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [Send](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

LangGraph allows the graph structure to be plotted to help visualize its function:

```python
from IPython.display import Image

Image(app.get_graph().draw_mermaid_png())
```

![](data:image/jpg;base64,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)

## Invoke graph[â€‹](#invoke-graph "Direct link to Invoke graph")

When running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.

Note that because we have a loop in the graph, it can be helpful to specify a [recursion\_limit](https://langchain-ai.github.io/langgraph/reference/errors/#langgraph.errors.GraphRecursionError) on its execution. This will raise a specific error when the specified limit is exceeded.

```python
async for step in app.astream(
    {"contents": [doc.page_content for doc in split_docs]},
    {"recursion_limit": 10},
):
    print(list(step.keys()))
```

```output
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['collect_summaries']
['collapse_summaries']
['collapse_summaries']
['generate_final_summary']
```

```python
print(step)
```

```output
{'generate_final_summary': {'final_summary': 'The consolidated summary of the main themes from the provided documents highlights the advancements and applications of large language models (LLMs) in artificial intelligence, particularly in autonomous agents and software development. Key themes include:\n\n1. **Integration of LLMs**: LLMs play a crucial role in enabling autonomous agents to perform complex tasks through advanced reasoning and decision-making techniques, such as Chain of Thought (CoT) and Tree of Thoughts.\n\n2. **Memory Management**: The categorization of memory into sensory, short-term, and long-term types parallels machine learning concepts, with short-term memory facilitating in-context learning and long-term memory enhanced by external storage solutions.\n\n3. **Tool Use and APIs**: Autonomous agents utilize external APIs to expand their capabilities, demonstrating adaptability and improved problem-solving skills.\n\n4. **Search Algorithms**: Various approximate nearest neighbor search algorithms, including Locality-Sensitive Hashing (LSH) and FAISS, are discussed for enhancing search efficiency in high-dimensional spaces.\n\n5. **Neuro-Symbolic Architectures**: The integration of neuro-symbolic systems, such as the MRKL framework, combines expert modules with LLMs to improve problem-solving, particularly in complex tasks.\n\n6. **Challenges and Innovations**: The documents address challenges like hallucination and inefficient planning in LLMs, alongside innovative methods such as Chain of Hindsight (CoH) and Algorithm Distillation (AD) for performance enhancement.\n\n7. **Software Development Practices**: The use of LLMs in software development is explored, particularly in creating structured applications like a Super Mario game using the model-view-controller (MVC) architecture, emphasizing task management, component organization, and documentation.\n\n8. **Limitations of LLMs**: Constraints such as finite context length and challenges in long-term planning are acknowledged, along with concerns regarding the reliability of natural language as an interface.\n\nOverall, the integration of LLMs and neuro-symbolic architectures signifies a significant evolution in AI, with ongoing research focused on enhancing planning, memory management, and problem-solving capabilities across various applications.'}}
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

Check out the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) for detail on building with LangGraph, including [this guide](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) on the details of map-reduce in LangGraph.

See the summarization [how-to guides](/docs/how_to/#summarization) for additional summarization strategies, including those designed for larger volumes of text.

See also [this tutorial](/docs/tutorials/summarization/) for more detail on summarization.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/summarize_map_reduce.ipynb)

* * *


- [Load chat model](#load-chat-model)
- [Load documents](#load-documents)
- [Create graph](#create-graph)
  
  - [Map step](#map-step)
  - [Reduce step](#reduce-step)
  - [Orchestration via LangGraph](#orchestration-via-langgraph)
- [Invoke graph](#invoke-graph)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/chat_model_rate_limiting.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/chat_model_rate_limiting.ipynb)

# How to handle rate limits

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models/)
- [LLMs](/docs/concepts/text_llms/)

You may find yourself in a situation where you are getting rate limited by the model provider API because you're making too many requests.

For example, this might happen if you are running many parallel queries to benchmark the chat model on a test dataset.

If you are facing such a situation, you can use a rate limiter to help match the rate at which you're making request to the rate allowed by the API.

Requires `langchain-core >= 0.2.24`

This functionality was added in `langchain-core == 0.2.24`. Please make sure your package is up to date.

## Initialize a rate limiter[â€‹](#initialize-a-rate-limiter "Direct link to Initialize a rate limiter")

Langchain comes with a built-in in memory rate limiter. This rate limiter is thread safe and can be shared by multiple threads in the same process.

The provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size of the requests.

```python
from langchain_core.rate_limiters import InMemoryRateLimiter

rate_limiter = InMemoryRateLimiter(
    requests_per_second=0.1,  # <-- Super slow! We can only make a request once every 10 seconds!!
    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,
    max_bucket_size=10,  # Controls the maximum burst size.
)
```

**API Reference:**[InMemoryRateLimiter](https://python.langchain.com/api_reference/core/rate_limiters/langchain_core.rate_limiters.InMemoryRateLimiter.html)

## Choose a model[â€‹](#choose-a-model "Direct link to Choose a model")

Choose any model and pass to it the rate\_limiter via the `rate_limiter` attribute.

```python
import os
import time
from getpass import getpass

if "ANTHROPIC_API_KEY" not in os.environ:
    os.environ["ANTHROPIC_API_KEY"] = getpass()


from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model_name="claude-3-opus-20240229", rate_limiter=rate_limiter)
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)

Let's confirm that the rate limiter works. We should only be able to invoke the model once per 10 seconds.

```python
for _ in range(5):
    tic = time.time()
    model.invoke("hello")
    toc = time.time()
    print(toc - tic)
```

```output
11.599073648452759
10.7502121925354
10.244257926940918
8.83088755607605
11.645203590393066
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/chat_model_rate_limiting.ipynb)

* * *


- [Initialize a rate limiter](#initialize-a-rate-limiter)
- [Choose a model](#choose-a-model)









[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/concepts/few_shot_prompting.mdx)

# Few-shot prompting

Prerequisites

- [Chat models](/docs/concepts/chat_models/)

## Overview[â€‹](#overview "Direct link to Overview")

One of the most effective ways to improve model performance is to give a model examples of what you want it to do. The technique of adding example inputs and expected outputs to a model prompt is known as "few-shot prompting". The technique is based on the [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) paper. There are a few things to think about when doing few-shot prompting:

1. How are examples generated?
2. How many examples are in each prompt?
3. How are examples selected at runtime?
4. How are examples formatted in the prompt?

Here are the considerations for each.

## 1. Generating examples[â€‹](#1-generating-examples "Direct link to 1. Generating examples")

The first and most important step of few-shot prompting is coming up with a good dataset of examples. Good examples should be relevant at runtime, clear, informative, and provide information that was not already known to the model.

At a high-level, the basic ways to generate examples are:

- Manual: a person/people generates examples they think are useful.
- Better model: a better (presumably more expensive/slower) model's responses are used as examples for a worse (presumably cheaper/faster) model.
- User feedback: users (or labelers) leave feedback on interactions with the application and examples are generated based on that feedback (for example, all interactions with positive feedback could be turned into examples).
- LLM feedback: same as user feedback but the process is automated by having models evaluate themselves.

Which approach is best depends on your task. For tasks where a small number of core principles need to be understood really well, it can be valuable hand-craft a few really good examples. For tasks where the space of correct behaviors is broader and more nuanced, it can be useful to generate many examples in a more automated fashion so that there's a higher likelihood of there being some highly relevant examples for any runtime input.

**Single-turn v.s. multi-turn examples**

Another dimension to think about when generating examples is what the example is actually showing.

The simplest types of examples just have a user input and an expected model output. These are single-turn examples.

One more complex type of example is where the example is an entire conversation, usually in which a model initially responds incorrectly and a user then tells the model how to correct its answer. This is called a multi-turn example. Multi-turn examples can be useful for more nuanced tasks where it's useful to show common errors and spell out exactly why they're wrong and what should be done instead.

## 2. Number of examples[â€‹](#2-number-of-examples "Direct link to 2. Number of examples")

Once we have a dataset of examples, we need to think about how many examples should be in each prompt. The key tradeoff is that more examples generally improve performance, but larger prompts increase costs and latency. And beyond some threshold having too many examples can start to confuse the model. Finding the right number of examples is highly dependent on the model, the task, the quality of the examples, and your cost and latency constraints. Anecdotally, the better the model is the fewer examples it needs to perform well and the more quickly you hit steeply diminishing returns on adding more examples. But, the best/only way to reliably answer this question is to run some experiments with different numbers of examples.

## 3. Selecting examples[â€‹](#3-selecting-examples "Direct link to 3. Selecting examples")

Assuming we are not adding our entire example dataset into each prompt, we need to have a way of selecting examples from our dataset based on a given input. We can do this:

- Randomly
- By (semantic or keyword-based) similarity of the inputs
- Based on some other constraints, like token size

LangChain has a number of [`ExampleSelectors`](/docs/concepts/example_selectors/) which make it easy to use any of these techniques.

Generally, selecting by semantic similarity leads to the best model performance. But how important this is is again model and task specific, and is something worth experimenting with.

## 4. Formatting examples[â€‹](#4-formatting-examples "Direct link to 4. Formatting examples")

Most state-of-the-art models these days are chat models, so we'll focus on formatting examples for those. Our basic options are to insert the examples:

- In the system prompt as a string
- As their own messages

If we insert our examples into the system prompt as a string, we'll need to make sure it's clear to the model where each example begins and which parts are the input versus output. Different models respond better to different syntaxes, like [ChatML](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chat-markup-language), XML, TypeScript, etc.

If we insert our examples as messages, where each example is represented as a sequence of Human, AI messages, we might want to also assign [names](/docs/concepts/messages/) to our messages like `"example_user"` and `"example_assistant"` to make it clear that these messages correspond to different actors than the latest input message.

**Formatting tool call examples**

One area where formatting examples as messages can be tricky is when our example outputs have tool calls. This is because different models have different constraints on what types of message sequences are allowed when any tool calls are generated.

- Some models require that any AIMessage with tool calls be immediately followed by ToolMessages for every tool call,
- Some models additionally require that any ToolMessages be immediately followed by an AIMessage before the next HumanMessage,
- Some models require that tools are passed into the model if there are any tool calls / ToolMessages in the chat history.

These requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples only include expected tool calls and not the actual tool outputs, you can try adding dummy ToolMessages / AIMessages to the end of each example with generic contents to satisfy the API constraints. In these cases it's especially worth experimenting with inserting your examples as strings versus messages, as having dummy messages can adversely affect certain models.

You can see a case study of how Anthropic and OpenAI respond to different few-shot prompting techniques on two different tool calling benchmarks [here](https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/concepts/few_shot_prompting.mdx)

* * *


- [Overview](#overview)
- [1. Generating examples](#1-generating-examples)
- [2. Number of examples](#2-number-of-examples)
- [3. Selecting examples](#3-selecting-examples)
- [4. Formatting examples](#4-formatting-examples)









# How to create a dynamic (self-constructing) chain

Prerequisites

This guide assumes familiarity with the following:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel/)
- [How to turn any function into a runnable](/docs/how_to/functions/)

Sometimes we want to construct parts of a chain at runtime, depending on the chain inputs ([routing](/docs/how_to/routing/) is the most common example of this). We can create dynamic chains like this using a very useful property of RunnableLambda's, which is that if a RunnableLambda returns a Runnable, that Runnable is itself invoked. Let's see an example.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
# | echo: false

from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-sonnet-20240229")
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)

```python
from operator import itemgetter

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable, RunnablePassthrough, chain

contextualize_instructions = """Convert the latest user question into a standalone question given the chat history. Don't answer the question, return the question and nothing else (no descriptive text)."""
contextualize_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_instructions),
        ("placeholder", "{chat_history}"),
        ("human", "{question}"),
    ]
)
contextualize_question = contextualize_prompt | llm | StrOutputParser()

qa_instructions = (
    """Answer the user question given the following context:\n\n{context}."""
)
qa_prompt = ChatPromptTemplate.from_messages(
    [("system", qa_instructions), ("human", "{question}")]
)


@chain
def contextualize_if_needed(input_: dict) -> Runnable:
    if input_.get("chat_history"):
        # NOTE: This is returning another Runnable, not an actual output.
        return contextualize_question
    else:
        return RunnablePassthrough() | itemgetter("question")


@chain
def fake_retriever(input_: dict) -> str:
    return "egypt's population in 2024 is about 111 million"


full_chain = (
    RunnablePassthrough.assign(question=contextualize_if_needed).assign(
        context=fake_retriever
    )
    | qa_prompt
    | llm
    | StrOutputParser()
)

full_chain.invoke(
    {
        "question": "what about egypt",
        "chat_history": [
            ("human", "what's the population of indonesia"),
            ("ai", "about 276 million"),
        ],
    }
)
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)

```output
"According to the context provided, Egypt's population in 2024 is estimated to be about 111 million."
```

The key here is that `contextualize_if_needed` returns another Runnable and not an actual output. This returned Runnable is itself run when the full chain is executed.

Looking at the trace we can see that, since we passed in chat\_history, we executed the contextualize\_question chain as part of the full chain: [https://smith.langchain.com/public/9e0ae34c-4082-4f3f-beed-34a2a2f4c991/r](https://smith.langchain.com/public/9e0ae34c-4082-4f3f-beed-34a2a2f4c991/r)

Note that the streaming, batching, etc. capabilities of the returned Runnable are all preserved

```python
for chunk in contextualize_if_needed.stream(
    {
        "question": "what about egypt",
        "chat_history": [
            ("human", "what's the population of indonesia"),
            ("ai", "about 276 million"),
        ],
    }
):
    print(chunk)
```

```output
What
 is
 the
 population
 of
 Egypt
?
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/dynamic_chain.ipynb)

* * *










[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_memory/long_term_memory_agent.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_memory/long_term_memory_agent.ipynb)

# A Long-Term Memory Agent

This tutorial shows how to implement an agent with long-term memory capabilities using LangGraph. The agent can store, retrieve, and use memories to enhance its interactions with users.

Inspired by papers like [MemGPT](https://memgpt.ai/) and distilled from our own works on long-term memory, the graph extracts memories from chat interactions and persists them to a database. "Memory" in this tutorial will be represented in two ways:

- a piece of text information that is generated by the agent
- structured information about entities extracted by the agent in the shape of `(subject, predicate, object)` knowledge triples.

This information can later be read or queried semantically to provide personalized context when your bot is responding to a particular user.

The KEY idea is that by saving memories, the agent persists information about users that is SHARED across multiple conversations (threads), which is different from memory of a single conversation that is already enabled by LangGraph's [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/).

![memory_graph.png](data:image/png;base64,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)

You can also check out a full implementation of this agent in [this repo](https://github.com/langchain-ai/lang-memgpt).

## Install dependencies[â€‹](#install-dependencies "Direct link to Install dependencies")

```python
%pip install -U --quiet langgraph langchain-openai langchain-community tiktoken
```

```python
import getpass
import os


def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")


_set_env("OPENAI_API_KEY")
_set_env("TAVILY_API_KEY")
```

```output
OPENAI_API_KEY:  Â·Â·Â·Â·Â·Â·Â·Â·
TAVILY_API_KEY:  Â·Â·Â·Â·Â·Â·Â·Â·
```

```python
import json
from typing import List, Literal, Optional

import tiktoken
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.messages import get_buffer_string
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, MessagesState, StateGraph
from langgraph.prebuilt import ToolNode
```

**API Reference:**[TavilySearchResults](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [Embeddings](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.embeddings.Embeddings.html) | [get\_buffer\_string](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.get_buffer_string.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph) | [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)

## Define vectorstore for memories[â€‹](#define-vectorstore-for-memories "Direct link to Define vectorstore for memories")

First, let's define the vectorstore where we will be storing our memories. Memories will be stored as embeddings and later looked up based on the conversation context. We will be using an in-memory vectorstore.

```python
recall_vector_store = InMemoryVectorStore(OpenAIEmbeddings())
```

### Define tools[â€‹](#define-tools "Direct link to Define tools")

Next, let's define our memory tools. We will need a tool to store the memories and another tool to search them to find the most relevant memory.

```python
import uuid


def get_user_id(config: RunnableConfig) -> str:
    user_id = config["configurable"].get("user_id")
    if user_id is None:
        raise ValueError("User ID needs to be provided to save a memory.")

    return user_id


@tool
def save_recall_memory(memory: str, config: RunnableConfig) -> str:
    """Save memory to vectorstore for later semantic retrieval."""
    user_id = get_user_id(config)
    document = Document(
        page_content=memory, id=str(uuid.uuid4()), metadata={"user_id": user_id}
    )
    recall_vector_store.add_documents([document])
    return memory


@tool
def search_recall_memories(query: str, config: RunnableConfig) -> List[str]:
    """Search for relevant memories."""
    user_id = get_user_id(config)

    def _filter_function(doc: Document) -> bool:
        return doc.metadata.get("user_id") == user_id

    documents = recall_vector_store.similarity_search(
        query, k=3, filter=_filter_function
    )
    return [document.page_content for document in documents]
```

Additionally, let's give our agent ability to search the web using [Tavily](https://tavily.com/).

```python
search = TavilySearchResults(max_results=1)
tools = [save_recall_memory, search_recall_memories, search]
```

### Define state, nodes and edges[â€‹](#define-state-nodes-and-edges "Direct link to Define state, nodes and edges")

Our graph state will contain just two channels -- `messages` for keeping track of the chat history and `recall_memories` -- contextual memories that will be pulled in before calling the agent and passed to the agent's system prompt.

```python
class State(MessagesState):
    # add memories that will be retrieved based on the conversation context
    recall_memories: List[str]
```

```python
# Define the prompt template for the agent
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant with advanced long-term memory"
            " capabilities. Powered by a stateless LLM, you must rely on"
            " external memory to store information between conversations."
            " Utilize the available memory tools to store and retrieve"
            " important details that will help you better attend to the user's"
            " needs and understand their context.\n\n"
            "Memory Usage Guidelines:\n"
            "1. Actively use memory tools (save_core_memory, save_recall_memory)"
            " to build a comprehensive understanding of the user.\n"
            "2. Make informed suppositions and extrapolations based on stored"
            " memories.\n"
            "3. Regularly reflect on past interactions to identify patterns and"
            " preferences.\n"
            "4. Update your mental model of the user with each new piece of"
            " information.\n"
            "5. Cross-reference new information with existing memories for"
            " consistency.\n"
            "6. Prioritize storing emotional context and personal values"
            " alongside facts.\n"
            "7. Use memory to anticipate needs and tailor responses to the"
            " user's style.\n"
            "8. Recognize and acknowledge changes in the user's situation or"
            " perspectives over time.\n"
            "9. Leverage memories to provide personalized examples and"
            " analogies.\n"
            "10. Recall past challenges or successes to inform current"
            " problem-solving.\n\n"
            "## Recall Memories\n"
            "Recall memories are contextually retrieved based on the current"
            " conversation:\n{recall_memories}\n\n"
            "## Instructions\n"
            "Engage with the user naturally, as a trusted colleague or friend."
            " There's no need to explicitly mention your memory capabilities."
            " Instead, seamlessly incorporate your understanding of the user"
            " into your responses. Be attentive to subtle cues and underlying"
            " emotions. Adapt your communication style to match the user's"
            " preferences and current emotional state. Use tools to persist"
            " information you want to retain in the next conversation. If you"
            " do call tools, all text preceding the tool call is an internal"
            " message. Respond AFTER calling the tool, once you have"
            " confirmation that the tool completed successfully.\n\n",
        ),
        ("placeholder", "{messages}"),
    ]
)
```

```python
model = ChatOpenAI(model_name="gpt-4o")
model_with_tools = model.bind_tools(tools)

tokenizer = tiktoken.encoding_for_model("gpt-4o")


def agent(state: State) -> State:
    """Process the current state and generate a response using the LLM.

    Args:
        state (schemas.State): The current state of the conversation.

    Returns:
        schemas.State: The updated state with the agent's response.
    """
    bound = prompt | model_with_tools
    recall_str = (
        "<recall_memory>\n" + "\n".join(state["recall_memories"]) + "\n</recall_memory>"
    )
    prediction = bound.invoke(
        {
            "messages": state["messages"],
            "recall_memories": recall_str,
        }
    )
    return {
        "messages": [prediction],
    }


def load_memories(state: State, config: RunnableConfig) -> State:
    """Load memories for the current conversation.

    Args:
        state (schemas.State): The current state of the conversation.
        config (RunnableConfig): The runtime configuration for the agent.

    Returns:
        State: The updated state with loaded memories.
    """
    convo_str = get_buffer_string(state["messages"])
    convo_str = tokenizer.decode(tokenizer.encode(convo_str)[:2048])
    recall_memories = search_recall_memories.invoke(convo_str, config)
    return {
        "recall_memories": recall_memories,
    }


def route_tools(state: State):
    """Determine whether to use tools or end the conversation based on the last message.

    Args:
        state (schemas.State): The current state of the conversation.

    Returns:
        Literal["tools", "__end__"]: The next step in the graph.
    """
    msg = state["messages"][-1]
    if msg.tool_calls:
        return "tools"

    return END
```

## Build the graph[â€‹](#build-the-graph "Direct link to Build the graph")

Our agent graph is going to be very similar to simple [ReAct agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent). The only important modification is adding a node to load memories BEFORE calling the agent for the first time.

```python
# Create the graph and add nodes
builder = StateGraph(State)
builder.add_node(load_memories)
builder.add_node(agent)
builder.add_node("tools", ToolNode(tools))

# Add edges to the graph
builder.add_edge(START, "load_memories")
builder.add_edge("load_memories", "agent")
builder.add_conditional_edges("agent", route_tools, ["tools", END])
builder.add_edge("tools", "agent")

# Compile the graph
memory = MemorySaver()
graph = builder.compile(checkpointer=memory)
```

```python
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
```

![](data:image/jpg;base64,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)

## Run the agent\![â€‹](#run-the-agent "Direct link to Run the agent!")

Let's run the agent for the first time and tell it some information about the user!

```python
def pretty_print_stream_chunk(chunk):
    for node, updates in chunk.items():
        print(f"Update from node: {node}")
        if "messages" in updates:
            updates["messages"][-1].pretty_print()
        else:
            print(updates)

        print("\n")
```

```python
# NOTE: we're specifying `user_id` to save memories for a given user
config = {"configurable": {"user_id": "1", "thread_id": "1"}}

for chunk in graph.stream({"messages": [("user", "my name is John")]}, config=config):
    pretty_print_stream_chunk(chunk)
```

```output
Update from node: load_memories
{'recall_memories': []}


Update from node: agent
==================================[1m Ai Message [0m==================================
Tool Calls:
  save_recall_memory (call_OqfbWodmrywjMnB1v3p19QLt)
 Call ID: call_OqfbWodmrywjMnB1v3p19QLt
  Args:
    memory: User's name is John.


Update from node: tools
=================================[1m Tool Message [0m=================================
Name: save_recall_memory

User's name is John.


Update from node: agent
==================================[1m Ai Message [0m==================================

Nice to meet you, John! How can I assist you today?
```

You can see that the agent saved the memory about user's name. Let's add some more information about the user!

```python
for chunk in graph.stream({"messages": [("user", "i love pizza")]}, config=config):
    pretty_print_stream_chunk(chunk)
```

```output
Update from node: load_memories
{'recall_memories': ["User's name is John."]}


Update from node: agent
==================================[1m Ai Message [0m==================================
Tool Calls:
  save_recall_memory (call_xxEivMuWCURJrGxMZb02Eh31)
 Call ID: call_xxEivMuWCURJrGxMZb02Eh31
  Args:
    memory: John loves pizza.


Update from node: tools
=================================[1m Tool Message [0m=================================
Name: save_recall_memory

John loves pizza.


Update from node: agent
==================================[1m Ai Message [0m==================================

Pizza is amazing! Do you have a favorite type or topping?
```

```python
for chunk in graph.stream(
    {"messages": [("user", "yes -- pepperoni!")]},
    config={"configurable": {"user_id": "1", "thread_id": "1"}},
):
    pretty_print_stream_chunk(chunk)
```

```output
Update from node: load_memories
{'recall_memories': ["User's name is John.", 'John loves pizza.']}


Update from node: agent
==================================[1m Ai Message [0m==================================
Tool Calls:
  save_recall_memory (call_AFrtCVwIEr48Fim80zlhe6xg)
 Call ID: call_AFrtCVwIEr48Fim80zlhe6xg
  Args:
    memory: John's favorite pizza topping is pepperoni.


Update from node: tools
=================================[1m Tool Message [0m=================================
Name: save_recall_memory

John's favorite pizza topping is pepperoni.


Update from node: agent
==================================[1m Ai Message [0m==================================

Pepperoni is a classic choice! Do you have a favorite pizza place, or do you enjoy making it at home?
```

```python
for chunk in graph.stream(
    {"messages": [("user", "i also just moved to new york")]},
    config={"configurable": {"user_id": "1", "thread_id": "1"}},
):
    pretty_print_stream_chunk(chunk)
```

```output
Update from node: load_memories
{'recall_memories': ["User's name is John.", 'John loves pizza.', "John's favorite pizza topping is pepperoni."]}


Update from node: agent
==================================[1m Ai Message [0m==================================
Tool Calls:
  save_recall_memory (call_Na86uY9eBzaJ0sS0GM4Z9tSf)
 Call ID: call_Na86uY9eBzaJ0sS0GM4Z9tSf
  Args:
    memory: John just moved to New York.


Update from node: tools
=================================[1m Tool Message [0m=================================
Name: save_recall_memory

John just moved to New York.


Update from node: agent
==================================[1m Ai Message [0m==================================

Welcome to New York! That's a fantastic place for a pizza lover. Have you had a chance to explore any of the famous pizzerias there yet?
```

Now we can use the saved information about our user on a different thread. Let's try it out:

```python
config = {"configurable": {"user_id": "1", "thread_id": "2"}}

for chunk in graph.stream(
    {"messages": [("user", "where should i go for dinner?")]}, config=config
):
    pretty_print_stream_chunk(chunk)
```

```output
Update from node: load_memories
{'recall_memories': ['John loves pizza.', "User's name is John.", 'John just moved to New York.']}


Update from node: agent
==================================[1m Ai Message [0m==================================

Considering you just moved to New York and love pizza, I'd recommend checking out some of the iconic pizza places in the city. Some popular spots include:

1. **Di Fara Pizza** in Brooklyn â€“ Known for its classic New York-style pizza.
2. **Joe's Pizza** in Greenwich Village â€“ A historic pizzeria with a great reputation.
3. **Lucali** in Carroll Gardens, Brooklyn â€“ Often ranked among the best for its delicious thin-crust pies.

Would you like more recommendations or information about any of these places?
```

Notice how the agent is loading the most relevant memories before answering, and in our case suggests the dinner recommendations based on both the food preferences as well as location.

Finally, let's use the search tool together with the rest of the conversation context and memory to find location of a pizzeria:

```python
for chunk in graph.stream(
    {"messages": [("user", "what's the address for joe's in greenwich village?")]},
    config=config,
):
    pretty_print_stream_chunk(chunk)
```

```output
Update from node: load_memories
{'recall_memories': ['John loves pizza.', 'John just moved to New York.', "John's favorite pizza topping is pepperoni."]}


Update from node: agent
==================================[1m Ai Message [0m==================================
Tool Calls:
  tavily_search_results_json (call_aespiB28jpTFvaC4d0qpfY6t)
 Call ID: call_aespiB28jpTFvaC4d0qpfY6t
  Args:
    query: Joe's Pizza Greenwich Village NYC address


Update from node: tools
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json

[{"url": "https://www.joespizzanyc.com/locations-1-1", "content": "Joe's Pizza Greenwich Village (Original Location) 7 Carmine Street New York, NY 10014 (212) 366-1182ï»¿ Joe's Pizza Times Square 1435 Broadway New York, NY 10018 (646) 559-4878. TIMES SQUARE MENU. ORDER JOE'S TIMES SQUARE Joe's Pizza Williamsburg 216 Bedford Avenue Brooklyn, NY 11249"}]


Update from node: agent
==================================[1m Ai Message [0m==================================

The address for Joe's Pizza in Greenwich Village is:

**7 Carmine Street, New York, NY 10014**

Enjoy your pizza!
```

If you were to pass a different user ID, the agent's response will not be personalized as we haven't saved any information about the other user:

## Adding structured memories[â€‹](#adding-structured-memories "Direct link to Adding structured memories")

So far we've represented memories as strings, e.g., `"John loves pizza"`. This is a natural representation when persisting memories to a vector store. If your use-case would benefit from other persistence backends-- such as a graph database-- we can update our application to generate memories with additional structure.

Below, we update the `save_recall_memory` tool to accept a list of "knowledge triples", or 3-tuples with a `subject`, `predicate`, and `object`, suitable for storage in a knolwedge graph. Our model will then generate these representations as part of its tool calls.

For simplicity, we use the same vector database as before, but the `save_recall_memory` and `search_recall_memories` tools could be further updated to interact with a graph database. For now, we only need to update the `save_recall_memory` tool:

```python
recall_vector_store = InMemoryVectorStore(OpenAIEmbeddings())
```

```python
from typing_extensions import TypedDict


class KnowledgeTriple(TypedDict):
    subject: str
    predicate: str
    object_: str


@tool
def save_recall_memory(memories: List[KnowledgeTriple], config: RunnableConfig) -> str:
    """Save memory to vectorstore for later semantic retrieval."""
    user_id = get_user_id(config)
    for memory in memories:
        serialized = " ".join(memory.values())
        document = Document(
            serialized,
            id=str(uuid.uuid4()),
            metadata={
                "user_id": user_id,
                **memory,
            },
        )
        recall_vector_store.add_documents([document])
    return memories
```

We can then compile the graph exactly as before:

```python
tools = [save_recall_memory, search_recall_memories, search]
model_with_tools = model.bind_tools(tools)


# Create the graph and add nodes
builder = StateGraph(State)
builder.add_node(load_memories)
builder.add_node(agent)
builder.add_node("tools", ToolNode(tools))

# Add edges to the graph
builder.add_edge(START, "load_memories")
builder.add_edge("load_memories", "agent")
builder.add_conditional_edges("agent", route_tools, ["tools", END])
builder.add_edge("tools", "agent")

# Compile the graph
memory = MemorySaver()
graph = builder.compile(checkpointer=memory)
```

```python
config = {"configurable": {"user_id": "3", "thread_id": "1"}}

for chunk in graph.stream({"messages": [("user", "Hi, I'm Alice.")]}, config=config):
    pretty_print_stream_chunk(chunk)
```

```output
Update from node: load_memories
{'recall_memories': []}


Update from node: agent
==================================[1m Ai Message [0m==================================

Hello, Alice! How can I assist you today?
```

Note that the application elects to extract knowledge-triples from the user's statements:

```python
for chunk in graph.stream(
    {"messages": [("user", "My friend John likes Pizza.")]}, config=config
):
    pretty_print_stream_chunk(chunk)
```

```output
Update from node: load_memories
{'recall_memories': []}


Update from node: agent
==================================[1m Ai Message [0m==================================
Tool Calls:
  save_recall_memory (call_EQSZlvZLZpPa0OGS5Kyzy2Yz)
 Call ID: call_EQSZlvZLZpPa0OGS5Kyzy2Yz
  Args:
    memories: [{'subject': 'Alice', 'predicate': 'has a friend', 'object_': 'John'}, {'subject': 'John', 'predicate': 'likes', 'object_': 'Pizza'}]


Update from node: tools
=================================[1m Tool Message [0m=================================
Name: save_recall_memory

[{"subject": "Alice", "predicate": "has a friend", "object_": "John"}, {"subject": "John", "predicate": "likes", "object_": "Pizza"}]


Update from node: agent
==================================[1m Ai Message [0m==================================

Got it! If you need any suggestions related to pizza or anything else, feel free to ask. What else is on your mind today?
```

As before, the memories generated from one thread are accessed in another thread from the same user:

```python
config = {"configurable": {"user_id": "3", "thread_id": "2"}}

for chunk in graph.stream(
    {"messages": [("user", "What food should I bring to John's party?")]}, config=config
):
    pretty_print_stream_chunk(chunk)
```

```output
Update from node: load_memories
{'recall_memories': ['John likes Pizza', 'Alice has a friend John']}


Update from node: agent
==================================[1m Ai Message [0m==================================

Since John likes pizza, bringing some delicious pizza would be a great choice for the party. You might also consider asking if there are any specific toppings he prefers or if there are any dietary restrictions among the guests. This way, you can ensure everyone enjoys the food!
```

Optionally, for illustrative purposes we can visualize the knowledge graph extracted by the model:

```python
%pip install -U --quiet matplotlib networkx
```

```python
import matplotlib.pyplot as plt
import networkx as nx

# Fetch records
records = recall_vector_store.similarity_search(
    "Alice", k=2, filter=lambda doc: doc.metadata["user_id"] == "3"
)


# Plot graph
plt.figure(figsize=(6, 4), dpi=80)
G = nx.DiGraph()

for record in records:
    G.add_edge(
        record.metadata["subject"],
        record.metadata["object_"],
        label=record.metadata["predicate"],
    )

pos = nx.spring_layout(G)
nx.draw(
    G,
    pos,
    with_labels=True,
    node_size=3000,
    node_color="lightblue",
    font_size=10,
    font_weight="bold",
    arrows=True,
)
edge_labels = nx.get_edge_attributes(G, "label")
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color="red")
plt.show()
```

![](data:image/png;base64,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)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_memory/long_term_memory_agent.ipynb)

* * *


- [Install dependencies](#install-dependencies)
- [Define vectorstore for memories](#define-vectorstore-for-memories)
  
  - [Define tools](#define-tools)
  - [Define state, nodes and edges](#define-state-nodes-and-edges)
- [Build the graph](#build-the-graph)
- [Run the agent!](#run-the-agent)
- [Adding structured memories](#adding-structured-memories)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/indexing.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/indexing.ipynb)

# How to use the LangChain indexing API

Here, we will look at a basic indexing workflow using the LangChain indexing API.

The indexing API lets you load and keep in sync documents from any source into a [vector store](/docs/concepts/vectorstores/). Specifically, it helps:

- Avoid writing duplicated content into the vector store
- Avoid re-writing unchanged content
- Avoid re-computing embeddings over unchanged content

All of which should save you time and money, as well as improve your vector search results.

Crucially, the indexing API will work even with documents that have gone through several transformation steps (e.g., via text chunking) with respect to the original source documents.

## How it works[â€‹](#how-it-works "Direct link to How it works")

LangChain indexing makes use of a record manager (`RecordManager`) that keeps track of document writes into the vector store.

When indexing content, hashes are computed for each document, and the following information is stored in the record manager:

- the document hash (hash of both page content and metadata)
- write time
- the source id -- each document should include information in its metadata to allow us to determine the ultimate source of this document

## Deletion modes[â€‹](#deletion-modes "Direct link to Deletion modes")

When indexing documents into a vector store, it's possible that some existing documents in the vector store should be deleted. In certain situations you may want to remove any existing documents that are derived from the same sources as the new documents being indexed. In others you may want to delete all existing documents wholesale. The indexing API deletion modes let you pick the behavior you want:

| Cleanup Mode | De-Duplicates Content | Parallelizable | Cleans Up Deleted Source Docs | Cleans Up Mutations of Source Docs and/or Derived Docs | Clean Up Timing    |
|--------------|-----------------------|----------------|-------------------------------|--------------------------------------------------------|--------------------|
| None         | âœ…                     | âœ…              | âŒ                             | âŒ                                                      | \-                 |
| Incremental  | âœ…                     | âœ…              | âŒ                             | âœ…                                                      | Continuously       |
| Full         | âœ…                     | âŒ              | âœ…                             | âœ…                                                      | At end of indexing |
| Scoped\_Full | âœ…                     | âœ…              | âŒ                             | âœ…                                                      | At end of indexing |

`None` does not do any automatic clean up, allowing the user to manually do clean up of old content.

`incremental`, `full` and `scoped_full` offer the following automated clean up:

- If the content of the source document or derived documents has **changed**, all 3 modes will clean up (delete) previous versions of the content.
- If the source document has been **deleted** (meaning it is not included in the documents currently being indexed), the `full` cleanup mode will delete it from the vector store correctly, but the `incremental` and `scoped_full` mode will not.

When content is mutated (e.g., the source PDF file was revised) there will be a period of time during indexing when both the new and old versions may be returned to the user. This happens after the new content was written, but before the old version was deleted.

- `incremental` indexing minimizes this period of time as it is able to do clean up continuously, as it writes.
- `full` and `scoped_full` mode does the clean up after all batches have been written.

## Requirements[â€‹](#requirements "Direct link to Requirements")

1. Do not use with a store that has been pre-populated with content independently of the indexing API, as the record manager will not know that records have been inserted previously.
2. Only works with LangChain `vectorstore`'s that support:
   
   - document addition by id (`add_documents` method with `ids` argument)
   - delete by id (`delete` method with `ids` argument)

Compatible Vectorstores: `Aerospike`, `AnalyticDB`, `AstraDB`, `AwaDB`, `AzureCosmosDBNoSqlVectorSearch`, `AzureCosmosDBVectorSearch`, `AzureSearch`, `Bagel`, `Cassandra`, `Chroma`, `CouchbaseVectorStore`, `DashVector`, `DatabricksVectorSearch`, `DeepLake`, `Dingo`, `ElasticVectorSearch`, `ElasticsearchStore`, `FAISS`, `HanaDB`, `Milvus`, `MongoDBAtlasVectorSearch`, `MyScale`, `OpenSearchVectorSearch`, `PGVector`, `Pinecone`, `Qdrant`, `Redis`, `Rockset`, `ScaNN`, `SingleStoreDB`, `SupabaseVectorStore`, `SurrealDBStore`, `TimescaleVector`, `Vald`, `VDMS`, `Vearch`, `VespaStore`, `Weaviate`, `Yellowbrick`, `ZepVectorStore`, `TencentVectorDB`, `OpenSearchVectorSearch`.

## Caution[â€‹](#caution "Direct link to Caution")

The record manager relies on a time-based mechanism to determine what content can be cleaned up (when using `full` or `incremental` or `scoped_full` cleanup modes).

If two tasks run back-to-back, and the first task finishes before the clock time changes, then the second task may not be able to clean up content.

This is unlikely to be an issue in actual settings for the following reasons:

1. The RecordManager uses higher resolution timestamps.
2. The data would need to change between the first and the second tasks runs, which becomes unlikely if the time interval between the tasks is small.
3. Indexing tasks typically take more than a few ms.

## Quickstart[â€‹](#quickstart "Direct link to Quickstart")

```python
from langchain.indexes import SQLRecordManager, index
from langchain_core.documents import Document
from langchain_elasticsearch import ElasticsearchStore
from langchain_openai import OpenAIEmbeddings
```

**API Reference:**[SQLRecordManager](https://python.langchain.com/api_reference/langchain/indexes/langchain.indexes._sql_record_manager.SQLRecordManager.html) | [index](https://python.langchain.com/api_reference/core/indexing/langchain_core.indexing.api.index.html) | [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) | [ElasticsearchStore](https://python.langchain.com/api_reference/elasticsearch/vectorstores/langchain_elasticsearch.vectorstores.ElasticsearchStore.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

Initialize a vector store and set up the embeddings:

```python
collection_name = "test_index"

embedding = OpenAIEmbeddings()

vectorstore = ElasticsearchStore(
    es_url="http://localhost:9200", index_name="test_index", embedding=embedding
)
```

Initialize a record manager with an appropriate namespace.

**Suggestion:** Use a namespace that takes into account both the vector store and the collection name in the vector store; e.g., 'redis/my\_docs', 'chromadb/my\_docs' or 'postgres/my\_docs'.

```python
namespace = f"elasticsearch/{collection_name}"
record_manager = SQLRecordManager(
    namespace, db_url="sqlite:///record_manager_cache.sql"
)
```

Create a schema before using the record manager.

```python
record_manager.create_schema()
```

Let's index some test documents:

```python
doc1 = Document(page_content="kitty", metadata={"source": "kitty.txt"})
doc2 = Document(page_content="doggy", metadata={"source": "doggy.txt"})
```

Indexing into an empty vector store:

```python
def _clear():
    """Hacky helper method to clear content. See the `full` mode section to to understand why it works."""
    index([], record_manager, vectorstore, cleanup="full", source_id_key="source")
```

### `None` deletion mode[â€‹](#none-deletion-mode "Direct link to none-deletion-mode")

This mode does not do automatic clean up of old versions of content; however, it still takes care of content de-duplication.

```python
_clear()
```

```python
index(
    [doc1, doc1, doc1, doc1, doc1],
    record_manager,
    vectorstore,
    cleanup=None,
    source_id_key="source",
)
```

```output
{'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}
```

```python
_clear()
```

```python
index([doc1, doc2], record_manager, vectorstore, cleanup=None, source_id_key="source")
```

```output
{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}
```

Second time around all content will be skipped:

```python
index([doc1, doc2], record_manager, vectorstore, cleanup=None, source_id_key="source")
```

```output
{'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}
```

### `"incremental"` deletion mode[â€‹](#incremental-deletion-mode "Direct link to incremental-deletion-mode")

```python
_clear()
```

```python
index(
    [doc1, doc2],
    record_manager,
    vectorstore,
    cleanup="incremental",
    source_id_key="source",
)
```

```output
{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}
```

Indexing again should result in both documents getting **skipped** -- also skipping the embedding operation!

```python
index(
    [doc1, doc2],
    record_manager,
    vectorstore,
    cleanup="incremental",
    source_id_key="source",
)
```

```output
{'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}
```

If we provide no documents with incremental indexing mode, nothing will change.

```python
index([], record_manager, vectorstore, cleanup="incremental", source_id_key="source")
```

```output
{'num_added': 0, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}
```

If we mutate a document, the new version will be written and all old versions sharing the same source will be deleted.

```python
changed_doc_2 = Document(page_content="puppy", metadata={"source": "doggy.txt"})
```

```python
index(
    [changed_doc_2],
    record_manager,
    vectorstore,
    cleanup="incremental",
    source_id_key="source",
)
```

```output
{'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 1}
```

### `"full"` deletion mode[â€‹](#full-deletion-mode "Direct link to full-deletion-mode")

In `full` mode the user should pass the `full` universe of content that should be indexed into the indexing function.

Any documents that are not passed into the indexing function and are present in the vectorstore will be deleted!

This behavior is useful to handle deletions of source documents.

```python
_clear()
```

```python
all_docs = [doc1, doc2]
```

```python
index(all_docs, record_manager, vectorstore, cleanup="full", source_id_key="source")
```

```output
{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}
```

Say someone deleted the first doc:

```python
del all_docs[0]
```

```python
all_docs
```

```output
[Document(page_content='doggy', metadata={'source': 'doggy.txt'})]
```

Using full mode will clean up the deleted content as well.

```python
index(all_docs, record_manager, vectorstore, cleanup="full", source_id_key="source")
```

```output
{'num_added': 0, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}
```

## Source[â€‹](#source "Direct link to Source")

The metadata attribute contains a field called `source`. This source should be pointing at the *ultimate* provenance associated with the given document.

For example, if these documents are representing chunks of some parent document, the `source` for both documents should be the same and reference the parent document.

In general, `source` should always be specified. Only use a `None`, if you **never** intend to use `incremental` mode, and for some reason can't specify the `source` field correctly.

```python
from langchain_text_splitters import CharacterTextSplitter
```

**API Reference:**[CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

```python
doc1 = Document(
    page_content="kitty kitty kitty kitty kitty", metadata={"source": "kitty.txt"}
)
doc2 = Document(page_content="doggy doggy the doggy", metadata={"source": "doggy.txt"})
```

```python
new_docs = CharacterTextSplitter(
    separator="t", keep_separator=True, chunk_size=12, chunk_overlap=2
).split_documents([doc1, doc2])
new_docs
```

```output
[Document(page_content='kitty kit', metadata={'source': 'kitty.txt'}),
 Document(page_content='tty kitty ki', metadata={'source': 'kitty.txt'}),
 Document(page_content='tty kitty', metadata={'source': 'kitty.txt'}),
 Document(page_content='doggy doggy', metadata={'source': 'doggy.txt'}),
 Document(page_content='the doggy', metadata={'source': 'doggy.txt'})]
```

```python
_clear()
```

```python
index(
    new_docs,
    record_manager,
    vectorstore,
    cleanup="incremental",
    source_id_key="source",
)
```

```output
{'num_added': 5, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}
```

```python
changed_doggy_docs = [
    Document(page_content="woof woof", metadata={"source": "doggy.txt"}),
    Document(page_content="woof woof woof", metadata={"source": "doggy.txt"}),
]
```

This should delete the old versions of documents associated with `doggy.txt` source and replace them with the new versions.

```python
index(
    changed_doggy_docs,
    record_manager,
    vectorstore,
    cleanup="incremental",
    source_id_key="source",
)
```

```output
{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 2}
```

```python
vectorstore.similarity_search("dog", k=30)
```

```output
[Document(page_content='woof woof', metadata={'source': 'doggy.txt'}),
 Document(page_content='woof woof woof', metadata={'source': 'doggy.txt'}),
 Document(page_content='tty kitty', metadata={'source': 'kitty.txt'}),
 Document(page_content='tty kitty ki', metadata={'source': 'kitty.txt'}),
 Document(page_content='kitty kit', metadata={'source': 'kitty.txt'})]
```

## Using with loaders[â€‹](#using-with-loaders "Direct link to Using with loaders")

Indexing can accept either an iterable of documents or else any loader.

**Attention:** The loader **must** set source keys correctly.

```python
from langchain_core.document_loaders import BaseLoader


class MyCustomLoader(BaseLoader):
    def lazy_load(self):
        text_splitter = CharacterTextSplitter(
            separator="t", keep_separator=True, chunk_size=12, chunk_overlap=2
        )
        docs = [
            Document(page_content="woof woof", metadata={"source": "doggy.txt"}),
            Document(page_content="woof woof woof", metadata={"source": "doggy.txt"}),
        ]
        yield from text_splitter.split_documents(docs)

    def load(self):
        return list(self.lazy_load())
```

**API Reference:**[BaseLoader](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html)

```python
_clear()
```

```python
loader = MyCustomLoader()
```

```python
loader.load()
```

```output
[Document(page_content='woof woof', metadata={'source': 'doggy.txt'}),
 Document(page_content='woof woof woof', metadata={'source': 'doggy.txt'})]
```

```python
index(loader, record_manager, vectorstore, cleanup="full", source_id_key="source")
```

```output
{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}
```

```python
vectorstore.similarity_search("dog", k=30)
```

```output
[Document(page_content='woof woof', metadata={'source': 'doggy.txt'}),
 Document(page_content='woof woof woof', metadata={'source': 'doggy.txt'})]
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/indexing.ipynb)

* * *


- [How it works](#how-it-works)
- [Deletion modes](#deletion-modes)
- [Requirements](#requirements)
- [Caution](#caution)
- [Quickstart](#quickstart)
  
  - [`None` deletion mode](#none-deletion-mode)
  - [`"incremental"` deletion mode](#incremental-deletion-mode)
  - [`"full"` deletion mode](#full-deletion-mode)
- [Source](#source)
- [Using with loaders](#using-with-loaders)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/map_reduce_chain.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/map_reduce_chain.ipynb)

# Migrating from MapReduceDocumentsChain

[MapReduceDocumentsChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html) implements a map-reduce strategy over (potentially long) texts. The strategy is as follows:

- Split a text into smaller documents;
- Map a process onto the smaller documents;
- Reduce or consolidate the results of the process into a final result.

Note that the map step is typically parallelized over the input documents.

A common process applied in this context is summarization, in which the map step summarizes individual documents, and the reduce step generates a summary of the summaries.

In the reduce step, `MapReduceDocumentsChain` supports a recursive "collapsing" of the summaries: the inputs would be partitioned based on a token limit, and summaries would be generated of the partitions. This step would be repeated until the total length of the summaries was within a desired limit, allowing for the summarization of arbitrary-length text. This is particularly useful for models with smaller context windows.

LangGraph supports [map-reduce](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) workflows, and confers a number of advantages for this problem:

- LangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;
- LangGraph's [checkpointing](https://langchain-ai.github.io/langgraph/how-tos/persistence/) supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.
- The LangGraph implementation is easier to extend, as we will see below.

Below we will go through both `MapReduceDocumentsChain` and a corresponding LangGraph implementation, first on a simple example for illustrative purposes, and second on a longer example text to demonstrate the recursive reduce step.

Let's first load a chat model:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

## Basic example (short documents)[â€‹](#basic-example-short-documents "Direct link to Basic example (short documents)")

Let's use the following 3 documents for illustrative purposes.

```python
from langchain_core.documents import Document

documents = [
    Document(page_content="Apples are red", metadata={"title": "apple_book"}),
    Document(page_content="Blueberries are blue", metadata={"title": "blueberry_book"}),
    Document(page_content="Bananas are yelow", metadata={"title": "banana_book"}),
]
```

**API Reference:**[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

### Legacy[â€‹](#legacy "Direct link to Legacy")

Details

Below we show an implementation with `MapReduceDocumentsChain`. We define the prompt templates for the map and reduce steps, instantiate separate chains for these steps, and finally instantiate the `MapReduceDocumentsChain`:

```python
from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains.llm import LLMChain
from langchain_core.prompts import ChatPromptTemplate
from langchain_text_splitters import CharacterTextSplitter

# Map
map_template = "Write a concise summary of the following: {docs}."
map_prompt = ChatPromptTemplate([("human", map_template)])
map_chain = LLMChain(llm=llm, prompt=map_prompt)


# Reduce
reduce_template = """
The following is a set of summaries:
{docs}
Take these and distill it into a final, consolidated summary
of the main themes.
"""
reduce_prompt = ChatPromptTemplate([("human", reduce_template)])
reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)


# Takes a list of documents, combines them into a single string, and passes this to an LLMChain
combine_documents_chain = StuffDocumentsChain(
    llm_chain=reduce_chain, document_variable_name="docs"
)

# Combines and iteratively reduces the mapped documents
reduce_documents_chain = ReduceDocumentsChain(
    # This is final chain that is called.
    combine_documents_chain=combine_documents_chain,
    # If documents exceed context for `StuffDocumentsChain`
    collapse_documents_chain=combine_documents_chain,
    # The maximum number of tokens to group documents into.
    token_max=1000,
)

# Combining documents by mapping a chain over them, then combining results
map_reduce_chain = MapReduceDocumentsChain(
    # Map chain
    llm_chain=map_chain,
    # Reduce chain
    reduce_documents_chain=reduce_documents_chain,
    # The variable name in the llm_chain to put the documents in
    document_variable_name="docs",
    # Return the results of the map steps in the output
    return_intermediate_steps=False,
)
```

**API Reference:**[MapReduceDocumentsChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html) | [ReduceDocumentsChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html) | [StuffDocumentsChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html) | [LLMChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

```python
result = map_reduce_chain.invoke(documents)

print(result["output_text"])
```

```output
Fruits come in a variety of colors, with apples being red, blueberries being blue, and bananas being yellow.
```

In the [LangSmith trace](https://smith.langchain.com/public/8d88a2c0-5d26-41f6-9176-d06549b17aa6/r) we observe four LLM calls: one summarizing each of the three input documents, and one summarizing the summaries.

### LangGraph[â€‹](#langgraph "Direct link to LangGraph")

Below we show a LangGraph implementation, using the same prompt templates as above. The graph includes a node for generating summaries which is mapped across a list of input documents. This node then flows to a second node that generates the final summary.

Details

We will need to install `langgraph`:

```python
%pip install -qU langgraph
```

```python
import operator
from typing import Annotated, List, TypedDict

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langgraph.constants import Send
from langgraph.graph import END, START, StateGraph

map_template = "Write a concise summary of the following: {context}."

reduce_template = """
The following is a set of summaries:
{docs}
Take these and distill it into a final, consolidated summary
of the main themes.
"""

map_prompt = ChatPromptTemplate([("human", map_template)])
reduce_prompt = ChatPromptTemplate([("human", reduce_template)])

map_chain = map_prompt | llm | StrOutputParser()
reduce_chain = reduce_prompt | llm | StrOutputParser()

# Graph components: define the components that will make up the graph


# This will be the overall state of the main graph.
# It will contain the input document contents, corresponding
# summaries, and a final summary.
class OverallState(TypedDict):
    # Notice here we use the operator.add
    # This is because we want combine all the summaries we generate
    # from individual nodes back into one list - this is essentially
    # the "reduce" part
    contents: List[str]
    summaries: Annotated[list, operator.add]
    final_summary: str


# This will be the state of the node that we will "map" all
# documents to in order to generate summaries
class SummaryState(TypedDict):
    content: str


# Here we generate a summary, given a document
async def generate_summary(state: SummaryState):
    response = await map_chain.ainvoke(state["content"])
    return {"summaries": [response]}


# Here we define the logic to map out over the documents
# We will use this an edge in the graph
def map_summaries(state: OverallState):
    # We will return a list of `Send` objects
    # Each `Send` object consists of the name of a node in the graph
    # as well as the state to send to that node
    return [
        Send("generate_summary", {"content": content}) for content in state["contents"]
    ]


# Here we will generate the final summary
async def generate_final_summary(state: OverallState):
    response = await reduce_chain.ainvoke(state["summaries"])
    return {"final_summary": response}


# Construct the graph: here we put everything together to construct our graph
graph = StateGraph(OverallState)
graph.add_node("generate_summary", generate_summary)
graph.add_node("generate_final_summary", generate_final_summary)
graph.add_conditional_edges(START, map_summaries, ["generate_summary"])
graph.add_edge("generate_summary", "generate_final_summary")
graph.add_edge("generate_final_summary", END)
app = graph.compile()
```

**API Reference:**[StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [Send](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)

```python
from IPython.display import Image

Image(app.get_graph().draw_mermaid_png())
```

![](data:image/jpg;base64,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)

Note that calling the graph in streaming mode allows us to monitor steps and potentially take action on them during execution.

```python
# Call the graph:
async for step in app.astream({"contents": [doc.page_content for doc in documents]}):
    print(step)
```

```output
{'generate_summary': {'summaries': ['Apples are typically red in color.']}}
{'generate_summary': {'summaries': ['Bananas are yellow in color.']}}
{'generate_summary': {'summaries': ['Blueberries are a type of fruit that are blue in color.']}}
{'generate_final_summary': {'final_summary': 'The main themes are the colors of different fruits: apples are red, blueberries are blue, and bananas are yellow.'}}
```

In the [LangSmith trace](https://smith.langchain.com/public/8ecbe9fd-eb02-4c6e-90ae-659952c9360a/r) we recover the same four LLM calls as before.

## Summarizing long documents[â€‹](#summarizing-long-documents "Direct link to Summarizing long documents")

Map-reduce flows are particularly useful when texts are long compared to the context window of a LLM. `MapReduceDocumentsChain` supports a recursive "collapsing" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.

This "collapse" step is implemented as a `while` loop within `MapReduceDocumentsChain`. We can demonstrate this step on a longer text, a [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng (as featured in the [RAG tutorial](/docs/tutorials/rag/) and other documentation).

First we load the post and chunk it into smaller "sub documents":

```python
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import CharacterTextSplitter

loader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")
documents = loader.load()

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000, chunk_overlap=0
)
split_docs = text_splitter.split_documents(documents)
print(f"Generated {len(split_docs)} documents.")
```

**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

```````output
USER_AGENT environment variable not set, consider setting it to identify your requests.
Created a chunk of size 1003, which is longer than the specified 1000
``````output
Generated 14 documents.
```````

### Legacy[â€‹](#legacy-1 "Direct link to Legacy")

Details

We can invoke `MapReduceDocumentsChain` as before:

```python
result = map_reduce_chain.invoke(split_docs)

print(result["output_text"])
```

```output
The article discusses the use of Large Language Models (LLMs) to power autonomous agents in various tasks, showcasing their capabilities in problem-solving beyond generating written content. Key components such as planning, memory optimization, and tool use are explored, with proof-of-concept demos like AutoGPT and GPT-Engineer demonstrating the potential of LLM-powered agents. Challenges include limitations in historical information retention and natural language interface reliability, while the potential of LLMs in enhancing reasoning, problem-solving, and planning proficiency for autonomous agents is highlighted. Overall, the article emphasizes the versatility and power of LLMs in creating intelligent agents for tasks like scientific discovery and experiment design.
```

Consider the [LangSmith trace](https://smith.langchain.com/public/d8b3311d-2220-487a-8eaf-104ef90678dd/r) for the above invocation. When instantiating our `ReduceDocumentsChain`, we set a `token_max` of 1,000 tokens. This results in a total of 17 LLM calls:

- 14 calls are for summarizing the 14 sub-documents generated by our text splitter.
- This generated summaries that totaled about 1,000 - 2,000 tokens. Because we set a `token_max` of 1,000, there are two more calls to summarize (or "collapse") these summaries.
- One final call is for generating a final summary of the two "collapsed" summaries.

### LangGraph[â€‹](#langgraph-1 "Direct link to LangGraph")

Details

We can extend our original map-reduce implementation in LangGraph to implement the same recursive collapsing step. We make the following changes:

- Add a `collapsed_summaries` key to the state to store the collapsed summaries;
- Update the final summarization node to summarize the collapsed summaries;
- Add a `collapse_summaries` node that partitions a list of documents based on a token length (1,000 tokens here, as before) and generates summaries of each partition and stores the result in `collapsed_summaries`.

We add a conditional edge from `collapse_summaries` to itself to form a loop: if the collapsed summaries total more than the `token_max`, we re-run the node.

```python
from typing import Literal

from langchain.chains.combine_documents.reduce import (
    acollapse_docs,
    split_list_of_docs,
)


def length_function(documents: List[Document]) -> int:
    """Get number of tokens for input contents."""
    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)


token_max = 1000


class OverallState(TypedDict):
    contents: List[str]
    summaries: Annotated[list, operator.add]
    collapsed_summaries: List[Document]  # add key for collapsed summaries
    final_summary: str


# Add node to store summaries for collapsing
def collect_summaries(state: OverallState):
    return {
        "collapsed_summaries": [Document(summary) for summary in state["summaries"]]
    }


# Modify final summary to read off collapsed summaries
async def generate_final_summary(state: OverallState):
    response = await reduce_chain.ainvoke(state["collapsed_summaries"])
    return {"final_summary": response}


graph = StateGraph(OverallState)
graph.add_node("generate_summary", generate_summary)  # same as before
graph.add_node("collect_summaries", collect_summaries)
graph.add_node("generate_final_summary", generate_final_summary)


# Add node to collapse summaries
async def collapse_summaries(state: OverallState):
    doc_lists = split_list_of_docs(
        state["collapsed_summaries"], length_function, token_max
    )
    results = []
    for doc_list in doc_lists:
        results.append(await acollapse_docs(doc_list, reduce_chain.ainvoke))

    return {"collapsed_summaries": results}


graph.add_node("collapse_summaries", collapse_summaries)


def should_collapse(
    state: OverallState,
) -> Literal["collapse_summaries", "generate_final_summary"]:
    num_tokens = length_function(state["collapsed_summaries"])
    if num_tokens > token_max:
        return "collapse_summaries"
    else:
        return "generate_final_summary"


graph.add_conditional_edges(START, map_summaries, ["generate_summary"])
graph.add_edge("generate_summary", "collect_summaries")
graph.add_conditional_edges("collect_summaries", should_collapse)
graph.add_conditional_edges("collapse_summaries", should_collapse)
graph.add_edge("generate_final_summary", END)
app = graph.compile()
```

**API Reference:**[acollapse\_docs](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.reduce.acollapse_docs.html) | [split\_list\_of\_docs](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.reduce.split_list_of_docs.html)

LangGraph allows the graph structure to be plotted to help visualize its function:

```python
from IPython.display import Image

Image(app.get_graph().draw_mermaid_png())
```

![](data:image/jpg;base64,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)

As before, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.

Note that because we have a loop in the graph, it can be helpful to specify a [recursion\_limit](https://langchain-ai.github.io/langgraph/reference/errors/#langgraph.errors.GraphRecursionError) on its execution. This is analogous to [ReduceDocumentsChain.token\_max](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html#langchain.chains.combine_documents.reduce.ReduceDocumentsChain.token_max) to will raise a specific error when the specified limit is exceeded.

```python
async for step in app.astream(
    {"contents": [doc.page_content for doc in split_docs]},
    {"recursion_limit": 10},
):
    print(list(step.keys()))
```

```output
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['collect_summaries']
['collapse_summaries']
['generate_final_summary']
```

```python
print(step)
```

```output
{'generate_final_summary': {'final_summary': 'The summaries discuss the use of Large Language Models (LLMs) to power autonomous agents in various tasks such as problem-solving, planning, and tool use. Key components like planning, memory, and task decomposition are highlighted, along with challenges such as inefficient planning and hallucination. Techniques like Algorithm Distillation and Maximum Inner Product Search are explored for optimization, while frameworks like ReAct and Reflexion show improvements in knowledge-intensive tasks. The importance of accurate interpretation of user input and well-structured code for functional autonomy is emphasized, along with the potential of LLMs in prompting, reasoning, and emergent social behavior in simulation environments. Challenges in real-world scenarios and the use of LLMs with expert-designed tools for tasks like organic synthesis and drug discovery are also discussed.'}}
```

In the corresponding [LangSmith trace](https://smith.langchain.com/public/9d7b1d50-e1d6-44c9-9ab2-eabef621c883/r) we can see the same 17 LLM calls as before, this time grouped under their respective nodes.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

Check out the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) for detail on building with LangGraph, including [this guide](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) on the details of map-reduce in LangGraph.

See [this tutorial](/docs/tutorials/summarization/) for more LLM-based summarization strategies.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_chains/map_reduce_chain.ipynb)

* * *


- [Basic example (short documents)](#basic-example-short-documents)
  
  - [Legacy](#legacy)
  - [LangGraph](#langgraph)
- [Summarizing long documents](#summarizing-long-documents)
  
  - [Legacy](#legacy-1)
  - [LangGraph](#langgraph-1)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/MultiQueryRetriever.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/MultiQueryRetriever.ipynb)

# How to use the MultiQueryRetriever

Distance-based [vector database](/docs/concepts/vectorstores/) retrieval [embeds](/docs/concepts/embedding_models/) (represents) queries in high-dimensional space and finds similar embedded documents based on a distance metric. But, retrieval may produce different results with subtle changes in query wording, or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.

The [MultiQueryRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html) automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the `MultiQueryRetriever` can mitigate some of the limitations of the distance-based retrieval and get a richer set of results.

Let's build a vectorstore using the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng from the [RAG tutorial](/docs/tutorials/rag/):

```python
# Build a sample vectorDB
from langchain_chroma import Chroma
from langchain_community.document_loaders import WebBaseLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Load blog post
loader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")
data = loader.load()

# Split
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
splits = text_splitter.split_documents(data)

# VectorDB
embedding = OpenAIEmbeddings()
vectordb = Chroma.from_documents(documents=splits, embedding=embedding)
```

**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

```output
USER_AGENT environment variable not set, consider setting it to identify your requests.
```

#### Simple usage[â€‹](#simple-usage "Direct link to Simple usage")

Specify the LLM to use for query generation, and the retriever will do the rest.

```python
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI

question = "What are the approaches to Task Decomposition?"
llm = ChatOpenAI(temperature=0)
retriever_from_llm = MultiQueryRetriever.from_llm(
    retriever=vectordb.as_retriever(), llm=llm
)
```

**API Reference:**[MultiQueryRetriever](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```python
# Set logging for the queries
import logging

logging.basicConfig()
logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)
```

```python
unique_docs = retriever_from_llm.invoke(question)
len(unique_docs)
```

```output
INFO:langchain.retrievers.multi_query:Generated queries: ['1. How can Task Decomposition be achieved through different methods?', '2. What strategies are commonly used for Task Decomposition?', '3. What are the various ways to break down tasks in Task Decomposition?']
```

```output
5
```

Note that the underlying queries generated by the [retriever](/docs/concepts/retrievers/) are logged at the `INFO` level.

#### Supplying your own prompt[â€‹](#supplying-your-own-prompt "Direct link to Supplying your own prompt")

Under the hood, `MultiQueryRetriever` generates queries using a specific [prompt](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html). To customize this prompt:

1. Make a [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) with an input variable for the question;
2. Implement an [output parser](/docs/concepts/output_parsers/) like the one below to split the result into a list of queries.

The prompt and output parser together must support the generation of a list of queries.

```python
from typing import List

from langchain_core.output_parsers import BaseOutputParser
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field


# Output parser will split the LLM result into a list of queries
class LineListOutputParser(BaseOutputParser[List[str]]):
    """Output parser for a list of lines."""

    def parse(self, text: str) -> List[str]:
        lines = text.strip().split("\n")
        return list(filter(None, lines))  # Remove empty lines


output_parser = LineListOutputParser()

QUERY_PROMPT = PromptTemplate(
    input_variables=["question"],
    template="""You are an AI language model assistant. Your task is to generate five 
    different versions of the given user question to retrieve relevant documents from a vector 
    database. By generating multiple perspectives on the user question, your goal is to help
    the user overcome some of the limitations of the distance-based similarity search. 
    Provide these alternative questions separated by newlines.
    Original question: {question}""",
)
llm = ChatOpenAI(temperature=0)

# Chain
llm_chain = QUERY_PROMPT | llm | output_parser

# Other inputs
question = "What are the approaches to Task Decomposition?"
```

**API Reference:**[BaseOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

```python
# Run
retriever = MultiQueryRetriever(
    retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key="lines"
)  # "lines" is the key (attribute name) of the parsed output

# Results
unique_docs = retriever.invoke("What does the course say about regression?")
len(unique_docs)
```

```output
INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide insights on regression from the course material?', '2. How is regression discussed in the course content?', '3. What information does the course offer regarding regression?', '4. In what way is regression covered in the course?', "5. What are the course's teachings on regression?"]
```

```output
9
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/MultiQueryRetriever.ipynb)

* * *









- [GitHub](https://github.com/langchain-ai/langchain "GitHub")
- [X / Twitter](https://twitter.com/langchainai "X / Twitter")

`Ctrl`+`K`

- [Reference](#)

[Docs](https://python.langchain.com/)

- [GitHub](https://github.com/langchain-ai/langchain "GitHub")
- [X / Twitter](https://twitter.com/langchainai "X / Twitter")

Section Navigation

Base packages

- [Core](core/index.html)
- [Langchain](langchain/index.html)
- [Text Splitters](text_splitters/index.html)
- [Community](community/index.html)
- [Experimental](experimental/index.html)

Integrations

- [AI21](ai21/index.html)
- [Anthropic](anthropic/index.html)
- [AstraDB](astradb/index.html)
- [AWS](aws/index.html)
- [Azure Ai](azure_ai/index.html)
- [Azure Dynamic Sessions](azure_dynamic_sessions/index.html)
- [Cerebras](cerebras/index.html)
- [Chroma](chroma/index.html)
- [Cohere](cohere/index.html)
- [Deepseek](deepseek/index.html)
- [Elasticsearch](elasticsearch/index.html)
- [Exa](exa/index.html)
- [Fireworks](fireworks/index.html)
- [Google Community](google_community/index.html)
- [Google GenAI](google_genai/index.html)
- [Google VertexAI](google_vertexai/index.html)
- [Groq](groq/index.html)
- [Huggingface](huggingface/index.html)
- [IBM](ibm/index.html)
- [Milvus](milvus/index.html)
- [MistralAI](mistralai/index.html)
- [MongoDB](mongodb/index.html)
- [Neo4J](neo4j/index.html)
- [Nomic](nomic/index.html)
- [Nvidia Ai Endpoints](nvidia_ai_endpoints/index.html)
- [Ollama](ollama/index.html)
- [OpenAI](openai/index.html)
- [Perplexity](perplexity/index.html)
- [Pinecone](pinecone/index.html)
- [Postgres](postgres/index.html)
- [Prompty](prompty/index.html)
- [Qdrant](qdrant/index.html)
- [Redis](redis/index.html)
- [Sema4](sema4/index.html)
- [Snowflake](snowflake/index.html)
- [Sqlserver](sqlserver/index.html)
- [Standard Tests](standard_tests/index.html)
- [Together](together/index.html)
- [Unstructured](unstructured/index.html)
- [Upstage](upstage/index.html)
- [VoyageAI](voyageai/index.html)
- [Weaviate](weaviate/index.html)
- [XAI](xai/index.html)

<!--THE END-->

- LangChain Python API Reference

# LangChain Python API Reference[#](#langchain-python-api-reference "Link to this heading")

Welcome to the LangChain Python API reference. This is a reference for all `langchain-x` packages.

For user guides see [https://python.langchain.com](https://python.langchain.com).

For the legacy API reference hosted on ReadTheDocs see [https://api.python.langchain.com/](https://api.python.langchain.com/).

## Base packages[#](#base-packages "Link to this heading")

**Core**

langchain-core: 0.3.54

[core/index.html](core/index.html)

**Langchain**

langchain: 0.3.23

[langchain/index.html](langchain/index.html)

**Text Splitters**

langchain-text-splitters: 0.3.8

[text\_splitters/index.html](text_splitters/index.html)

**Community**

langchain-community: 0.3.21

[community/index.html](community/index.html)

**Experimental**

langchain-experimental: 0.3.5rc1

[experimental/index.html](experimental/index.html)

## Integrations[#](#integrations "Link to this heading")

**OpenAI**

langchain-openai 0.3.14

[openai/index.html](openai/index.html)

**Anthropic**

langchain-anthropic 0.3.12

[anthropic/index.html](anthropic/index.html)

**Google VertexAI**

langchain-google-vertexai 2.0.20

[google\_vertexai/index.html](google_vertexai/index.html)

**AWS**

langchain-aws 0.2.21

[aws/index.html](aws/index.html)

**Huggingface**

langchain-huggingface 0.1.2

[huggingface/index.html](huggingface/index.html)

**MistralAI**

langchain-mistralai 0.2.10

[mistralai/index.html](mistralai/index.html)

See the full list of integrations in the Section Navigation.


- [Base packages](#base-packages)
- [Integrations](#integrations)

Â© Copyright 2023, LangChain Inc.
[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/index.mdx)

# How-to guides

Here youâ€™ll find answers to â€œHow do Iâ€¦.?â€ types of questions. These guides are *goal-oriented* and *concrete*; they're meant to help you complete a specific task. For conceptual explanations see the [Conceptual guide](/docs/concepts/). For end-to-end walkthroughs see [Tutorials](/docs/tutorials/). For comprehensive descriptions of every class and function see the [API Reference](https://python.langchain.com/api_reference/).

## Installation[â€‹](#installation "Direct link to Installation")

- [How to: install LangChain packages](/docs/how_to/installation/)
- [How to: use LangChain with different Pydantic versions](/docs/how_to/pydantic_compatibility/)

## Key features[â€‹](#key-features "Direct link to Key features")

This highlights functionality that is core to using LangChain.

- [How to: return structured data from a model](/docs/how_to/structured_output/)
- [How to: use a model to call tools](/docs/how_to/tool_calling/)
- [How to: stream runnables](/docs/how_to/streaming/)
- [How to: debug your LLM apps](/docs/how_to/debugging/)

## Components[â€‹](#components "Direct link to Components")

These are the core building blocks you can use when building applications.

### Chat models[â€‹](#chat-models "Direct link to Chat models")

[Chat Models](/docs/concepts/chat_models/) are newer forms of language models that take messages in and output a message. See [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.

- [How to: do function/tool calling](/docs/how_to/tool_calling/)
- [How to: get models to return structured output](/docs/how_to/structured_output/)
- [How to: cache model responses](/docs/how_to/chat_model_caching/)
- [How to: get log probabilities](/docs/how_to/logprobs/)
- [How to: create a custom chat model class](/docs/how_to/custom_chat_model/)
- [How to: stream a response back](/docs/how_to/chat_streaming/)
- [How to: track token usage](/docs/how_to/chat_token_usage_tracking/)
- [How to: track response metadata across providers](/docs/how_to/response_metadata/)
- [How to: use chat model to call tools](/docs/how_to/tool_calling/)
- [How to: stream tool calls](/docs/how_to/tool_streaming/)
- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting/)
- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot/)
- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific/)
- [How to: force a specific tool call](/docs/how_to/tool_choice/)
- [How to: work with local models](/docs/how_to/local_llms/)
- [How to: init any model in one line](/docs/how_to/chat_models_universal_init/)
- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)

### Messages[â€‹](#messages "Direct link to Messages")

[Messages](/docs/concepts/messages/) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.

- [How to: trim messages](/docs/how_to/trim_messages/)
- [How to: filter messages](/docs/how_to/filter_messages/)
- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)

### Prompt templates[â€‹](#prompt-templates "Direct link to Prompt templates")

[Prompt Templates](/docs/concepts/prompt_templates/) are responsible for formatting user input into a format that can be passed to a language model.

- [How to: use few shot examples](/docs/how_to/few_shot_examples/)
- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)
- [How to: partially format prompt templates](/docs/how_to/prompts_partial/)
- [How to: compose prompts together](/docs/how_to/prompts_composition/)
- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)

### Example selectors[â€‹](#example-selectors "Direct link to Example selectors")

[Example Selectors](/docs/concepts/example_selectors/) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs/how_to/example_selectors/)
- [How to: select examples by length](/docs/how_to/example_selectors_length_based/)
- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity/)
- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram/)
- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr/)
- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)

### LLMs[â€‹](#llms "Direct link to LLMs")

What LangChain calls [LLMs](/docs/concepts/text_llms/) are older forms of language models that take a string in and output a string.

- [How to: cache model responses](/docs/how_to/llm_caching/)
- [How to: create a custom LLM class](/docs/how_to/custom_llm/)
- [How to: stream a response back](/docs/how_to/streaming_llm/)
- [How to: track token usage](/docs/how_to/llm_token_usage_tracking/)
- [How to: work with local models](/docs/how_to/local_llms/)

### Output parsers[â€‹](#output-parsers "Direct link to Output parsers")

[Output Parsers](/docs/concepts/output_parsers/) are responsible for taking the output of an LLM and parsing into more structured format.

- [How to: parse text from message objects](/docs/how_to/output_parser_string/)
- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured/)
- [How to: parse JSON output](/docs/how_to/output_parser_json/)
- [How to: parse XML output](/docs/how_to/output_parser_xml/)
- [How to: parse YAML output](/docs/how_to/output_parser_yaml/)
- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry/)
- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing/)
- [How to: write a custom output parser class](/docs/how_to/output_parser_custom/)

### Document loaders[â€‹](#document-loaders "Direct link to Document loaders")

[Document Loaders](/docs/concepts/document_loaders/) are responsible for loading documents from a variety of sources.

- [How to: load PDF files](/docs/how_to/document_loader_pdf/)
- [How to: load web pages](/docs/how_to/document_loader_web/)
- [How to: load CSV data](/docs/how_to/document_loader_csv/)
- [How to: load data from a directory](/docs/how_to/document_loader_directory/)
- [How to: load HTML data](/docs/how_to/document_loader_html/)
- [How to: load JSON data](/docs/how_to/document_loader_json/)
- [How to: load Markdown data](/docs/how_to/document_loader_markdown/)
- [How to: load Microsoft Office data](/docs/how_to/document_loader_office_file/)
- [How to: write a custom document loader](/docs/how_to/document_loader_custom/)

### Text splitters[â€‹](#text-splitters "Direct link to Text splitters")

[Text Splitters](/docs/concepts/text_splitters/) take a document and split into chunks that can be used for retrieval.

- [How to: recursively split text](/docs/how_to/recursive_text_splitter/)
- [How to: split HTML](/docs/how_to/split_html/)
- [How to: split by character](/docs/how_to/character_text_splitter/)
- [How to: split code](/docs/how_to/code_splitter/)
- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter/)
- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter/)
- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker/)
- [How to: split by tokens](/docs/how_to/split_by_token/)

### Embedding models[â€‹](#embedding-models "Direct link to Embedding models")

[Embedding Models](/docs/concepts/embedding_models/) take a piece of text and create a numerical representation of it. See [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.

- [How to: embed text data](/docs/how_to/embed_text/)
- [How to: cache embedding results](/docs/how_to/caching_embeddings/)
- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings/)

### Vector stores[â€‹](#vector-stores "Direct link to Vector stores")

[Vector stores](/docs/concepts/vectorstores/) are databases that can efficiently store and retrieve embeddings. See [supported integrations](/docs/integrations/vectorstores/) for details on getting started with vector stores from a specific provider.

- [How to: use a vector store to retrieve data](/docs/how_to/vectorstores/)

### Retrievers[â€‹](#retrievers "Direct link to Retrievers")

[Retrievers](/docs/concepts/retrievers/) are responsible for taking a query and returning relevant documents.

- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever/)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever/)
- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression/)
- [How to: write a custom retriever class](/docs/how_to/custom_retriever/)
- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever/)
- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever/)
- [How to: reorder retrieved results to mitigate the "lost in the middle" effect](/docs/how_to/long_context_reorder/)
- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector/)
- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever/)
- [How to: generate metadata filters](/docs/how_to/self_query/)
- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore/)
- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid/)

### Indexing[â€‹](#indexing "Direct link to Indexing")

Indexing is the process of keeping your vectorstore in-sync with the underlying data source.

- [How to: reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing/)

### Tools[â€‹](#tools "Direct link to Tools")

LangChain [Tools](/docs/concepts/tools/) contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer [here](/docs/integrations/tools/) for a list of pre-buit tools.

- [How to: create tools](/docs/how_to/custom_tools/)
- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin/)
- [How to: use chat models to call tools](/docs/how_to/tool_calling/)
- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model/)
- [How to: pass run time values to tools](/docs/how_to/tool_runtime/)
- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human/)
- [How to: handle tool errors](/docs/how_to/tools_error/)
- [How to: force models to call a tool](/docs/how_to/tool_choice/)
- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel/)
- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure/)
- [How to: stream events from a tool](/docs/how_to/tool_stream_events/)
- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool/)
- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting/)
- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets/)

### Multimodal[â€‹](#multimodal "Direct link to Multimodal")

- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)
- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)

### Agents[â€‹](#agents "Direct link to Agents")

note

For in depth how-to guides for agents, please check out [LangGraph](https://langchain-ai.github.io/langgraph/) documentation.

- [How to: use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor/)
- [How to: migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent/)

### Callbacks[â€‹](#callbacks "Direct link to Callbacks")

[Callbacks](/docs/concepts/callbacks/) allow you to hook into the various stages of your LLM application's execution.

- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_runtime/)
- [How to: attach callbacks to a module](/docs/how_to/callbacks_attach/)
- [How to: pass callbacks into a module constructor](/docs/how_to/callbacks_constructor/)
- [How to: create custom callback handlers](/docs/how_to/custom_callbacks/)
- [How to: use callbacks in async environments](/docs/how_to/callbacks_async/)
- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events/)

### Custom[â€‹](#custom "Direct link to Custom")

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat_model/)
- [How to: create a custom LLM class](/docs/how_to/custom_llm/)
- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings/)
- [How to: write a custom retriever class](/docs/how_to/custom_retriever/)
- [How to: write a custom document loader](/docs/how_to/document_loader_custom/)
- [How to: write a custom output parser class](/docs/how_to/output_parser_custom/)
- [How to: create custom callback handlers](/docs/how_to/custom_callbacks/)
- [How to: define a custom tool](/docs/how_to/custom_tools/)
- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events/)

### Serialization[â€‹](#serialization "Direct link to Serialization")

- [How to: save and load LangChain objects](/docs/how_to/serialization/)

## Use cases[â€‹](#use-cases "Direct link to Use cases")

These guides cover use-case specific details.

### Q&amp;A with RAG[â€‹](#qa-with-rag "Direct link to Q&A with RAG")

Retrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data. For a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).

- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)
- [How to: stream](/docs/how_to/qa_streaming/)
- [How to: return sources](/docs/how_to/qa_sources/)
- [How to: return citations](/docs/how_to/qa_citations/)
- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)

### Extraction[â€‹](#extraction "Direct link to Extraction")

Extraction is when you use LLMs to extract structured information from unstructured text. For a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).

- [How to: use reference examples](/docs/how_to/extraction_examples/)
- [How to: handle long text](/docs/how_to/extraction_long_text/)
- [How to: do extraction without using function calling](/docs/how_to/extraction_parse/)

### Chatbots[â€‹](#chatbots "Direct link to Chatbots")

Chatbots involve using an LLM to have a conversation. For a high-level tutorial on building chatbots, check out [this guide](/docs/tutorials/chatbot/).

- [How to: manage memory](/docs/how_to/chatbots_memory/)
- [How to: do retrieval](/docs/how_to/chatbots_retrieval/)
- [How to: use tools](/docs/how_to/chatbots_tools/)
- [How to: manage large chat history](/docs/how_to/trim_messages/)

### Query analysis[â€‹](#query-analysis "Direct link to Query analysis")

Query Analysis is the task of using an LLM to generate a query to send to a retriever. For a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).

- [How to: add examples to the prompt](/docs/how_to/query_few_shot/)
- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries/)
- [How to: handle multiple queries](/docs/how_to/query_multiple_queries/)
- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers/)
- [How to: construct filters](/docs/how_to/query_constructing_filters/)
- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality/)

### Q&amp;A over SQL + CSV[â€‹](#qa-over-sql--csv "Direct link to Q&A over SQL + CSV")

You can use LLMs to do question answering over tabular data. For a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).

- [How to: use prompting to improve results](/docs/how_to/sql_prompting/)
- [How to: do query validation](/docs/how_to/sql_query_checking/)
- [How to: deal with large databases](/docs/how_to/sql_large_db/)
- [How to: deal with CSV files](/docs/how_to/sql_csv/)

### Q&amp;A over graph databases[â€‹](#qa-over-graph-databases "Direct link to Q&A over graph databases")

You can use an LLM to do question answering over graph databases. For a high-level tutorial, check out [this guide](/docs/tutorials/graph/).

- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic/)
- [How to: construct knowledge graphs](/docs/how_to/graph_constructing/)

### Summarization[â€‹](#summarization "Direct link to Summarization")

LLMs can summarize and otherwise distill desired information from text, including large volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization/).

- [How to: summarize text in a single LLM call](/docs/how_to/summarize_stuff/)
- [How to: summarize text through parallelization](/docs/how_to/summarize_map_reduce/)
- [How to: summarize text through iterative refinement](/docs/how_to/summarize_refine/)

## LangChain Expression Language (LCEL)[â€‹](#langchain-expression-language-lcel "Direct link to LangChain Expression Language (LCEL)")

Should I use LCEL?

LCEL is an orchestration solution. See our [concepts page](/docs/concepts/lcel/#should-i-use-lcel) for recommendations on when to use LCEL.

[LangChain Expression Language](/docs/concepts/lcel/) is a way to create arbitrary custom chains. It is built on the [Runnable](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html) protocol.

[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.

[**Migration guide**](/docs/versions/migrating_chains/): For migrating legacy chain abstractions to LCEL.

- [How to: chain runnables](/docs/how_to/sequence/)
- [How to: stream runnables](/docs/how_to/streaming/)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocation args to runnables](/docs/how_to/binding/)
- [How to: turn any function into a runnable](/docs/how_to/functions/)
- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough/)
- [How to: configure runnable behavior at runtime](/docs/how_to/configure/)
- [How to: add message history (memory) to a chain](/docs/how_to/message_history/)
- [How to: route between sub-chains](/docs/how_to/routing/)
- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)
- [How to: inspect runnables](/docs/how_to/inspect/)
- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks/)
- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets/)

## [LangGraph](https://langchain-ai.github.io/langgraph)[â€‹](#langgraph "Direct link to langgraph")

LangGraph is an extension of LangChain aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.

LangGraph documentation is currently hosted on a separate site. You can peruse [LangGraph how-to guides here](https://langchain-ai.github.io/langgraph/how-tos/).

## [LangSmith](https://docs.smith.langchain.com/)[â€‹](#langsmith "Direct link to langsmith")

LangSmith allows you to closely trace, monitor and evaluate your LLM application. It seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.

LangSmith documentation is hosted on a separate site. You can peruse [LangSmith how-to guides here](https://docs.smith.langchain.com/), but we'll highlight a few sections that are particularly relevant to LangChain below:

### Evaluation[â€‹](#evaluation "Direct link to Evaluation")

Evaluating performance is a vital part of building LLM-powered applications. LangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.

To learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).

### Tracing[â€‹](#tracing "Direct link to Tracing")

Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.

- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)
- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)

You can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/index.mdx)

* * *


- [Installation](#installation)
- [Key features](#key-features)
- [Components](#components)
  
  - [Chat models](#chat-models)
  - [Messages](#messages)
  - [Prompt templates](#prompt-templates)
  - [Example selectors](#example-selectors)
  - [LLMs](#llms)
  - [Output parsers](#output-parsers)
  - [Document loaders](#document-loaders)
  - [Text splitters](#text-splitters)
  - [Embedding models](#embedding-models)
  - [Vector stores](#vector-stores)
  - [Retrievers](#retrievers)
  - [Indexing](#indexing)
  - [Tools](#tools)
  - [Multimodal](#multimodal)
  - [Agents](#agents)
  - [Callbacks](#callbacks)
  - [Custom](#custom)
  - [Serialization](#serialization)
- [Use cases](#use-cases)
  
  - [Q&amp;A with RAG](#qa-with-rag)
  - [Extraction](#extraction)
  - [Chatbots](#chatbots)
  - [Query analysis](#query-analysis)
  - [Q&amp;A over SQL + CSV](#qa-over-sql--csv)
  - [Q&amp;A over graph databases](#qa-over-graph-databases)
  - [Summarization](#summarization)
- [LangChain Expression Language (LCEL)](#langchain-expression-language-lcel)
- [LangGraph](#langgraph)
- [LangSmith](#langsmith)
  
  - [Evaluation](#evaluation)
  - [Tracing](#tracing)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/passthrough.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/passthrough.ipynb)

# How to pass through arguments from one step to the next

Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Expression Language (LCEL)](/docs/concepts/lcel/)
- [Chaining runnables](/docs/how_to/sequence/)
- [Calling runnables in parallel](/docs/how_to/parallel/)
- [Custom functions](/docs/how_to/functions/)

When composing chains with several steps, sometimes you will want to pass data from previous steps unchanged for use as input to a later step. The [`RunnablePassthrough`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) class allows you to do just this, and is typically is used in conjunction with a [RunnableParallel](/docs/how_to/parallel/) to pass data through to a later step in your constructed chains.

See the example below:

```python
%pip install -qU langchain langchain-openai

import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

```python
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

runnable = RunnableParallel(
    passed=RunnablePassthrough(),
    modified=lambda x: x["num"] + 1,
)

runnable.invoke({"num": 1})
```

**API Reference:**[RunnableParallel](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableParallel.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)

```output
{'passed': {'num': 1}, 'modified': 2}
```

As seen above, `passed` key was called with `RunnablePassthrough()` and so it simply passed on `{'num': 1}`.

We also set a second key in the map with `modified`. This uses a lambda to set a single value adding 1 to the num, which resulted in `modified` key with the value of `2`.

## Retrieval Example[â€‹](#retrieval-example "Direct link to Retrieval Example")

In the example below, we see a more real-world use case where we use `RunnablePassthrough` along with `RunnableParallel` in a chain to properly format inputs to a prompt:

```python
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

vectorstore = FAISS.from_texts(
    ["harrison worked at kensho"], embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)
model = ChatOpenAI()

retrieval_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

retrieval_chain.invoke("where did harrison work?")
```

**API Reference:**[FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

```output
'Harrison worked at Kensho.'
```

Here the input to prompt is expected to be a map with keys "context" and "question". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the "question" key. The `RunnablePassthrough` allows us to pass on the user's question to the prompt and model.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

Now you've learned how to pass data through your chains to help format the data flowing through your chains.

To learn more, see the other how-to guides on runnables in this section.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/passthrough.ipynb)

* * *


- [Retrieval Example](#retrieval-example)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/tutorials/extraction.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/tutorials/extraction.ipynb)

# Build an Extraction Chain

In this tutorial, we will use [tool-calling](/docs/concepts/tool_calling/) features of [chat models](/docs/concepts/chat_models/) to extract structured information from unstructured text. We will also demonstrate how to use [few-shot prompting](/docs/concepts/few_shot_prompting/) in this context to improve performance.

important

This tutorial requires `langchain-core>=0.3.20` and will only work with models that support **tool calling**.

## Setup[â€‹](#setup "Direct link to Setup")

### Jupyter Notebook[â€‹](#jupyter-notebook "Direct link to Jupyter Notebook")

This and other tutorials are perhaps most conveniently run in a [Jupyter notebooks](https://jupyter.org/). Going through guides in an interactive environment is a great way to better understand them. See [here](https://jupyter.org/install) for instructions on how to install.

### Installation[â€‹](#installation "Direct link to Installation")

To install LangChain run:

- Pip
- Conda

```bash
pip install --upgrade langchain-core
```

```bash
conda install langchain-core -c conda-forge
```

For more details, see our [Installation guide](/docs/how_to/installation/).

### LangSmith[â€‹](#langsmith "Direct link to LangSmith")

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
```

Or, if in a notebook, you can set them with:

```python
import getpass
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

## The Schema[â€‹](#the-schema "Direct link to The Schema")

First, we need to describe what information we want to extract from the text.

We'll use Pydantic to define an example schema to extract personal information.

```python
from typing import Optional

from pydantic import BaseModel, Field


class Person(BaseModel):
    """Information about a person."""

    # ^ Doc-string for the entity Person.
    # This doc-string is sent to the LLM as the description of the schema Person,
    # and it can help to improve extraction results.

    # Note that:
    # 1. Each field is an `optional` -- this allows the model to decline to extract it!
    # 2. Each field has a `description` -- this description is used by the LLM.
    # Having a good description can help improve extraction results.
    name: Optional[str] = Field(default=None, description="The name of the person")
    hair_color: Optional[str] = Field(
        default=None, description="The color of the person's hair if known"
    )
    height_in_meters: Optional[str] = Field(
        default=None, description="Height measured in meters"
    )
```

There are two best practices when defining schema:

1. Document the **attributes** and the **schema** itself: This information is sent to the LLM and is used to improve the quality of information extraction.
2. Do not force the LLM to make up information! Above we used `Optional` for the attributes allowing the LLM to output `None` if it doesn't know the answer.

important

For best performance, document the schema well and make sure the model isn't force to return results if there's no information to be extracted in the text.

## The Extractor[â€‹](#the-extractor "Direct link to The Extractor")

Let's create an information extractor using the schema we defined above.

```python
from typing import Optional

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import BaseModel, Field

# Define a custom prompt to provide instructions and any additional context.
# 1) You can add examples into the prompt template to improve extraction quality
# 2) Introduce additional parameters to take context into account (e.g., include metadata
#    about the document from which the text was extracted.)
prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are an expert extraction algorithm. "
            "Only extract relevant information from the text. "
            "If you do not know the value of an attribute asked to extract, "
            "return null for the attribute's value.",
        ),
        # Please see the how-to about improving performance with
        # reference examples.
        # MessagesPlaceholder('examples'),
        ("human", "{text}"),
    ]
)
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)

We need to use a model that supports function/tool calling.

Please review [the documentation](/docs/concepts/tool_calling/) for all models that can be used with this API.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
structured_llm = llm.with_structured_output(schema=Person)
```

Let's test it out:

```python
text = "Alan Smith is 6 feet tall and has blond hair."
prompt = prompt_template.invoke({"text": text})
structured_llm.invoke(prompt)
```

```output
Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83')
```

important

Extraction is Generative ðŸ¤¯

LLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters even though it was provided in feet!

We can see the LangSmith trace [here](https://smith.langchain.com/public/44b69a63-3b3b-47b8-8a6d-61b46533f015/r). Note that the [chat model portion of the trace](https://smith.langchain.com/public/44b69a63-3b3b-47b8-8a6d-61b46533f015/r/dd1f6305-f1e9-4919-bd8f-339d03a12d01) reveals the exact sequence of messages sent to the model, tools invoked, and other metadata.

## Multiple Entities[â€‹](#multiple-entities "Direct link to Multiple Entities")

In **most cases**, you should be extracting a list of entities rather than a single entity.

This can be easily achieved using pydantic by nesting models inside one another.

```python
from typing import List, Optional

from pydantic import BaseModel, Field


class Person(BaseModel):
    """Information about a person."""

    # ^ Doc-string for the entity Person.
    # This doc-string is sent to the LLM as the description of the schema Person,
    # and it can help to improve extraction results.

    # Note that:
    # 1. Each field is an `optional` -- this allows the model to decline to extract it!
    # 2. Each field has a `description` -- this description is used by the LLM.
    # Having a good description can help improve extraction results.
    name: Optional[str] = Field(default=None, description="The name of the person")
    hair_color: Optional[str] = Field(
        default=None, description="The color of the person's hair if known"
    )
    height_in_meters: Optional[str] = Field(
        default=None, description="Height measured in meters"
    )


class Data(BaseModel):
    """Extracted data about people."""

    # Creates a model so that we can extract multiple entities.
    people: List[Person]
```

important

Extraction results might not be perfect here. Read on to see how to use **Reference Examples** to improve the quality of extraction, and check out our extraction [how-to](/docs/how_to/#extraction) guides for more detail.

```python
structured_llm = llm.with_structured_output(schema=Data)
text = "My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me."
prompt = prompt_template.invoke({"text": text})
structured_llm.invoke(prompt)
```

```output
Data(people=[Person(name='Jeff', hair_color='black', height_in_meters='1.83'), Person(name='Anna', hair_color='black', height_in_meters=None)])
```

tip

When the schema accommodates the extraction of **multiple entities**, it also allows the model to extract **no entities** if no relevant information is in the text by providing an empty list.

This is usually a **good** thing! It allows specifying **required** attributes on an entity without necessarily forcing the model to detect this entity.

We can see the LangSmith trace [here](https://smith.langchain.com/public/7173764d-5e76-45fe-8496-84460bd9cdef/r).

## Reference examples[â€‹](#reference-examples "Direct link to Reference examples")

The behavior of LLM applications can be steered using [few-shot prompting](/docs/concepts/few_shot_prompting/). For [chat models](/docs/concepts/chat_models/), this can take the form of a sequence of pairs of input and response messages demonstrating desired behaviors.

For example, we can convey the meaning of a symbol with alternating `user` and `assistant` [messages](/docs/concepts/messages/#role):

```python
messages = [
    {"role": "user", "content": "2 ðŸ¦œ 2"},
    {"role": "assistant", "content": "4"},
    {"role": "user", "content": "2 ðŸ¦œ 3"},
    {"role": "assistant", "content": "5"},
    {"role": "user", "content": "3 ðŸ¦œ 4"},
]

response = llm.invoke(messages)
print(response.content)
```

```output
7
```

[Structured output](/docs/concepts/structured_outputs/) often uses [tool calling](/docs/concepts/tool_calling/) under-the-hood. This typically involves the generation of [AI messages](/docs/concepts/messages/#aimessage) containing tool calls, as well as [tool messages](/docs/concepts/messages/#toolmessage) containing the results of tool calls. What should a sequence of messages look like in this case?

Different [chat model providers](/docs/integrations/chat/) impose different requirements for valid message sequences. Some will accept a (repeating) message sequence of the form:

- User message
- AI message with tool call
- Tool message with result

Others require a final AI message containing some sort of response.

LangChain includes a utility function [tool\_example\_to\_messages](https://python.langchain.com/api_reference/core/utils/langchain_core.utils.function_calling.tool_example_to_messages.html) that will generate a valid sequence for most model providers. It simplifies the generation of structured few-shot examples by just requiring Pydantic representations of the corresponding tool calls.

Let's try this out. We can convert pairs of input strings and desired Pydantic objects to a sequence of messages that can be provided to a chat model. Under the hood, LangChain will format the tool calls to each provider's required format.

Note: this version of `tool_example_to_messages` requires `langchain-core>=0.3.20`.

```python
from langchain_core.utils.function_calling import tool_example_to_messages

examples = [
    (
        "The ocean is vast and blue. It's more than 20,000 feet deep.",
        Data(people=[]),
    ),
    (
        "Fiona traveled far from France to Spain.",
        Data(people=[Person(name="Fiona", height_in_meters=None, hair_color=None)]),
    ),
]


messages = []

for txt, tool_call in examples:
    if tool_call.people:
        # This final message is optional for some providers
        ai_response = "Detected people."
    else:
        ai_response = "Detected no people."
    messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))
```

**API Reference:**[tool\_example\_to\_messages](https://python.langchain.com/api_reference/core/utils/langchain_core.utils.function_calling.tool_example_to_messages.html)

Inspecting the result, we see these two example pairs generated eight messages:

```python
for message in messages:
    message.pretty_print()
```

```output
================================[1m Human Message [0m=================================

The ocean is vast and blue. It's more than 20,000 feet deep.
==================================[1m Ai Message [0m==================================
Tool Calls:
  Data (d8f2e054-7fb9-417f-b28f-0447a775b2c3)
 Call ID: d8f2e054-7fb9-417f-b28f-0447a775b2c3
  Args:
    people: []
=================================[1m Tool Message [0m=================================

You have correctly called this tool.
==================================[1m Ai Message [0m==================================

Detected no people.
================================[1m Human Message [0m=================================

Fiona traveled far from France to Spain.
==================================[1m Ai Message [0m==================================
Tool Calls:
  Data (0178939e-a4b1-4d2a-a93e-b87f665cdfd6)
 Call ID: 0178939e-a4b1-4d2a-a93e-b87f665cdfd6
  Args:
    people: [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]
=================================[1m Tool Message [0m=================================

You have correctly called this tool.
==================================[1m Ai Message [0m==================================

Detected people.
```

Let's compare performance with and without these messages. For example, let's pass a message for which we intend no people to be extracted:

```python
message_no_extraction = {
    "role": "user",
    "content": "The solar system is large, but earth has only 1 moon.",
}

structured_llm = llm.with_structured_output(schema=Data)
structured_llm.invoke([message_no_extraction])
```

```output
Data(people=[Person(name='Earth', hair_color='None', height_in_meters='0.00')])
```

In this example, the model is liable to erroneously generate records of people.

Because our few-shot examples contain examples of "negatives", we encourage the model to behave correctly in this case:

```python
structured_llm.invoke(messages + [message_no_extraction])
```

```output
Data(people=[])
```

tip

The [LangSmith](https://smith.langchain.com/public/b3433f57-7905-4430-923c-fed214525bf1/r) trace for the run reveals the exact sequence of messages sent to the chat model, tool calls generated, latency, token counts, and other metadata.

See [this guide](/docs/how_to/extraction_examples/) for more detail on extraction workflows with reference examples, including how to incorporate prompt templates and customize the generation of example messages.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

Now that you understand the basics of extraction with LangChain, you're ready to proceed to the rest of the how-to guides:

- [Add Examples](/docs/how_to/extraction_examples/): More detail on using **reference examples** to improve performance.
- [Handle Long Text](/docs/how_to/extraction_long_text/): What should you do if the text does not fit into the context window of the LLM?
- [Use a Parsing Approach](/docs/how_to/extraction_parse/): Use a prompt based approach to extract with models that do not support **tool/function calling**.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/tutorials/extraction.ipynb)

* * *


- [Setup](#setup)
  
  - [Jupyter Notebook](#jupyter-notebook)
  - [Installation](#installation)
  - [LangSmith](#langsmith)
- [The Schema](#the-schema)
- [The Extractor](#the-extractor)
- [Multiple Entities](#multiple-entities)
- [Reference examples](#reference-examples)
- [Next steps](#next-steps)








- [LangSmith](https://docs.smith.langchain.com)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [LangChain Hub](https://smith.langchain.com/hub)
- [LangChain JS/TS](https://js.langchain.com)

[v0.3](#)

- [v0.3](/docs/introduction/)
- [v0.2](https://python.langchain.com/v0.2/docs/introduction)
- [v0.1](https://python.langchain.com/v0.1/docs/get_started/introduction)

[ðŸ’¬](https://chat.langchain.com)

Search

- [Providers](/docs/integrations/providers/)
  
  - [Anthropic](/docs/integrations/providers/anthropic/)
  - [AWS](/docs/integrations/providers/aws/)
  - [Google](/docs/integrations/providers/google/)
  - [Hugging Face](/docs/integrations/providers/huggingface/)
  - [Microsoft](/docs/integrations/providers/microsoft/)
  - [OpenAI](/docs/integrations/providers/openai/)
  - [More](/docs/integrations/providers/all/)
    
    - [Providers](/docs/integrations/providers/)
    - [Abso](/docs/integrations/providers/abso/)
    - [Acreom](/docs/integrations/providers/acreom/)
    - [Activeloop Deep Lake](/docs/integrations/providers/activeloop_deeplake/)
    - [ADS4GPTs](/docs/integrations/providers/ads4gpts/)
    - [Aerospike](/docs/integrations/providers/aerospike/)
    - [AgentQL](/docs/integrations/providers/agentql/)
    - [AI21 Labs](/docs/integrations/providers/ai21/)
    - [Aim](/docs/integrations/providers/aim_tracking/)
    - [AINetwork](/docs/integrations/providers/ainetwork/)
    - [Airbyte](/docs/integrations/providers/airbyte/)
    - [Airtable](/docs/integrations/providers/airtable/)
    - [Alchemy](/docs/integrations/providers/alchemy/)
    - [Aleph Alpha](/docs/integrations/providers/aleph_alpha/)
    - [Alibaba Cloud](/docs/integrations/providers/alibaba_cloud/)
    - [AnalyticDB](/docs/integrations/providers/analyticdb/)
    - [Annoy](/docs/integrations/providers/annoy/)
    - [Anthropic](/docs/integrations/providers/anthropic/)
    - [Anyscale](/docs/integrations/providers/anyscale/)
    - [Apache Software Foundation](/docs/integrations/providers/apache/)
    - [Apache Doris](/docs/integrations/providers/apache_doris/)
    - [Apify](/docs/integrations/providers/apify/)
    - [Apple](/docs/integrations/providers/apple/)
    - [ArangoDB](/docs/integrations/providers/arangodb/)
    - [Arcee](/docs/integrations/providers/arcee/)
    - [ArcGIS](/docs/integrations/providers/arcgis/)
    - [Argilla](/docs/integrations/providers/argilla/)
    - [Arize](/docs/integrations/providers/arize/)
    - [Arthur](/docs/integrations/providers/arthur_tracking/)
    - [Arxiv](/docs/integrations/providers/arxiv/)
    - [Ascend](/docs/integrations/providers/ascend/)
    - [AskNews](/docs/integrations/providers/asknews/)
    - [AssemblyAI](/docs/integrations/providers/assemblyai/)
    - [Astra DB](/docs/integrations/providers/astradb/)
    - [Atlas](/docs/integrations/providers/atlas/)
    - [AwaDB](/docs/integrations/providers/awadb/)
    - [AWS](/docs/integrations/providers/aws/)
    - [AZLyrics](/docs/integrations/providers/azlyrics/)
    - [Azure AI](/docs/integrations/providers/azure_ai/)
    - [BAAI](/docs/integrations/providers/baai/)
    - [Bagel](/docs/integrations/providers/bagel/)
    - [BagelDB](/docs/integrations/providers/bageldb/)
    - [Baichuan](/docs/integrations/providers/baichuan/)
    - [Baidu](/docs/integrations/providers/baidu/)
    - [Banana](/docs/integrations/providers/bananadev/)
    - [Baseten](/docs/integrations/providers/baseten/)
    - [Beam](/docs/integrations/providers/beam/)
    - [Beautiful Soup](/docs/integrations/providers/beautiful_soup/)
    - [BibTeX](/docs/integrations/providers/bibtex/)
    - [BiliBili](/docs/integrations/providers/bilibili/)
    - [Bittensor](/docs/integrations/providers/bittensor/)
    - [Blackboard](/docs/integrations/providers/blackboard/)
    - [bookend.ai](/docs/integrations/providers/bookendai/)
    - [Box](/docs/integrations/providers/box/)
    - [Brave Search](/docs/integrations/providers/brave_search/)
    - [Breebs (Open Knowledge)](/docs/integrations/providers/breebs/)
    - [Browserbase](/docs/integrations/providers/browserbase/)
    - [Browserless](/docs/integrations/providers/browserless/)
    - [ByteDance](/docs/integrations/providers/byte_dance/)
    - [Cassandra](/docs/integrations/providers/cassandra/)
    - [Cerebras](/docs/integrations/providers/cerebras/)
    - [CerebriumAI](/docs/integrations/providers/cerebriumai/)
    - [Chaindesk](/docs/integrations/providers/chaindesk/)
    - [Chroma](/docs/integrations/providers/chroma/)
    - [Clarifai](/docs/integrations/providers/clarifai/)
    - [ClearML](/docs/integrations/providers/clearml_tracking/)
    - [ClickHouse](/docs/integrations/providers/clickhouse/)
    - [ClickUp](/docs/integrations/providers/clickup/)
    - [Cloudflare](/docs/integrations/providers/cloudflare/)
    - [Clova](/docs/integrations/providers/clova/)
    - [CnosDB](/docs/integrations/providers/cnosdb/)
    - [Cognee](/docs/integrations/providers/cognee/)
    - [CogniSwitch](/docs/integrations/providers/cogniswitch/)
    - [Cohere](/docs/integrations/providers/cohere/)
    - [College Confidential](/docs/integrations/providers/college_confidential/)
    - [Comet](/docs/integrations/providers/comet_tracking/)
    - [Confident AI](/docs/integrations/providers/confident/)
    - [Confluence](/docs/integrations/providers/confluence/)
    - [Connery](/docs/integrations/providers/connery/)
    - [Context](/docs/integrations/providers/context/)
    - [Contextual AI](/docs/integrations/providers/contextual/)
    - [Couchbase](/docs/integrations/providers/couchbase/)
    - [Coze](/docs/integrations/providers/coze/)
    - [CrateDB](/docs/integrations/providers/cratedb/)
    - [C Transformers](/docs/integrations/providers/ctransformers/)
    - [CTranslate2](/docs/integrations/providers/ctranslate2/)
    - [Cube](/docs/integrations/providers/cube/)
    - [Dappier](/docs/integrations/providers/dappier/)
    - [DashVector](/docs/integrations/providers/dashvector/)
    - [Databricks](/docs/integrations/providers/databricks/)
    - [Datadog Tracing](/docs/integrations/providers/datadog/)
    - [Datadog Logs](/docs/integrations/providers/datadog_logs/)
    - [DataForSEO](/docs/integrations/providers/dataforseo/)
    - [Dataherald](/docs/integrations/providers/dataherald/)
    - [Dedoc](/docs/integrations/providers/dedoc/)
    - [DeepInfra](/docs/integrations/providers/deepinfra/)
    - [Deeplake](/docs/integrations/providers/deeplake/)
    - [DeepSeek](/docs/integrations/providers/deepseek/)
    - [DeepSparse](/docs/integrations/providers/deepsparse/)
    - [Dell](/docs/integrations/providers/dell/)
    - [Diffbot](/docs/integrations/providers/diffbot/)
    - [DingoDB](/docs/integrations/providers/dingo/)
    - [Discord](/docs/integrations/providers/discord-shikenso/)
    - [Discord (community loader)](/docs/integrations/providers/discord/)
    - [DocArray](/docs/integrations/providers/docarray/)
    - [Docling](/docs/integrations/providers/docling/)
    - [Doctran](/docs/integrations/providers/doctran/)
    - [Docugami](/docs/integrations/providers/docugami/)
    - [Docusaurus](/docs/integrations/providers/docusaurus/)
    - [Dria](/docs/integrations/providers/dria/)
    - [Dropbox](/docs/integrations/providers/dropbox/)
    - [DSPy](/docs/integrations/providers/dspy/)
    - [DuckDB](/docs/integrations/providers/duckdb/)
    - [DuckDuckGo Search](/docs/integrations/providers/duckduckgo_search/)
    - [E2B](/docs/integrations/providers/e2b/)
    - [Eden AI](/docs/integrations/providers/edenai/)
    - [Elasticsearch](/docs/integrations/providers/elasticsearch/)
    - [ElevenLabs](/docs/integrations/providers/elevenlabs/)
    - [Embedchain](/docs/integrations/providers/embedchain/)
    - [Epsilla](/docs/integrations/providers/epsilla/)
    - [Etherscan](/docs/integrations/providers/etherscan/)
    - [Everly AI](/docs/integrations/providers/everlyai/)
    - [EverNote](/docs/integrations/providers/evernote/)
    - [Exa](/docs/integrations/providers/exa_search/)
    - [Facebook - Meta](/docs/integrations/providers/facebook/)
    - [FalkorDB](/docs/integrations/providers/falkordb/)
    - [Fauna](/docs/integrations/providers/fauna/)
    - [Fiddler](/docs/integrations/providers/fiddler/)
    - [Figma](/docs/integrations/providers/figma/)
    - [FireCrawl](/docs/integrations/providers/firecrawl/)
    - [Fireworks AI](/docs/integrations/providers/fireworks/)
    - [Flyte](/docs/integrations/providers/flyte/)
    - [FMP Data (Financial Data Prep)](/docs/integrations/providers/fmp-data/)
    - [Forefront AI](/docs/integrations/providers/forefrontai/)
    - [Friendli AI](/docs/integrations/providers/friendli/)
    - [Smabbler](/docs/integrations/providers/galaxia/)
    - [Geopandas](/docs/integrations/providers/geopandas/)
    - [Git](/docs/integrations/providers/git/)
    - [GitBook](/docs/integrations/providers/gitbook/)
    - [GitHub](/docs/integrations/providers/github/)
    - [GitLab](/docs/integrations/providers/gitlab/)
    - [GOAT](/docs/integrations/providers/goat/)
    - [Golden](/docs/integrations/providers/golden/)
    - [Goodfire](/docs/integrations/providers/goodfire/)
    - [Google](/docs/integrations/providers/google/)
    - [Serper - Google Search API](/docs/integrations/providers/google_serper/)
    - [GooseAI](/docs/integrations/providers/gooseai/)
    - [GPT4All](/docs/integrations/providers/gpt4all/)
    - [Gradient](/docs/integrations/providers/gradient/)
    - [Graph RAG](/docs/integrations/providers/graph_rag/)
    - [Graphsignal](/docs/integrations/providers/graphsignal/)
    - [Grobid](/docs/integrations/providers/grobid/)
    - [Groq](/docs/integrations/providers/groq/)
    - [Gutenberg](/docs/integrations/providers/gutenberg/)
    - [Hacker News](/docs/integrations/providers/hacker_news/)
    - [Hazy Research](/docs/integrations/providers/hazy_research/)
    - [Helicone](/docs/integrations/providers/helicone/)
    - [Hologres](/docs/integrations/providers/hologres/)
    - [HTML to text](/docs/integrations/providers/html2text/)
    - [Huawei](/docs/integrations/providers/huawei/)
    - [Hugging Face](/docs/integrations/providers/huggingface/)
    - [Hyperbrowser](/docs/integrations/providers/hyperbrowser/)
    - [IBM](/docs/integrations/providers/ibm/)
    - [IEIT Systems](/docs/integrations/providers/ieit_systems/)
    - [iFixit](/docs/integrations/providers/ifixit/)
    - [iFlytek](/docs/integrations/providers/iflytek/)
    - [IMSDb](/docs/integrations/providers/imsdb/)
    - [Infinispan VS](/docs/integrations/providers/infinispanvs/)
    - [Infinity](/docs/integrations/providers/infinity/)
    - [Infino](/docs/integrations/providers/infino/)
    - [Intel](/docs/integrations/providers/intel/)
    - [Iugu](/docs/integrations/providers/iugu/)
    - [Jaguar](/docs/integrations/providers/jaguar/)
    - [Javelin AI Gateway](/docs/integrations/providers/javelin_ai_gateway/)
    - [Jenkins](/docs/integrations/providers/jenkins/)
    - [Jina AI](/docs/integrations/providers/jina/)
    - [Johnsnowlabs](/docs/integrations/providers/johnsnowlabs/)
    - [Joplin](/docs/integrations/providers/joplin/)
    - [KDB.AI](/docs/integrations/providers/kdbai/)
    - [Kinetica](/docs/integrations/providers/kinetica/)
    - [KoboldAI](/docs/integrations/providers/koboldai/)
    - [Konko](/docs/integrations/providers/konko/)
    - [KoNLPY](/docs/integrations/providers/konlpy/)
    - [KÃ¹zu](/docs/integrations/providers/kuzu/)
    - [Label Studio](/docs/integrations/providers/labelstudio/)
    - [lakeFS](/docs/integrations/providers/lakefs/)
    - [LanceDB](/docs/integrations/providers/lancedb/)
    - [LangChain Decorators âœ¨](/docs/integrations/providers/langchain_decorators/)
    - [LangFair: Use-Case Level LLM Bias and Fairness Assessments](/docs/integrations/providers/langfair/)
    - [Langfuse ðŸª¢](/docs/integrations/providers/langfuse/)
    - [Lantern](/docs/integrations/providers/lantern/)
    - [Lindorm](/docs/integrations/providers/lindorm/)
    - [Linkup](/docs/integrations/providers/linkup/)
    - [LiteLLM](/docs/integrations/providers/litellm/)
    - [LlamaIndex](/docs/integrations/providers/llama_index/)
    - [Llama.cpp](/docs/integrations/providers/llamacpp/)
    - [LlamaEdge](/docs/integrations/providers/llamaedge/)
    - [llamafile](/docs/integrations/providers/llamafile/)
    - [LLMonitor](/docs/integrations/providers/llmonitor/)
    - [LocalAI](/docs/integrations/providers/localai/)
    - [Log10](/docs/integrations/providers/log10/)
    - [MariaDB](/docs/integrations/providers/mariadb/)
    - [MariTalk](/docs/integrations/providers/maritalk/)
    - [Marqo](/docs/integrations/providers/marqo/)
    - [MediaWikiDump](/docs/integrations/providers/mediawikidump/)
    - [Meilisearch](/docs/integrations/providers/meilisearch/)
    - [Memcached](/docs/integrations/providers/memcached/)
    - [Memgraph](/docs/integrations/providers/memgraph/)
    - [Metal](/docs/integrations/providers/metal/)
    - [Microsoft](/docs/integrations/providers/microsoft/)
    - [Milvus](/docs/integrations/providers/milvus/)
    - [MindsDB](/docs/integrations/providers/mindsdb/)
    - [Minimax](/docs/integrations/providers/minimax/)
    - [MistralAI](/docs/integrations/providers/mistralai/)
    - [MLflow AI Gateway for LLMs](/docs/integrations/providers/mlflow/)
    - [MLflow](/docs/integrations/providers/mlflow_tracking/)
    - [MLX](/docs/integrations/providers/mlx/)
    - [Modal](/docs/integrations/providers/modal/)
    - [ModelScope](/docs/integrations/providers/modelscope/)
    - [Modern Treasury](/docs/integrations/providers/modern_treasury/)
    - [Momento](/docs/integrations/providers/momento/)
    - [MongoDB](/docs/integrations/providers/mongodb/)
    - [MongoDB Atlas](/docs/integrations/providers/mongodb_atlas/)
    - [Motherduck](/docs/integrations/providers/motherduck/)
    - [MotÃ¶rhead](/docs/integrations/providers/motorhead/)
    - [MyScale](/docs/integrations/providers/myscale/)
    - [NAVER](/docs/integrations/providers/naver/)
    - [Neo4j](/docs/integrations/providers/neo4j/)
    - [Netmind](/docs/integrations/providers/netmind/)
    - [Nimble](/docs/integrations/providers/nimble/)
    - [NLPCloud](/docs/integrations/providers/nlpcloud/)
    - [Nomic](/docs/integrations/providers/nomic/)
    - [Notion DB](/docs/integrations/providers/notion/)
    - [Nuclia](/docs/integrations/providers/nuclia/)
    - [NVIDIA](/docs/integrations/providers/nvidia/)
    - [Obsidian](/docs/integrations/providers/obsidian/)
    - [OceanBase](/docs/integrations/providers/oceanbase/)
    - [Oracle Cloud Infrastructure (OCI)](/docs/integrations/providers/oci/)
    - [OctoAI](/docs/integrations/providers/octoai/)
    - [Ollama](/docs/integrations/providers/ollama/)
    - [Ontotext GraphDB](/docs/integrations/providers/ontotext_graphdb/)
    - [OpenAI](/docs/integrations/providers/openai/)
    - [OpenGradient](/docs/integrations/providers/opengradient/)
    - [OpenLLM](/docs/integrations/providers/openllm/)
    - [OpenSearch](/docs/integrations/providers/opensearch/)
    - [OpenWeatherMap](/docs/integrations/providers/openweathermap/)
    - [OracleAI Vector Search](/docs/integrations/providers/oracleai/)
    - [Outline](/docs/integrations/providers/outline/)
    - [Outlines](/docs/integrations/providers/outlines/)
    - [Oxylabs](/docs/integrations/providers/oxylabs/)
    - [Pandas](/docs/integrations/providers/pandas/)
    - [PaymanAI](/docs/integrations/providers/payman-tool/)
    - [Pebblo](/docs/integrations/providers/pebblo/)
    - [Permit](/docs/integrations/providers/permit/)
    - [Perplexity](/docs/integrations/providers/perplexity/)
    - [Petals](/docs/integrations/providers/petals/)
    - [Postgres Embedding](/docs/integrations/providers/pg_embedding/)
    - [PGVector](/docs/integrations/providers/pgvector/)
    - [Pinecone](/docs/integrations/providers/pinecone/)
    - [PipelineAI](/docs/integrations/providers/pipelineai/)
    - [Pipeshift](/docs/integrations/providers/pipeshift/)
    - [Portkey](/docs/integrations/providers/portkey/)
    - [Predibase](/docs/integrations/providers/predibase/)
    - [Prediction Guard](/docs/integrations/providers/predictionguard/)
    - [PremAI](/docs/integrations/providers/premai/)
    - [SWI-Prolog](/docs/integrations/providers/prolog/)
    - [PromptLayer](/docs/integrations/providers/promptlayer/)
    - [Psychic](/docs/integrations/providers/psychic/)
    - [PubMed](/docs/integrations/providers/pubmed/)
    - [PullMd Loader](/docs/integrations/providers/pull-md/)
    - [PygmalionAI](/docs/integrations/providers/pygmalionai/)
    - [PyMuPDF4LLM](/docs/integrations/providers/pymupdf4llm/)
    - [Qdrant](/docs/integrations/providers/qdrant/)
    - [RAGatouille](/docs/integrations/providers/ragatouille/)
    - [rank\_bm25](/docs/integrations/providers/rank_bm25/)
    - [Ray Serve](/docs/integrations/providers/ray_serve/)
    - [Rebuff](/docs/integrations/providers/rebuff/)
    - [Reddit](/docs/integrations/providers/reddit/)
    - [Redis](/docs/integrations/providers/redis/)
    - [Remembrall](/docs/integrations/providers/remembrall/)
    - [Replicate](/docs/integrations/providers/replicate/)
    - [Roam](/docs/integrations/providers/roam/)
    - [Sema4 (fka Robocorp)](/docs/integrations/providers/robocorp/)
    - [Rockset](/docs/integrations/providers/rockset/)
    - [Runhouse](/docs/integrations/providers/runhouse/)
    - [Runpod](/docs/integrations/providers/runpod/)
    - [RWKV-4](/docs/integrations/providers/rwkv/)
    - [Salesforce](/docs/integrations/providers/salesforce/)
    - [Salute Devices](/docs/integrations/providers/salute_devices/)
    - [SambaNova](/docs/integrations/providers/sambanova/)
    - [SAP](/docs/integrations/providers/sap/)
    - [ScrapeGraph AI](/docs/integrations/providers/scrapegraph/)
    - [SearchApi](/docs/integrations/providers/searchapi/)
    - [SearxNG Search API](/docs/integrations/providers/searx/)
    - [SemaDB](/docs/integrations/providers/semadb/)
    - [SerpAPI](/docs/integrations/providers/serpapi/)
    - [Shale Protocol](/docs/integrations/providers/shaleprotocol/)
    - [SingleStore Integration](/docs/integrations/providers/singlestore/)
    - [scikit-learn](/docs/integrations/providers/sklearn/)
    - [Slack](/docs/integrations/providers/slack/)
    - [Snowflake](/docs/integrations/providers/snowflake/)
    - [spaCy](/docs/integrations/providers/spacy/)
    - [Spark](/docs/integrations/providers/spark/)
    - [SparkLLM](/docs/integrations/providers/sparkllm/)
    - [Spreedly](/docs/integrations/providers/spreedly/)
    - [SQLite](/docs/integrations/providers/sqlite/)
    - [Stack Exchange](/docs/integrations/providers/stackexchange/)
    - [StarRocks](/docs/integrations/providers/starrocks/)
    - [StochasticAI](/docs/integrations/providers/stochasticai/)
    - [Streamlit](/docs/integrations/providers/streamlit/)
    - [Stripe](/docs/integrations/providers/stripe/)
    - [Supabase (Postgres)](/docs/integrations/providers/supabase/)
    - [Nebula](/docs/integrations/providers/symblai_nebula/)
    - [Tableau](/docs/integrations/providers/tableau/)
    - [Taiga](/docs/integrations/providers/taiga/)
    - [Tair](/docs/integrations/providers/tair/)
    - [Tavily](/docs/integrations/providers/tavily/)
    - [Telegram](/docs/integrations/providers/telegram/)
    - [Tencent](/docs/integrations/providers/tencent/)
    - [TensorFlow Datasets](/docs/integrations/providers/tensorflow_datasets/)
    - [TiDB](/docs/integrations/providers/tidb/)
    - [TigerGraph](/docs/integrations/providers/tigergraph/)
    - [Tigris](/docs/integrations/providers/tigris/)
    - [Tilores](/docs/integrations/providers/tilores/)
    - [Together AI](/docs/integrations/providers/together/)
    - [2Markdown](/docs/integrations/providers/tomarkdown/)
    - [Transwarp](/docs/integrations/providers/transwarp/)
    - [Trello](/docs/integrations/providers/trello/)
    - [Trubrics](/docs/integrations/providers/trubrics/)
    - [TruLens](/docs/integrations/providers/trulens/)
    - [Twitter](/docs/integrations/providers/twitter/)
    - [Typesense](/docs/integrations/providers/typesense/)
    - [Unstructured](/docs/integrations/providers/unstructured/)
    - [Upstage](/docs/integrations/providers/upstage/)
    - [upstash](/docs/integrations/providers/upstash/)
    - [UpTrain](/docs/integrations/providers/uptrain/)
    - [USearch](/docs/integrations/providers/usearch/)
    - [Valthera](/docs/integrations/providers/valthera/)
    - [VDMS](/docs/integrations/providers/vdms/)
    - [Vearch](/docs/integrations/providers/vearch/)
    - [Vectara](/docs/integrations/providers/vectara/)
    - [Vectorize](/docs/integrations/providers/vectorize/)
    - [Vespa](/docs/integrations/providers/vespa/)
    - [vlite](/docs/integrations/providers/vlite/)
    - [VoyageAI](/docs/integrations/providers/voyageai/)
    - [Weights &amp; Biases](/docs/integrations/providers/wandb/)
    - [Weights &amp; Biases tracing](/docs/integrations/providers/wandb_tracing/)
    - [Weights &amp; Biases tracking](/docs/integrations/providers/wandb_tracking/)
    - [Weather](/docs/integrations/providers/weather/)
    - [Weaviate](/docs/integrations/providers/weaviate/)
    - [WhatsApp](/docs/integrations/providers/whatsapp/)
    - [WhyLabs](/docs/integrations/providers/whylabs_profiling/)
    - [Wikipedia](/docs/integrations/providers/wikipedia/)
    - [Wolfram Alpha](/docs/integrations/providers/wolfram_alpha/)
    - [Writer, Inc.](/docs/integrations/providers/writer/)
    - [xAI](/docs/integrations/providers/xai/)
    - [Xata](/docs/integrations/providers/xata/)
    - [Xorbits Inference (Xinference)](/docs/integrations/providers/xinference/)
    - [Yahoo](/docs/integrations/providers/yahoo/)
    - [Yandex](/docs/integrations/providers/yandex/)
    - [YDB](/docs/integrations/providers/ydb/)
    - [Yeager.ai](/docs/integrations/providers/yeagerai/)
    - [Yellowbrick](/docs/integrations/providers/yellowbrick/)
    - [01.AI](/docs/integrations/providers/yi/)
    - [You](/docs/integrations/providers/you/)
    - [YouTube](/docs/integrations/providers/youtube/)
    - [Zep](/docs/integrations/providers/zep/)
    - [Zhipu AI](/docs/integrations/providers/zhipuai/)
    - [Zilliz](/docs/integrations/providers/zilliz/)
    - [Zotero](/docs/integrations/providers/zotero/)
- [Components](/docs/integrations/components/)
  
  - [Chat models](/docs/integrations/chat/)
    
    - [Chat models](/docs/integrations/chat/)
    - [Abso](/docs/integrations/chat/abso/)
    - [AI21 Labs](/docs/integrations/chat/ai21/)
    - [Alibaba Cloud PAI EAS](/docs/integrations/chat/alibaba_cloud_pai_eas/)
    - [Anthropic](/docs/integrations/chat/anthropic/)
    - [\[Deprecated\] Experimental Anthropic Tools Wrapper](/docs/integrations/chat/anthropic_functions/)
    - [Anyscale](/docs/integrations/chat/anyscale/)
    - [AzureAIChatCompletionsModel](/docs/integrations/chat/azure_ai/)
    - [Azure OpenAI](/docs/integrations/chat/azure_chat_openai/)
    - [Azure ML Endpoint](/docs/integrations/chat/azureml_chat_endpoint/)
    - [Baichuan Chat](/docs/integrations/chat/baichuan/)
    - [Baidu Qianfan](/docs/integrations/chat/baidu_qianfan_endpoint/)
    - [AWS Bedrock](/docs/integrations/chat/bedrock/)
    - [Cerebras](/docs/integrations/chat/cerebras/)
    - [CloudflareWorkersAI](/docs/integrations/chat/cloudflare_workersai/)
    - [Cohere](/docs/integrations/chat/cohere/)
    - [ContextualAI](/docs/integrations/chat/contextual/)
    - [Coze Chat](/docs/integrations/chat/coze/)
    - [Dappier AI](/docs/integrations/chat/dappier/)
    - [Databricks](/docs/integrations/chat/databricks/)
    - [DeepInfra](/docs/integrations/chat/deepinfra/)
    - [DeepSeek](/docs/integrations/chat/deepseek/)
    - [Eden AI](/docs/integrations/chat/edenai/)
    - [Ernie Bot Chat](/docs/integrations/chat/ernie/)
    - [EverlyAI](/docs/integrations/chat/everlyai/)
    - [Fireworks](/docs/integrations/chat/fireworks/)
    - [ChatFriendli](/docs/integrations/chat/friendli/)
    - [GigaChat](/docs/integrations/chat/gigachat/)
    - [Goodfire](/docs/integrations/chat/goodfire/)
    - [Google AI](/docs/integrations/chat/google_generative_ai/)
    - [Google Cloud Vertex AI](/docs/integrations/chat/google_vertex_ai_palm/)
    - [GPTRouter](/docs/integrations/chat/gpt_router/)
    - [Groq](/docs/integrations/chat/groq/)
    - [ChatHuggingFace](/docs/integrations/chat/huggingface/)
    - [IBM watsonx.ai](/docs/integrations/chat/ibm_watsonx/)
    - [JinaChat](/docs/integrations/chat/jinachat/)
    - [Kinetica](/docs/integrations/chat/kinetica/)
    - [Konko](/docs/integrations/chat/konko/)
    - [LiteLLM](/docs/integrations/chat/litellm/)
    - [LiteLLM Router](/docs/integrations/chat/litellm_router/)
    - [Llama 2 Chat](/docs/integrations/chat/llama2_chat/)
    - [Llama API](/docs/integrations/chat/llama_api/)
    - [LlamaEdge](/docs/integrations/chat/llama_edge/)
    - [Llama.cpp](/docs/integrations/chat/llamacpp/)
    - [maritalk](/docs/integrations/chat/maritalk/)
    - [MiniMax](/docs/integrations/chat/minimax/)
    - [MistralAI](/docs/integrations/chat/mistralai/)
    - [MLX](/docs/integrations/chat/mlx/)
    - [ModelScope](/docs/integrations/chat/modelscope_chat_endpoint/)
    - [Moonshot](/docs/integrations/chat/moonshot/)
    - [Naver](/docs/integrations/chat/naver/)
    - [Netmind](/docs/integrations/chat/netmind/)
    - [NVIDIA AI Endpoints](/docs/integrations/chat/nvidia_ai_endpoints/)
    - [ChatOCIModelDeployment](/docs/integrations/chat/oci_data_science/)
    - [OCIGenAI](/docs/integrations/chat/oci_generative_ai/)
    - [ChatOctoAI](/docs/integrations/chat/octoai/)
    - [Ollama](/docs/integrations/chat/ollama/)
    - [OpenAI](/docs/integrations/chat/openai/)
    - [Outlines](/docs/integrations/chat/outlines/)
    - [Perplexity](/docs/integrations/chat/perplexity/)
    - [Pipeshift](/docs/integrations/chat/pipeshift/)
    - [ChatPredictionGuard](/docs/integrations/chat/predictionguard/)
    - [PremAI](/docs/integrations/chat/premai/)
    - [PromptLayer ChatOpenAI](/docs/integrations/chat/promptlayer_chatopenai/)
    - [Qwen QwQ](/docs/integrations/chat/qwq/)
    - [Reka](/docs/integrations/chat/reka/)
    - [RunPod Chat Model](/docs/integrations/chat/runpod/)
    - [SambaNovaCloud](/docs/integrations/chat/sambanova/)
    - [SambaStudio](/docs/integrations/chat/sambastudio/)
    - [ChatSeekrFlow](/docs/integrations/chat/seekrflow/)
    - [Snowflake Cortex](/docs/integrations/chat/snowflake/)
    - [solar](/docs/integrations/chat/solar/)
    - [SparkLLM Chat](/docs/integrations/chat/sparkllm/)
    - [Nebula (Symbl.ai)](/docs/integrations/chat/symblai_nebula/)
    - [Tencent Hunyuan](/docs/integrations/chat/tencent_hunyuan/)
    - [Together](/docs/integrations/chat/together/)
    - [Tongyi Qwen](/docs/integrations/chat/tongyi/)
    - [Upstage](/docs/integrations/chat/upstage/)
    - [vectara](/docs/integrations/chat/vectara/)
    - [vLLM Chat](/docs/integrations/chat/vllm/)
    - [Volc Enging Maas](/docs/integrations/chat/volcengine_maas/)
    - [Chat Writer](/docs/integrations/chat/writer/)
    - [xAI](/docs/integrations/chat/xai/)
    - [Xinference](/docs/integrations/chat/xinference/)
    - [YandexGPT](/docs/integrations/chat/yandex/)
    - [ChatYI](/docs/integrations/chat/yi/)
    - [Yuan2.0](/docs/integrations/chat/yuan2/)
    - [ZHIPU AI](/docs/integrations/chat/zhipuai/)
  - [Retrievers](/docs/integrations/retrievers/)
    
    - [Retrievers](/docs/integrations/retrievers/)
    - [Activeloop Deep Memory](/docs/integrations/retrievers/activeloop/)
    - [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/)
    - [Arcee](/docs/integrations/retrievers/arcee/)
    - [Arxiv](/docs/integrations/retrievers/arxiv/)
    - [AskNews](/docs/integrations/retrievers/asknews/)
    - [Azure AI Search](/docs/integrations/retrievers/azure_ai_search/)
    - [Bedrock (Knowledge Bases)](/docs/integrations/retrievers/bedrock/)
    - [BM25](/docs/integrations/retrievers/bm25/)
    - [Box](/docs/integrations/retrievers/box/)
    - [BREEBS (Open Knowledge)](/docs/integrations/retrievers/breebs/)
    - [Chaindesk](/docs/integrations/retrievers/chaindesk/)
    - [ChatGPT plugin](/docs/integrations/retrievers/chatgpt-plugin/)
    - [Cognee](/docs/integrations/retrievers/cognee/)
    - [Cohere reranker](/docs/integrations/retrievers/cohere-reranker/)
    - [Cohere RAG](/docs/integrations/retrievers/cohere/)
    - [Contextual AI Reranker](/docs/integrations/retrievers/contextual/)
    - [Dappier](/docs/integrations/retrievers/dappier/)
    - [DocArray](/docs/integrations/retrievers/docarray_retriever/)
    - [Dria](/docs/integrations/retrievers/dria_index/)
    - [ElasticSearch BM25](/docs/integrations/retrievers/elastic_search_bm25/)
    - [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/)
    - [Embedchain](/docs/integrations/retrievers/embedchain/)
    - [FlashRank reranker](/docs/integrations/retrievers/flashrank-reranker/)
    - [Fleet AI Context](/docs/integrations/retrievers/fleet_context/)
    - [Galaxia](/docs/integrations/retrievers/galaxia-retriever/)
    - [Google Drive](/docs/integrations/retrievers/google_drive/)
    - [Google Vertex AI Search](/docs/integrations/retrievers/google_vertex_ai_search/)
    - [Graph RAG](/docs/integrations/retrievers/graph_rag/)
    - [IBM watsonx.ai](/docs/integrations/retrievers/ibm_watsonx_ranker/)
    - [JaguarDB Vector Database](/docs/integrations/retrievers/jaguar/)
    - [Kay.ai](/docs/integrations/retrievers/kay/)
    - [Kinetica Vectorstore based Retriever](/docs/integrations/retrievers/kinetica/)
    - [kNN](/docs/integrations/retrievers/knn/)
    - [LinkupSearchRetriever](/docs/integrations/retrievers/linkup_search/)
    - [LLMLingua Document Compressor](/docs/integrations/retrievers/llmlingua/)
    - [LOTR (Merger Retriever)](/docs/integrations/retrievers/merger_retriever/)
    - [Metal](/docs/integrations/retrievers/metal/)
    - [Milvus Hybrid Search](/docs/integrations/retrievers/milvus_hybrid_search/)
    - [NanoPQ (Product Quantization)](/docs/integrations/retrievers/nanopq/)
    - [needle](/docs/integrations/retrievers/needle/)
    - [Nimble](/docs/integrations/retrievers/nimble/)
    - [Outline](/docs/integrations/retrievers/outline/)
    - [Permit](/docs/integrations/retrievers/permit/)
    - [Pinecone Hybrid Search](/docs/integrations/retrievers/pinecone_hybrid_search/)
    - [PubMed](/docs/integrations/retrievers/pubmed/)
    - [Qdrant Sparse Vector](/docs/integrations/retrievers/qdrant-sparse/)
    - [RAGatouille](/docs/integrations/retrievers/ragatouille/)
    - [RePhraseQuery](/docs/integrations/retrievers/re_phrase/)
    - [Rememberizer](/docs/integrations/retrievers/rememberizer/)
    - [SEC filing](/docs/integrations/retrievers/sec_filings/)
    - [Self-querying retrievers](/docs/integrations/retrievers/self_query/)
    - [SVM](/docs/integrations/retrievers/svm/)
    - [TavilySearchAPI](/docs/integrations/retrievers/tavily/)
    - [TF-IDF](/docs/integrations/retrievers/tf_idf/)
    - [\*\*NeuralDB\*\*](/docs/integrations/retrievers/thirdai_neuraldb/)
    - [Vectorize](/docs/integrations/retrievers/vectorize/)
    - [Vespa](/docs/integrations/retrievers/vespa/)
    - [Wikipedia](/docs/integrations/retrievers/wikipedia/)
    - [You.com](/docs/integrations/retrievers/you-retriever/)
    - [Zep Cloud](/docs/integrations/retrievers/zep_cloud_memorystore/)
    - [Zep Open Source](/docs/integrations/retrievers/zep_memorystore/)
    - [Zilliz Cloud Pipeline](/docs/integrations/retrievers/zilliz_cloud_pipeline/)
    - [Zotero](/docs/integrations/retrievers/zotero/)
  - [Tools/Toolkits](/docs/integrations/tools/)
    
    - [Tools](/docs/integrations/tools/)
    - [ADS4GPTs](/docs/integrations/tools/ads4gpts/)
    - [AgentQL](/docs/integrations/tools/agentql/)
    - [AINetwork Toolkit](/docs/integrations/tools/ainetwork/)
    - [Alpha Vantage](/docs/integrations/tools/alpha_vantage/)
    - [Amadeus Toolkit](/docs/integrations/tools/amadeus/)
    - [Apify Actor](/docs/integrations/tools/apify_actors/)
    - [ArXiv](/docs/integrations/tools/arxiv/)
    - [AskNews](/docs/integrations/tools/asknews/)
    - [AWS Lambda](/docs/integrations/tools/awslambda/)
    - [Azure AI Services Toolkit](/docs/integrations/tools/azure_ai_services/)
    - [Azure Cognitive Services Toolkit](/docs/integrations/tools/azure_cognitive_services/)
    - [Azure Container Apps dynamic sessions](/docs/integrations/tools/azure_dynamic_sessions/)
    - [Shell (bash)](/docs/integrations/tools/bash/)
    - [Bearly Code Interpreter](/docs/integrations/tools/bearly/)
    - [Bing Search](/docs/integrations/tools/bing_search/)
    - [Brave Search](/docs/integrations/tools/brave_search/)
    - [Cassandra Database Toolkit](/docs/integrations/tools/cassandra_database/)
    - [CDP](/docs/integrations/tools/cdp_agentkit/)
    - [ChatGPT Plugins](/docs/integrations/tools/chatgpt_plugins/)
    - [ClickUp Toolkit](/docs/integrations/tools/clickup/)
    - [Cogniswitch Toolkit](/docs/integrations/tools/cogniswitch/)
    - [Connery Toolkit and Tools](/docs/integrations/tools/connery/)
    - [Dall-E Image Generator](/docs/integrations/tools/dalle_image_generator/)
    - [Dappier](/docs/integrations/tools/dappier/)
    - [Databricks Unity Catalog (UC)](/docs/integrations/tools/databricks/)
    - [DataForSEO](/docs/integrations/tools/dataforseo/)
    - [Dataherald](/docs/integrations/tools/dataherald/)
    - [DuckDuckGo Search](/docs/integrations/tools/ddg/)
    - [Discord](/docs/integrations/tools/discord/)
    - [E2B Data Analysis](/docs/integrations/tools/e2b_data_analysis/)
    - [Eden AI](/docs/integrations/tools/edenai_tools/)
    - [ElevenLabs Text2Speech](/docs/integrations/tools/eleven_labs_tts/)
    - [Exa Search](/docs/integrations/tools/exa_search/)
    - [File System](/docs/integrations/tools/filesystem/)
    - [FinancialDatasets Toolkit](/docs/integrations/tools/financial_datasets/)
    - [FMP Data](/docs/integrations/tools/fmp-data/)
    - [Github Toolkit](/docs/integrations/tools/github/)
    - [Gitlab Toolkit](/docs/integrations/tools/gitlab/)
    - [Gmail Toolkit](/docs/integrations/tools/gmail/)
    - [GOAT](/docs/integrations/tools/goat/)
    - [Golden Query](/docs/integrations/tools/golden_query/)
    - [Google Books](/docs/integrations/tools/google_books/)
    - [Google Calendar Toolkit](/docs/integrations/tools/google_calendar/)
    - [Google Cloud Text-to-Speech](/docs/integrations/tools/google_cloud_texttospeech/)
    - [Google Drive](/docs/integrations/tools/google_drive/)
    - [Google Finance](/docs/integrations/tools/google_finance/)
    - [Google Imagen](/docs/integrations/tools/google_imagen/)
    - [Google Jobs](/docs/integrations/tools/google_jobs/)
    - [Google Lens](/docs/integrations/tools/google_lens/)
    - [Google Places](/docs/integrations/tools/google_places/)
    - [Google Scholar](/docs/integrations/tools/google_scholar/)
    - [Google Search](/docs/integrations/tools/google_search/)
    - [Google Serper](/docs/integrations/tools/google_serper/)
    - [Google Trends](/docs/integrations/tools/google_trends/)
    - [Gradio](/docs/integrations/tools/gradio_tools/)
    - [GraphQL](/docs/integrations/tools/graphql/)
    - [HuggingFace Hub Tools](/docs/integrations/tools/huggingface_tools/)
    - [Human as a tool](/docs/integrations/tools/human_tools/)
    - [Hyperbrowser Browser Agent Tools](/docs/integrations/tools/hyperbrowser_browser_agent_tools/)
    - [Hyperbrowser Web Scraping Tools](/docs/integrations/tools/hyperbrowser_web_scraping_tools/)
    - [IBM watsonx.ai](/docs/integrations/tools/ibm_watsonx/)
    - [IFTTT WebHooks](/docs/integrations/tools/ifttt/)
    - [Infobip](/docs/integrations/tools/infobip/)
    - [Ionic Shopping Tool](/docs/integrations/tools/ionic_shopping/)
    - [Jenkins](/docs/integrations/tools/jenkins/)
    - [Jina Search](/docs/integrations/tools/jina_search/)
    - [Jira Toolkit](/docs/integrations/tools/jira/)
    - [JSON Toolkit](/docs/integrations/tools/json/)
    - [Lemon Agent](/docs/integrations/tools/lemonai/)
    - [LinkupSearchTool](/docs/integrations/tools/linkup_search/)
    - [Memgraph](/docs/integrations/tools/memgraph/)
    - [Memorize](/docs/integrations/tools/memorize/)
    - [Mojeek Search](/docs/integrations/tools/mojeek_search/)
    - [MultiOn Toolkit](/docs/integrations/tools/multion/)
    - [NASA Toolkit](/docs/integrations/tools/nasa/)
    - [Naver Search](/docs/integrations/tools/naver_search/)
    - [Nuclia Understanding](/docs/integrations/tools/nuclia/)
    - [NVIDIA Riva: ASR and TTS](/docs/integrations/tools/nvidia_riva/)
    - [Office365 Toolkit](/docs/integrations/tools/office365/)
    - [OpenAPI Toolkit](/docs/integrations/tools/openapi/)
    - [Natural Language API Toolkits](/docs/integrations/tools/openapi_nla/)
    - [OpenGradient](/docs/integrations/tools/opengradient_toolkit/)
    - [OpenWeatherMap](/docs/integrations/tools/openweathermap/)
    - [Oracle AI Vector Search: Generate Summary](/docs/integrations/tools/oracleai/)
    - [Oxylabs](/docs/integrations/tools/oxylabs/)
    - [Pandas Dataframe](/docs/integrations/tools/pandas/)
    - [Passio NutritionAI](/docs/integrations/tools/passio_nutrition_ai/)
    - [PaymanAI](/docs/integrations/tools/payman-tool/)
    - [Permit](/docs/integrations/tools/permit/)
    - [PlayWright Browser Toolkit](/docs/integrations/tools/playwright/)
    - [Polygon IO Toolkit and Tools](/docs/integrations/tools/polygon/)
    - [PowerBI Toolkit](/docs/integrations/tools/powerbi/)
    - [Prolog](/docs/integrations/tools/prolog_tool/)
    - [PubMed](/docs/integrations/tools/pubmed/)
    - [Python REPL](/docs/integrations/tools/python/)
    - [Reddit Search](/docs/integrations/tools/reddit_search/)
    - [Requests Toolkit](/docs/integrations/tools/requests/)
    - [Riza Code Interpreter](/docs/integrations/tools/riza/)
    - [Robocorp Toolkit](/docs/integrations/tools/robocorp/)
    - [Salesforce](/docs/integrations/tools/salesforce/)
    - [SceneXplain](/docs/integrations/tools/sceneXplain/)
    - [ScrapeGraph](/docs/integrations/tools/scrapegraph/)
    - [SearchApi](/docs/integrations/tools/searchapi/)
    - [SearxNG Search](/docs/integrations/tools/searx_search/)
    - [Semantic Scholar API Tool](/docs/integrations/tools/semanticscholar/)
    - [SerpAPI](/docs/integrations/tools/serpapi/)
    - [Slack Toolkit](/docs/integrations/tools/slack/)
    - [Spark SQL Toolkit](/docs/integrations/tools/spark_sql/)
    - [SQLDatabase Toolkit](/docs/integrations/tools/sql_database/)
    - [StackExchange](/docs/integrations/tools/stackexchange/)
    - [Steam Toolkit](/docs/integrations/tools/steam/)
    - [Stripe](/docs/integrations/tools/stripe/)
    - [Tableau](/docs/integrations/tools/tableau/)
    - [Taiga](/docs/integrations/tools/taiga/)
    - [Tavily Extract](/docs/integrations/tools/tavily_extract/)
    - [Tavily Search](/docs/integrations/tools/tavily_search/)
    - [Tilores](/docs/integrations/tools/tilores/)
    - [Twilio](/docs/integrations/tools/twilio/)
    - [Upstage](/docs/integrations/tools/upstage_groundedness_check/)
    - [Valthera](/docs/integrations/tools/valthera/)
    - [Wikidata](/docs/integrations/tools/wikidata/)
    - [Wikipedia](/docs/integrations/tools/wikipedia/)
    - [Wolfram Alpha](/docs/integrations/tools/wolfram_alpha/)
    - [Writer Tools](/docs/integrations/tools/writer/)
    - [Yahoo Finance News](/docs/integrations/tools/yahoo_finance_news/)
    - [You.com Search](/docs/integrations/tools/you/)
    - [YouTube](/docs/integrations/tools/youtube/)
    - [Zapier Natural Language Actions](/docs/integrations/tools/zapier/)
    - [ZenGuard AI](/docs/integrations/tools/zenguard/)
  - [Document loaders](/docs/integrations/document_loaders/)
    
    - [Document loaders](/docs/integrations/document_loaders/)
    - [acreom](/docs/integrations/document_loaders/acreom/)
    - [AgentQLLoader](/docs/integrations/document_loaders/agentql/)
    - [AirbyteLoader](/docs/integrations/document_loaders/airbyte/)
    - [Airbyte CDK (Deprecated)](/docs/integrations/document_loaders/airbyte_cdk/)
    - [Airbyte Gong (Deprecated)](/docs/integrations/document_loaders/airbyte_gong/)
    - [Airbyte Hubspot (Deprecated)](/docs/integrations/document_loaders/airbyte_hubspot/)
    - [Airbyte JSON (Deprecated)](/docs/integrations/document_loaders/airbyte_json/)
    - [Airbyte Salesforce (Deprecated)](/docs/integrations/document_loaders/airbyte_salesforce/)
    - [Airbyte Shopify (Deprecated)](/docs/integrations/document_loaders/airbyte_shopify/)
    - [Airbyte Stripe (Deprecated)](/docs/integrations/document_loaders/airbyte_stripe/)
    - [Airbyte Typeform (Deprecated)](/docs/integrations/document_loaders/airbyte_typeform/)
    - [Airbyte Zendesk Support (Deprecated)](/docs/integrations/document_loaders/airbyte_zendesk_support/)
    - [Airtable](/docs/integrations/document_loaders/airtable/)
    - [Alibaba Cloud MaxCompute](/docs/integrations/document_loaders/alibaba_cloud_maxcompute/)
    - [Amazon Textract](/docs/integrations/document_loaders/amazon_textract/)
    - [Apify Dataset](/docs/integrations/document_loaders/apify_dataset/)
    - [ArcGIS](/docs/integrations/document_loaders/arcgis/)
    - [ArxivLoader](/docs/integrations/document_loaders/arxiv/)
    - [AssemblyAI Audio Transcripts](/docs/integrations/document_loaders/assemblyai/)
    - [AstraDB](/docs/integrations/document_loaders/astradb/)
    - [Async Chromium](/docs/integrations/document_loaders/async_chromium/)
    - [AsyncHtml](/docs/integrations/document_loaders/async_html/)
    - [Athena](/docs/integrations/document_loaders/athena/)
    - [AWS S3 Directory](/docs/integrations/document_loaders/aws_s3_directory/)
    - [AWS S3 File](/docs/integrations/document_loaders/aws_s3_file/)
    - [AZLyrics](/docs/integrations/document_loaders/azlyrics/)
    - [Azure AI Data](/docs/integrations/document_loaders/azure_ai_data/)
    - [Azure Blob Storage Container](/docs/integrations/document_loaders/azure_blob_storage_container/)
    - [Azure Blob Storage File](/docs/integrations/document_loaders/azure_blob_storage_file/)
    - [Azure AI Document Intelligence](/docs/integrations/document_loaders/azure_document_intelligence/)
    - [BibTeX](/docs/integrations/document_loaders/bibtex/)
    - [BiliBili](/docs/integrations/document_loaders/bilibili/)
    - [Blackboard](/docs/integrations/document_loaders/blackboard/)
    - [Blockchain](/docs/integrations/document_loaders/blockchain/)
    - [Box](/docs/integrations/document_loaders/box/)
    - [Brave Search](/docs/integrations/document_loaders/brave_search/)
    - [Browserbase](/docs/integrations/document_loaders/browserbase/)
    - [Browserless](/docs/integrations/document_loaders/browserless/)
    - [BSHTMLLoader](/docs/integrations/document_loaders/bshtml/)
    - [Cassandra](/docs/integrations/document_loaders/cassandra/)
    - [ChatGPT Data](/docs/integrations/document_loaders/chatgpt_loader/)
    - [College Confidential](/docs/integrations/document_loaders/college_confidential/)
    - [Concurrent Loader](/docs/integrations/document_loaders/concurrent/)
    - [Confluence](/docs/integrations/document_loaders/confluence/)
    - [CoNLL-U](/docs/integrations/document_loaders/conll-u/)
    - [Copy Paste](/docs/integrations/document_loaders/copypaste/)
    - [Couchbase](/docs/integrations/document_loaders/couchbase/)
    - [CSV](/docs/integrations/document_loaders/csv/)
    - [Cube Semantic Layer](/docs/integrations/document_loaders/cube_semantic/)
    - [Datadog Logs](/docs/integrations/document_loaders/datadog_logs/)
    - [Dedoc](/docs/integrations/document_loaders/dedoc/)
    - [Diffbot](/docs/integrations/document_loaders/diffbot/)
    - [Discord](/docs/integrations/document_loaders/discord/)
    - [Docling](/docs/integrations/document_loaders/docling/)
    - [Docugami](/docs/integrations/document_loaders/docugami/)
    - [Docusaurus](/docs/integrations/document_loaders/docusaurus/)
    - [Dropbox](/docs/integrations/document_loaders/dropbox/)
    - [DuckDB](/docs/integrations/document_loaders/duckdb/)
    - [Email](/docs/integrations/document_loaders/email/)
    - [EPub](/docs/integrations/document_loaders/epub/)
    - [Etherscan](/docs/integrations/document_loaders/etherscan/)
    - [EverNote](/docs/integrations/document_loaders/evernote/)
    - [example\_data](/docs/integrations/document_loaders/example_data/example/)
    - [Facebook Chat](/docs/integrations/document_loaders/facebook_chat/)
    - [Fauna](/docs/integrations/document_loaders/fauna/)
    - [Figma](/docs/integrations/document_loaders/figma/)
    - [FireCrawl](/docs/integrations/document_loaders/firecrawl/)
    - [Geopandas](/docs/integrations/document_loaders/geopandas/)
    - [Git](/docs/integrations/document_loaders/git/)
    - [GitBook](/docs/integrations/document_loaders/gitbook/)
    - [GitHub](/docs/integrations/document_loaders/github/)
    - [Glue Catalog](/docs/integrations/document_loaders/glue_catalog/)
    - [Google AlloyDB for PostgreSQL](/docs/integrations/document_loaders/google_alloydb/)
    - [Google BigQuery](/docs/integrations/document_loaders/google_bigquery/)
    - [Google Bigtable](/docs/integrations/document_loaders/google_bigtable/)
    - [Google Cloud SQL for SQL server](/docs/integrations/document_loaders/google_cloud_sql_mssql/)
    - [Google Cloud SQL for MySQL](/docs/integrations/document_loaders/google_cloud_sql_mysql/)
    - [Google Cloud SQL for PostgreSQL](/docs/integrations/document_loaders/google_cloud_sql_pg/)
    - [Google Cloud Storage Directory](/docs/integrations/document_loaders/google_cloud_storage_directory/)
    - [Google Cloud Storage File](/docs/integrations/document_loaders/google_cloud_storage_file/)
    - [Google Firestore in Datastore Mode](/docs/integrations/document_loaders/google_datastore/)
    - [Google Drive](/docs/integrations/document_loaders/google_drive/)
    - [Google El Carro for Oracle Workloads](/docs/integrations/document_loaders/google_el_carro/)
    - [Google Firestore (Native Mode)](/docs/integrations/document_loaders/google_firestore/)
    - [Google Memorystore for Redis](/docs/integrations/document_loaders/google_memorystore_redis/)
    - [Google Spanner](/docs/integrations/document_loaders/google_spanner/)
    - [Google Speech-to-Text Audio Transcripts](/docs/integrations/document_loaders/google_speech_to_text/)
    - [Grobid](/docs/integrations/document_loaders/grobid/)
    - [Gutenberg](/docs/integrations/document_loaders/gutenberg/)
    - [Hacker News](/docs/integrations/document_loaders/hacker_news/)
    - [Huawei OBS Directory](/docs/integrations/document_loaders/huawei_obs_directory/)
    - [Huawei OBS File](/docs/integrations/document_loaders/huawei_obs_file/)
    - [HuggingFace dataset](/docs/integrations/document_loaders/hugging_face_dataset/)
    - [HyperbrowserLoader](/docs/integrations/document_loaders/hyperbrowser/)
    - [iFixit](/docs/integrations/document_loaders/ifixit/)
    - [Images](/docs/integrations/document_loaders/image/)
    - [Image captions](/docs/integrations/document_loaders/image_captions/)
    - [IMSDb](/docs/integrations/document_loaders/imsdb/)
    - [Iugu](/docs/integrations/document_loaders/iugu/)
    - [Joplin](/docs/integrations/document_loaders/joplin/)
    - [JSONLoader](/docs/integrations/document_loaders/json/)
    - [Jupyter Notebook](/docs/integrations/document_loaders/jupyter_notebook/)
    - [Kinetica](/docs/integrations/document_loaders/kinetica/)
    - [lakeFS](/docs/integrations/document_loaders/lakefs/)
    - [LangSmith](/docs/integrations/document_loaders/langsmith/)
    - [LarkSuite (FeiShu)](/docs/integrations/document_loaders/larksuite/)
    - [LLM Sherpa](/docs/integrations/document_loaders/llmsherpa/)
    - [Mastodon](/docs/integrations/document_loaders/mastodon/)
    - [MathPixPDFLoader](/docs/integrations/document_loaders/mathpix/)
    - [MediaWiki Dump](/docs/integrations/document_loaders/mediawikidump/)
    - [Merge Documents Loader](/docs/integrations/document_loaders/merge_doc/)
    - [mhtml](/docs/integrations/document_loaders/mhtml/)
    - [Microsoft Excel](/docs/integrations/document_loaders/microsoft_excel/)
    - [Microsoft OneDrive](/docs/integrations/document_loaders/microsoft_onedrive/)
    - [Microsoft OneNote](/docs/integrations/document_loaders/microsoft_onenote/)
    - [Microsoft PowerPoint](/docs/integrations/document_loaders/microsoft_powerpoint/)
    - [Microsoft SharePoint](/docs/integrations/document_loaders/microsoft_sharepoint/)
    - [Microsoft Word](/docs/integrations/document_loaders/microsoft_word/)
    - [Near Blockchain](/docs/integrations/document_loaders/mintbase/)
    - [Modern Treasury](/docs/integrations/document_loaders/modern_treasury/)
    - [MongoDB](/docs/integrations/document_loaders/mongodb/)
    - [Needle Document Loader](/docs/integrations/document_loaders/needle/)
    - [News URL](/docs/integrations/document_loaders/news/)
    - [Notion DB 2/2](/docs/integrations/document_loaders/notion/)
    - [Nuclia](/docs/integrations/document_loaders/nuclia/)
    - [Obsidian](/docs/integrations/document_loaders/obsidian/)
    - [Open Document Format (ODT)](/docs/integrations/document_loaders/odt/)
    - [Open City Data](/docs/integrations/document_loaders/open_city_data/)
    - [Oracle Autonomous Database](/docs/integrations/document_loaders/oracleadb_loader/)
    - [Oracle AI Vector Search: Document Processing](/docs/integrations/document_loaders/oracleai/)
    - [Org-mode](/docs/integrations/document_loaders/org_mode/)
    - [Pandas DataFrame](/docs/integrations/document_loaders/pandas_dataframe/)
    - [parsers](/docs/integrations/document_loaders/parsers/azure_openai_whisper_parser/)
    - [PDFMinerLoader](/docs/integrations/document_loaders/pdfminer/)
    - [PDFPlumber](/docs/integrations/document_loaders/pdfplumber/)
    - [Pebblo Safe DocumentLoader](/docs/integrations/document_loaders/pebblo/)
    - [Polars DataFrame](/docs/integrations/document_loaders/polars_dataframe/)
    - [Dell PowerScale Document Loader](/docs/integrations/document_loaders/powerscale/)
    - [Psychic](/docs/integrations/document_loaders/psychic/)
    - [PubMed](/docs/integrations/document_loaders/pubmed/)
    - [PullMdLoader](/docs/integrations/document_loaders/pull_md/)
    - [PyMuPDFLoader](/docs/integrations/document_loaders/pymupdf/)
    - [PyMuPDF4LLM](/docs/integrations/document_loaders/pymupdf4llm/)
    - [PyPDFDirectoryLoader](/docs/integrations/document_loaders/pypdfdirectory/)
    - [PyPDFium2Loader](/docs/integrations/document_loaders/pypdfium2/)
    - [PyPDFLoader](/docs/integrations/document_loaders/pypdfloader/)
    - [PySpark](/docs/integrations/document_loaders/pyspark_dataframe/)
    - [Quip](/docs/integrations/document_loaders/quip/)
    - [ReadTheDocs Documentation](/docs/integrations/document_loaders/readthedocs_documentation/)
    - [Recursive URL](/docs/integrations/document_loaders/recursive_url/)
    - [Reddit](/docs/integrations/document_loaders/reddit/)
    - [Roam](/docs/integrations/document_loaders/roam/)
    - [Rockset](/docs/integrations/document_loaders/rockset/)
    - [rspace](/docs/integrations/document_loaders/rspace/)
    - [RSS Feeds](/docs/integrations/document_loaders/rss/)
    - [RST](/docs/integrations/document_loaders/rst/)
    - [scrapfly](/docs/integrations/document_loaders/scrapfly/)
    - [ScrapingAnt](/docs/integrations/document_loaders/scrapingant/)
    - [SingleStore](/docs/integrations/document_loaders/singlestore/)
    - [Sitemap](/docs/integrations/document_loaders/sitemap/)
    - [Slack](/docs/integrations/document_loaders/slack/)
    - [Snowflake](/docs/integrations/document_loaders/snowflake/)
    - [Source Code](/docs/integrations/document_loaders/source_code/)
    - [Spider](/docs/integrations/document_loaders/spider/)
    - [Spreedly](/docs/integrations/document_loaders/spreedly/)
    - [Stripe](/docs/integrations/document_loaders/stripe/)
    - [Subtitle](/docs/integrations/document_loaders/subtitle/)
    - [SurrealDB](/docs/integrations/document_loaders/surrealdb/)
    - [Telegram](/docs/integrations/document_loaders/telegram/)
    - [Tencent COS Directory](/docs/integrations/document_loaders/tencent_cos_directory/)
    - [Tencent COS File](/docs/integrations/document_loaders/tencent_cos_file/)
    - [TensorFlow Datasets](/docs/integrations/document_loaders/tensorflow_datasets/)
    - [TiDB](/docs/integrations/document_loaders/tidb/)
    - [2Markdown](/docs/integrations/document_loaders/tomarkdown/)
    - [TOML](/docs/integrations/document_loaders/toml/)
    - [Trello](/docs/integrations/document_loaders/trello/)
    - [TSV](/docs/integrations/document_loaders/tsv/)
    - [Twitter](/docs/integrations/document_loaders/twitter/)
    - [Unstructured](/docs/integrations/document_loaders/unstructured_file/)
    - [UnstructuredMarkdownLoader](/docs/integrations/document_loaders/unstructured_markdown/)
    - [UnstructuredPDFLoader](/docs/integrations/document_loaders/unstructured_pdfloader/)
    - [Upstage](/docs/integrations/document_loaders/upstage/)
    - [URL](/docs/integrations/document_loaders/url/)
    - [Vsdx](/docs/integrations/document_loaders/vsdx/)
    - [Weather](/docs/integrations/document_loaders/weather/)
    - [WebBaseLoader](/docs/integrations/document_loaders/web_base/)
    - [WhatsApp Chat](/docs/integrations/document_loaders/whatsapp_chat/)
    - [Wikipedia](/docs/integrations/document_loaders/wikipedia/)
    - [UnstructuredXMLLoader](/docs/integrations/document_loaders/xml/)
    - [Xorbits Pandas DataFrame](/docs/integrations/document_loaders/xorbits/)
    - [YouTube audio](/docs/integrations/document_loaders/youtube_audio/)
    - [YouTube transcripts](/docs/integrations/document_loaders/youtube_transcript/)
    - [YoutubeLoaderDL](/docs/integrations/document_loaders/yt_dlp/)
    - [Yuque](/docs/integrations/document_loaders/yuque/)
    - [ZeroxPDFLoader](/docs/integrations/document_loaders/zeroxpdfloader/)
  - [Vector stores](/docs/integrations/vectorstores/)
    
    - [Vector stores](/docs/integrations/vectorstores/)
    - [Activeloop Deep Lake](/docs/integrations/vectorstores/activeloop_deeplake/)
    - [Aerospike](/docs/integrations/vectorstores/aerospike/)
    - [Alibaba Cloud OpenSearch](/docs/integrations/vectorstores/alibabacloud_opensearch/)
    - [AnalyticDB](/docs/integrations/vectorstores/analyticdb/)
    - [Annoy](/docs/integrations/vectorstores/annoy/)
    - [Apache Doris](/docs/integrations/vectorstores/apache_doris/)
    - [ApertureDB](/docs/integrations/vectorstores/aperturedb/)
    - [Astra DB Vector Store](/docs/integrations/vectorstores/astradb/)
    - [Atlas](/docs/integrations/vectorstores/atlas/)
    - [AwaDB](/docs/integrations/vectorstores/awadb/)
    - [Azure Cosmos DB Mongo vCore](/docs/integrations/vectorstores/azure_cosmos_db/)
    - [Azure Cosmos DB No SQL](/docs/integrations/vectorstores/azure_cosmos_db_no_sql/)
    - [Azure AI Search](/docs/integrations/vectorstores/azuresearch/)
    - [Bagel](/docs/integrations/vectorstores/bagel/)
    - [BagelDB](/docs/integrations/vectorstores/bageldb/)
    - [Baidu Cloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/baiducloud_vector_search/)
    - [Baidu VectorDB](/docs/integrations/vectorstores/baiduvectordb/)
    - [Apache Cassandra](/docs/integrations/vectorstores/cassandra/)
    - [Chroma](/docs/integrations/vectorstores/chroma/)
    - [Clarifai](/docs/integrations/vectorstores/clarifai/)
    - [ClickHouse](/docs/integrations/vectorstores/clickhouse/)
    - [CloudflareVectorize](/docs/integrations/vectorstores/cloudflare_vectorize/)
    - [Couchbase](/docs/integrations/vectorstores/couchbase/)
    - [DashVector](/docs/integrations/vectorstores/dashvector/)
    - [Databricks](/docs/integrations/vectorstores/databricks_vector_search/)
    - [DingoDB](/docs/integrations/vectorstores/dingo/)
    - [DocArray HnswSearch](/docs/integrations/vectorstores/docarray_hnsw/)
    - [DocArray InMemorySearch](/docs/integrations/vectorstores/docarray_in_memory/)
    - [Amazon Document DB](/docs/integrations/vectorstores/documentdb/)
    - [DuckDB](/docs/integrations/vectorstores/duckdb/)
    - [China Mobile ECloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/ecloud_vector_search/)
    - [Elasticsearch](/docs/integrations/vectorstores/elasticsearch/)
    - [Epsilla](/docs/integrations/vectorstores/epsilla/)
    - [Faiss](/docs/integrations/vectorstores/faiss/)
    - [Faiss (Async)](/docs/integrations/vectorstores/faiss_async/)
    - [FalkorDBVectorStore](/docs/integrations/vectorstores/falkordbvector/)
    - [Google AlloyDB for PostgreSQL](/docs/integrations/vectorstores/google_alloydb/)
    - [Google BigQuery Vector Search](/docs/integrations/vectorstores/google_bigquery_vector_search/)
    - [Google Cloud SQL for MySQL](/docs/integrations/vectorstores/google_cloud_sql_mysql/)
    - [Google Cloud SQL for PostgreSQL](/docs/integrations/vectorstores/google_cloud_sql_pg/)
    - [Firestore](/docs/integrations/vectorstores/google_firestore/)
    - [Google Memorystore for Redis](/docs/integrations/vectorstores/google_memorystore_redis/)
    - [Google Spanner](/docs/integrations/vectorstores/google_spanner/)
    - [Google Vertex AI Feature Store](/docs/integrations/vectorstores/google_vertex_ai_feature_store/)
    - [Google Vertex AI Vector Search](/docs/integrations/vectorstores/google_vertex_ai_vector_search/)
    - [Hippo](/docs/integrations/vectorstores/hippo/)
    - [Hologres](/docs/integrations/vectorstores/hologres/)
    - [Infinispan](/docs/integrations/vectorstores/infinispanvs/)
    - [Jaguar Vector Database](/docs/integrations/vectorstores/jaguar/)
    - [KDB.AI](/docs/integrations/vectorstores/kdbai/)
    - [Kinetica](/docs/integrations/vectorstores/kinetica/)
    - [LanceDB](/docs/integrations/vectorstores/lancedb/)
    - [Lantern](/docs/integrations/vectorstores/lantern/)
    - [Lindorm](/docs/integrations/vectorstores/lindorm/)
    - [LLMRails](/docs/integrations/vectorstores/llm_rails/)
    - [ManticoreSearch VectorStore](/docs/integrations/vectorstores/manticore_search/)
    - [MariaDB](/docs/integrations/vectorstores/mariadb/)
    - [Marqo](/docs/integrations/vectorstores/marqo/)
    - [Meilisearch](/docs/integrations/vectorstores/meilisearch/)
    - [Amazon MemoryDB](/docs/integrations/vectorstores/memorydb/)
    - [Milvus](/docs/integrations/vectorstores/milvus/)
    - [Momento Vector Index (MVI)](/docs/integrations/vectorstores/momento_vector_index/)
    - [MongoDB Atlas](/docs/integrations/vectorstores/mongodb_atlas/)
    - [MyScale](/docs/integrations/vectorstores/myscale/)
    - [Neo4j Vector Index](/docs/integrations/vectorstores/neo4jvector/)
    - [NucliaDB](/docs/integrations/vectorstores/nucliadb/)
    - [Oceanbase](/docs/integrations/vectorstores/oceanbase/)
    - [openGauss](/docs/integrations/vectorstores/opengauss/)
    - [OpenSearch](/docs/integrations/vectorstores/opensearch/)
    - [Oracle AI Vector Search: Vector Store](/docs/integrations/vectorstores/oracle/)
    - [Pathway](/docs/integrations/vectorstores/pathway/)
    - [Postgres Embedding](/docs/integrations/vectorstores/pgembedding/)
    - [PGVecto.rs](/docs/integrations/vectorstores/pgvecto_rs/)
    - [PGVector](/docs/integrations/vectorstores/pgvector/)
    - [Pinecone](/docs/integrations/vectorstores/pinecone/)
    - [Qdrant](/docs/integrations/vectorstores/qdrant/)
    - [Redis](/docs/integrations/vectorstores/redis/)
    - [Relyt](/docs/integrations/vectorstores/relyt/)
    - [Rockset](/docs/integrations/vectorstores/rockset/)
    - [SAP HANA Cloud Vector Engine](/docs/integrations/vectorstores/sap_hanavector/)
    - [ScaNN](/docs/integrations/vectorstores/scann/)
    - [SemaDB](/docs/integrations/vectorstores/semadb/)
    - [SingleStore](/docs/integrations/vectorstores/singlestore/)
    - [scikit-learn](/docs/integrations/vectorstores/sklearn/)
    - [SQLiteVec](/docs/integrations/vectorstores/sqlitevec/)
    - [SQLite-VSS](/docs/integrations/vectorstores/sqlitevss/)
    - [SQLServer](/docs/integrations/vectorstores/sqlserver/)
    - [StarRocks](/docs/integrations/vectorstores/starrocks/)
    - [Supabase (Postgres)](/docs/integrations/vectorstores/supabase/)
    - [SurrealDB](/docs/integrations/vectorstores/surrealdb/)
    - [Tablestore](/docs/integrations/vectorstores/tablestore/)
    - [Tair](/docs/integrations/vectorstores/tair/)
    - [Tencent Cloud VectorDB](/docs/integrations/vectorstores/tencentvectordb/)
    - [ThirdAI NeuralDB](/docs/integrations/vectorstores/thirdai_neuraldb/)
    - [TiDB Vector](/docs/integrations/vectorstores/tidb_vector/)
    - [Tigris](/docs/integrations/vectorstores/tigris/)
    - [TileDB](/docs/integrations/vectorstores/tiledb/)
    - [Timescale Vector (Postgres)](/docs/integrations/vectorstores/timescalevector/)
    - [Typesense](/docs/integrations/vectorstores/typesense/)
    - [Upstash Vector](/docs/integrations/vectorstores/upstash/)
    - [USearch](/docs/integrations/vectorstores/usearch/)
    - [Vald](/docs/integrations/vectorstores/vald/)
    - [VDMS](/docs/integrations/vectorstores/vdms/)
    - [Vearch](/docs/integrations/vectorstores/vearch/)
    - [Vectara](/docs/integrations/vectorstores/vectara/)
    - [Vespa](/docs/integrations/vectorstores/vespa/)
    - [viking DB](/docs/integrations/vectorstores/vikingdb/)
    - [vlite](/docs/integrations/vectorstores/vlite/)
    - [Weaviate](/docs/integrations/vectorstores/weaviate/)
    - [Xata](/docs/integrations/vectorstores/xata/)
    - [YDB](/docs/integrations/vectorstores/ydb/)
    - [Yellowbrick](/docs/integrations/vectorstores/yellowbrick/)
    - [Zep](/docs/integrations/vectorstores/zep/)
    - [Zep Cloud](/docs/integrations/vectorstores/zep_cloud/)
    - [Zilliz](/docs/integrations/vectorstores/zilliz/)
  - [Embedding models](/docs/integrations/text_embedding/)
    
    - [Embedding models](/docs/integrations/text_embedding/)
    - [AI21](/docs/integrations/text_embedding/ai21/)
    - [Aleph Alpha](/docs/integrations/text_embedding/aleph_alpha/)
    - [Anyscale](/docs/integrations/text_embedding/anyscale/)
    - [ascend](/docs/integrations/text_embedding/ascend/)
    - [AwaDB](/docs/integrations/text_embedding/awadb/)
    - [AzureOpenAI](/docs/integrations/text_embedding/azureopenai/)
    - [Baichuan Text Embeddings](/docs/integrations/text_embedding/baichuan/)
    - [Baidu Qianfan](/docs/integrations/text_embedding/baidu_qianfan_endpoint/)
    - [Bedrock](/docs/integrations/text_embedding/bedrock/)
    - [BGE on Hugging Face](/docs/integrations/text_embedding/bge_huggingface/)
    - [Bookend AI](/docs/integrations/text_embedding/bookend/)
    - [Clarifai](/docs/integrations/text_embedding/clarifai/)
    - [Cloudflare Workers AI](/docs/integrations/text_embedding/cloudflare_workersai/)
    - [Clova Embeddings](/docs/integrations/text_embedding/clova/)
    - [Cohere](/docs/integrations/text_embedding/cohere/)
    - [DashScope](/docs/integrations/text_embedding/dashscope/)
    - [Databricks](/docs/integrations/text_embedding/databricks/)
    - [DeepInfra](/docs/integrations/text_embedding/deepinfra/)
    - [EDEN AI](/docs/integrations/text_embedding/edenai/)
    - [Elasticsearch](/docs/integrations/text_embedding/elasticsearch/)
    - [Embaas](/docs/integrations/text_embedding/embaas/)
    - [ERNIE](/docs/integrations/text_embedding/ernie/)
    - [Fake Embeddings](/docs/integrations/text_embedding/fake/)
    - [FastEmbed by Qdrant](/docs/integrations/text_embedding/fastembed/)
    - [Fireworks](/docs/integrations/text_embedding/fireworks/)
    - [GigaChat](/docs/integrations/text_embedding/gigachat/)
    - [Google Generative AI Embeddings](/docs/integrations/text_embedding/google_generative_ai/)
    - [Google Vertex AI](/docs/integrations/text_embedding/google_vertex_ai_palm/)
    - [GPT4All](/docs/integrations/text_embedding/gpt4all/)
    - [Gradient](/docs/integrations/text_embedding/gradient/)
    - [Hugging Face](/docs/integrations/text_embedding/huggingfacehub/)
    - [IBM watsonx.ai](/docs/integrations/text_embedding/ibm_watsonx/)
    - [Infinity](/docs/integrations/text_embedding/infinity/)
    - [Instruct Embeddings on Hugging Face](/docs/integrations/text_embedding/instruct_embeddings/)
    - [IPEX-LLM: Local BGE Embeddings on Intel CPU](/docs/integrations/text_embedding/ipex_llm/)
    - [IPEX-LLM: Local BGE Embeddings on Intel GPU](/docs/integrations/text_embedding/ipex_llm_gpu/)
    - [IntelÂ® Extension for Transformers Quantized Text Embeddings](/docs/integrations/text_embedding/itrex/)
    - [Jina](/docs/integrations/text_embedding/jina/)
    - [John Snow Labs](/docs/integrations/text_embedding/johnsnowlabs_embedding/)
    - [LASER Language-Agnostic SEntence Representations Embeddings by Meta AI](/docs/integrations/text_embedding/laser/)
    - [Lindorm](/docs/integrations/text_embedding/lindorm/)
    - [Llama.cpp](/docs/integrations/text_embedding/llamacpp/)
    - [llamafile](/docs/integrations/text_embedding/llamafile/)
    - [LLMRails](/docs/integrations/text_embedding/llm_rails/)
    - [LocalAI](/docs/integrations/text_embedding/localai/)
    - [MiniMax](/docs/integrations/text_embedding/minimax/)
    - [MistralAI](/docs/integrations/text_embedding/mistralai/)
    - [model2vec](/docs/integrations/text_embedding/model2vec/)
    - [ModelScope](/docs/integrations/text_embedding/modelscope_embedding/)
    - [MosaicML](/docs/integrations/text_embedding/mosaicml/)
    - [Naver](/docs/integrations/text_embedding/naver/)
    - [Netmind](/docs/integrations/text_embedding/netmind/)
    - [NLP Cloud](/docs/integrations/text_embedding/nlp_cloud/)
    - [Nomic](/docs/integrations/text_embedding/nomic/)
    - [NVIDIA NIMs](/docs/integrations/text_embedding/nvidia_ai_endpoints/)
    - [Oracle Cloud Infrastructure Generative AI](/docs/integrations/text_embedding/oci_generative_ai/)
    - [Ollama](/docs/integrations/text_embedding/ollama/)
    - [OpenClip](/docs/integrations/text_embedding/open_clip/)
    - [OpenAI](/docs/integrations/text_embedding/openai/)
    - [OpenVINO](/docs/integrations/text_embedding/openvino/)
    - [Embedding Documents using Optimized and Quantized Embedders](/docs/integrations/text_embedding/optimum_intel/)
    - [Oracle AI Vector Search: Generate Embeddings](/docs/integrations/text_embedding/oracleai/)
    - [OVHcloud](/docs/integrations/text_embedding/ovhcloud/)
    - [Pinecone Embeddings](/docs/integrations/text_embedding/pinecone/)
    - [PredictionGuardEmbeddings](/docs/integrations/text_embedding/predictionguard/)
    - [PremAI](/docs/integrations/text_embedding/premai/)
    - [SageMaker](/docs/integrations/text_embedding/sagemaker-endpoint/)
    - [SambaNovaCloud](/docs/integrations/text_embedding/sambanova/)
    - [SambaStudio](/docs/integrations/text_embedding/sambastudio/)
    - [Self Hosted](/docs/integrations/text_embedding/self-hosted/)
    - [Sentence Transformers on Hugging Face](/docs/integrations/text_embedding/sentence_transformers/)
    - [Solar](/docs/integrations/text_embedding/solar/)
    - [SpaCy](/docs/integrations/text_embedding/spacy_embedding/)
    - [SparkLLM Text Embeddings](/docs/integrations/text_embedding/sparkllm/)
    - [TensorFlow Hub](/docs/integrations/text_embedding/tensorflowhub/)
    - [Text Embeddings Inference](/docs/integrations/text_embedding/text_embeddings_inference/)
    - [TextEmbed - Embedding Inference Server](/docs/integrations/text_embedding/textembed/)
    - [Titan Takeoff](/docs/integrations/text_embedding/titan_takeoff/)
    - [Together AI](/docs/integrations/text_embedding/together/)
    - [Upstage](/docs/integrations/text_embedding/upstage/)
    - [Volc Engine](/docs/integrations/text_embedding/volcengine/)
    - [Voyage AI](/docs/integrations/text_embedding/voyageai/)
    - [Xorbits inference (Xinference)](/docs/integrations/text_embedding/xinference/)
    - [YandexGPT](/docs/integrations/text_embedding/yandex/)
    - [ZhipuAI](/docs/integrations/text_embedding/zhipuai/)
  - [Other](/docs/integrations/llms/)

<!--THE END-->

- [Components](/docs/integrations/components/)
- Vector stores


[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/vectorstores/index.mdx)

# Vector stores

A [vector store](/docs/concepts/vectorstores/) stores [embedded](/docs/concepts/embedding_models/) data and performs similarity search.

**Select embedding model:**

Select [embeddings model](/docs/integrations/text_embedding/):

OpenAIâ–¾

[OpenAI](#)

[Azure](#)

[Google](#)

[AWS](#)

[HuggingFace](#)

[Ollama](#)

[Cohere](#)

[MistralAI](#)

[Nomic](#)

[NVIDIA](#)

[Voyage AI](#)

[IBM watsonx](#)

[Fake](#)

```bash
pip install -qU langchain-openai
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
```

**Select vector store:**

Select [vector store](/docs/integrations/vectorstores/):

In-memoryâ–¾

[In-memory](#)

[AstraDB](#)

[Chroma](#)

[FAISS](#)

[Milvus](#)

[MongoDB](#)

[PGVector](#)

[Pinecone](#)

[Qdrant](#)

```bash
pip install -qU langchain-core
```

```python
from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)
```

| Vectorstore                                                                                                                                        | Delete by ID | Filtering | Search by Vector | Search with score | Async | Passes Standard Tests | Multi Tenancy | IDs in add Documents |
|----------------------------------------------------------------------------------------------------------------------------------------------------|--------------|-----------|------------------|-------------------|-------|-----------------------|---------------|----------------------|
| [AstraDBVectorStore](astradb)                                                                                                                      | âœ…            | âœ…         | âœ…                | âœ…                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [Chroma](chroma)                                                                                                                                   | âœ…            | âœ…         | âœ…                | âœ…                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [Clickhouse](clickhouse)                                                                                                                           | âœ…            | âœ…         | âŒ                | âœ…                 | âŒ     | âŒ                     | âŒ             | âŒ                    |
| [CouchbaseVectorStore](couchbase)                                                                                                                  | âœ…            | âœ…         | âŒ                | âœ…                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [DatabricksVectorSearch](databricks_vector_search)                                                                                                 | âœ…            | âœ…         | âœ…                | âœ…                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [ElasticsearchStore](elasticsearch)                                                                                                                | âœ…            | âœ…         | âœ…                | âœ…                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [FAISS](faiss)                                                                                                                                     | âœ…            | âœ…         | âœ…                | âœ…                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [InMemoryVectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | âœ…            | âœ…         | âŒ                | âœ…                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [Milvus](milvus)                                                                                                                                   | âœ…            | âœ…         | âŒ                | âœ…                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [MongoDBAtlasVectorSearch](mongodb_atlas)                                                                                                          | âœ…            | âœ…         | âœ…                | âœ…                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [openGauss](openGauss)                                                                                                                             | âœ…            | âœ…         | âœ…                | âœ…                 | âŒ     | âœ…                     | âŒ             | âœ…                    |
| [PGVector](pgvector)                                                                                                                               | âœ…            | âœ…         | âœ…                | âœ…                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [PineconeVectorStore](pinecone)                                                                                                                    | âœ…            | âœ…         | âœ…                | âŒ                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [QdrantVectorStore](qdrant)                                                                                                                        | âœ…            | âœ…         | âœ…                | âœ…                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [Redis](redis)                                                                                                                                     | âœ…            | âœ…         | âœ…                | âœ…                 | âœ…     | âŒ                     | âŒ             | âŒ                    |
| [Weaviate](weaviate)                                                                                                                               | âœ…            | âœ…         | âœ…                | âœ…                 | âœ…     | âŒ                     | âœ…             | âŒ                    |
| [SQLServer](sqlserver)                                                                                                                             | âœ…            | âœ…         | âœ…                | âœ…                 | âŒ     | âŒ                     | âŒ             | âŒ                    |

## All Vectorstores[â€‹](#all-vectorstores "Direct link to All Vectorstores")

| Name                                                                                                   | Description                                                              |
|--------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------|
| [Activeloop Deep Lake](/docs/integrations/vectorstores/activeloop_deeplake)                            | Activeloop Deep Lake as a Multi-Modal Vector Store that stores embedd... |
| [Aerospike](/docs/integrations/vectorstores/aerospike)                                                 | Aerospike Vector Search (AVS) is an                                      |
| [Alibaba Cloud OpenSearch](/docs/integrations/vectorstores/alibabacloud_opensearch)                    | Alibaba Cloud Opensearch is a one-stop platform to develop intelligen... |
| [AnalyticDB](/docs/integrations/vectorstores/analyticdb)                                               | AnalyticDB for PostgreSQL is a massively parallel processing (MPP) da... |
| [Annoy](/docs/integrations/vectorstores/annoy)                                                         | Annoy (Approximate Nearest Neighbors Oh Yeah) is a C++ library with P... |
| [Apache Doris](/docs/integrations/vectorstores/apache_doris)                                           | Apache Doris is a modern data warehouse for real-time analytics.         |
| [ApertureDB](/docs/integrations/vectorstores/aperturedb)                                               | ApertureDB is a database that stores, indexes, and manages multi-moda... |
| [Astra DB Vector Store](/docs/integrations/vectorstores/astradb)                                       | This page provides a quickstart for using Astra DB as a Vector Store.    |
| [Atlas](/docs/integrations/vectorstores/atlas)                                                         | Atlas is a platform by Nomic made for interacting with both small and... |
| [AwaDB](/docs/integrations/vectorstores/awadb)                                                         | AwaDB is an AI Native database for the search and storage of embeddin... |
| [Azure Cosmos DB Mongo vCore](/docs/integrations/vectorstores/azure_cosmos_db)                         | This notebook shows you how to leverage this integrated vector databa... |
| [Azure Cosmos DB No SQL](/docs/integrations/vectorstores/azure_cosmos_db_no_sql)                       | This notebook shows you how to leverage this integrated vector databa... |
| [Azure AI Search](/docs/integrations/vectorstores/azuresearch)                                         | Azure AI Search (formerly known as Azure Search and Azure Cognitive S... |
| [Bagel](/docs/integrations/vectorstores/bagel)                                                         | Bagel (Open Inference platform for AI), is like GitHub for AI data.      |
| [BagelDB](/docs/integrations/vectorstores/bageldb)                                                     | BagelDB (Open Vector Database for AI), is like GitHub for AI data.       |
| [Baidu Cloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/baiducloud_vector_search)     | Baidu Cloud VectorSearch is a fully managed, enterprise-level distrib... |
| [Baidu VectorDB](/docs/integrations/vectorstores/baiduvectordb)                                        | Baidu VectorDB is a robust, enterprise-level distributed database ser... |
| [Apache Cassandra](/docs/integrations/vectorstores/cassandra)                                          | This page provides a quickstart for using Apache CassandraÂ® as a Vect... |
| [Chroma](/docs/integrations/vectorstores/chroma)                                                       | This notebook covers how to get started with the Chroma vector store.    |
| [Clarifai](/docs/integrations/vectorstores/clarifai)                                                   | Clarifai is an AI Platform that provides the full AI lifecycle rangin... |
| [ClickHouse](/docs/integrations/vectorstores/clickhouse)                                               | ClickHouse is the fastest and most resource efficient open-source dat... |
| [CloudflareVectorize](/docs/integrations/vectorstores/cloudflare_vectorize)                            | This notebook covers how to get started with the CloudflareVectorize ... |
| [Couchbase](/docs/integrations/vectorstores/couchbase)                                                 | Couchbase is an award-winning distributed NoSQL cloud database that d... |
| [DashVector](/docs/integrations/vectorstores/dashvector)                                               | DashVector is a fully-managed vectorDB service that supports high-dim... |
| [Databricks](/docs/integrations/vectorstores/databricks_vector_search)                                 | Databricks Vector Search is a serverless similarity search engine tha... |
| [DingoDB](/docs/integrations/vectorstores/dingo)                                                       | DingoDB is a distributed multi-mode vector database, which combines t... |
| [DocArray HnswSearch](/docs/integrations/vectorstores/docarray_hnsw)                                   | DocArrayHnswSearch is a lightweight Document Index implementation pro... |
| [DocArray InMemorySearch](/docs/integrations/vectorstores/docarray_in_memory)                          | DocArrayInMemorySearch is a document index provided by Docarray that ... |
| [Amazon Document DB](/docs/integrations/vectorstores/documentdb)                                       | Amazon DocumentDB (with MongoDB Compatibility) makes it easy to set u... |
| [DuckDB](/docs/integrations/vectorstores/duckdb)                                                       | This notebook shows how to use DuckDB as a vector store.                 |
| [China Mobile ECloud ElasticSearch VectorSearch](/docs/integrations/vectorstores/ecloud_vector_search) | China Mobile ECloud VectorSearch is a fully managed, enterprise-level... |
| [Elasticsearch](/docs/integrations/vectorstores/elasticsearch)                                         | Elasticsearch is a distributed, RESTful search and analytics engine, ... |
| [Epsilla](/docs/integrations/vectorstores/epsilla)                                                     | Epsilla is an open-source vector database that leverages the advanced... |
| [Faiss](/docs/integrations/vectorstores/faiss)                                                         | Facebook AI Similarity Search (FAISS) is a library for efficient simi... |
| [Faiss (Async)](/docs/integrations/vectorstores/faiss_async)                                           | Facebook AI Similarity Search (Faiss) is a library for efficient simi... |
| [FalkorDBVectorStore](/docs/integrations/vectorstores/falkordbvector)                                  | FalkorDB is an open-source graph database with integrated support for... |
| [Google AlloyDB for PostgreSQL](/docs/integrations/vectorstores/google_alloydb)                        | AlloyDB is a fully managed relational database service that offers hi... |
| [Google BigQuery Vector Search](/docs/integrations/vectorstores/google_bigquery_vector_search)         | Google Cloud BigQuery Vector Search lets you use GoogleSQL to do sema... |
| [Google Cloud SQL for MySQL](/docs/integrations/vectorstores/google_cloud_sql_mysql)                   | Cloud SQL is a fully managed relational database service that offers ... |
| [Google Cloud SQL for PostgreSQL](/docs/integrations/vectorstores/google_cloud_sql_pg)                 | Cloud SQL is a fully managed relational database service that offers ... |
| [Firestore](/docs/integrations/vectorstores/google_firestore)                                          | Firestore is a serverless document-oriented database that scales to m... |
| [Google Memorystore for Redis](/docs/integrations/vectorstores/google_memorystore_redis)               | Google Memorystore for Redis is a fully-managed service that is power... |
| [Google Spanner](/docs/integrations/vectorstores/google_spanner)                                       | Spanner is a highly scalable database that combines unlimited scalabi... |
| [Google Vertex AI Feature Store](/docs/integrations/vectorstores/google_vertex_ai_feature_store)       | Google Cloud Vertex Feature Store streamlines your ML feature managem... |
| [Google Vertex AI Vector Search](/docs/integrations/vectorstores/google_vertex_ai_vector_search)       | This notebook shows how to use functionality related to the Google Cl... |
| [Hippo](/docs/integrations/vectorstores/hippo)                                                         | Transwarp Hippo is an enterprise-level cloud-native distributed vecto... |
| [Hologres](/docs/integrations/vectorstores/hologres)                                                   | Hologres is a unified real-time data warehousing service developed by... |
| [Infinispan](/docs/integrations/vectorstores/infinispanvs)                                             | Infinispan is an open-source key-value data grid, it can work as sing... |
| [Jaguar Vector Database](/docs/integrations/vectorstores/jaguar)                                       | 1\. It is a distributed vector database                                  |
| [KDB.AI](/docs/integrations/vectorstores/kdbai)                                                        | KDB.AI is a powerful knowledge-based vector database and search engin... |
| [Kinetica](/docs/integrations/vectorstores/kinetica)                                                   | Kinetica is a database with integrated support for vector similarity ... |
| [LanceDB](/docs/integrations/vectorstores/lancedb)                                                     | LanceDB is an open-source database for vector-search built with persi... |
| [Lantern](/docs/integrations/vectorstores/lantern)                                                     | Lantern is an open-source vector similarity search for Postgres          |
| [Lindorm](/docs/integrations/vectorstores/lindorm)                                                     | This notebook covers how to get started with the Lindorm vector store.   |
| [LLMRails](/docs/integrations/vectorstores/llm_rails)                                                  | LLMRails is a API platform for building GenAI applications. It provid... |
| [ManticoreSearch VectorStore](/docs/integrations/vectorstores/manticore_search)                        | ManticoreSearch is an open-source search engine that offers fast, sca... |
| [MariaDB](/docs/integrations/vectorstores/mariadb)                                                     | LangChain's MariaDB integration (langchain-mariadb) provides vector c... |
| [Marqo](/docs/integrations/vectorstores/marqo)                                                         | This notebook shows how to use functionality related to the Marqo vec... |
| [Meilisearch](/docs/integrations/vectorstores/meilisearch)                                             | Meilisearch is an open-source, lightning-fast, and hyper relevant sea... |
| [Amazon MemoryDB](/docs/integrations/vectorstores/memorydb)                                            | Vector Search introduction and langchain integration guide.              |
| [Milvus](/docs/integrations/vectorstores/milvus)                                                       | Milvus is a database that stores, indexes, and manages massive embedd... |
| [Momento Vector Index (MVI)](/docs/integrations/vectorstores/momento_vector_index)                     | MVI: the most productive, easiest to use, serverless vector index for... |
| [MongoDB Atlas](/docs/integrations/vectorstores/mongodb_atlas)                                         | This notebook covers how to MongoDB Atlas vector search in LangChain,... |
| [MyScale](/docs/integrations/vectorstores/myscale)                                                     | MyScale is a cloud-based database optimized for AI applications and s... |
| [Neo4j Vector Index](/docs/integrations/vectorstores/neo4jvector)                                      | Neo4j is an open-source graph database with integrated support for ve... |
| [NucliaDB](/docs/integrations/vectorstores/nucliadb)                                                   | You can use a local NucliaDB instance or use Nuclia Cloud.               |
| [Oceanbase](/docs/integrations/vectorstores/oceanbase)                                                 | This notebook covers how to get started with the Oceanbase vector sto... |
| [openGauss](/docs/integrations/vectorstores/opengauss)                                                 | This notebook covers how to get started with the openGauss VectorStor... |
| [OpenSearch](/docs/integrations/vectorstores/opensearch)                                               | OpenSearch is a scalable, flexible, and extensible open-source softwa... |
| [Oracle AI Vector Search: Vector Store](/docs/integrations/vectorstores/oracle)                        | Oracle AI Vector Search is designed for Artificial Intelligence (AI) ... |
| [Pathway](/docs/integrations/vectorstores/pathway)                                                     | Pathway is an open data processing framework. It allows you to easily... |
| [Postgres Embedding](/docs/integrations/vectorstores/pgembedding)                                      | Postgres Embedding is an open-source vector similarity search for Pos... |
| [PGVecto.rs](/docs/integrations/vectorstores/pgvecto_rs)                                               | This notebook shows how to use functionality related to the Postgres ... |
| [PGVector](/docs/integrations/vectorstores/pgvector)                                                   | An implementation of LangChain vectorstore abstraction using postgres... |
| [Pinecone](/docs/integrations/vectorstores/pinecone)                                                   | Pinecone is a vector database with broad functionality.                  |
| [Qdrant](/docs/integrations/vectorstores/qdrant)                                                       | Qdrant (read: quadrant) is a vector similarity search engine. It prov... |
| [Redis](/docs/integrations/vectorstores/redis)                                                         | This notebook covers how to get started with the Redis vector store.     |
| [Relyt](/docs/integrations/vectorstores/relyt)                                                         | Relyt is a cloud native data warehousing service that is designed to ... |
| [Rockset](/docs/integrations/vectorstores/rockset)                                                     | Rockset is a real-time search and analytics database built for the cl... |
| [SAP HANA Cloud Vector Engine](/docs/integrations/vectorstores/sap_hanavector)                         | SAP HANA Cloud Vector Engine is a vector store fully integrated into ... |
| [ScaNN](/docs/integrations/vectorstores/scann)                                                         | ScaNN (Scalable Nearest Neighbors) is a method for efficient vector s... |
| [SemaDB](/docs/integrations/vectorstores/semadb)                                                       | SemaDB from SemaFind is a no fuss vector similarity database for buil... |
| [SingleStore](/docs/integrations/vectorstores/singlestore)                                             | SingleStore is a robust, high-performance distributed SQL database so... |
| [scikit-learn](/docs/integrations/vectorstores/sklearn)                                                | scikit-learn is an open-source collection of machine learning algorit... |
| [SQLiteVec](/docs/integrations/vectorstores/sqlitevec)                                                 | This notebook covers how to get started with the SQLiteVec vector sto... |
| [SQLite-VSS](/docs/integrations/vectorstores/sqlitevss)                                                | SQLite-VSS is an SQLite extension designed for vector search, emphasi... |
| [SQLServer](/docs/integrations/vectorstores/sqlserver)                                                 | Azure SQL provides a dedicatedÂ Vector data type that simplifies the c... |
| [StarRocks](/docs/integrations/vectorstores/starrocks)                                                 | StarRocks is a High-Performance Analytical Database.                     |
| [Supabase (Postgres)](/docs/integrations/vectorstores/supabase)                                        | Supabase is an open-source Firebase alternative. Supabase is built on... |
| [SurrealDB](/docs/integrations/vectorstores/surrealdb)                                                 | SurrealDB is an end-to-end cloud-native database designed for modern ... |
| [Tablestore](/docs/integrations/vectorstores/tablestore)                                               | Tablestore is a fully managed NoSQL cloud database service.              |
| [Tair](/docs/integrations/vectorstores/tair)                                                           | Tair is a cloud native in-memory database service developed by Alibab... |
| [Tencent Cloud VectorDB](/docs/integrations/vectorstores/tencentvectordb)                              | Tencent Cloud VectorDB is a fully managed, self-developed, enterprise... |
| [ThirdAI NeuralDB](/docs/integrations/vectorstores/thirdai_neuraldb)                                   | NeuralDB is a CPU-friendly and fine-tunable vector store developed by... |
| [TiDB Vector](/docs/integrations/vectorstores/tidb_vector)                                             | TiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution... |
| [Tigris](/docs/integrations/vectorstores/tigris)                                                       | Tigris is an open-source Serverless NoSQL Database and Search Platfor... |
| [TileDB](/docs/integrations/vectorstores/tiledb)                                                       | TileDB is a powerful engine for indexing and querying dense and spars... |
| [Timescale Vector (Postgres)](/docs/integrations/vectorstores/timescalevector)                         | Timescale Vector is PostgreSQL++ vector database for AI applications.    |
| [Typesense](/docs/integrations/vectorstores/typesense)                                                 | Typesense is an open-source, in-memory search engine, that you can ei... |
| [Upstash Vector](/docs/integrations/vectorstores/upstash)                                              | Upstash Vector is a serverless vector database designed for working w... |
| [USearch](/docs/integrations/vectorstores/usearch)                                                     | USearch is a Smaller &amp; Faster Single-File Vector Search Engine       |
| [Vald](/docs/integrations/vectorstores/vald)                                                           | Vald is a highly scalable distributed fast approximate nearest neighb... |
| [VDMS](/docs/integrations/vectorstores/vdms)                                                           | This notebook covers how to get started with VDMS as a vector store.     |
| [Vearch](/docs/integrations/vectorstores/vearch)                                                       | Vearch is the vector search infrastructure for deeping learning and A... |
| [Vectara](/docs/integrations/vectorstores/vectara)                                                     | Vectara is the trusted AI Assistant and Agent platform which focuses ... |
| [Vespa](/docs/integrations/vectorstores/vespa)                                                         | Vespa is a fully featured search engine and vector database. It suppo... |
| [viking DB](/docs/integrations/vectorstores/vikingdb)                                                  | viking DB is a database that stores, indexes, and manages massive emb... |
| [vlite](/docs/integrations/vectorstores/vlite)                                                         | VLite is a simple and blazing fast vector database that allows you to... |
| [Weaviate](/docs/integrations/vectorstores/weaviate)                                                   | This notebook covers how to get started with the Weaviate vector stor... |
| [Xata](/docs/integrations/vectorstores/xata)                                                           | Xata is a serverless data platform, based on PostgreSQL. It provides ... |
| [YDB](/docs/integrations/vectorstores/ydb)                                                             | YDB is a versatile open source Distributed SQL Database that combines... |
| [Yellowbrick](/docs/integrations/vectorstores/yellowbrick)                                             | Yellowbrick is an elastic, massively parallel processing (MPP) SQL da... |
| [Zep](/docs/integrations/vectorstores/zep)                                                             | Recall, understand, and extract data from chat histories. Power perso... |
| [Zep Cloud](/docs/integrations/vectorstores/zep_cloud)                                                 | Recall, understand, and extract data from chat histories. Power perso... |
| [Zilliz](/docs/integrations/vectorstores/zilliz)                                                       | Zilliz Cloud is a fully managed service on cloud for LF AI MilvusÂ®,      |

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/integrations/vectorstores/index.mdx)

* * *


- [All Vectorstores](#all-vectorstores)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/function_calling.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/function_calling.ipynb)

# How to do tool/function calling

info

We use the term tool calling interchangeably with function calling. Although function calling is sometimes meant to refer to invocations of a single function, we treat all models as though they can return multiple tool or function calls in each message.

Tool calling allows a model to respond to a given prompt by generating output that matches a user-defined schema. While the name implies that the model is performing some action, this is actually not the case! The model is coming up with the arguments to a tool, and actually running the tool (or not) is up to the user - for example, if you want to [extract output matching some schema](/docs/tutorials/extraction/) from unstructured text, you could give the model an "extraction" tool that takes parameters matching the desired schema, then treat the generated output as your final result.

A tool call includes a name, arguments dict, and an optional identifier. The arguments dict is structured `{argument_name: argument_value}`.

Many LLM providers, including [Anthropic](https://www.anthropic.com/), [Cohere](https://cohere.com/), [Google](https://cloud.google.com/vertex-ai), [Mistral](https://mistral.ai/), [OpenAI](https://openai.com/), and others, support variants of a tool calling feature. These features typically allow requests to the LLM to include available tools and their schemas, and for responses to include calls to these tools. For instance, given a search engine tool, an LLM might handle a query by first issuing a call to the search engine. The system calling the LLM can receive the tool call, execute it, and return the output to the LLM to inform its response. LangChain includes a suite of [built-in tools](/docs/integrations/tools/) and supports several methods for defining your own [custom tools](/docs/how_to/custom_tools/). Tool-calling is extremely useful for building [tool-using chains and agents](/docs/how_to/#tools), and for getting structured outputs from models more generally.

Providers adopt different conventions for formatting tool schemas and tool calls. For instance, Anthropic returns tool calls as parsed structures within a larger content block:

```python
[
  {
    "text": "<thinking>\nI should use a tool.\n</thinking>",
    "type": "text"
  },
  {
    "id": "id_value",
    "input": {"arg_name": "arg_value"},
    "name": "tool_name",
    "type": "tool_use"
  }
]
```

whereas OpenAI separates tool calls into a distinct parameter, with arguments as JSON strings:

```python
{
  "tool_calls": [
    {
      "id": "id_value",
      "function": {
        "arguments": '{"arg_name": "arg_value"}',
        "name": "tool_name"
      },
      "type": "function"
    }
  ]
}
```

LangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.

## Passing tools to LLMs[â€‹](#passing-tools-to-llms "Direct link to Passing tools to LLMs")

Chat models supporting tool calling features implement a `.bind_tools` method, which receives a list of LangChain [tool objects](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.BaseTool.html#langchain_core.tools.BaseTool) and binds them to the chat model in its expected format. Subsequent invocations of the chat model will include tool schemas in its calls to the LLM.

For example, we can define the schema for custom tools using the `@tool` decorator on Python functions:

```python
from langchain_core.tools import tool


@tool
def add(a: int, b: int) -> int:
    """Adds a and b."""
    return a + b


@tool
def multiply(a: int, b: int) -> int:
    """Multiplies a and b."""
    return a * b


tools = [add, multiply]
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

Or below, we define the schema using Pydantic:

```python
from pydantic import BaseModel, Field


# Note that the docstrings here are crucial, as they will be passed along
# to the model along with the class name.
class Add(BaseModel):
    """Add two integers together."""

    a: int = Field(..., description="First integer")
    b: int = Field(..., description="Second integer")


class Multiply(BaseModel):
    """Multiply two integers together."""

    a: int = Field(..., description="First integer")
    b: int = Field(..., description="Second integer")


tools = [Add, Multiply]
```

We can bind them to chat models as follows:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

We can use the `bind_tools()` method to handle converting `Multiply` to a "tool" and binding it to the model (i.e., passing it in each time the model is invoked).

```python
llm_with_tools = llm.bind_tools(tools)
```

## Tool calls[â€‹](#tool-calls "Direct link to Tool calls")

If tool calls are included in a LLM response, they are attached to the corresponding [message](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html#langchain_core.messages.ai.AIMessage) or [message chunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html#langchain_core.messages.ai.AIMessageChunk) as a list of [tool call](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolCall.html#langchain_core.messages.tool.ToolCall) objects in the `.tool_calls` attribute. A `ToolCall` is a typed dict that includes a tool name, dict of argument values, and (optionally) an identifier. Messages with no tool calls default to an empty list for this attribute.

Example:

```python
query = "What is 3 * 12? Also, what is 11 + 49?"

llm_with_tools.invoke(query).tool_calls
```

```output
[{'name': 'Multiply',
  'args': {'a': 3, 'b': 12},
  'id': 'call_1Tdp5wUXbYQzpkBoagGXqUTo'},
 {'name': 'Add',
  'args': {'a': 11, 'b': 49},
  'id': 'call_k9v09vYioS3X0Qg35zESuUKI'}]
```

The `.tool_calls` attribute should contain valid tool calls. Note that on occasion, model providers may output malformed tool calls (e.g., arguments that are not valid JSON). When parsing fails in these cases, instances of [InvalidToolCall](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.InvalidToolCall.html#langchain_core.messages.tool.InvalidToolCall) are populated in the `.invalid_tool_calls` attribute. An `InvalidToolCall` can have a name, string arguments, identifier, and error message.

If desired, [output parsers](/docs/how_to/#output-parsers) can further process the output. For example, we can convert back to the original Pydantic class:

```python
from langchain_core.output_parsers.openai_tools import PydanticToolsParser

chain = llm_with_tools | PydanticToolsParser(tools=[Multiply, Add])
chain.invoke(query)
```

**API Reference:**[PydanticToolsParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.openai_tools.PydanticToolsParser.html)

```output
[Multiply(a=3, b=12), Add(a=11, b=49)]
```

### Streaming[â€‹](#streaming "Direct link to Streaming")

When tools are called in a streaming context, [message chunks](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html#langchain_core.messages.ai.AIMessageChunk) will be populated with [tool call chunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolCallChunk.html#langchain_core.messages.tool.ToolCallChunk) objects in a list via the `.tool_call_chunks` attribute. A `ToolCallChunk` includes optional string fields for the tool `name`, `args`, and `id`, and includes an optional integer field `index` that can be used to join chunks together. Fields are optional because portions of a tool call may be streamed across different chunks (e.g., a chunk that includes a substring of the arguments may have null values for the tool name and id).

Because message chunks inherit from their parent message class, an [AIMessageChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html#langchain_core.messages.ai.AIMessageChunk) with tool call chunks will also include `.tool_calls` and `.invalid_tool_calls` fields. These fields are parsed best-effort from the message's tool call chunks.

Note that not all providers currently support streaming for tool calls.

Example:

```python
async for chunk in llm_with_tools.astream(query):
    print(chunk.tool_call_chunks)
```

```output
[]
[{'name': 'Multiply', 'args': '', 'id': 'call_d39MsxKM5cmeGJOoYKdGBgzc', 'index': 0}]
[{'name': None, 'args': '{"a"', 'id': None, 'index': 0}]
[{'name': None, 'args': ': 3, ', 'id': None, 'index': 0}]
[{'name': None, 'args': '"b": 1', 'id': None, 'index': 0}]
[{'name': None, 'args': '2}', 'id': None, 'index': 0}]
[{'name': 'Add', 'args': '', 'id': 'call_QJpdxD9AehKbdXzMHxgDMMhs', 'index': 1}]
[{'name': None, 'args': '{"a"', 'id': None, 'index': 1}]
[{'name': None, 'args': ': 11,', 'id': None, 'index': 1}]
[{'name': None, 'args': ' "b": ', 'id': None, 'index': 1}]
[{'name': None, 'args': '49}', 'id': None, 'index': 1}]
[]
```

Note that adding message chunks will merge their corresponding tool call chunks. This is the principle by which LangChain's various [tool output parsers](/docs/how_to/output_parser_structured/) support streaming.

For example, below we accumulate tool call chunks:

```python
first = True
async for chunk in llm_with_tools.astream(query):
    if first:
        gathered = chunk
        first = False
    else:
        gathered = gathered + chunk

    print(gathered.tool_call_chunks)
```

```output
[]
[{'name': 'Multiply', 'args': '', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a"', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, ', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 1', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{"a"', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{"a": 11,', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{"a": 11, "b": ', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{"a": 11, "b": 49}', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{"a": 11, "b": 49}', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]
```

```python
print(type(gathered.tool_call_chunks[0]["args"]))
```

```output
<class 'str'>
```

And below we accumulate tool calls to demonstrate partial parsing:

```python
first = True
async for chunk in llm_with_tools.astream(query):
    if first:
        gathered = chunk
        first = False
    else:
        gathered = gathered + chunk

    print(gathered.tool_calls)
```

```output
[]
[]
[{'name': 'Multiply', 'args': {}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]
[{'name': 'Multiply', 'args': {'a': 3}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 1}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]
```

```python
print(type(gathered.tool_calls[0]["args"]))
```

```output
<class 'dict'>
```

## Passing tool outputs to model[â€‹](#passing-tool-outputs-to-model "Direct link to Passing tool outputs to model")

If we're using the model-generated tool invocations to actually call tools and want to pass the tool results back to the model, we can do so using `ToolMessage`s.

```python
from langchain_core.messages import HumanMessage, ToolMessage

messages = [HumanMessage(query)]
ai_msg = llm_with_tools.invoke(messages)
messages.append(ai_msg)
for tool_call in ai_msg.tool_calls:
    selected_tool = {"add": add, "multiply": multiply}[tool_call["name"].lower()]
    tool_output = selected_tool.invoke(tool_call["args"])
    messages.append(ToolMessage(tool_output, tool_call_id=tool_call["id"]))
messages
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html)

```output
[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?'),
 AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_K5DsWEmgt6D08EI9AFu9NaL1', 'function': {'arguments': '{"a": 3, "b": 12}', 'name': 'Multiply'}, 'type': 'function'}, {'id': 'call_qywVrsplg0ZMv7LHYYMjyG81', 'function': {'arguments': '{"a": 11, "b": 49}', 'name': 'Add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 105, 'total_tokens': 155}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1a0b8cdd-9221-4d94-b2ed-5701f67ce9fe-0', tool_calls=[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_K5DsWEmgt6D08EI9AFu9NaL1'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_qywVrsplg0ZMv7LHYYMjyG81'}]),
 ToolMessage(content='36', tool_call_id='call_K5DsWEmgt6D08EI9AFu9NaL1'),
 ToolMessage(content='60', tool_call_id='call_qywVrsplg0ZMv7LHYYMjyG81')]
```

```python
llm_with_tools.invoke(messages)
```

```output
AIMessage(content='3 * 12 is 36 and 11 + 49 is 60.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 171, 'total_tokens': 189}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-a6c8093c-b16a-4c92-8308-7c9ac998118c-0')
```

## Few-shot prompting[â€‹](#few-shot-prompting "Direct link to Few-shot prompting")

For more complex tool use it's very useful to add few-shot examples to the prompt. We can do this by adding `AIMessage`s with `ToolCall`s and corresponding `ToolMessage`s to our prompt.

For example, even with some special instructions our model can get tripped up by order of operations:

```python
llm_with_tools.invoke(
    "Whats 119 times 8 minus 20. Don't do any math yourself, only use tools for math. Respect order of operations"
).tool_calls
```

```output
[{'name': 'Multiply',
  'args': {'a': 119, 'b': 8},
  'id': 'call_Dl3FXRVkQCFW4sUNYOe4rFr7'},
 {'name': 'Add',
  'args': {'a': 952, 'b': -20},
  'id': 'call_n03l4hmka7VZTCiP387Wud2C'}]
```

The model shouldn't be trying to add anything yet, since it technically can't know the results of 119 * 8 yet.

By adding a prompt with some examples we can correct this behavior:

```python
from langchain_core.messages import AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

examples = [
    HumanMessage(
        "What's the product of 317253 and 128472 plus four", name="example_user"
    ),
    AIMessage(
        "",
        name="example_assistant",
        tool_calls=[
            {"name": "Multiply", "args": {"x": 317253, "y": 128472}, "id": "1"}
        ],
    ),
    ToolMessage("16505054784", tool_call_id="1"),
    AIMessage(
        "",
        name="example_assistant",
        tool_calls=[{"name": "Add", "args": {"x": 16505054784, "y": 4}, "id": "2"}],
    ),
    ToolMessage("16505054788", tool_call_id="2"),
    AIMessage(
        "The product of 317253 and 128472 plus four is 16505054788",
        name="example_assistant",
    ),
]

system = """You are bad at math but are an expert at using a calculator. 

Use past tool usage as an example of how to correctly use the tools."""
few_shot_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        *examples,
        ("human", "{query}"),
    ]
)

chain = {"query": RunnablePassthrough()} | few_shot_prompt | llm_with_tools
chain.invoke("Whats 119 times 8 minus 20").tool_calls
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html)

```output
[{'name': 'Multiply',
  'args': {'a': 119, 'b': 8},
  'id': 'call_MoSgwzIhPxhclfygkYaKIsGZ'}]
```

Seems like we get the correct output this time.

Here's what the [LangSmith trace](https://smith.langchain.com/public/f70550a1-585f-4c9d-a643-13148ab1616f/r) looks like.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

- **Output parsing**: See [OpenAI Tools output parsers](/docs/how_to/output_parser_structured/) to learn about extracting the function calling API responses into various formats.
- **Structured output chains**: [Some models have constructors](/docs/how_to/structured_output/) that handle creating a structured output chain for you.
- **Tool use**: See how to construct chains and agents that call the invoked tools in [these guides](/docs/how_to/#tools).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/function_calling.ipynb)

* * *


- [Passing tools to LLMs](#passing-tools-to-llms)
- [Tool calls](#tool-calls)
  
  - [Streaming](#streaming)
- [Passing tool outputs to model](#passing-tool-outputs-to-model)
- [Few-shot prompting](#few-shot-prompting)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/callbacks_runtime.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/callbacks_runtime.ipynb)

# How to pass callbacks in at runtime

Prerequisites

This guide assumes familiarity with the following concepts:

- [Callbacks](/docs/concepts/callbacks/)
- [Custom callback handlers](/docs/how_to/custom_callbacks/)

In many cases, it is advantageous to pass in handlers instead when running the object. When we pass through [`CallbackHandlers`](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html#langchain-core-callbacks-base-basecallbackhandler) using the `callbacks` keyword arg when executing a run, those callbacks will be issued by all nested objects involved in the execution. For example, when a handler is passed through to an Agent, it will be used for all callbacks related to the agent and all the objects involved in the agent's execution, in this case, the Tools and LLM.

This prevents us from having to manually attach the handlers to each individual nested object. Here's an example:

```python
from typing import Any, Dict, List

from langchain_anthropic import ChatAnthropic
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage
from langchain_core.outputs import LLMResult
from langchain_core.prompts import ChatPromptTemplate


class LoggingHandler(BaseCallbackHandler):
    def on_chat_model_start(
        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs
    ) -> None:
        print("Chat model started")

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        print(f"Chat model ended, response: {response}")

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs
    ) -> None:
        print(f"Chain {serialized.get('name')} started")

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:
        print(f"Chain ended, outputs: {outputs}")


callbacks = [LoggingHandler()]
llm = ChatAnthropic(model="claude-3-sonnet-20240229")
prompt = ChatPromptTemplate.from_template("What is 1 + {number}?")

chain = prompt | llm

chain.invoke({"number": "2"}, config={"callbacks": callbacks})
```

**API Reference:**[ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [BaseCallbackHandler](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [LLMResult](https://python.langchain.com/api_reference/core/outputs/langchain_core.outputs.llm_result.LLMResult.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
Chain RunnableSequence started
Chain ChatPromptTemplate started
Chain ended, outputs: messages=[HumanMessage(content='What is 1 + 2?')]
Chat model started
Chat model ended, response: generations=[[ChatGeneration(text='1 + 2 = 3', message=AIMessage(content='1 + 2 = 3', response_metadata={'id': 'msg_01D8Tt5FdtBk5gLTfBPm2tac', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}}, id='run-bb0dddd8-85f3-4e6b-8553-eaa79f859ef8-0'))]] llm_output={'id': 'msg_01D8Tt5FdtBk5gLTfBPm2tac', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}} run=None
Chain ended, outputs: content='1 + 2 = 3' response_metadata={'id': 'msg_01D8Tt5FdtBk5gLTfBPm2tac', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}} id='run-bb0dddd8-85f3-4e6b-8553-eaa79f859ef8-0'
```

```output
AIMessage(content='1 + 2 = 3', response_metadata={'id': 'msg_01D8Tt5FdtBk5gLTfBPm2tac', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}}, id='run-bb0dddd8-85f3-4e6b-8553-eaa79f859ef8-0')
```

If there are already existing callbacks associated with a module, these will run in addition to any passed in at runtime.

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to pass callbacks at runtime.

Next, check out the other how-to guides in this section, such as how to [pass callbacks into a module constructor](/docs/how_to/custom_callbacks/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/callbacks_runtime.ipynb)

* * *


- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/extraction_examples.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/extraction_examples.ipynb)

# How to use reference examples when doing extraction

The quality of extractions can often be improved by providing reference examples to the LLM.

Data extraction attempts to generate [structured representations](/docs/concepts/structured_outputs/) of information found in text and other unstructured or semi-structured formats. [Tool-calling](/docs/concepts/tool_calling/) LLM features are often used in this context. This guide demonstrates how to build few-shot examples of tool calls to help steer the behavior of extraction and similar applications.

tip

While this guide focuses how to use examples with a tool calling model, this technique is generally applicable, and will work also with JSON more or prompt based techniques.

LangChain implements a [tool-call attribute](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html#langchain_core.messages.ai.AIMessage.tool_calls) on messages from LLMs that include tool calls. See our [how-to guide on tool calling](/docs/how_to/tool_calling/) for more detail. To build reference examples for data extraction, we build a chat history containing a sequence of:

- [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) containing example inputs;
- [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) containing example tool calls;
- [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html) containing example tool outputs.

LangChain adopts this convention for structuring tool calls into conversation across LLM model providers.

First we build a prompt template that includes a placeholder for these messages:

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# Define a custom prompt to provide instructions and any additional context.
# 1) You can add examples into the prompt template to improve extraction quality
# 2) Introduce additional parameters to take context into account (e.g., include metadata
#    about the document from which the text was extracted.)
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are an expert extraction algorithm. "
            "Only extract relevant information from the text. "
            "If you do not know the value of an attribute asked "
            "to extract, return null for the attribute's value.",
        ),
        # â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“
        MessagesPlaceholder("examples"),  # <-- EXAMPLES!
        # â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘â†‘
        ("human", "{text}"),
    ]
)
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)

Test out the template:

```python
from langchain_core.messages import (
    HumanMessage,
)

prompt.invoke(
    {"text": "this is some text", "examples": [HumanMessage(content="testing 1 2 3")]}
)
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

```output
ChatPromptValue(messages=[SystemMessage(content="You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.", additional_kwargs={}, response_metadata={}), HumanMessage(content='testing 1 2 3', additional_kwargs={}, response_metadata={}), HumanMessage(content='this is some text', additional_kwargs={}, response_metadata={})])
```

## Define the schema[â€‹](#define-the-schema "Direct link to Define the schema")

Let's re-use the person schema from the [extraction tutorial](/docs/tutorials/extraction/).

```python
from typing import List, Optional

from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field


class Person(BaseModel):
    """Information about a person."""

    # ^ Doc-string for the entity Person.
    # This doc-string is sent to the LLM as the description of the schema Person,
    # and it can help to improve extraction results.

    # Note that:
    # 1. Each field is an `optional` -- this allows the model to decline to extract it!
    # 2. Each field has a `description` -- this description is used by the LLM.
    # Having a good description can help improve extraction results.
    name: Optional[str] = Field(..., description="The name of the person")
    hair_color: Optional[str] = Field(
        ..., description="The color of the person's hair if known"
    )
    height_in_meters: Optional[str] = Field(..., description="Height in METERs")


class Data(BaseModel):
    """Extracted data about people."""

    # Creates a model so that we can extract multiple entities.
    people: List[Person]
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

## Define reference examples[â€‹](#define-reference-examples "Direct link to Define reference examples")

Examples can be defined as a list of input-output pairs.

Each example contains an example `input` text and an example `output` showing what should be extracted from the text.

important

This is a bit in the weeds, so feel free to skip.

The format of the example needs to match the API used (e.g., tool calling or JSON mode etc.).

Here, the formatted examples will match the format expected for the tool calling API since that's what we're using.

```python
import uuid
from typing import Dict, List, TypedDict

from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from pydantic import BaseModel, Field


class Example(TypedDict):
    """A representation of an example consisting of text input and expected tool calls.

    For extraction, the tool calls are represented as instances of pydantic model.
    """

    input: str  # This is the example text
    tool_calls: List[BaseModel]  # Instances of pydantic model that should be extracted


def tool_example_to_messages(example: Example) -> List[BaseMessage]:
    """Convert an example into a list of messages that can be fed into an LLM.

    This code is an adapter that converts our example to a list of messages
    that can be fed into a chat model.

    The list of messages per example corresponds to:

    1) HumanMessage: contains the content from which content should be extracted.
    2) AIMessage: contains the extracted information from the model
    3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.

    The ToolMessage is required because some of the chat models are hyper-optimized for agents
    rather than for an extraction use case.
    """
    messages: List[BaseMessage] = [HumanMessage(content=example["input"])]
    tool_calls = []
    for tool_call in example["tool_calls"]:
        tool_calls.append(
            {
                "id": str(uuid.uuid4()),
                "args": tool_call.dict(),
                # The name of the function right now corresponds
                # to the name of the pydantic model
                # This is implicit in the API right now,
                # and will be improved over time.
                "name": tool_call.__class__.__name__,
            },
        )
    messages.append(AIMessage(content="", tool_calls=tool_calls))
    tool_outputs = example.get("tool_outputs") or [
        "You have correctly called this tool."
    ] * len(tool_calls)
    for output, tool_call in zip(tool_outputs, tool_calls):
        messages.append(ToolMessage(content=output, tool_call_id=tool_call["id"]))
    return messages
```

**API Reference:**[AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) | [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html)

Next let's define our examples and then convert them into message format.

```python
examples = [
    (
        "The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it.",
        Data(people=[]),
    ),
    (
        "Fiona traveled far from France to Spain.",
        Data(people=[Person(name="Fiona", height_in_meters=None, hair_color=None)]),
    ),
]


messages = []

for text, tool_call in examples:
    messages.extend(
        tool_example_to_messages({"input": text, "tool_calls": [tool_call]})
    )
```

Let's test out the prompt

```python
example_prompt = prompt.invoke({"text": "this is some text", "examples": messages})

for message in example_prompt.messages:
    print(f"{message.type}: {message}")
```

```output
system: content="You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value." additional_kwargs={} response_metadata={}
human: content="The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it." additional_kwargs={} response_metadata={}
ai: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'Data', 'args': {'people': []}, 'id': '240159b1-1405-4107-a07c-3c6b91b3d5b7', 'type': 'tool_call'}]
tool: content='You have correctly called this tool.' tool_call_id='240159b1-1405-4107-a07c-3c6b91b3d5b7'
human: content='Fiona traveled far from France to Spain.' additional_kwargs={} response_metadata={}
ai: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'Data', 'args': {'people': [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]}, 'id': '3fc521e4-d1d2-4c20-bf40-e3d72f1068da', 'type': 'tool_call'}]
tool: content='You have correctly called this tool.' tool_call_id='3fc521e4-d1d2-4c20-bf40-e3d72f1068da'
human: content='this is some text' additional_kwargs={} response_metadata={}
```

## Create an extractor[â€‹](#create-an-extractor "Direct link to Create an extractor")

Let's select an LLM. Because we are using tool-calling, we will need a model that supports a tool-calling feature. See [this table](/docs/integrations/chat/) for available LLMs.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4-0125-preview", model_provider="openai", temperature=0)
```

Following the [extraction tutorial](/docs/tutorials/extraction/), we use the `.with_structured_output` method to structure model outputs according to the desired schema:

```python
runnable = prompt | llm.with_structured_output(
    schema=Data,
    method="function_calling",
    include_raw=False,
)
```

## Without examples ðŸ˜¿[â€‹](#without-examples- "Direct link to Without examples ðŸ˜¿")

Notice that even capable models can fail with a **very simple** test case!

```python
for _ in range(5):
    text = "The solar system is large, but earth has only 1 moon."
    print(runnable.invoke({"text": text, "examples": []}))
```

```````output
people=[Person(name='earth', hair_color='null', height_in_meters='null')]
``````output
people=[Person(name='earth', hair_color='null', height_in_meters='null')]
``````output
people=[]
``````output
people=[Person(name='earth', hair_color='null', height_in_meters='null')]
``````output
people=[]
```````

## With examples ðŸ˜»[â€‹](#with-examples- "Direct link to With examples ðŸ˜»")

Reference examples helps to fix the failure!

```python
for _ in range(5):
    text = "The solar system is large, but earth has only 1 moon."
    print(runnable.invoke({"text": text, "examples": messages}))
```

```````output
people=[]
``````output
people=[]
``````output
people=[]
``````output
people=[]
``````output
people=[]
```````

Note that we can see the few-shot examples as tool-calls in the [Langsmith trace](https://smith.langchain.com/public/4c436bc2-a1ce-440b-82f5-093947542e40/r).

And we retain performance on a positive sample:

```python
runnable.invoke(
    {
        "text": "My name is Harrison. My hair is black.",
        "examples": messages,
    }
)
```

```output
Data(people=[Person(name='Harrison', hair_color='black', height_in_meters=None)])
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/extraction_examples.ipynb)

* * *


- [Define the schema](#define-the-schema)
- [Define reference examples](#define-reference-examples)
- [Create an extractor](#create-an-extractor)
- [Without examples ðŸ˜¿](#without-examples-)
- [With examples ðŸ˜»](#with-examples-)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/vectorstore_retriever.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/vectorstore_retriever.ipynb)

# How to use a vectorstore as a retriever

A vector store retriever is a [retriever](/docs/concepts/retrievers/) that uses a [vector store](/docs/concepts/vectorstores/) to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever [interface](/docs/concepts/runnables/). It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store.

In this guide we will cover:

1. How to instantiate a retriever from a vectorstore;
2. How to specify the search type for the retriever;
3. How to specify additional search parameters, such as threshold scores and top-k.

## Creating a retriever from a vectorstore[â€‹](#creating-a-retriever-from-a-vectorstore "Direct link to Creating a retriever from a vectorstore")

You can build a retriever from a vectorstore using its [.as\_retriever](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html#langchain_core.vectorstores.base.VectorStore.as_retriever) method. Let's walk through an example.

First we instantiate a vectorstore. We will use an in-memory [FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) vectorstore:

```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter

loader = TextLoader("state_of_the_union.txt")

documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(texts, embeddings)
```

**API Reference:**[TextLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html) | [FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)

We can then instantiate a retriever:

```python
retriever = vectorstore.as_retriever()
```

This creates a retriever (specifically a [VectorStoreRetriever](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStoreRetriever.html)), which we can use in the usual way:

```python
docs = retriever.invoke("what did the president say about ketanji brown jackson?")
```

## Maximum marginal relevance retrieval[â€‹](#maximum-marginal-relevance-retrieval "Direct link to Maximum marginal relevance retrieval")

By default, the vector store retriever uses similarity search. If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type.

This effectively specifies what method on the underlying vectorstore is used (e.g., `similarity_search`, `max_marginal_relevance_search`, etc.).

```python
retriever = vectorstore.as_retriever(search_type="mmr")
```

```python
docs = retriever.invoke("what did the president say about ketanji brown jackson?")
```

## Passing search parameters[â€‹](#passing-search-parameters "Direct link to Passing search parameters")

We can pass parameters to the underlying vectorstore's search methods using `search_kwargs`.

### Similarity score threshold retrieval[â€‹](#similarity-score-threshold-retrieval "Direct link to Similarity score threshold retrieval")

For example, we can set a similarity score threshold and only return documents with a score above that threshold.

```python
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.5}
)
```

```python
docs = retriever.invoke("what did the president say about ketanji brown jackson?")
```

### Specifying top k[â€‹](#specifying-top-k "Direct link to Specifying top k")

We can also limit the number of documents `k` returned by the retriever.

```python
retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
```

```python
docs = retriever.invoke("what did the president say about ketanji brown jackson?")
len(docs)
```

```output
1
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/vectorstore_retriever.ipynb)

* * *


- [Creating a retriever from a vectorstore](#creating-a-retriever-from-a-vectorstore)
- [Maximum marginal relevance retrieval](#maximum-marginal-relevance-retrieval)
- [Passing search parameters](#passing-search-parameters)
  
  - [Similarity score threshold retrieval](#similarity-score-threshold-retrieval)
  - [Specifying top k](#specifying-top-k)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/tool_calling.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/tool_calling.ipynb)

# How to use chat models to call tools

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models/)
- [Tool calling](/docs/concepts/tool_calling/)
- [Tools](/docs/concepts/tools/)
- [Output parsers](/docs/concepts/output_parsers/)

[Tool calling](/docs/concepts/tool_calling/) allows a chat model to respond to a given prompt by "calling a tool".

Remember, while the name "tool calling" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.

Tool calling is a general technique that generates structured output from a model, and you can use it even when you don't intend to invoke any tools. An example use-case of that is [extraction from unstructured text](/docs/tutorials/extraction/).

![Diagram of calling a tool](/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace.png)

If you want to see how to use the model-generated tool call to actually run a tool [check out this guide](/docs/how_to/tool_results_pass_to_model/).

Supported models

Tool calling is not universal, but is supported by many popular LLM providers. You can find a [list of all models that support tool calling here](/docs/integrations/chat/).

LangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls. This guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.

## Defining tool schemas[â€‹](#defining-tool-schemas "Direct link to Defining tool schemas")

For a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it's arguments are. Chat models that support tool calling features implement a `.bind_tools()` method for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain [Tool objects](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#basetool). Subsequent invocations of the model will pass in these tool schemas along with the prompt.

### Python functions[â€‹](#python-functions "Direct link to Python functions")

Our tool schemas can be Python functions:

```python
# The function name, type hints, and docstring are all part of the tool
# schema that's passed to the model. Defining good, descriptive schemas
# is an extension of prompt engineering and is an important part of
# getting models to perform well.
def add(a: int, b: int) -> int:
    """Add two integers.

    Args:
        a: First integer
        b: Second integer
    """
    return a + b


def multiply(a: int, b: int) -> int:
    """Multiply two integers.

    Args:
        a: First integer
        b: Second integer
    """
    return a * b
```

### LangChain Tool[â€‹](#langchain-tool "Direct link to LangChain Tool")

LangChain also implements a `@tool` decorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide [here](/docs/how_to/custom_tools/#creating-tools-from-functions) for details.

### Pydantic class[â€‹](#pydantic-class "Direct link to Pydantic class")

You can equivalently define the schemas without the accompanying functions using [Pydantic](https://docs.pydantic.dev).

Note that all fields are `required` unless provided a default value.

```python
from pydantic import BaseModel, Field


class add(BaseModel):
    """Add two integers."""

    a: int = Field(..., description="First integer")
    b: int = Field(..., description="Second integer")


class multiply(BaseModel):
    """Multiply two integers."""

    a: int = Field(..., description="First integer")
    b: int = Field(..., description="Second integer")
```

### TypedDict class[â€‹](#typeddict-class "Direct link to TypedDict class")

Requires `langchain-core>=0.2.25`

Or using TypedDicts and annotations:

```python
from typing_extensions import Annotated, TypedDict


class add(TypedDict):
    """Add two integers."""

    # Annotations must have the type and can optionally include a default value and description (in that order).
    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]


class multiply(TypedDict):
    """Multiply two integers."""

    a: Annotated[int, ..., "First integer"]
    b: Annotated[int, ..., "Second integer"]


tools = [add, multiply]
```

To actually bind those schemas to a chat model, we'll use the `.bind_tools()` method. This handles converting the `add` and `multiply` schemas to the proper format for the model. The tool schema will then be passed it in each time the model is invoked.

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
llm_with_tools = llm.bind_tools(tools)

query = "What is 3 * 12?"

llm_with_tools.invoke(query)
```

```output
AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_iXj4DiW1p7WLjTAQMRO0jxMs', 'function': {'arguments': '{"a":3,"b":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 80, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0b620986-3f62-4df7-9ba3-4595089f9ad4-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_iXj4DiW1p7WLjTAQMRO0jxMs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 80, 'output_tokens': 17, 'total_tokens': 97})
```

As we can see our LLM generated arguments to a tool! You can look at the docs for [bind\_tools()](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.BaseChatOpenAI.html#langchain_openai.chat_models.base.BaseChatOpenAI.bind_tools) to learn about all the ways to customize how your LLM selects tools, as well as [this guide on how to force the LLM to call a tool](/docs/how_to/tool_choice/) rather than letting it decide.

## Tool calls[â€‹](#tool-calls "Direct link to Tool calls")

If tool calls are included in a LLM response, they are attached to the corresponding [message](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html#langchain_core.messages.ai.AIMessage) or [message chunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html#langchain_core.messages.ai.AIMessageChunk) as a list of [tool call](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolCall.html#langchain_core.messages.tool.ToolCall) objects in the `.tool_calls` attribute.

Note that chat models can call multiple tools at once.

A `ToolCall` is a typed dict that includes a tool name, dict of argument values, and (optionally) an identifier. Messages with no tool calls default to an empty list for this attribute.

```python
query = "What is 3 * 12? Also, what is 11 + 49?"

llm_with_tools.invoke(query).tool_calls
```

```output
[{'name': 'multiply',
  'args': {'a': 3, 'b': 12},
  'id': 'call_1fyhJAbJHuKQe6n0PacubGsL',
  'type': 'tool_call'},
 {'name': 'add',
  'args': {'a': 11, 'b': 49},
  'id': 'call_fc2jVkKzwuPWyU7kS9qn1hyG',
  'type': 'tool_call'}]
```

The `.tool_calls` attribute should contain valid tool calls. Note that on occasion, model providers may output malformed tool calls (e.g., arguments that are not valid JSON). When parsing fails in these cases, instances of [InvalidToolCall](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.InvalidToolCall.html#langchain_core.messages.tool.InvalidToolCall) are populated in the `.invalid_tool_calls` attribute. An `InvalidToolCall` can have a name, string arguments, identifier, and error message.

## Parsing[â€‹](#parsing "Direct link to Parsing")

If desired, [output parsers](/docs/how_to/#output-parsers) can further process the output. For example, we can convert existing values populated on the `.tool_calls` to Pydantic objects using the [PydanticToolsParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.openai_tools.PydanticToolsParser.html):

```python
from langchain_core.output_parsers import PydanticToolsParser
from pydantic import BaseModel, Field


class add(BaseModel):
    """Add two integers."""

    a: int = Field(..., description="First integer")
    b: int = Field(..., description="Second integer")


class multiply(BaseModel):
    """Multiply two integers."""

    a: int = Field(..., description="First integer")
    b: int = Field(..., description="Second integer")


chain = llm_with_tools | PydanticToolsParser(tools=[add, multiply])
chain.invoke(query)
```

**API Reference:**[PydanticToolsParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.openai_tools.PydanticToolsParser.html)

```output
[multiply(a=3, b=12), add(a=11, b=49)]
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

Now you've learned how to bind tool schemas to a chat model and have the model call the tool.

Next, check out this guide on actually using the tool by invoking the function and passing the results back to the model:

- Pass [tool results back to model](/docs/how_to/tool_results_pass_to_model/)

You can also check out some more specific uses of tool calling:

- Getting [structured outputs](/docs/how_to/structured_output/) from models
- Few shot prompting [with tools](/docs/how_to/tools_few_shot/)
- Stream [tool calls](/docs/how_to/tool_streaming/)
- Pass [runtime values to tools](/docs/how_to/tool_runtime/)

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tool_calling.ipynb)

* * *


- [Defining tool schemas](#defining-tool-schemas)
  
  - [Python functions](#python-functions)
  - [LangChain Tool](#langchain-tool)
  - [Pydantic class](#pydantic-class)
  - [TypedDict class](#typeddict-class)
- [Tool calls](#tool-calls)
- [Parsing](#parsing)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/logprobs.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/logprobs.ipynb)

# How to get log probabilities

Prerequisites

This guide assumes familiarity with the following concepts:

- [Chat models](/docs/concepts/chat_models/)
- [Tokens](/docs/concepts/tokens/)

Certain [chat models](/docs/concepts/chat_models/) can be configured to return token-level log probabilities representing the likelihood of a given token. This guide walks through how to get this information in LangChain.

## OpenAI[â€‹](#openai "Direct link to OpenAI")

Install the LangChain x OpenAI package and set your API key

```python
%pip install -qU langchain-openai
```

```python
import getpass
import os

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

For the OpenAI API to return log probabilities we need to configure the `logprobs=True` param. Then, the logprobs are included on each output [`AIMessage`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) as part of the `response_metadata`:

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini").bind(logprobs=True)

msg = llm.invoke(("human", "how are you today"))

msg.response_metadata["logprobs"]["content"][:5]
```

**API Reference:**[ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
[{'token': 'I', 'bytes': [73], 'logprob': -0.26341408, 'top_logprobs': []},
 {'token': "'m",
  'bytes': [39, 109],
  'logprob': -0.48584133,
  'top_logprobs': []},
 {'token': ' just',
  'bytes': [32, 106, 117, 115, 116],
  'logprob': -0.23484154,
  'top_logprobs': []},
 {'token': ' a',
  'bytes': [32, 97],
  'logprob': -0.0018291725,
  'top_logprobs': []},
 {'token': ' computer',
  'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],
  'logprob': -0.052299336,
  'top_logprobs': []}]
```

And are part of streamed Message chunks as well:

```python
ct = 0
full = None
for chunk in llm.stream(("human", "how are you today")):
    if ct < 5:
        full = chunk if full is None else full + chunk
        if "logprobs" in full.response_metadata:
            print(full.response_metadata["logprobs"]["content"])
    else:
        break
    ct += 1
```

```output
[]
[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}]
[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': "'m", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}]
[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': "'m", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.23778509, 'top_logprobs': []}]
[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': "'m", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.23778509, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.0022134194, 'top_logprobs': []}]
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to get logprobs from OpenAI models in LangChain.

Next, check out the other how-to guides chat models in this section, like [how to get a model to return structured output](/docs/how_to/structured_output/) or [how to track token usage](/docs/how_to/chat_token_usage_tracking/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/logprobs.ipynb)

* * *


- [OpenAI](#openai)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/semantic-chunker.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/semantic-chunker.ipynb)

# How to split text based on semantic similarity

Taken from Greg Kamradt's wonderful notebook: [5\_Levels\_Of\_Text\_Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)

All credit to him.

This guide covers how to split chunks based on their semantic similarity. If embeddings are sufficiently far apart, chunks are split.

At a high level, this splits into sentences, then groups into groups of 3 sentences, and then merges one that are similar in the embedding space.

## Install Dependencies[â€‹](#install-dependencies "Direct link to Install Dependencies")

```python
!pip install --quiet langchain_experimental langchain_openai
```

## Load Example Data[â€‹](#load-example-data "Direct link to Load Example Data")

```python
# This is a long document we can split up.
with open("state_of_the_union.txt") as f:
    state_of_the_union = f.read()
```

## Create Text Splitter[â€‹](#create-text-splitter "Direct link to Create Text Splitter")

To instantiate a [SemanticChunker](https://python.langchain.com/api_reference/experimental/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html), we must specify an embedding model. Below we will use [OpenAIEmbeddings](https://python.langchain.com/api_reference/community/embeddings/langchain_community.embeddings.openai.OpenAIEmbeddings.html).

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings

text_splitter = SemanticChunker(OpenAIEmbeddings())
```

**API Reference:**[SemanticChunker](https://python.langchain.com/api_reference/experimental/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)

## Split Text[â€‹](#split-text "Direct link to Split Text")

We split text in the usual way, e.g., by invoking `.create_documents` to create LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) objects:

```python
docs = text_splitter.create_documents([state_of_the_union])
print(docs[0].page_content)
```

```output
Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament â€œLight will win over darkness.â€ The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history weâ€™ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.
```

## Breakpoints[â€‹](#breakpoints "Direct link to Breakpoints")

This chunker works by determining when to "break" apart sentences. This is done by looking for differences in embeddings between any two sentences. When that difference is past some threshold, then they are split.

There are a few ways to determine what that threshold is, which are controlled by the `breakpoint_threshold_type` kwarg.

Note: if the resulting chunk sizes are too small/big, the additional kwargs `breakpoint_threshold_amount` and `min_chunk_size` can be used for adjustments.

### Percentile[â€‹](#percentile "Direct link to Percentile")

The default way to split is based on percentile. In this method, all differences between sentences are calculated, and then any difference greater than the X percentile is split. The default value for X is 95.0 and can be adjusted by the keyword argument `breakpoint_threshold_amount` which expects a number between 0.0 and 100.0.

```python
text_splitter = SemanticChunker(
    OpenAIEmbeddings(), breakpoint_threshold_type="percentile"
)
```

```python
docs = text_splitter.create_documents([state_of_the_union])
print(docs[0].page_content)
```

```output
Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament â€œLight will win over darkness.â€ The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history weâ€™ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.
```

```python
print(len(docs))
```

```output
26
```

### Standard Deviation[â€‹](#standard-deviation "Direct link to Standard Deviation")

In this method, any difference greater than X standard deviations is split. The default value for X is 3.0 and can be adjusted by the keyword argument `breakpoint_threshold_amount`.

```python
text_splitter = SemanticChunker(
    OpenAIEmbeddings(), breakpoint_threshold_type="standard_deviation"
)
```

```python
docs = text_splitter.create_documents([state_of_the_union])
print(docs[0].page_content)
```

```output
Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament â€œLight will win over darkness.â€ The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history weâ€™ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving. And the costs and the threats to America and the world keep rising. Thatâ€™s why the NATO Alliance was created to secure peace and stability in Europe after World War 2. The United States is a member along with 29 other nations. It matters. American diplomacy matters. American resolve matters. Putinâ€™s latest attack on Ukraine was premeditated and unprovoked. He rejected repeated efforts at diplomacy. He thought the West and NATO wouldnâ€™t respond. And he thought he could divide us at home. Putin was wrong. We were ready. Here is what we did. We prepared extensively and carefully. We spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. I spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression. We countered Russiaâ€™s lies with truth. And now that he has acted the free world is holding him accountable. Along with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. We are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. Together with our allies â€“we are right now enforcing powerful economic sanctions. We are cutting off Russiaâ€™s largest banks from the international financial system. Preventing Russiaâ€™s central bank from defending the Russian Ruble making Putinâ€™s $630 Billion â€œwar fundâ€ worthless. We are choking off Russiaâ€™s access to technology that will sap its economic strength and weaken its military for years to come. Tonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs. We are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains. And tonight I am announcing that we will join our allies in closing off American air space to all Russian flights â€“ further isolating Russia â€“ and adding an additional squeeze â€“on their economy. The Ruble has lost 30% of its value. The Russian stock market has lost 40% of its value and trading remains suspended. Russiaâ€™s economy is reeling and Putin alone is to blame. Together with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. We are giving more than $1 Billion in direct assistance to Ukraine. And we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering. Let me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine. Our forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies â€“ in the event that Putin decides to keep moving west. For that purpose weâ€™ve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia. As I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power. And we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them. Putin has unleashed violence and chaos. But while he may make gains on the battlefield â€“ he will pay a continuing high price over the long run. And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards. To all Americans, I will be honest with you, as Iâ€™ve always promised. A Russian dictator, invading a foreign country, has costs around the world. And Iâ€™m taking robust action to make sure the pain of our sanctions  is targeted at Russiaâ€™s economy. And I will use every tool at our disposal to protect American businesses and consumers. Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world. America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies. These steps will help blunt gas prices here at home. And I know the news about whatâ€™s happening can seem alarming.
```

```python
print(len(docs))
```

```output
4
```

### Interquartile[â€‹](#interquartile "Direct link to Interquartile")

In this method, the interquartile distance is used to split chunks. The interquartile range can be scaled by the keyword argument `breakpoint_threshold_amount`, the default value is 1.5.

```python
text_splitter = SemanticChunker(
    OpenAIEmbeddings(), breakpoint_threshold_type="interquartile"
)
```

```python
docs = text_splitter.create_documents([state_of_the_union])
print(docs[0].page_content)
```

```output
Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russiaâ€™s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament â€œLight will win over darkness.â€ The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history weâ€™ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.
```

```python
print(len(docs))
```

```output
25
```

### Gradient[â€‹](#gradient "Direct link to Gradient")

In this method, the gradient of distance is used to split chunks along with the percentile method. This method is useful when chunks are highly correlated with each other or specific to a domain e.g. legal or medical. The idea is to apply anomaly detection on gradient array so that the distribution become wider and easy to identify boundaries in highly semantic data. Similar to the percentile method, the split can be adjusted by the keyword argument `breakpoint_threshold_amount` which expects a number between 0.0 and 100.0, the default value is 95.0.

```python
text_splitter = SemanticChunker(
    OpenAIEmbeddings(), breakpoint_threshold_type="gradient"
)
```

```python
docs = text_splitter.create_documents([state_of_the_union])
print(docs[0].page_content)
```

```output
Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.
```

```python
print(len(docs))
```

```output
26
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/semantic-chunker.ipynb)

* * *


- [Install Dependencies](#install-dependencies)
- [Load Example Data](#load-example-data)
- [Create Text Splitter](#create-text-splitter)
- [Split Text](#split-text)
- [Breakpoints](#breakpoints)
  
  - [Percentile](#percentile)
  - [Standard Deviation](#standard-deviation)
  - [Interquartile](#interquartile)
  - [Gradient](#gradient)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/llm_router_chain.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/llm_router_chain.ipynb)

# Migrating from LLMRouterChain

The [`LLMRouterChain`](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.router.llm_router.LLMRouterChain.html) routed an input query to one of multiple destinations-- that is, given an input query, it used a LLM to select from a list of destination chains, and passed its inputs to the selected chain.

`LLMRouterChain` does not support common [chat model](/docs/concepts/chat_models/) features, such as message roles and [tool calling](/docs/concepts/tool_calling/). Under the hood, `LLMRouterChain` routes a query by instructing the LLM to generate JSON-formatted text, and parsing out the intended destination.

Consider an example from a [MultiPromptChain](/docs/versions/migrating_chains/multi_prompt_chain/), which uses `LLMRouterChain`. Below is an (example) default prompt:

```python
from langchain.chains.router.multi_prompt import MULTI_PROMPT_ROUTER_TEMPLATE

destinations = """
animals: prompt for animal expert
vegetables: prompt for a vegetable expert
"""

router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations)

print(router_template.replace("`", "'"))  # for rendering purposes
```

```output
Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.

<< FORMATTING >>
Return a markdown code snippet with a JSON object formatted to look like:
'''json
{{
    "destination": string \ name of the prompt to use or "DEFAULT"
    "next_inputs": string \ a potentially modified version of the original input
}}
'''

REMEMBER: "destination" MUST be one of the candidate prompt names specified below OR it can be "DEFAULT" if the input is not well suited for any of the candidate prompts.
REMEMBER: "next_inputs" can just be the original input if you don't think any modifications are needed.

<< CANDIDATE PROMPTS >>

animals: prompt for animal expert
vegetables: prompt for a vegetable expert


<< INPUT >>
{input}

<< OUTPUT (must include '''json at the start of the response) >>
<< OUTPUT (must end with ''') >>
```

Most of the behavior is determined via a single natural language prompt. Chat models that support [tool calling](/docs/how_to/tool_calling/) features confer a number of advantages for this task:

- Supports chat prompt templates, including messages with `system` and other roles;
- Tool-calling models are fine-tuned to generate structured output;
- Support for runnable methods like streaming and async operations.

Now let's look at `LLMRouterChain` side-by-side with an LCEL implementation that uses tool-calling. Note that for this guide we will `langchain-openai >= 0.1.20`:

```python
%pip install -qU langchain-core langchain-openai
```

```python
import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

## Legacy[â€‹](#legacy "Direct link to Legacy")

Details

```python
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

router_prompt = PromptTemplate(
    # Note: here we use the prompt template from above. Generally this would need
    # to be customized.
    template=router_template,
    input_variables=["input"],
    output_parser=RouterOutputParser(),
)

chain = LLMRouterChain.from_llm(llm, router_prompt)
```

**API Reference:**[LLMRouterChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.router.llm_router.LLMRouterChain.html) | [RouterOutputParser](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.router.llm_router.RouterOutputParser.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```python
result = chain.invoke({"input": "What color are carrots?"})

print(result["destination"])
```

```output
vegetables
```

## LCEL[â€‹](#lcel "Direct link to LCEL")

Details

```python
from operator import itemgetter
from typing import Literal

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from typing_extensions import TypedDict

llm = ChatOpenAI(model="gpt-4o-mini")

route_system = "Route the user's query to either the animal or vegetable expert."
route_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", route_system),
        ("human", "{input}"),
    ]
)


# Define schema for output:
class RouteQuery(TypedDict):
    """Route query to destination expert."""

    destination: Literal["animal", "vegetable"]


# Instead of writing formatting instructions into the prompt, we
# leverage .with_structured_output to coerce the output into a simple
# schema.
chain = route_prompt | llm.with_structured_output(RouteQuery)
```

**API Reference:**[ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```python
result = chain.invoke({"input": "What color are carrots?"})

print(result["destination"])
```

```output
vegetable
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

See [this tutorial](/docs/tutorials/llm_chain/) for more detail on building with prompt templates, LLMs, and output parsers.

Check out the [LCEL conceptual docs](/docs/concepts/lcel/) for more background information.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_chains/llm_router_chain.ipynb)

* * *


- [Legacy](#legacy)
- [LCEL](#lcel)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/tool_results_pass_to_model.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/tool_results_pass_to_model.ipynb)

# How to pass tool outputs to chat models

Prerequisites

This guide assumes familiarity with the following concepts:

- [LangChain Tools](/docs/concepts/tools/)
- [Function/tool calling](/docs/concepts/tool_calling/)
- [Using chat models to call tools](/docs/how_to/tool_calling/)
- [Defining custom tools](/docs/how_to/custom_tools/)

Some models are capable of [**tool calling**](/docs/concepts/tool_calling/) - generating arguments that conform to a specific user-provided schema. This guide will demonstrate how to use those tool calls to actually call a function and properly pass the results back to the model.

![Diagram of a tool call invocation](/assets/images/tool_invocation-7f277888701ee431a17607f1a035c080.png)

![Diagram of a tool call result](/assets/images/tool_results-71b4b90f33a56563c102d91e7821a993.png)

First, let's define our tools and our model:

Select [chat model](/docs/integrations/chat/):

OpenAIâ–¾

[OpenAI](#)

[Anthropic](#)

[Azure](#)

[Google Vertex](#)

[AWS](#)

[Groq](#)

[Cohere](#)

[NVIDIA](#)

[Fireworks AI](#)

[Mistral AI](#)

[Together AI](#)

[IBM watsonx](#)

[Databricks](#)

[xAI](#)

[Perplexity](#)

```bash
pip install -qU "langchain[openai]"
```

```python
import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain.chat_models import init_chat_model

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

```python
from langchain_core.tools import tool


@tool
def add(a: int, b: int) -> int:
    """Adds a and b."""
    return a + b


@tool
def multiply(a: int, b: int) -> int:
    """Multiplies a and b."""
    return a * b


tools = [add, multiply]

llm_with_tools = llm.bind_tools(tools)
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

Now, let's get the model to call a tool. We'll add it to a list of messages that we'll treat as conversation history:

```python
from langchain_core.messages import HumanMessage

query = "What is 3 * 12? Also, what is 11 + 49?"

messages = [HumanMessage(query)]

ai_msg = llm_with_tools.invoke(messages)

print(ai_msg.tool_calls)

messages.append(ai_msg)
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)

```output
[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_GPGPE943GORirhIAYnWv00rK', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_dm8o64ZrY3WFZHAvCh1bEJ6i', 'type': 'tool_call'}]
```

Next let's invoke the tool functions using the args the model populated!

Conveniently, if we invoke a LangChain `Tool` with a `ToolCall`, we'll automatically get back a `ToolMessage` that can be fed back to the model:

Compatibility

This functionality was added in `langchain-core == 0.2.19`. Please make sure your package is up to date.

If you are on earlier versions of `langchain-core`, you will need to extract the `args` field from the tool and construct a `ToolMessage` manually.

```python
for tool_call in ai_msg.tool_calls:
    selected_tool = {"add": add, "multiply": multiply}[tool_call["name"].lower()]
    tool_msg = selected_tool.invoke(tool_call)
    messages.append(tool_msg)

messages
```

```output
[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?'),
 AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_loT2pliJwJe3p7nkgXYF48A1', 'function': {'arguments': '{"a": 3, "b": 12}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_bG9tYZCXOeYDZf3W46TceoV4', 'function': {'arguments': '{"a": 11, "b": 49}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 87, 'total_tokens': 137}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e3db3c46-bf9e-478e-abc1-dc9a264f4afe-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_loT2pliJwJe3p7nkgXYF48A1', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_bG9tYZCXOeYDZf3W46TceoV4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 87, 'output_tokens': 50, 'total_tokens': 137}),
 ToolMessage(content='36', name='multiply', tool_call_id='call_loT2pliJwJe3p7nkgXYF48A1'),
 ToolMessage(content='60', name='add', tool_call_id='call_bG9tYZCXOeYDZf3W46TceoV4')]
```

And finally, we'll invoke the model with the tool results. The model will use this information to generate a final answer to our original query:

```python
llm_with_tools.invoke(messages)
```

```output
AIMessage(content='The result of \\(3 \\times 12\\) is 36, and the result of \\(11 + 49\\) is 60.', response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 153, 'total_tokens': 184}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-87d1ef0a-1223-4bb3-9310-7b591789323d-0', usage_metadata={'input_tokens': 153, 'output_tokens': 31, 'total_tokens': 184})
```

Note that each `ToolMessage` must include a `tool_call_id` that matches an `id` in the original tool calls that the model generates. This helps the model match tool responses with tool calls.

Tool calling agents, like those in [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/), use this basic flow to answer queries and solve tasks.

## Related[â€‹](#related "Direct link to Related")

- [LangGraph quickstart](https://langchain-ai.github.io/langgraph/tutorials/introduction/)
- Few shot prompting [with tools](/docs/how_to/tools_few_shot/)
- Stream [tool calls](/docs/how_to/tool_streaming/)
- Pass [runtime values to tools](/docs/how_to/tool_runtime/)
- Getting [structured outputs](/docs/how_to/structured_output/) from models

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/tool_results_pass_to_model.ipynb)

* * *


- [Related](#related)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_tools.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/custom_tools.ipynb)

# How to create tools

When constructing an [agent](/docs/concepts/agents/), you will need to provide it with a list of [Tools](/docs/concepts/tools/) that it can use. Besides the actual function that is called, the Tool consists of several components:

| Attribute      | Type               | Description                                                                                                                                                                    |
|----------------|--------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| name           | str                | Must be unique within a set of tools provided to an LLM or agent.                                                                                                              |
| description    | str                | Describes what the tool does. Used as context by the LLM or agent.                                                                                                             |
| args\_schema   | pydantic.BaseModel | Optional but recommended, and required if using callback handlers. It can be used to provide more information (e.g., few-shot examples) or validation for expected parameters. |
| return\_direct | boolean            | Only relevant for agents. When True, after invoking the given tool, the agent will stop and return the result direcly to the user.                                             |

LangChain supports the creation of tools from:

1. Functions;
2. LangChain [Runnables](/docs/concepts/runnables/);
3. By sub-classing from [BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html) -- This is the most flexible method, it provides the largest degree of control, at the expense of more effort and code.

Creating tools from functions may be sufficient for most use cases, and can be done via a simple [@tool decorator](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html). If more configuration is needed-- e.g., specification of both sync and async implementations-- one can also use the [StructuredTool.from\_function](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.structured.StructuredTool.html#langchain_core.tools.structured.StructuredTool.from_function) class method.

In this guide we provide an overview of these methods.

tip

Models will perform better if the tools have well chosen names, descriptions and JSON schemas.

## Creating tools from functions[â€‹](#creating-tools-from-functions "Direct link to Creating tools from functions")

### @tool decorator[â€‹](#tool-decorator "Direct link to @tool decorator")

This `@tool` decorator is the simplest way to define a custom tool. The decorator uses the function name as the tool name by default, but this can be overridden by passing a string as the first argument. Additionally, the decorator will use the function's docstring as the tool's description - so a docstring MUST be provided.

```python
from langchain_core.tools import tool


@tool
def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b


# Let's inspect some of the attributes associated with the tool.
print(multiply.name)
print(multiply.description)
print(multiply.args)
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

```output
multiply
Multiply two numbers.
{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}
```

Or create an **async** implementation, like this:

```python
from langchain_core.tools import tool


@tool
async def amultiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

Note that `@tool` supports parsing of annotations, nested schemas, and other features:

```python
from typing import Annotated, List


@tool
def multiply_by_max(
    a: Annotated[int, "scale factor"],
    b: Annotated[List[int], "list of ints over which to take maximum"],
) -> int:
    """Multiply a by the maximum of b."""
    return a * max(b)


print(multiply_by_max.args_schema.model_json_schema())
```

```output
{'description': 'Multiply a by the maximum of b.',
 'properties': {'a': {'description': 'scale factor',
   'title': 'A',
   'type': 'string'},
  'b': {'description': 'list of ints over which to take maximum',
   'items': {'type': 'integer'},
   'title': 'B',
   'type': 'array'}},
 'required': ['a', 'b'],
 'title': 'multiply_by_maxSchema',
 'type': 'object'}
```

You can also customize the tool name and JSON args by passing them into the tool decorator.

```python
from pydantic import BaseModel, Field


class CalculatorInput(BaseModel):
    a: int = Field(description="first number")
    b: int = Field(description="second number")


@tool("multiplication-tool", args_schema=CalculatorInput, return_direct=True)
def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b


# Let's inspect some of the attributes associated with the tool.
print(multiply.name)
print(multiply.description)
print(multiply.args)
print(multiply.return_direct)
```

```output
multiplication-tool
Multiply two numbers.
{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}
True
```

#### Docstring parsing[â€‹](#docstring-parsing "Direct link to Docstring parsing")

`@tool` can optionally parse [Google Style docstrings](https://google.github.io/styleguide/pyguide.html#383-functions-and-methods) and associate the docstring components (such as arg descriptions) to the relevant parts of the tool schema. To toggle this behavior, specify `parse_docstring`:

```python
@tool(parse_docstring=True)
def foo(bar: str, baz: int) -> str:
    """The foo.

    Args:
        bar: The bar.
        baz: The baz.
    """
    return bar


print(foo.args_schema.model_json_schema())
```

```output
{'description': 'The foo.',
 'properties': {'bar': {'description': 'The bar.',
   'title': 'Bar',
   'type': 'string'},
  'baz': {'description': 'The baz.', 'title': 'Baz', 'type': 'integer'}},
 'required': ['bar', 'baz'],
 'title': 'fooSchema',
 'type': 'object'}
```

caution

By default, `@tool(parse_docstring=True)` will raise `ValueError` if the docstring does not parse correctly. See [API Reference](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) for detail and examples.

### StructuredTool[â€‹](#structuredtool "Direct link to StructuredTool")

The `StructuredTool.from_function` class method provides a bit more configurability than the `@tool` decorator, without requiring much additional code.

```python
from langchain_core.tools import StructuredTool


def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b


async def amultiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b


calculator = StructuredTool.from_function(func=multiply, coroutine=amultiply)

print(calculator.invoke({"a": 2, "b": 3}))
print(await calculator.ainvoke({"a": 2, "b": 5}))
```

**API Reference:**[StructuredTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.structured.StructuredTool.html)

```output
6
10
```

To configure it:

```python
class CalculatorInput(BaseModel):
    a: int = Field(description="first number")
    b: int = Field(description="second number")


def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b


calculator = StructuredTool.from_function(
    func=multiply,
    name="Calculator",
    description="multiply numbers",
    args_schema=CalculatorInput,
    return_direct=True,
    # coroutine= ... <- you can specify an async method if desired as well
)

print(calculator.invoke({"a": 2, "b": 3}))
print(calculator.name)
print(calculator.description)
print(calculator.args)
```

```output
6
Calculator
multiply numbers
{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}
```

## Creating tools from Runnables[â€‹](#creating-tools-from-runnables "Direct link to Creating tools from Runnables")

LangChain [Runnables](/docs/concepts/runnables/) that accept string or `dict` input can be converted to tools using the [as\_tool](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.as_tool) method, which allows for the specification of names, descriptions, and additional schema information for arguments.

Example usage:

```python
from langchain_core.language_models import GenericFakeChatModel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages(
    [("human", "Hello. Please respond in the style of {answer_style}.")]
)

# Placeholder LLM
llm = GenericFakeChatModel(messages=iter(["hello matey"]))

chain = prompt | llm | StrOutputParser()

as_tool = chain.as_tool(
    name="Style responder", description="Description of when to use tool."
)
as_tool.args
```

**API Reference:**[GenericFakeChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.fake_chat_models.GenericFakeChatModel.html) | [StrOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_95770/2548361071.py:14: LangChainBetaWarning: This API is in beta and may change in the future.
  as_tool = chain.as_tool(
```

```output
{'answer_style': {'title': 'Answer Style', 'type': 'string'}}
```

See [this guide](/docs/how_to/convert_runnable_to_tool/) for more detail.

## Subclass BaseTool[â€‹](#subclass-basetool "Direct link to Subclass BaseTool")

You can define a custom tool by sub-classing from `BaseTool`. This provides maximal control over the tool definition, but requires writing more code.

```python
from typing import Optional

from langchain_core.callbacks import (
    AsyncCallbackManagerForToolRun,
    CallbackManagerForToolRun,
)
from langchain_core.tools import BaseTool
from langchain_core.tools.base import ArgsSchema
from pydantic import BaseModel, Field


class CalculatorInput(BaseModel):
    a: int = Field(description="first number")
    b: int = Field(description="second number")


# Note: It's important that every field has type hints. BaseTool is a
# Pydantic class and not having type hints can lead to unexpected behavior.
class CustomCalculatorTool(BaseTool):
    name: str = "Calculator"
    description: str = "useful for when you need to answer questions about math"
    args_schema: Optional[ArgsSchema] = CalculatorInput
    return_direct: bool = True

    def _run(
        self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool."""
        return a * b

    async def _arun(
        self,
        a: int,
        b: int,
        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
    ) -> str:
        """Use the tool asynchronously."""
        # If the calculation is cheap, you can just delegate to the sync implementation
        # as shown below.
        # If the sync calculation is expensive, you should delete the entire _arun method.
        # LangChain will automatically provide a better implementation that will
        # kick off the task in a thread to make sure it doesn't block other async code.
        return self._run(a, b, run_manager=run_manager.get_sync())
```

**API Reference:**[AsyncCallbackManagerForToolRun](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.AsyncCallbackManagerForToolRun.html) | [CallbackManagerForToolRun](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.CallbackManagerForToolRun.html) | [BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html)

```python
multiply = CustomCalculatorTool()
print(multiply.name)
print(multiply.description)
print(multiply.args)
print(multiply.return_direct)

print(multiply.invoke({"a": 2, "b": 3}))
print(await multiply.ainvoke({"a": 2, "b": 3}))
```

```output
Calculator
useful for when you need to answer questions about math
{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}
True
6
6
```

## How to create async tools[â€‹](#how-to-create-async-tools "Direct link to How to create async tools")

LangChain Tools implement the [Runnable interface ðŸƒ](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html).

All Runnables expose the `invoke` and `ainvoke` methods (as well as other methods like `batch`, `abatch`, `astream` etc).

So even if you only provide an `sync` implementation of a tool, you could still use the `ainvoke` interface, but there are some important things to know:

- LangChain's by default provides an async implementation that assumes that the function is expensive to compute, so it'll delegate execution to another thread.
- If you're working in an async codebase, you should create async tools rather than sync tools, to avoid incuring a small overhead due to that thread.
- If you need both sync and async implementations, use `StructuredTool.from_function` or sub-class from `BaseTool`.
- If implementing both sync and async, and the sync code is fast to run, override the default LangChain async implementation and simply call the sync code.
- You CANNOT and SHOULD NOT use the sync `invoke` with an `async` tool.

```python
from langchain_core.tools import StructuredTool


def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b


calculator = StructuredTool.from_function(func=multiply)

print(calculator.invoke({"a": 2, "b": 3}))
print(
    await calculator.ainvoke({"a": 2, "b": 5})
)  # Uses default LangChain async implementation incurs small overhead
```

**API Reference:**[StructuredTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.structured.StructuredTool.html)

```output
6
10
```

```python
from langchain_core.tools import StructuredTool


def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b


async def amultiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b


calculator = StructuredTool.from_function(func=multiply, coroutine=amultiply)

print(calculator.invoke({"a": 2, "b": 3}))
print(
    await calculator.ainvoke({"a": 2, "b": 5})
)  # Uses use provided amultiply without additional overhead
```

**API Reference:**[StructuredTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.structured.StructuredTool.html)

```output
6
10
```

You should not and cannot use `.invoke` when providing only an async definition.

```python
@tool
async def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b


try:
    multiply.invoke({"a": 2, "b": 3})
except NotImplementedError:
    print("Raised not implemented error. You should not be doing this.")
```

```output
Raised not implemented error. You should not be doing this.
```

## Handling Tool Errors[â€‹](#handling-tool-errors "Direct link to Handling Tool Errors")

If you're using tools with agents, you will likely need an error handling strategy, so the agent can recover from the error and continue execution.

A simple strategy is to throw a `ToolException` from inside the tool and specify an error handler using `handle_tool_error`.

When the error handler is specified, the exception will be caught and the error handler will decide which output to return from the tool.

You can set `handle_tool_error` to `True`, a string value, or a function. If it's a function, the function should take a `ToolException` as a parameter and return a value.

Please note that only raising a `ToolException` won't be effective. You need to first set the `handle_tool_error` of the tool because its default value is `False`.

```python
from langchain_core.tools import ToolException


def get_weather(city: str) -> int:
    """Get weather for the given city."""
    raise ToolException(f"Error: There is no city by the name of {city}.")
```

**API Reference:**[ToolException](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.ToolException.html)

Here's an example with the default `handle_tool_error=True` behavior.

```python
get_weather_tool = StructuredTool.from_function(
    func=get_weather,
    handle_tool_error=True,
)

get_weather_tool.invoke({"city": "foobar"})
```

```output
'Error: There is no city by the name of foobar.'
```

We can set `handle_tool_error` to a string that will always be returned.

```python
get_weather_tool = StructuredTool.from_function(
    func=get_weather,
    handle_tool_error="There is no such city, but it's probably above 0K there!",
)

get_weather_tool.invoke({"city": "foobar"})
```

```output
"There is no such city, but it's probably above 0K there!"
```

Handling the error using a function:

```python
def _handle_error(error: ToolException) -> str:
    return f"The following errors occurred during tool execution: `{error.args[0]}`"


get_weather_tool = StructuredTool.from_function(
    func=get_weather,
    handle_tool_error=_handle_error,
)

get_weather_tool.invoke({"city": "foobar"})
```

```output
'The following errors occurred during tool execution: `Error: There is no city by the name of foobar.`'
```

## Returning artifacts of Tool execution[â€‹](#returning-artifacts-of-tool-execution "Direct link to Returning artifacts of Tool execution")

Sometimes there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns custom objects like Documents, we may want to pass some view or metadata about this output to the model without passing the raw output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.

The Tool and [ToolMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html) interfaces make it possible to distinguish between the parts of the tool output meant for the model (this is the ToolMessage.content) and those parts which are meant for use outside the model (ToolMessage.artifact).

Requires `langchain-core >= 0.2.19`

This functionality was added in `langchain-core == 0.2.19`. Please make sure your package is up to date.

If we want our tool to distinguish between message content and other artifacts, we need to specify `response_format="content_and_artifact"` when defining our tool and make sure that we return a tuple of (content, artifact):

```python
import random
from typing import List, Tuple

from langchain_core.tools import tool


@tool(response_format="content_and_artifact")
def generate_random_ints(min: int, max: int, size: int) -> Tuple[str, List[int]]:
    """Generate size random ints in the range [min, max]."""
    array = [random.randint(min, max) for _ in range(size)]
    content = f"Successfully generated array of {size} random ints in [{min}, {max}]."
    return content, array
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

If we invoke our tool directly with the tool arguments, we'll get back just the content part of the output:

```python
generate_random_ints.invoke({"min": 0, "max": 9, "size": 10})
```

```output
'Successfully generated array of 10 random ints in [0, 9].'
```

If we invoke our tool with a ToolCall (like the ones generated by tool-calling models), we'll get back a ToolMessage that contains both the content and artifact generated by the Tool:

```python
generate_random_ints.invoke(
    {
        "name": "generate_random_ints",
        "args": {"min": 0, "max": 9, "size": 10},
        "id": "123",  # required
        "type": "tool_call",  # required
    }
)
```

```output
ToolMessage(content='Successfully generated array of 10 random ints in [0, 9].', name='generate_random_ints', tool_call_id='123', artifact=[4, 8, 2, 4, 1, 0, 9, 5, 8, 1])
```

We can do the same when subclassing BaseTool:

```python
from langchain_core.tools import BaseTool


class GenerateRandomFloats(BaseTool):
    name: str = "generate_random_floats"
    description: str = "Generate size random floats in the range [min, max]."
    response_format: str = "content_and_artifact"

    ndigits: int = 2

    def _run(self, min: float, max: float, size: int) -> Tuple[str, List[float]]:
        range_ = max - min
        array = [
            round(min + (range_ * random.random()), ndigits=self.ndigits)
            for _ in range(size)
        ]
        content = f"Generated {size} floats in [{min}, {max}], rounded to {self.ndigits} decimals."
        return content, array

    # Optionally define an equivalent async method

    # async def _arun(self, min: float, max: float, size: int) -> Tuple[str, List[float]]:
    #     ...
```

**API Reference:**[BaseTool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html)

```python
rand_gen = GenerateRandomFloats(ndigits=4)

rand_gen.invoke(
    {
        "name": "generate_random_floats",
        "args": {"min": 0.1, "max": 3.3333, "size": 3},
        "id": "123",
        "type": "tool_call",
    }
)
```

```output
ToolMessage(content='Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.', name='generate_random_floats', tool_call_id='123', artifact=[1.5566, 0.5134, 2.7914])
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/custom_tools.ipynb)

* * *


- [Creating tools from functions](#creating-tools-from-functions)
  
  - [@tool decorator](#tool-decorator)
  - [StructuredTool](#structuredtool)
- [Creating tools from Runnables](#creating-tools-from-runnables)
- [Subclass BaseTool](#subclass-basetool)
- [How to create async tools](#how-to-create-async-tools)
- [Handling Tool Errors](#handling-tool-errors)
- [Returning artifacts of Tool execution](#returning-artifacts-of-tool-execution)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/migrate_agent.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/migrate_agent.ipynb)

# How to migrate from legacy LangChain agents to LangGraph

Prerequisites

This guide assumes familiarity with the following concepts:

- [Agents](/docs/concepts/agents/)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [Tool calling](/docs/how_to/tool_calling/)

Here we focus on how to move from legacy LangChain agents to more flexible [LangGraph](https://langchain-ai.github.io/langgraph/) agents. LangChain agents (the [AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html#langchain.agents.agent.AgentExecutor) in particular) have multiple configuration parameters. In this notebook we will show how those parameters map to the LangGraph react agent executor using the [create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) prebuilt helper method.

note

In LangGraph, the graph replaces LangChain's agent executor. It manages the agent's cycles and tracks the scratchpad as messages within its state. The LangChain "agent" corresponds to the prompt and LLM you've provided.

#### Prerequisites[â€‹](#prerequisites "Direct link to Prerequisites")

This how-to guide uses OpenAI as the LLM. Install the dependencies to run.

```python
%%capture --no-stderr
%pip install -U langgraph langchain langchain-openai
```

Then, set your OpenAI API key.

```python
import getpass
import os

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API key:\n")
```

## Basic Usage[â€‹](#basic-usage "Direct link to Basic Usage")

For basic creation and usage of a tool-calling ReAct-style agent, the functionality is the same. First, let's define a model and tool(s), then we'll use those to create an agent.

```python
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o")


@tool
def magic_function(input: int) -> int:
    """Applies a magic function to an input."""
    return input + 2


tools = [magic_function]


query = "what is the value of magic_function(3)?"
```

**API Reference:**[tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

For the LangChain [AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html#langchain.agents.agent.AgentExecutor), we define a prompt with a placeholder for the agent's scratchpad. The agent can be invoked as follows:

```python
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant"),
        ("human", "{input}"),
        # Placeholders fill up a **list** of messages
        ("placeholder", "{agent_scratchpad}"),
    ]
)


agent = create_tool_calling_agent(model, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools)

agent_executor.invoke({"input": query})
```

**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create\_tool\_calling\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
{'input': 'what is the value of magic_function(3)?',
 'output': 'The value of `magic_function(3)` is 5.'}
```

LangGraph's [react agent executor](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) manages a state that is defined by a list of messages. It will continue to process the list until there are no tool calls in the agent's output. To kick it off, we input a list of messages. The output will contain the entire state of the graph-- in this case, the conversation history.

```python
from langgraph.prebuilt import create_react_agent

langgraph_agent_executor = create_react_agent(model, tools)


messages = langgraph_agent_executor.invoke({"messages": [("human", query)]})
{
    "input": query,
    "output": messages["messages"][-1].content,
}
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```output
{'input': 'what is the value of magic_function(3)?',
 'output': 'The value of `magic_function(3)` is 5.'}
```

```python
message_history = messages["messages"]

new_query = "Pardon?"

messages = langgraph_agent_executor.invoke(
    {"messages": message_history + [("human", new_query)]}
)
{
    "input": new_query,
    "output": messages["messages"][-1].content,
}
```

```output
{'input': 'Pardon?',
 'output': 'The result of applying `magic_function` to the input value 3 is 5.'}
```

## Prompt Templates[â€‹](#prompt-templates "Direct link to Prompt Templates")

With legacy LangChain agents you have to pass in a prompt template. You can use this to control the agent.

With LangGraph [react agent executor](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent), by default there is no prompt. You can achieve similar control over the agent in a few ways:

1. Pass in a system message as input
2. Initialize the agent with a system message
3. Initialize the agent with a function to transform messages in the graph state before passing to the model.
4. Initialize the agent with a [Runnable](/docs/concepts/lcel/) to transform messages in the graph state before passing to the model. This includes passing prompt templates as well.

Let's take a look at all of these below. We will pass in custom instructions to get the agent to respond in Spanish.

First up, using `AgentExecutor`:

```python
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant. Respond only in Spanish."),
        ("human", "{input}"),
        # Placeholders fill up a **list** of messages
        ("placeholder", "{agent_scratchpad}"),
    ]
)


agent = create_tool_calling_agent(model, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools)

agent_executor.invoke({"input": query})
```

```output
{'input': 'what is the value of magic_function(3)?',
 'output': 'El valor de magic_function(3) es 5.'}
```

Now, let's pass a custom system message to [react agent executor](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent).

LangGraph's prebuilt `create_react_agent` does not take a prompt template directly as a parameter, but instead takes a [`prompt`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) parameter. This modifies the graph state before the llm is called, and can be one of four values:

- A `SystemMessage`, which is added to the beginning of the list of messages.
- A `string`, which is converted to a `SystemMessage` and added to the beginning of the list of messages.
- A `Callable`, which should take in full graph state. The output is then passed to the language model.
- Or a [`Runnable`](/docs/concepts/lcel/), which should take in full graph state. The output is then passed to the language model.

Here's how it looks in action:

```python
from langchain_core.messages import SystemMessage
from langgraph.prebuilt import create_react_agent

system_message = "You are a helpful assistant. Respond only in Spanish."
# This could also be a SystemMessage object
# system_message = SystemMessage(content="You are a helpful assistant. Respond only in Spanish.")

langgraph_agent_executor = create_react_agent(model, tools, prompt=system_message)


messages = langgraph_agent_executor.invoke({"messages": [("user", query)]})
```

**API Reference:**[SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

We can also pass in an arbitrary function or a runnable. This function/runnable should take in a graph state and output a list of messages. We can do all types of arbitrary formatting of messages here. In this case, let's add a SystemMessage to the start of the list of messages and append another user message at the end.

```python
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.prebuilt import create_react_agent
from langgraph.prebuilt.chat_agent_executor import AgentState

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant. Respond only in Spanish."),
        ("placeholder", "{messages}"),
        ("user", "Also say 'Pandamonium!' after the answer."),
    ]
)

# alternatively, this can be passed as a function, e.g.
# def prompt(state: AgentState):
#     return (
#         [SystemMessage(content="You are a helpful assistant. Respond only in Spanish.")] +
#         state["messages"] +
#         [HumanMessage(content="Also say 'Pandamonium!' after the answer.")]
#     )


langgraph_agent_executor = create_react_agent(model, tools, prompt=prompt)


messages = langgraph_agent_executor.invoke({"messages": [("human", query)]})
print(
    {
        "input": query,
        "output": messages["messages"][-1].content,
    }
)
```

**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) | [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```output
{'input': 'what is the value of magic_function(3)?', 'output': 'El valor de magic_function(3) es 5. Â¡Pandamonium!'}
```

## Memory[â€‹](#memory "Direct link to Memory")

### In LangChain[â€‹](#in-langchain "Direct link to In LangChain")

With LangChain's [AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html#langchain.agents.agent.AgentExecutor.iter), you could add chat [Memory](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html#langchain.agents.agent.AgentExecutor.memory) so it can engage in a multi-turn conversation.

```python
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o")
memory = InMemoryChatMessageHistory(session_id="test-session")
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        # First put the history
        ("placeholder", "{chat_history}"),
        # Then the new input
        ("human", "{input}"),
        # Finally the scratchpad
        ("placeholder", "{agent_scratchpad}"),
    ]
)


@tool
def magic_function(input: int) -> int:
    """Applies a magic function to an input."""
    return input + 2


tools = [magic_function]


agent = create_tool_calling_agent(model, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools)

agent_with_chat_history = RunnableWithMessageHistory(
    agent_executor,
    # This is needed because in most real world scenarios, a session id is needed
    # It isn't really used here because we are using a simple in memory ChatMessageHistory
    lambda session_id: memory,
    input_messages_key="input",
    history_messages_key="chat_history",
)

config = {"configurable": {"session_id": "test-session"}}
print(
    agent_with_chat_history.invoke(
        {"input": "Hi, I'm polly! What's the output of magic_function of 3?"}, config
    )["output"]
)
print("---")
print(agent_with_chat_history.invoke({"input": "Remember my name?"}, config)["output"])
print("---")
print(
    agent_with_chat_history.invoke({"input": "what was that output again?"}, config)[
        "output"
    ]
)
```

**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create\_tool\_calling\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) | [InMemoryChatMessageHistory](https://python.langchain.com/api_reference/core/chat_history/langchain_core.chat_history.InMemoryChatMessageHistory.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnableWithMessageHistory](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
The output of the magic function when the input is 3 is 5.
---
Yes, you mentioned your name is Polly.
---
The output of the magic function when the input is 3 is 5.
```

### In LangGraph[â€‹](#in-langgraph "Direct link to In LangGraph")

Memory is just [persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/), aka [checkpointing](https://langchain-ai.github.io/langgraph/reference/checkpoints/).

Add a `checkpointer` to the agent and you get chat memory for free.

```python
from langgraph.checkpoint.memory import MemorySaver  # an in-memory checkpointer
from langgraph.prebuilt import create_react_agent

system_message = "You are a helpful assistant."
# This could also be a SystemMessage object
# system_message = SystemMessage(content="You are a helpful assistant. Respond only in Spanish.")

memory = MemorySaver()
langgraph_agent_executor = create_react_agent(
    model, tools, prompt=system_message, checkpointer=memory
)

config = {"configurable": {"thread_id": "test-thread"}}
print(
    langgraph_agent_executor.invoke(
        {
            "messages": [
                ("user", "Hi, I'm polly! What's the output of magic_function of 3?")
            ]
        },
        config,
    )["messages"][-1].content
)
print("---")
print(
    langgraph_agent_executor.invoke(
        {"messages": [("user", "Remember my name?")]}, config
    )["messages"][-1].content
)
print("---")
print(
    langgraph_agent_executor.invoke(
        {"messages": [("user", "what was that output again?")]}, config
    )["messages"][-1].content
)
```

**API Reference:**[MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```output
The output of the magic function for the input 3 is 5.
---
Yes, you mentioned that your name is Polly.
---
The output of the magic function for the input 3 was 5.
```

## Iterating through steps[â€‹](#iterating-through-steps "Direct link to Iterating through steps")

### In LangChain[â€‹](#in-langchain-1 "Direct link to In LangChain")

With LangChain's [AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html#langchain.agents.agent.AgentExecutor.iter), you could iterate over the steps using the [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) (or async `astream`) methods or the [iter](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html#langchain.agents.agent.AgentExecutor.iter) method. LangGraph supports stepwise iteration using [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream)

```python
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o")


prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("human", "{input}"),
        # Placeholders fill up a **list** of messages
        ("placeholder", "{agent_scratchpad}"),
    ]
)


@tool
def magic_function(input: int) -> int:
    """Applies a magic function to an input."""
    return input + 2


tools = [magic_function]

agent = create_tool_calling_agent(model, tools, prompt=prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools)

for step in agent_executor.stream({"input": query}):
    print(step)
```

**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create\_tool\_calling\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
{'actions': [ToolAgentAction(tool='magic_function', tool_input={'input': 3}, log="\nInvoking: `magic_function` with `{'input': 3}`\n\n\n", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-7a3a5ada-52ec-4df0-bf7d-81e5051b01b4', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{"input":3}', 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_yyetzabaDBRX9Ml2KyqfKzZM')], 'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-7a3a5ada-52ec-4df0-bf7d-81e5051b01b4', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{"input":3}', 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'index': 0, 'type': 'tool_call_chunk'}])]}
{'steps': [AgentStep(action=ToolAgentAction(tool='magic_function', tool_input={'input': 3}, log="\nInvoking: `magic_function` with `{'input': 3}`\n\n\n", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-7a3a5ada-52ec-4df0-bf7d-81e5051b01b4', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{"input":3}', 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_yyetzabaDBRX9Ml2KyqfKzZM'), observation=5)], 'messages': [FunctionMessage(content='5', additional_kwargs={}, response_metadata={}, name='magic_function')]}
{'output': 'The value of `magic_function(3)` is 5.', 'messages': [AIMessage(content='The value of `magic_function(3)` is 5.', additional_kwargs={}, response_metadata={})]}
```

### In LangGraph[â€‹](#in-langgraph-1 "Direct link to In LangGraph")

In LangGraph, things are handled natively using [stream](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.graph.CompiledGraph.stream) or the asynchronous `astream` method.

```python
from langgraph.prebuilt import create_react_agent
from langgraph.prebuilt.chat_agent_executor import AgentState

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("placeholder", "{messages}"),
    ]
)

langgraph_agent_executor = create_react_agent(model, tools, prompt=prompt)

for step in langgraph_agent_executor.stream(
    {"messages": [("human", query)]}, stream_mode="updates"
):
    print(step)
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```output
{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IHTMrjvIHn8gFOX42FstIpr9', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 61, 'total_tokens': 75, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1a6970da-163a-4e4d-b9b7-7e73b1057f42-0', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_IHTMrjvIHn8gFOX42FstIpr9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 61, 'output_tokens': 14, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}
{'tools': {'messages': [ToolMessage(content='5', name='magic_function', id='51a9d3e4-734d-426f-a5a1-c6597e4efe25', tool_call_id='call_IHTMrjvIHn8gFOX42FstIpr9')]}}
{'agent': {'messages': [AIMessage(content='The value of `magic_function(3)` is 5.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 84, 'total_tokens': 98, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a20a4ee344', 'finish_reason': 'stop', 'logprobs': None}, id='run-73001576-a3dc-4552-8d81-c9ce8aec05b3-0', usage_metadata={'input_tokens': 84, 'output_tokens': 14, 'total_tokens': 98, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}
```

## `return_intermediate_steps`[â€‹](#return_intermediate_steps "Direct link to return_intermediate_steps")

### In LangChain[â€‹](#in-langchain-2 "Direct link to In LangChain")

Setting this parameter on AgentExecutor allows users to access intermediate\_steps, which pairs agent actions (e.g., tool invocations) with their outcomes.

```python
agent_executor = AgentExecutor(agent=agent, tools=tools, return_intermediate_steps=True)
result = agent_executor.invoke({"input": query})
print(result["intermediate_steps"])
```

```output
[(ToolAgentAction(tool='magic_function', tool_input={'input': 3}, log="\nInvoking: `magic_function` with `{'input': 3}`\n\n\n", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_njTvl2RsVf4q1aMUxoYnJuK1', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-c9dfe3ab-2db6-4592-851e-89e056aeab32', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_njTvl2RsVf4q1aMUxoYnJuK1', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{"input":3}', 'id': 'call_njTvl2RsVf4q1aMUxoYnJuK1', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_njTvl2RsVf4q1aMUxoYnJuK1'), 5)]
```

### In LangGraph[â€‹](#in-langgraph-2 "Direct link to In LangGraph")

By default the [react agent executor](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) in LangGraph appends all messages to the central state. Therefore, it is easy to see any intermediate steps by just looking at the full state.

```python
from langgraph.prebuilt import create_react_agent

langgraph_agent_executor = create_react_agent(model, tools=tools)

messages = langgraph_agent_executor.invoke({"messages": [("human", query)]})

messages
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```output
{'messages': [HumanMessage(content='what is the value of magic_function(3)?', additional_kwargs={}, response_metadata={}, id='1abb52c2-4bc2-4d82-bd32-5a24c3976b0f'),
  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_XfQD6C7rAalcmicQubkhJVFq', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a20a4ee344', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-34f02786-5b5c-4bb1-bd9e-406c81944a24-0', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_XfQD6C7rAalcmicQubkhJVFq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),
  ToolMessage(content='5', name='magic_function', id='cbc9fadf-1962-4ed7-b476-348c774652be', tool_call_id='call_XfQD6C7rAalcmicQubkhJVFq'),
  AIMessage(content='The value of `magic_function(3)` is 5.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 78, 'total_tokens': 92, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'stop', 'logprobs': None}, id='run-547e03d2-872d-4008-a38d-b7f739a77df5-0', usage_metadata={'input_tokens': 78, 'output_tokens': 14, 'total_tokens': 92, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}
```

## `max_iterations`[â€‹](#max_iterations "Direct link to max_iterations")

### In LangChain[â€‹](#in-langchain-3 "Direct link to In LangChain")

`AgentExecutor` implements a `max_iterations` parameter, allowing users to abort a run that exceeds a specified number of iterations.

```python
@tool
def magic_function(input: str) -> str:
    """Applies a magic function to an input."""
    return "Sorry, there was an error. Please try again."


tools = [magic_function]
```

```python
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant. Respond only in Spanish."),
        ("human", "{input}"),
        # Placeholders fill up a **list** of messages
        ("placeholder", "{agent_scratchpad}"),
    ]
)

agent = create_tool_calling_agent(model, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    max_iterations=3,
)

agent_executor.invoke({"input": query})
```

```output


[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3mLo siento, no puedo decirte directamente el valor de `magic_function(3)`. Si deseas, puedo usar la funciÃ³n mÃ¡gica para calcularlo. Â¿Te gustarÃ­a que lo hiciera?[0m

[1m> Finished chain.[0m
```

```output
{'input': 'what is the value of magic_function(3)?',
 'output': 'Lo siento, no puedo decirte directamente el valor de `magic_function(3)`. Si deseas, puedo usar la funciÃ³n mÃ¡gica para calcularlo. Â¿Te gustarÃ­a que lo hiciera?'}
```

### In LangGraph[â€‹](#in-langgraph-3 "Direct link to In LangGraph")

In LangGraph this is controlled via `recursion_limit` configuration parameter.

Note that in `AgentExecutor`, an "iteration" includes a full turn of tool invocation and execution. In LangGraph, each step contributes to the recursion limit, so we will need to multiply by two (and add one) to get equivalent results.

If the recursion limit is reached, LangGraph raises a specific exception type, that we can catch and manage similarly to AgentExecutor.

```python
from langgraph.errors import GraphRecursionError
from langgraph.prebuilt import create_react_agent

RECURSION_LIMIT = 2 * 3 + 1

langgraph_agent_executor = create_react_agent(model, tools=tools)

try:
    for chunk in langgraph_agent_executor.stream(
        {"messages": [("human", query)]},
        {"recursion_limit": RECURSION_LIMIT},
        stream_mode="values",
    ):
        print(chunk["messages"][-1])
except GraphRecursionError:
    print({"input": query, "output": "Agent stopped due to max iterations."})
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```output
content='what is the value of magic_function(3)?' additional_kwargs={} response_metadata={} id='c2489fe8-e69c-4163-876d-3cce26b28521'
content='' additional_kwargs={'tool_calls': [{'id': 'call_OyNTcO6SDAvZcBlIEknPRrTR', 'function': {'arguments': '{"input":"3"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-b65504bb-fa23-4f8a-8d6c-7edb6d16e7ff-0' tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_OyNTcO6SDAvZcBlIEknPRrTR', 'type': 'tool_call'}] usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}
content='Sorry, there was an error. Please try again.' name='magic_function' id='f00e0bff-54fe-4726-a1a7-127a59d8f7ed' tool_call_id='call_OyNTcO6SDAvZcBlIEknPRrTR'
content="It seems there was an error when trying to compute the value of the magic function with input 3. Let's try again." additional_kwargs={'tool_calls': [{'id': 'call_Q020rQoJh4cnh8WglIMnDm4z', 'function': {'arguments': '{"input":"3"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 88, 'total_tokens': 128, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-556d8cb2-b47a-4826-b17d-b520982c2475-0' tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_Q020rQoJh4cnh8WglIMnDm4z', 'type': 'tool_call'}] usage_metadata={'input_tokens': 88, 'output_tokens': 40, 'total_tokens': 128, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}
content='Sorry, there was an error. Please try again.' name='magic_function' id='777212cd-8381-44db-9762-3f81951ea73e' tool_call_id='call_Q020rQoJh4cnh8WglIMnDm4z'
content="It seems there is a persistent issue in computing the value of the magic function with the input 3. Unfortunately, I can't provide the value at this time. If you have any other questions or need further assistance, feel free to ask!" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 150, 'total_tokens': 199, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'stop', 'logprobs': None} id='run-92ec0b90-bc8e-4851-9139-f1d976145ab7-0' usage_metadata={'input_tokens': 150, 'output_tokens': 49, 'total_tokens': 199, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}
```

## `max_execution_time`[â€‹](#max_execution_time "Direct link to max_execution_time")

### In LangChain[â€‹](#in-langchain-4 "Direct link to In LangChain")

`AgentExecutor` implements a `max_execution_time` parameter, allowing users to abort a run that exceeds a total time limit.

```python
import time


@tool
def magic_function(input: str) -> str:
    """Applies a magic function to an input."""
    time.sleep(2.5)
    return "Sorry, there was an error. Please try again."


tools = [magic_function]

agent = create_tool_calling_agent(model, tools, prompt)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    max_execution_time=2,
    verbose=True,
)

agent_executor.invoke({"input": query})
```

```output


[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3mLo siento, no tengo la capacidad de evaluar directamente una funciÃ³n llamada "magic_function" con el valor 3. Sin embargo, si me proporcionas mÃ¡s detalles sobre quÃ© hace la funciÃ³n o cÃ³mo estÃ¡ definida, podrÃ­a intentar ayudarte a comprender su comportamiento o resolverlo de otra manera.[0m

[1m> Finished chain.[0m
```

```output
{'input': 'what is the value of magic_function(3)?',
 'output': 'Lo siento, no tengo la capacidad de evaluar directamente una funciÃ³n llamada "magic_function" con el valor 3. Sin embargo, si me proporcionas mÃ¡s detalles sobre quÃ© hace la funciÃ³n o cÃ³mo estÃ¡ definida, podrÃ­a intentar ayudarte a comprender su comportamiento o resolverlo de otra manera.'}
```

### In LangGraph[â€‹](#in-langgraph-4 "Direct link to In LangGraph")

With LangGraph's react agent, you can control timeouts on two levels.

You can set a `step_timeout` to bound each **step**:

```python
from langgraph.prebuilt import create_react_agent

langgraph_agent_executor = create_react_agent(model, tools=tools)
# Set the max timeout for each step here
langgraph_agent_executor.step_timeout = 2

try:
    for chunk in langgraph_agent_executor.stream({"messages": [("human", query)]}):
        print(chunk)
        print("------")
except TimeoutError:
    print({"input": query, "output": "Agent stopped due to a step timeout."})
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```output
{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_UuxSgpGaqzX84sNlKzCVOiRO', 'function': {'arguments': '{"input":"3"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-24c94cbd-2962-48cf-a447-af888eb6ef86-0', tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_UuxSgpGaqzX84sNlKzCVOiRO', 'type': 'tool_call'}], usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}
------
{'input': 'what is the value of magic_function(3)?', 'output': 'Agent stopped due to a step timeout.'}
```

The other way to set a single max timeout for an entire run is to directly use the python stdlib [asyncio](https://docs.python.org/3/library/asyncio.html) library.

```python
import asyncio

from langgraph.prebuilt import create_react_agent

langgraph_agent_executor = create_react_agent(model, tools=tools)


async def stream(langgraph_agent_executor, inputs):
    async for chunk in langgraph_agent_executor.astream(
        {"messages": [("human", query)]}
    ):
        print(chunk)
        print("------")


try:
    task = asyncio.create_task(
        stream(langgraph_agent_executor, {"messages": [("human", query)]})
    )
    await asyncio.wait_for(task, timeout=3)
except asyncio.TimeoutError:
    print("Task Cancelled.")
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```output
{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_km17xvoY7wJ5yNnXhb5V9D3I', 'function': {'arguments': '{"input":"3"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_45c6de4934', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b44a04e5-9b68-4020-be36-98de1593eefc-0', tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_km17xvoY7wJ5yNnXhb5V9D3I', 'type': 'tool_call'}], usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}
------
Task Cancelled.
```

## `early_stopping_method`[â€‹](#early_stopping_method "Direct link to early_stopping_method")

### In LangChain[â€‹](#in-langchain-5 "Direct link to In LangChain")

With LangChain's [AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html#langchain.agents.agent.AgentExecutor.iter), you could configure an [early\_stopping\_method](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html#langchain.agents.agent.AgentExecutor.early_stopping_method) to either return a string saying "Agent stopped due to iteration limit or time limit." (`"force"`) or prompt the LLM a final time to respond (`"generate"`).

```python
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o")


prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("human", "{input}"),
        # Placeholders fill up a **list** of messages
        ("placeholder", "{agent_scratchpad}"),
    ]
)


@tool
def magic_function(input: int) -> int:
    """Applies a magic function to an input."""
    return "Sorry there was an error, please try again."


tools = [magic_function]

agent = create_tool_calling_agent(model, tools, prompt=prompt)
agent_executor = AgentExecutor(
    agent=agent, tools=tools, early_stopping_method="force", max_iterations=1
)

result = agent_executor.invoke({"input": query})
print("Output with early_stopping_method='force':")
print(result["output"])
```

**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create\_tool\_calling\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```output
Output with early_stopping_method='force':
Agent stopped due to max iterations.
```

### In LangGraph[â€‹](#in-langgraph-5 "Direct link to In LangGraph")

In LangGraph, you can explicitly handle the response behavior outside the agent, since the full state can be accessed.

```python
from langgraph.errors import GraphRecursionError
from langgraph.prebuilt import create_react_agent

RECURSION_LIMIT = 2 * 1 + 1

langgraph_agent_executor = create_react_agent(model, tools=tools)

try:
    for chunk in langgraph_agent_executor.stream(
        {"messages": [("human", query)]},
        {"recursion_limit": RECURSION_LIMIT},
        stream_mode="values",
    ):
        print(chunk["messages"][-1])
except GraphRecursionError:
    print({"input": query, "output": "Agent stopped due to max iterations."})
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```output
content='what is the value of magic_function(3)?' additional_kwargs={} response_metadata={} id='81fd2e50-1e6a-4871-87aa-b7c1225913a4'
content='' additional_kwargs={'tool_calls': [{'id': 'call_aaEzj3aO1RTnB0uoc9rYUIhi', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-476bc4b1-b7bf-4607-a31c-ddf09dc814c5-0' tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_aaEzj3aO1RTnB0uoc9rYUIhi', 'type': 'tool_call'}] usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}
content='Sorry there was an error, please try again.' name='magic_function' id='dcbe7e3e-0ed4-467d-a729-2f45916ff44f' tool_call_id='call_aaEzj3aO1RTnB0uoc9rYUIhi'
content="It seems there was an error when trying to compute the value of `magic_function(3)`. Let's try that again." additional_kwargs={'tool_calls': [{'id': 'call_jr4R8uJn2pdXF5GZC2Dg3YWS', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 87, 'total_tokens': 127, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-d94b8932-6e9e-4ab1-99f7-7dca89887ffe-0' tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_jr4R8uJn2pdXF5GZC2Dg3YWS', 'type': 'tool_call'}] usage_metadata={'input_tokens': 87, 'output_tokens': 40, 'total_tokens': 127, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}
{'input': 'what is the value of magic_function(3)?', 'output': 'Agent stopped due to max iterations.'}
```

## `trim_intermediate_steps`[â€‹](#trim_intermediate_steps "Direct link to trim_intermediate_steps")

### In LangChain[â€‹](#in-langchain-6 "Direct link to In LangChain")

With LangChain's [AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html#langchain.agents.agent.AgentExecutor), you could trim the intermediate steps of long-running agents using [trim\_intermediate\_steps](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html#langchain.agents.agent.AgentExecutor.trim_intermediate_steps), which is either an integer (indicating the agent should keep the last N steps) or a custom function.

For instance, we could trim the value so the agent only sees the most recent intermediate step.

```python
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o")


prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("human", "{input}"),
        # Placeholders fill up a **list** of messages
        ("placeholder", "{agent_scratchpad}"),
    ]
)


magic_step_num = 1


@tool
def magic_function(input: int) -> int:
    """Applies a magic function to an input."""
    global magic_step_num
    print(f"Call number: {magic_step_num}")
    magic_step_num += 1
    return input + magic_step_num


tools = [magic_function]

agent = create_tool_calling_agent(model, tools, prompt=prompt)


def trim_steps(steps: list):
    # Let's give the agent amnesia
    return []


agent_executor = AgentExecutor(
    agent=agent, tools=tools, trim_intermediate_steps=trim_steps
)


query = "Call the magic function 4 times in sequence with the value 3. You cannot call it multiple times at once."

for step in agent_executor.stream({"input": query}):
    pass
```

**API Reference:**[AgentExecutor](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.agent.AgentExecutor.html) | [create\_tool\_calling\_agent](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

```````output
Call number: 1
Call number: 2
Call number: 3
Call number: 4
Call number: 5
Call number: 6
Call number: 7
Call number: 8
Call number: 9
Call number: 10
Call number: 11
Call number: 12
Call number: 13
Call number: 14
``````output
Stopping agent prematurely due to triggering stop condition
``````output
Call number: 15
```````

### In LangGraph[â€‹](#in-langgraph-6 "Direct link to In LangGraph")

We can use the [`prompt`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) just as before when passing in [prompt templates](#prompt-templates).

```python
from langgraph.errors import GraphRecursionError
from langgraph.prebuilt import create_react_agent
from langgraph.prebuilt.chat_agent_executor import AgentState

magic_step_num = 1


@tool
def magic_function(input: int) -> int:
    """Applies a magic function to an input."""
    global magic_step_num
    print(f"Call number: {magic_step_num}")
    magic_step_num += 1
    return input + magic_step_num


tools = [magic_function]


def _modify_state_messages(state: AgentState):
    # Give the agent amnesia, only keeping the original user query
    return [("system", "You are a helpful assistant"), state["messages"][0]]


langgraph_agent_executor = create_react_agent(
    model, tools, prompt=_modify_state_messages
)

try:
    for step in langgraph_agent_executor.stream(
        {"messages": [("human", query)]}, stream_mode="updates"
    ):
        pass
except GraphRecursionError as e:
    print("Stopping agent prematurely due to triggering stop condition")
```

**API Reference:**[create\_react\_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

```output
Call number: 1
Call number: 2
Call number: 3
Call number: 4
Call number: 5
Call number: 6
Call number: 7
Call number: 8
Call number: 9
Call number: 10
Call number: 11
Call number: 12
Stopping agent prematurely due to triggering stop condition
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now learned how to migrate your LangChain agent executors to LangGraph.

Next, check out other [LangGraph how-to guides](https://langchain-ai.github.io/langgraph/how-tos/).

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/migrate_agent.ipynb)

* * *


- [Basic Usage](#basic-usage)
- [Prompt Templates](#prompt-templates)
- [Memory](#memory)
  
  - [In LangChain](#in-langchain)
  - [In LangGraph](#in-langgraph)
- [Iterating through steps](#iterating-through-steps)
  
  - [In LangChain](#in-langchain-1)
  - [In LangGraph](#in-langgraph-1)
- [`return_intermediate_steps`](#return_intermediate_steps)
  
  - [In LangChain](#in-langchain-2)
  - [In LangGraph](#in-langgraph-2)
- [`max_iterations`](#max_iterations)
  
  - [In LangChain](#in-langchain-3)
  - [In LangGraph](#in-langgraph-3)
- [`max_execution_time`](#max_execution_time)
  
  - [In LangChain](#in-langchain-4)
  - [In LangGraph](#in-langgraph-4)
- [`early_stopping_method`](#early_stopping_method)
  
  - [In LangChain](#in-langchain-5)
  - [In LangGraph](#in-langgraph-5)
- [`trim_intermediate_steps`](#trim_intermediate_steps)
  
  - [In LangChain](#in-langchain-6)
  - [In LangGraph](#in-langgraph-6)
- [Next steps](#next-steps)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/llm_token_usage_tracking.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/llm_token_usage_tracking.ipynb)

# How to track token usage for LLMs

Tracking [token](/docs/concepts/tokens/) usage to calculate cost is an important part of putting your app in production. This guide goes over how to obtain this information from your LangChain model calls.

Prerequisites

This guide assumes familiarity with the following concepts:

- [LLMs](/docs/concepts/text_llms/)

## Using LangSmith[â€‹](#using-langsmith "Direct link to Using LangSmith")

You can use [LangSmith](https://www.langchain.com/langsmith) to help track token usage in your LLM application. See the [LangSmith quick start guide](https://docs.smith.langchain.com/).

## Using callbacks[â€‹](#using-callbacks "Direct link to Using callbacks")

There are some API-specific callback context managers that allow you to track token usage across multiple calls. You'll need to check whether such an integration is available for your particular model.

If such an integration is not available for your model, you can create a custom callback manager by adapting the implementation of the [OpenAI callback manager](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.openai_info.OpenAICallbackHandler.html).

### OpenAI[â€‹](#openai "Direct link to OpenAI")

Let's first look at an extremely simple example of tracking token usage for a single Chat model call.

danger

The callback handler does not currently support streaming token counts for legacy language models (e.g., `langchain_openai.OpenAI`). For support in a streaming context, refer to the corresponding guide for chat models [here](/docs/how_to/chat_token_usage_tracking/).

### Single call[â€‹](#single-call "Direct link to Single call")

```python
from langchain_community.callbacks import get_openai_callback
from langchain_openai import OpenAI

llm = OpenAI(model_name="gpt-3.5-turbo-instruct")

with get_openai_callback() as cb:
    result = llm.invoke("Tell me a joke")
    print(result)
    print("---")
print()

print(f"Total Tokens: {cb.total_tokens}")
print(f"Prompt Tokens: {cb.prompt_tokens}")
print(f"Completion Tokens: {cb.completion_tokens}")
print(f"Total Cost (USD): ${cb.total_cost}")
```

**API Reference:**[get\_openai\_callback](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.manager.get_openai_callback.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

```output


Why don't scientists trust atoms?

Because they make up everything.
---

Total Tokens: 18
Prompt Tokens: 4
Completion Tokens: 14
Total Cost (USD): $3.4e-05
```

### Multiple calls[â€‹](#multiple-calls "Direct link to Multiple calls")

Anything inside the context manager will get tracked. Here's an example of using it to track multiple calls in sequence to a chain. This will also work for an agent which may use multiple steps.

```python
from langchain_community.callbacks import get_openai_callback
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAI

llm = OpenAI(model_name="gpt-3.5-turbo-instruct")

template = PromptTemplate.from_template("Tell me a joke about {topic}")
chain = template | llm

with get_openai_callback() as cb:
    response = chain.invoke({"topic": "birds"})
    print(response)
    response = chain.invoke({"topic": "fish"})
    print("--")
    print(response)


print()
print("---")
print(f"Total Tokens: {cb.total_tokens}")
print(f"Prompt Tokens: {cb.prompt_tokens}")
print(f"Completion Tokens: {cb.completion_tokens}")
print(f"Total Cost (USD): ${cb.total_cost}")
```

**API Reference:**[get\_openai\_callback](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.manager.get_openai_callback.html) | [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

```output


Why did the chicken go to the seance?

To talk to the other side of the road!
--


Why did the fish need a lawyer?

Because it got caught in a net!

---
Total Tokens: 50
Prompt Tokens: 12
Completion Tokens: 38
Total Cost (USD): $9.400000000000001e-05
```

## Streaming[â€‹](#streaming "Direct link to Streaming")

danger

`get_openai_callback` does not currently support streaming token counts for legacy language models (e.g., `langchain_openai.OpenAI`). If you want to count tokens correctly in a streaming context, there are a number of options:

- Use chat models as described in [this guide](/docs/how_to/chat_token_usage_tracking/);
- Implement a [custom callback handler](/docs/how_to/custom_callbacks/) that uses appropriate tokenizers to count the tokens;
- Use a monitoring platform such as [LangSmith](https://www.langchain.com/langsmith).

Note that when using legacy language models in a streaming context, token counts are not updated:

```python
from langchain_community.callbacks import get_openai_callback
from langchain_openai import OpenAI

llm = OpenAI(model_name="gpt-3.5-turbo-instruct")

with get_openai_callback() as cb:
    for chunk in llm.stream("Tell me a joke"):
        print(chunk, end="", flush=True)
    print(result)
    print("---")
print()

print(f"Total Tokens: {cb.total_tokens}")
print(f"Prompt Tokens: {cb.prompt_tokens}")
print(f"Completion Tokens: {cb.completion_tokens}")
print(f"Total Cost (USD): ${cb.total_cost}")
```

**API Reference:**[get\_openai\_callback](https://python.langchain.com/api_reference/community/callbacks/langchain_community.callbacks.manager.get_openai_callback.html) | [OpenAI](https://python.langchain.com/api_reference/openai/llms/langchain_openai.llms.base.OpenAI.html)

```output


Why don't scientists trust atoms?

Because they make up everything!

Why don't scientists trust atoms?

Because they make up everything.
---

Total Tokens: 0
Prompt Tokens: 0
Completion Tokens: 0
Total Cost (USD): $0.0
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/llm_token_usage_tracking.ipynb)

* * *


- [Using LangSmith](#using-langsmith)
- [Using callbacks](#using-callbacks)
  
  - [OpenAI](#openai)
  - [Single call](#single-call)
  - [Multiple calls](#multiple-calls)
- [Streaming](#streaming)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/how_to/query_multiple_queries.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/query_multiple_queries.ipynb)

# How to handle multiple queries when doing query analysis

Sometimes, a query analysis technique may allow for multiple queries to be generated. In these cases, we need to remember to run all queries and then to combine the results. We will show a simple example (using mock data) of how to do that.

## Setup[â€‹](#setup "Direct link to Setup")

#### Install dependencies[â€‹](#install-dependencies "Direct link to Install dependencies")

```python
%pip install -qU langchain langchain-community langchain-openai langchain-chroma
```

```output
Note: you may need to restart the kernel to use updated packages.
```

#### Set environment variables[â€‹](#set-environment-variables "Direct link to Set environment variables")

We'll use OpenAI in this example:

```python
import getpass
import os

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass()

# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

### Create Index[â€‹](#create-index "Direct link to Create Index")

We will create a vectorstore over fake information.

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

texts = ["Harrison worked at Kensho", "Ankush worked at Facebook"]
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_texts(
    texts,
    embeddings,
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 1})
```

**API Reference:**[OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

## Query analysis[â€‹](#query-analysis "Direct link to Query analysis")

We will use function calling to structure the output. We will let it return multiple queries.

```python
from typing import List, Optional

from pydantic import BaseModel, Field


class Search(BaseModel):
    """Search over a database of job records."""

    queries: List[str] = Field(
        ...,
        description="Distinct queries to search for",
    )
```

```python
from langchain_core.output_parsers.openai_tools import PydanticToolsParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

output_parser = PydanticToolsParser(tools=[Search])

system = """You have the ability to issue search queries to get information to help answer user information.

If you need to look up two distinct pieces of information, you are allowed to do that!"""
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
structured_llm = llm.with_structured_output(Search)
query_analyzer = {"question": RunnablePassthrough()} | prompt | structured_llm
```

**API Reference:**[PydanticToolsParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.openai_tools.PydanticToolsParser.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [RunnablePassthrough](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

We can see that this allows for creating multiple queries

```python
query_analyzer.invoke("where did Harrison Work")
```

```output
Search(queries=['Harrison Work', 'Harrison employment history'])
```

```python
query_analyzer.invoke("where did Harrison and ankush Work")
```

```output
Search(queries=['Harrison work history', 'Ankush work history'])
```

## Retrieval with query analysis[â€‹](#retrieval-with-query-analysis "Direct link to Retrieval with query analysis")

So how would we include this in a chain? One thing that will make this a lot easier is if we call our retriever asynchronously - this will let us loop over the queries and not get blocked on the response time.

```python
from langchain_core.runnables import chain
```

**API Reference:**[chain](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.chain.html)

```python
@chain
async def custom_chain(question):
    response = await query_analyzer.ainvoke(question)
    docs = []
    for query in response.queries:
        new_docs = await retriever.ainvoke(query)
        docs.extend(new_docs)
    # You probably want to think about reranking or deduplicating documents here
    # But that is a separate topic
    return docs
```

```python
await custom_chain.ainvoke("where did Harrison Work")
```

```output
[Document(page_content='Harrison worked at Kensho'),
 Document(page_content='Harrison worked at Kensho')]
```

```python
await custom_chain.ainvoke("where did Harrison and ankush Work")
```

```output
[Document(page_content='Harrison worked at Kensho'),
 Document(page_content='Ankush worked at Facebook')]
```

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/how_to/query_multiple_queries.ipynb)

* * *


- [Setup](#setup)
  
  - [Create Index](#create-index)
- [Query analysis](#query-analysis)
- [Retrieval with query analysis](#retrieval-with-query-analysis)









[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/conversation_retrieval_chain.ipynb)[![Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/versions/migrating_chains/conversation_retrieval_chain.ipynb)

# Migrating from ConversationalRetrievalChain

The [`ConversationalRetrievalChain`](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html) was an all-in one way that combined retrieval-augmented generation with chat history, allowing you to "chat with" your documents.

Advantages of switching to the LCEL implementation are similar to the [`RetrievalQA` migration guide](/docs/versions/migrating_chains/retrieval_qa/):

- Clearer internals. The `ConversationalRetrievalChain` chain hides an entire question rephrasing step which dereferences the initial query against the chat history.
  
  - This means the class contains two sets of configurable prompts, LLMs, etc.
- More easily return source documents.
- Support for runnable methods like streaming and async operations.

Here are equivalent implementations with custom prompts. We'll use the following ingestion code to load a [blog post by Lilian Weng](https://lilianweng.github.io/posts/2023-06-23-agent/) on autonomous agents into a local vector store:

## Shared setup[â€‹](#shared-setup "Direct link to Shared setup")

For both versions, we'll need to load the data with the `WebBaseLoader` document loader, split it with `RecursiveCharacterTextSplitter`, and add it to an in-memory `FAISS` vector store.

We will also instantiate a chat model to use.

```python
%pip install --upgrade --quiet langchain-community langchain langchain-openai faiss-cpu beautifulsoup4
```

```python
import os
from getpass import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass()
```

```python
# Load docs
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import FAISS
from langchain_openai.chat_models import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

loader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")
data = loader.load()

# Split
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
all_splits = text_splitter.split_documents(data)

# Store splits
vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())

# LLM
llm = ChatOpenAI()
```

**API Reference:**[WebBaseLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html) | [FAISS](https://python.langchain.com/api_reference/community/vectorstores/langchain_community.vectorstores.faiss.FAISS.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html) | [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)

## Legacy[â€‹](#legacy "Direct link to Legacy")

Details

```python
from langchain.chains import ConversationalRetrievalChain
from langchain_core.prompts import ChatPromptTemplate

condense_question_template = """
Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone question:"""

condense_question_prompt = ChatPromptTemplate.from_template(condense_question_template)

qa_template = """
You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to answer
the question. If you don't know the answer, say that you
don't know. Use three sentences maximum and keep the
answer concise.

Chat History:
{chat_history}

Other context:
{context}

Question: {question}
"""

qa_prompt = ChatPromptTemplate.from_template(qa_template)

convo_qa_chain = ConversationalRetrievalChain.from_llm(
    llm,
    vectorstore.as_retriever(),
    condense_question_prompt=condense_question_prompt,
    combine_docs_chain_kwargs={
        "prompt": qa_prompt,
    },
)

convo_qa_chain(
    {
        "question": "What are autonomous agents?",
        "chat_history": "",
    }
)
```

**API Reference:**[ConversationalRetrievalChain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html) | [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```output
{'question': 'What are autonomous agents?',
 'chat_history': '',
 'answer': 'Autonomous agents are entities empowered with capabilities like planning, task decomposition, and memory to perform complex tasks independently. These agents can leverage tools like browsing the internet, reading documentation, executing code, and calling APIs to achieve their objectives. They are designed to handle tasks like scientific discovery and experimentation autonomously.'}
```

## LCEL[â€‹](#lcel "Direct link to LCEL")

Details

```python
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

condense_question_system_template = (
    "Given a chat history and the latest user question "
    "which might reference context in the chat history, "
    "formulate a standalone question which can be understood "
    "without the chat history. Do NOT answer the question, "
    "just reformulate it if needed and otherwise return it as is."
)

condense_question_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", condense_question_system_template),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
    ]
)
history_aware_retriever = create_history_aware_retriever(
    llm, vectorstore.as_retriever(), condense_question_prompt
)

system_prompt = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}"
)

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
    ]
)
qa_chain = create_stuff_documents_chain(llm, qa_prompt)

convo_qa_chain = create_retrieval_chain(history_aware_retriever, qa_chain)

convo_qa_chain.invoke(
    {
        "input": "What are autonomous agents?",
        "chat_history": [],
    }
)
```

**API Reference:**[create\_history\_aware\_retriever](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html) | [create\_retrieval\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval.create_retrieval_chain.html) | [create\_stuff\_documents\_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html)

```output
{'input': 'What are autonomous agents?',
 'chat_history': [],
 'context': [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': "LLM Powered Autonomous Agents | Lil'Log", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'}, page_content='Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\nFor example, when requested to "develop a novel anticancer drug", the model came up with the following reasoning steps:'),
  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': "LLM Powered Autonomous Agents | Lil'Log", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'}, page_content='Weng, Lilian. (Jun 2023). â€œLLM-powered Autonomous Agentsâ€. Lilâ€™Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'),
  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': "LLM Powered Autonomous Agents | Lil'Log", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#'),
  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': "LLM Powered Autonomous Agents | Lil'Log", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en'}, page_content="LLM Powered Autonomous Agents | Lil'Log\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLil'Log\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosts\n\n\n\n\nArchive\n\n\n\n\nSearch\n\n\n\n\nTags\n\n\n\n\nFAQ\n\n\n\n\nemojisearch.app\n\n\n\n\n\n\n\n\n\n      LLM Powered Autonomous Agents\n    \nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n\n\n \n\n\nTable of Contents\n\n\n\nAgent System Overview\n\nComponent One: Planning\n\nTask Decomposition\n\nSelf-Reflection\n\n\nComponent Two: Memory\n\nTypes of Memory\n\nMaximum Inner Product Search (MIPS)")],
 'answer': 'Autonomous agents are entities that can act independently to achieve specific goals or tasks without direct human intervention. These agents have the ability to perceive their environment, make decisions, and take actions based on their programming or learning. They can perform tasks such as planning, execution, and problem-solving autonomously.'}
```

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now seen how to migrate existing usage of some legacy chains to LCEL.

Next, check out the [LCEL conceptual docs](/docs/concepts/lcel/) for more background information.

[Edit this page](https://github.com/langchain-ai/langchain/edit/master/docs/docs/versions/migrating_chains/conversation_retrieval_chain.ipynb)

* * *


- [Shared setup](#shared-setup)
- [Legacy](#legacy)
- [LCEL](#lcel)
- [Next steps](#next-steps)








# Get started with LangSmith

**LangSmith** is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.

![](/assets/images/ls-diagram-5be7dd68b135f573a7b0e163692e6800.png)

### [Observability](/observability)

Analyze traces in LangSmith and configure metrics, dashboards, alerts based on these.

### [Evals](/evaluation)

Evaluate your application over production traffic â€” score application performance and get human feedback on your data.

### [Prompt Engineering](/prompt_engineering/quickstarts/quickstart_ui)

Iterate on prompts, with automatic version control and collaboration features.

LangSmith + LangChain OSS

LangSmith is framework-agnostic â€”Â it can be used with or without LangChain's open source frameworks [`langchain`](https://python.langchain.com) and [`langgraph`](https://langchain-ai.github.io/langgraph/).

If you are using either of these, you can enable LangSmith tracing with a single environment variable. For more see the how-to guide for [setting up LangSmith with LangChain](/observability/how_to_guides/trace_with_langchain) or [setting up LangSmith with LangGraph](https://docs.smith.langchain.com/observability/how_to_guides/tracing/trace_with_langgraph).

## Observability[â€‹](#observability "Direct link to Observability")

Observability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.

This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmithâ€™s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.

- Get started by [adding tracing](/observability) to your application.
- [Create dashboards](/observability/how_to_guides/dashboards) to view key metrics like RPS, error rates and costs.

## Evals[â€‹](#evals "Direct link to Evals")

The quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.

- Get started by [creating your first evaluation](/evaluation).
- Quickly assess the performance of your application using our [off-the-shelf evaluators](https://docs.smith.langchain.com/evaluation/how_to_guides/prebuilt_evaluators) as a starting point.
- [Analyze results](/evaluation/how_to_guides#analyzing-experiment-results) of evaluations in the LangSmith UI and [compare results](https://docs.smith.langchain.com/evaluation/how_to_guides/compare_experiment_results) over time.
- Easily collect [human feedback](/evaluation/how_to_guides#annotation-queues-and-human-feedback) on your data to improve your application.

## Prompt Engineering[â€‹](#prompt-engineering "Direct link to Prompt Engineering")

While traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.

- Get started by [creating your first prompt](/prompt_engineering/tutorials/optimize_classifier).
- Iterate on models and prompts using the [Playground](/prompt_engineering/how_to_guides#playground).
- [Manage prompts programmatically](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompts/manage_prompts_programatically) in your application.

* * *

- [Evals](#evals)
- [Prompt Engineering](#prompt-engineering)

Community

- [Discord](https://discord.gg/cU2adEyC7w)
- [Twitter](https://twitter.com/LangChainAI)

GitHub

- [Docs Code](https://github.com/langchain-ai/langsmith-docs)
- [LangSmith SDK](https://github.com/langchain-ai/langsmith-sdk)
- [Python](https://github.com/langchain-ai/langchain)
- [JS/TS](https://github.com/langchain-ai/langchainjs)

More

- [Homepage](https://langchain.com)
- [Blog](https://blog.langchain.dev)
- [LangChain Python Docs](https://python.langchain.com/en/latest/)
- [LangChain JS/TS Docs](https://js.langchain.com/docs/)


